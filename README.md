My life and work are reported, changed, and governed by these [notes](#notes), [memos](#memos), and [hints](#hints).

# HINTS

## WHAT TO DO
1. Discover, predict, and control changes (in counts, rates, and accelerations)
2. as selections from variations (on physical, chemical, biological, behavioral, and cultural scales)
3. by making and maintaining strong practices (mediated by strong people marked by strong principles)
4. from the sciences (of 
    - logic, e.g. denotative, Boolean, functor;
    - mathematics, e.g. calculi, collections, categories;
    - physics, e.g. quantum, thermodynamic, gravitional;
    - chemistry, e.g. phyiscal, biophysical, biological;
    - biology, e.g. oranelles, organisms, environments;
    - behavior, e.g. biological, biosocial, social; and
    - culture, e.g. history, science, technology).

## HOW TO DO IT

Conclude from contemplation on templates:
1. Notes link templates to conclusions.
2. Memos link contemplations to templates.
3. Hints link conclusions to contemplations.
4. Good notes accurately report.
5. Good memos accelerate change.
6. Good hints adaptively govern.

## HOW TO WRITE IT WELL 99% OF THE TIME

1. Read "Writing that Works" by Joel Raphaelson and Kenneth Roman.
2. Read "The Elements of Style" by E. B. White and William Strunk Jr.
3. Go to 1.

## HOW TO WRITE IT WELL THE OTHER 1% OF THE TIME

Journal your thoughts and feelings.

## HOW TO MAKE IT LOGICAL

1. List the words used to talk about it.
2. Define as many from as few as you can.
3. List true sentences made from those few.
4. Conclude as many from as few as you can.
5. Go to 1.

## HOW LOGIC WORKS

1. Predicates denote or don't (are true or false of) where they occur (Aristotle); 
2. grammars generate categories from recurrent connective construction on itemized atomic lexicons (Frege, Hilbert, von Neumann);
3. supplemented lexical substitutions (SLS) of validities denote everywhere (Quine); and 
4. denotata are inherited (Tarski) alethically, i.e.
    1. joint denials denote where and only where (waow) each of their components don't (Peirce, Sheffer),
    2. existential closures denote waow there is some where denoted by their component (Boole, Quine, Hailperin), and
    3. existential croppings denote the left part of waow their compound does with some item, with the right part of the same whereabouts (Frege, Russell, Quine), and
5. recombically (Russell, SchÃ¶nfinkel, Curry, Bernays, Tarski, Quine, Charles H. Moore), i.e.
    1. components of drops denote the left part of the left part of waow their comopund does with the right part of the same whereabouts,
    2. components of pushes denote the left part of waow their comopund does with the left part of the right part of the same whereabouts, with the right part of the right part of the same whereabouts, and
    3. components of hems denote the left part of waow their compound does, with the right part of the left part of the left part of the same whereabouts with the right part of the same whereabouts.

## HOW THE ABBREVIATIONS OF LOGIC WORK

1. The truth-functional, better 'denotative-functional', abbreviations are
    1. negations are self joint denials,
    2. alternations are negations of joint denials,
    3. converse conditionals are alternations of their consequent with the negation of their antecedent,
    4. complementary converse conditionals are negations of converse conditionals,
    5. complementary conditionals are swapped complementary converse conditionals,
    6. conditionals are negations of complementary conditionals,
    7. alternative denials are conditionals with negated consequents,
    8. conjunctions are negations of alternative denials,
    9. exclusive alternations are conjunctions of alternative denials with matching alternations,
    10. biconditionals are negations of exclusive alternations, and 
    11. cedents are conditionals of conjunctions with alternations.
2. The Boolean abbreviations are
    1. universal closures are negations of existential closures of negations,
    2. inclusions are universal closures of conditionals,
    3. converse inclusions are swapped inclusions,
    4. proper inclusions are conjunctions of inclusions with the negation of  their converse, and
    5. coextensions are conjunctions of inclusions with their converse.
3. The quantificational abbreviation is
    - universal croppings are negations of existential croppings of negations.
4. The recombic abbreviations are
    1. dushes are drops of pushes,
    2. props are pushes of drops,
    3. overs are hems of pushes,
    4. oems are overs of hems,
    5. dupes are oems of dushes,
    6. pops are oems of two drops,
    7. nip ns are n pops of a drop of n pushes,
    8. digs are hems of nips,
    9. bury ns are n digs of n pushes,
    10. unbury ns are n bury ns,
    11. (i,m)-clockwise turns (CWs) are m pushes of one bury (i+m) of m pops, and
    12. (i,m)-counterclockwise turns (CCWs) are m pushes of one unbury (i+m) of m pops. 
5. The sameness abbreviations are
    1. (i+1,m)-indiscernabilities are i, universal croppings of bury 2s of, m, universal croppings of pops of, conjunctions of each biconditional of, drops of with nip 1s of, the 1st thru (i+m)th (i,m)-CWs of their component (Leibniz, Quine).
    2. identity is the conjunction of each indiscernability of a lexical atom(Leibniz, Quine).
    3. ((i+1)\*2)-identities are conjunctions of each i nip 2s of i nip 1s of the 1st thru ith (i,0)-CWs of their component. 
    4. 2-nonidentities are negations of identities, and 
    5. (i+3)-nonidenties are conjunctions of i+2 dupes of ((i+3)\*2)-identities, with the drop of (i+2)-nonidentity,
6. The compositional abbreviations are
    1. (i,m)-projections are i, existential croppings of bury 1s of, m, existential croppings of pops,
    2. (i,m)-fields are alternations fo each (i,m)-projection of the first thru (i+m)th (i,m)-CWs,    
    3. (j,k,m)-resultants are j existential croppings of j bury (j+k)s of the conjunction of k drops with m props.
    4. first (j\*2)-iterates are their component,
    5. (n+2)th (j\*2)-iterates are (j,j,0)-resultants of their component with j nip (j\*2)s of (n+1)th (j\*2)-iterates of their component,
    6. (n+1)th (j\*2)-inverse iterates are (n+1)th (j\*2)-iterates of j bury (j\*2)s.
    7. zeroth ((j+1)\*2)-iterates are conjunctions of ((j+1)\*2)-identities with each first thru ((j+1)\*2)th ((j+1)\*2)-CWs of (j\*2 +1) drops of the ((j+1)\*2,0)-field of their component.
7. The functional abbreviations are
    1. i-functionalities are inclusions of ((1+i)\*2)-identities with the conjunction of i drops with i nip i's of their component
    2. i-totalities are universal closures of i existential croppings
    3. i-partialities are negations of i-totalities
    4. (i,j)-injectivities are inclusions of ((1+i)\*2)-identities with the conjunction of i nip js with i nip (i+j)s of their component
    5. (i,j)-surjectivities are universal closures of i existential croppings of j (i+j,0)-CWs of their component
    6. (i,j)-bijectivities are the conjunction of the (i,j)-injectivity with the (i,j)-surjectivity of its component
    7. (i,j)-correlations are the conjunction of the i-functionality with the (i,j)-injectivity of their component
8. The graph abbreviations are
    1. i-symmetrics are inclusions of their component with the first (i\*2)-inverse iterate of their component
    2. i-nonsymmetrics are negations of i-symmetrics
    3. i-asymmetrics are inclusions of their component with the negation fo the first (i\*2)-inverse iterate of their component
    4. i-transitivities are inclusions of the second (i\*2)-iterate of their component with their component,
    5. i-nontransitivities are negations of i-transitivities,
    6. i-intransitivities are inclusions of the second (i\*2)-iterate of their component with the negation of their component,
    7. i-reflexivities are inclusions of the zeroth i-iterate of their component with their component
    8. i-nonreflexivities are negations of i-reflexivities
    9. i-irreflexivities are inclusions of their component with i-nonidentities.
    10. i-totalreflexivities are inclusions of i-identities with their component

## HOW PREDICATE ABSTRACTS WORK

1. The English relative clause 'who loves Dick' and the pidgin 'x such that x loves Dick' are uniformly paraphrased by the *predicate abstract* '{x: x loves Dick}'.
2. It *abstracts* 'Tom' from 'Tom loves Dick' by *binding* the *free* occurrence of 'x' in the *free* sentence 'x loves Dick' with the prefix 'x:'.
3. The *predication* '{x:x loves Dick}Tom' *concretes* to 'Tom loves Dick'.
4. **predicational completeness** *whatever can be said of a thing can be said by predicating a predicate of it*

## HOW TO COOK IT

Use a thermometer.

## HOW TO FIND A GOOD PSYCHOLOGIST

1. **Never** settle for a bad one.
2. Good ones are exceptional people who happen to be psychologists.
3. They are cultural liasons.

## OTHERS

- The conditional connective 'only if' is a grammatical partical and 'implies' is a predicate.

- A predicate is not a singular term which puports to designate its extension as the class (or relation) of each item (or list of items) which it denotes.

# NOTES

## \#2025-0919-1514

## 2025-0917-1724

> It took me this long to figure out that I can write the dashes which turn up in the hyperlinking anchors as part of the subsection title (or I could just smoosh all the numbers together, but I find that hard to read at a glance or while I'm scrolling quikckly back and forth in time).
> I'm contemplating not even making each of these notes a subsection and just leaving them as their own sections under a top most section titled 'NOTES' which may or may not contain some short sentences on how notes work, though I've already explaiend that in the hint [#how-to-do-it](#how-to-do-it).
>
> It also occurred to me that I can put the pound at the front of the entry as well e.g. '\#2025-0917-1724' instead of '2025-0917-1724'.
> This would make it even easier to copy and paste hyperlink anchors.
> All of this has reignited by intense desire to present the page as a javascript snippet that you must manually copy and paste into your browser to effectively use.
> Ah, they joys of obscurity.

This continues notes from [#2025-0915-1331](#2025-0915-1331) on the first volume of Whitehead and Russell's 'Principia Mathematica'.

1. In the last note I presented, at the end, the premises of a logical theory of the grammar of logical theories which did not depend upon concatenation, e.g. as in Quine's theory of protosyntax, nor trees, e.g. as in Feferman's theory of finitary inductively presented logics.

2. I was quick to see that uniqueness of existential joint denials had not been guarenteed by this initial list of premisses.
It was also apparent that a single predicate in the lexicon could potentially be predicated in more than one way.

3. The predicates of predication, e.g. 'w, x, y, z such that w predicates x with y and with z' and 'u, v, w, x, y, and z such that u predicates v with w, x, y, and with z', are an example of a peculiarity of most grammars of logic which is often overlooked because of the general utility of concatenation or tupling: there is not a single gramamtical construction called 'predication', rather there are grammatical constructions of predication for each of the many places of each predicate in the lexicon e.g. 'is red' is traditionally predicated of just one pronoun (a better name than 'variable') and 'fathered' is traditionally predicate of just two pronouns (perhaps identically spelled out as when we write 'each item is (x such that not x fathered x)').

4. So, first I shall write out the first iteration of the initial premises that I suspect are most likely to resolve the problems already pointed out.
I've also collected the premises based more on the part they come to play when included in an analysis of truth like that of Tarski's disquotational methods.
    1. pronominal premises
        1. just 1 item is (x such that x is lexically pronominal)
        2. each item is (x such that x is lexically pronominal only if x is pronominal)
        3. each item is (x, y, and z such that x accents y and z accents y, only if x is identical to z)
        4. each item is (x such that some item is (y such that x is pronominal only if y accents x))
        5. each item is (x and y such that x accents y and x is pronominal, if and only if y is pronominal)
    2. predicational premises
        1. just ..+n items are (x such that x is lexically predicable)
        2. .. and each item is (w, x, y, .., and z such that w predicates n place x with .., and with z and y predicates x with .., and with z, only if x is identical with y)
        3. .. and just n items are (x such that 

5. It's at this moment that I just jumped ahead to predicate functor logics like those I've contemplated lately, rather than go through the depths of shoehorning some theory of sequences and satisfaction into a theory like the one above.

6. I am glad that I worked out some of the corrections along this direction because it indirectly forced me to finally give a definition of the counting quantifiers in a truth functional and quantificational logic (TQL) which then prompted me to render the appropriate abbreviation for the same in predicate functor logic.

7. I have not done a good job of capturing all the notes that I've made, and all the reading that I've done, that have gone into this and other work e.g. it is only from reading in tandem Russell's "An Inquiry into Meaning and Truth", "Human Knowledge: its scope and limits", "Principles of Mathematics" and 'section B, theory of apparent variables. \*9. Extension of the theory of deduction from lower to higher types of propositions' starting on page 132 of PM1 along with chapters 2 and 3 of PM1 that I've seen some way of connecting some of Russell's more philosophical arguments with the tried and true methods of predicate logic like those in Quine's later works.


## 2025 0915 1331

This continues notes from [#2025-0913-1705](#2025-0913-1705) on Whitehead and Russell's 'Principia Mathematica'.

1. Quine more than anyone else emphasized that validitiy is the synthesis of grammar and denotation.
In his "Principles of Philosophy Second Edition" (POL) he gives a preformal description of grammar and truth (truth is degenerated denotation as shown in Quine's "From Stimulus to Science" (FSS)).
What Quine calls "Tarski's definition of truth" in POL is properly the presentation of a theory of truth.
In FSS Quine takes greater care to emphasize that a consistent theory of denotation involves a hierarchy of theories of denotation.
Neither in FSS nor in POL does Quine take the same kind of care he gives to the hierarchy of theories of denotation with his theory of grammar.

2. Russell and Whitehead's type theory imposes restrictions upon what propositions are to be taken as meaningful.
The meaning of meaning is mysterious, and, thankfully, it comes out as something amenable to paraphrase into grammar.

3. For Russel, and almost anyone else, meaning is far more than anything that can be caught by grammar e.g. synonymy is more than can be read from a grammatical analysis.

4. What Quine shows in POL is that grammar is as much 'about the world' as denotation.
This is in harmony with Russell's repeated reminder in 'An Inquiry into Meaning and Truth' (IMT) that language is part of the same world that the nonlinguistic world is i.e. unlike the division of the world into, what some philosophers have mandated as seperate but equal, the mental and the material, the division of the world into linguistic and nonlinguistic is not as seperate or equal as it may seem.

5. For Russell, experience is immediately self evident (at least in theory) and language is merely an expression of experience (as far as that goes).
Hence, he distances himself from the logical positivists that permit their protocol sentences to be the raw material from which, following Mach, the mental and material worlds are logically constructed e.g. as in Carnap's 'Aufbau'.

6. Russell's propositional methods in 'Principia Mathematica Volume One' (PM1) are under a similar control as to those he presents in IMT except perhaps that IMT is a much more careful consideration of all that propositional methods have to give all the sciences and not just mathematics in particular.

7. My aim here is to give a more detailed explination of the parallels between theories of grammar and theories of denotation which bring Quine's methods closer to Russell's in PM and IMT.

8. Beyond Quine and Russell's methods are the calculi and automata that set up grammars in modern works in theories of mathematical logic and computation.
And yet, these are not so far from Quine's methods at the end of his 'Mathematical Logic' (ML) where a theory of protosyntax provides, a necessarily incomplete, explication of the formal methods from which grammar has benefited.

9. My methods are the slightest generalization of Quine's protosyntax with an eye to accomodating the propositionalists who cannot let go of their nonextensional methods.
Instead of building concatenation conspicuously into grammar as Quine does with his protosyntax and instead of building trees into grammar as abstract syntax trees do, I prefer to deal with grammar in the same way that Tarski deals with truth, and closer to how Quine describes grammar in POL and Russell describes it, indirectly, in PM (in the course of his explanation of his theory of types) and, similarly, in IMT.

10. Although grammar is mentioned by Russell, it is so strongly distinguished from meaning that he misses those parts of his analysis of meaning which actually turn into analyses of grammar.
Hopefully I can bring out those points more conspicuously.

11. Thus, I would perhaps call my methods those of a logical theory of logical grammars.
The grammars are logical in that they are constructed to accomodate logical practices as opposed to those of English, German, French, and other purportedly less formal languages.

12. The theory is logical in that its compounds are composed from a list of basic predicates (as integral words or phrases such as 'fathered', 'red', 'is next to'), predicated of an appropriate string of varibles, and assembled by the grammatical construction of iterated existential joint denial (where the existential joint denial of grammatical compounds with a variable not occuring in them is the joint denial of them).

13. The difference between my methods and those of Quine at the end of ML in the chapter on protosyntax is that rather than define the predicates of grammar from the predicates of a covert theory of concatenation I leave them undefined.

14. In Tarski's analysis of truth, the predicate being analyzed is 'is true'.
It is found that there can be no consistent theory of a predicate 'is true' when the disquotational principle is regimented with respect to the predicate 'is true'.
The principle of diquotation is exemplified by the classic example: 'snow is white' is true if and only if snow is white.
It is precisely on the matter of diquotation that Russell, and almost all others, have made their mistakes.

15. The mistake is the result of mistaking the grammar of logical schema with the grammar of logical sentences e.g. predicate letters in a schema of quantificational and truth functional logic are not variables.
Beyond mistaking letters of a schema for variables of a sentence, there is a partcularly tricky situation which confused both Russell and Wittgenstein and which is brought up in IMT: logical sentences about logical schema.

16. To bring up the problem of theories of logical schema is premature, but points in the direction that my work on grammar shall go as I work over PM and IMT.

17. A logical theory of grammar gives the premises which connect grammatical lexicon, construction, and categories: lexicon are the raw material from which different categories are constructed.

18. It shall be seen from this particular example of a theory of a grammar of a logical theory that Quine's arguments that the predicate 'is grammatical' does not have a transcendent definition is further justified by the problems posed by, and solved by, specifying this theory.

19. As with Tarski's theory of truth, the lexical elements are the trickiest.
In the case of Tarski's analysis, disquotation comes through as a template upon which relevant premises are presented.
In the case of my analysis of grammar, there is nothing quite like diquotation available because, and this is important, the subject of the theory is purely linguistic.

20. Russell rightly makes much noise about the difference in how to treat the linguistic parts of our world and their links with the nonlinguistic parts of the world.
What Russell says is a difference between a word, or sentence, and its quotation, is seen by Quine as a difference between a word, or a sentence, and spelling out words, or sentences.

21. The first collection of premises which presente themselves to me are based in part upon the so called 'counting quantifiers' which may be defined in terms of the standard quantifiers (or, what is the same, the corresponding predicate functors applied to the appropriate predicate abstract, both of which I shall make free use) as follows:
    1. 'just 0 items are (x such that Fx)' for 'each item is (x such that not Fx)'
    2. 'just 1+n items are (x such that Fx)' for 'some item is (x such that Fx and just n items are (y such that Fy and not x is identical to y))'

22. Note that the definition of the counting quantifiers is made with respect to the predicate 'is identical to' and that, in standard logical langauges, we can take any theory which does or does not have a predicate of identity as coextensive with or defined by (respectively) indiscernability with respect to that language's lexical predicates.

23. I shall begin by giving the premises as they occurred to me and then contemplate them and their consequences with an aim at unearthing any missing premises.
My past has primed me to begin with a fragmenetary collection of premises, no matter how weak or poorly considered they may later end up showing themselves as, rather than trying to capture each key premise from the haze of preformal contemplations up to that point.

24. The premises of a logical theory of the grammar of a logical theory with pronminal variables (as contrasted with those logical theories set up without variables and with predicate functors):
    1. just 1 item is (x such that x is lexically pronominal)
    2. just m+..+n items are (x such that x is lexically predicable)
    3. 'is lexical' for 'x such that x is lexically pronominal or x is lexically predicable'
    4. each item is (x such that x is lexically pronominal only if x is pronominal)
    5. each item is (x, y, and z such that such that x accents y and z accents y, only if x is identical to z)
    6. each item is (x and y such that x accents y and x is pronominal if and only if y is pronominal)
    7. The premises of many place predications:
        1. just m items are (x such that each item is (y,.., and u such that y predicates x with .. and with u and x is lexically predicable, if and only if .., and u is pronominal)
        2. ..
        3. just n items are (x such that each item is (y, .., and v such that y predicates x with .. and with v and x is lexically predicable, if and only if .., and v is pronominal)
    8. 'is predicational' for 'x such that some items are (y, .., and u such that y is lexically predicable and x predicates y with .. and with u) .., or some items are (y, .., and v such that y is lexically predicable and x predicates y with .. and with v))'
    9. each item is (x such that x is a predication only if x is grammatical)
    10. each item is (w, x, y, and z such that w existentially jointly denies x with y with respect to z and w is grammatical, if and only if y is grammatical and z is pronominal)

25. Already as I wrote them it appeared to me that there were insufficient premises to proved for the unique decomposition of an existential joint denial into its binding pronoun (i.e. variable) and grammatical subcomponents.

26. It also occurred to me that there is no guarentee that a lexical predicate is not predicable in more than one way e.g. schematically, 'Fx' and 'Fxy' are permitted by my premises and yet they are not common in logical practice.

27. In the next note I shall make the appropriate corrections, as they occur to me, and perhaps subject them to further contemplation or perhaps go on to present premises of a logical theory of grammars of logical theories with predicate functors and without pronominal variables.


## 2025 0913 1705

This continues notes from [#2025-0911-1654](#2025-0911-1654) on Whitehead and Russell's 'Principia Mathematica'.

1. For lack of a better method of quoting the notation of Russell and Whitehead's 'Principia Mathematical Volume 1' (PM1), I shall use the notation from Landon's TeX package, but I shall drop the initial 'pm' attached to each command (it is only there to distinguish its commands from others native to tex and latex).
    - <https://ctan.math.hamburg/macros/latex/contrib/principia/principia.pdf>

2. Part I of PM1 is on mathematical logic which is to extend its symbolism into the propositions of mathematics.

3. Propositions are presupposed by Russel as that which the symbolisms are about when the symbolic constructions are sensible.

4. Point (2) of page 2 distinguishes symbolism from language. This distinction is no longer appropriate (why?).

5. Point (3) reveals the broad commitment to a kind of mentalism:
    > "aids the intuition in regions too abstract for the imagination readily to present to the mind the true relation between ideas employed." pg.2

    > "And thus the mind is finally led to construct trains of reasoning in regions of thought in which the imagination would be entirely unable to sustain itself without symbolic help. ORdinary langauge yields no help. ITs grammatical structure does not represent uniquely the relations between the ideas involved." pg. 2

    > "(a) Most mathematical investigation is concerned not with the analysis of the complete process of reasoning, but with the presentation of such and abstract of the proof as is sufficient to convince teh properly instructed mind." pg. 3

6. My method, in contrast to Russell's, is to step back to grammar prior to logic: grammar being the method by which we distinguish logic as a language from languages such as English and German.
Grammar does not tell us whether an item is true or false, only whether it is a grammatical compound of a logical language or not.

7. Grammar is no starting or stopping point e.g. autoclitic behavior can be examined prior to grammatical behavior and grammatical behavior can be constructed from it (thanks to Skinner's work in 'Verbal Behavior').

8. This is far more significant than it may seem.
While Russell in, e.g., 'An Inquiry into Meaning and Truth', confronts the psychological origins of knowledge, he leaves them quite far out from his purportedly direct knowledge of the world through his personal experience.
It shall come as no surprise that I trace the origins of his particular blend of explication, experience, and experiment to his carelessness with quotation marks.
But, more of that when it comes.

9. The notation of PM is that of Peano's 'Formulario Mathematico' along with additions carefully crafted by Russell and Whitehead from the deferred consequences of thier use.

10. The first grand difference between Russell's outlook in PM and mine is with respect to the introductory explanations of the word 'variable'.
    > "In ordinary mathematics, a variable generally stands for an undetermined number or quantity." pg.4

    > "In mathematical logic, any symbol whose meaning is not determinate is called a *variable*, and the various determinations of which its meaning is susceptable are called the *values* of the variables." pg. 4

11. In the traditional view, symbols symbolize or mean something other than whatever it is that a symbol presents itself as e.g. be it an assembly of marks on the page or those strange elements of Jung's archetypes that haunt images and our purported unconscious experiences with them.

12. The link between a symbol and what it symbolizes is presented as a kind of puzzle, like that of more modern works on naming objects which emphasize a cosmic blessing of conspicuous contact between an utterance and an object.

13. Presumably the meaning of a symbol can be different from what the symbol symbolizes: this much is made clear by Russell's definition of 'variable' as 
'any symbol whose meaning is not determinate'.

14. Variables are purportedly special symbols whose meanings are on the boundary of obscurity.
The meanings of a variable can, purportedly in some cases, be determined from further scrutiny (perhaps by perfection of some perceptive apparatus along with some evolution of a reasoning organ), and as far as they can be so determined, such meanings are especially called 'values of the variables'.

15. The phrase 'values of variables' and related articulations are legion in any realm of philosophy which leverages notation as a tool for thought.
Quine is even known more for his principle 'To be is to be the value of a variable' than he is for his more profound 'To be is to be denoted by a one place predicate.' which follows from his work on predicate functor logic.

16. For me, a variable is nothing more than a part of speech most closely aligned with that of pronouns i.e. variables are grammatical items of a grammatical cagtegory.
In setting up the grammar of a logical theory, only one item belongs initially to the lexicon of the grammatical language and the grammatical category of variables e.g. the letter ex. 
    - It has occurred to me multiple times that most people do not know the names of the letters of the English alphabet and that this is part of the origin of confusion like that of Russell's with respect to the passage back and forth between 'that'-clauses and outright quotations.
    - A table which I reference often gives the names of letters (both their English name and their code names in the NATO phonetic alphabet):
    <http://www.englishlessonsbrighton.co.uk/wp-content/uploads/2014/08/names-letters-english-alphabet.jpg>.

17. The rest of the variables are ushered in by the grammatical construction of accentuation (which presumably involves a grammatical partical called an accent).

18. Though this grammatical paraphrase of Russell's mystical definition of 'variable' appears weak it is spartan.
It is strong enough to carry through the elaborate arguments involved in talk of bondage, freedom, quantification, substitution, etc.

19. Talk of variables as more than bits of grammar has enhanced the hazy mist of not determinant, or indeterminate, meanings which carry with them a spirit of variability or variation.
This plays itself out in PM (and almost everywhere else) as a problem of variables having sometime one meaning, sometimes another, and sometimes, as I have said twice before, a mystical haze or mass of indeterminate meaning that has some structural resemblance to a whole which is somehow distinct from the mereological sum of its propositional parts i.e. an indeterminate whole which is somehow distinct from the mereological sum of its determinate meanings.

20. When compared to such talk of meanings my grammatical methods are hardly loaded with the heaps of history that we inherit through the etymology of the mentalistic phrases of our native language.
Perception, judgement, reason, inference, and other mysterious operations of the thinking mind are profound when compared to the largely incomplete methods of modern sciences of behavior.
Yet, there is now, more than ever, an alternative, provided largely by Skinner's works, to the experiential literature which floods the sciences of philosophy and psychology.

21. I'm hopeful that further careful reading of Russell's work and those earlier and later works which pass through it shall reveal to me an effective way of freeing those like Russell and Carnap from their phenomenalism.

22. How does talk of the meaning of variables come out in a predicate functor world i.e. when variables are properly eliminated from logic as they are Quine's predicate functor methods what is left of the problems posed by variables either with respect to Russell's definition or the grammatical definition?

23. The lexicon of a predicate functor language is populated entirely of predicates as integral words or phrases such as 'red' and 'round' and 'atop'.
The question as to whether there are intensional or extensional items named by, or, strictly, *designated by* these lexical elements is a question about the theory of the world in which such a language occurs.
The items of the world are purportedly those which are *denoted by* the one place predicates of the predicate functor language.
So, the question as to whether there exist items designated by the predicates of the lexicon of the language is a question about a special part of the theory of the world: the theory of denotation.

24. Those familiar with PM will notice that something like a hierarchy of langauges is forced upon anyone examining truth or denotation as a result of Tarski's analysis of truth and that this is anticipated by Russell's type theory in PM and further emphasized in "An Inquiry into Meaning and Truth" (IMT).

25. In both PM and IMT there is recourse to experiences and expressions which obscures an almost neutral analysis of the conclusions which link a theory of grammar, a theory of denotation, and a theory of the world.

26. Presumably, the theory of the world must, in addition to being consistent, contain the theory of its language's grammar and its langauge's truth.
Something like such a theory is analyzed here as part of the proper paraphrasing of the premises and conclusions of PM.

27. Turning now to values of variables.
Talk of values of variables is difficult for me to pin down precisely.
In the sentences of English, relative clauses do the work of binding pronouns, e.g. 'it such that it is red and it is round'.
This is distinguished from the dangling pronouns of an otherwise complete sentence, e.g. 'It is red and it is round.'

28. In the latter sentence with the dangling pronouns, the grammarian asks "Who or what is it?" or, better, "What does 'it' refer to in the sentence 'It is red and it is round.'?"
If the immediately preceeding sentence is 'He dropped the block.' then may answer "'it' refers to 'the block'" or, perhaps, "'it' refers to the block"?
How about "'it' refers back to 'the block'" or "'it' refers to the same thing as 'the block'"?
Even if 'the block' refers to no thing?

29. There is much more to be said about 'range of values of a variable' and its connection to 'significance'.

30. I shall return to the preface and the introduction and the beginning of Chapter 1 "Preliminary Explanations of Ideas and Notation", but now wish to leap ahead, still in chapter one, to the primitive propositions that play such a central role in PM and as access points to the peculiarities of the phenomenology promoted by Russell's 'ideas and notation'.

31. Translation of "primitive propositions employed in the calculus of propositions." pg. 13

32. "(1) Anything implied by a true premise is true Pp" (pg.13) is paraphrased as
    - each item is (x, y, and z such that x is true, z conditionalizes x with y, and z is true, only if y is true)

33. The paraphrase reveals some of the predicates found in the language of a logical theory of the grammar and truth (better, denotation, but that shall be left for later) of a logical language.

34. For those unfamiliar with a phrase such as 'each item is (x and y such that some item is (z such that z pairs x with y)' then read these hints:
    - The phrase 'who loves Dick' is, as part of English grammar, a relative clause.
    - Relative clauses are predicates e.g. the relative clause 'who loves Dick' is a one place predicate.
    - Predication of a name, e.g. 'Tom', with a relative clause, e.g. 'who loves Dick' yields a complete sentence 'Tom is who loves Dick'.
    - This complete sentence says the same thing as 'Tom loves Dick'.
    - In math, where there are rarely 'who's, instead of 'who loves Dick' the 'such that' phrase, e.g. 'it such that it loves Dick' or 'x such that x loves Dick', turns up as a canonical method of paraphrasing relative clauses into a regimented sentence.
    - Thus paraphrased, it is found that all the problems of the freedom and bondage of pronouns occurs without the complexity of quantification with respect to a pronoun (or, unhappily, variable).
    - The sentence 'It is round and it is red.' is complete even though its pronouns dangle, and the relative clause 'it such that it is round and it is red' is an incomplete sentence since it is just a predicate.
    - This distinction organizes a host of confused and conflicting methods which occur throughout PM and throughout most books on logic and mathematics e.g. the free pronouns in a sentence like 'It is round and it is red.' can play the part of dummy singular terms so that in a given argument we may safely contemplate the consequences of supposing 'it' does or does not designate (i.e. does or does not purport to name one and only one item) and the bound pronouns in a relative clause like 'it such that it is red and it is round' can play the part of, the unhapilly named, variables under the careful restrictions described by Quine as follows
        1. "The variable comes to appear in its true light as purely a means of identifying and distinguishing the referential places in a sentence" pg 32 of "From Stimulus to Science" (FSS)
        2. "As for the etymology of 'variable', it should be studiously ignored; it rests on a pernicious old mathematical metaphor that is happily fading." pg. 32 FSS
        3. "Here the variable serves merely to mark places where the same thing is referred to, and single it out as denoted by the newly abstracted predicate." pg. 33

35. The predicate 'x, y, and z such that x conditionalizes y with z' is a predicate in a logical theory of grammar where the grammatical construction of conditoinalization is either defined, e.g. with respect to negation and alternation or just alternative denial or perhaps just joint denial, or is part of the lexicon.
In either case, it can be taken by those who are wedded to propositions as being "about them" even though they play no proper part in my paraphrase.

36. Properly, Tarski's analysis of truth yields a heirarchy of theories of grammatical and truth predicates, though, until now, Quine has obscured the purely grammatical elements with his and Tarski's principle of disquotation.

37. Though what I have said of my own work is perhaps obscure with respect to what has been quoted from PM, I can assure the intreped reader that each of the technical distinctions I've brought up appear in one form or another in PM (some explicitly).

38. Back to the primitive propositions from page 13:
    > (1) Anything implied by a true premise is true \pp
    >
    > this is the rule which justifies inference.
    >
    > (2) \thm \dott p \or p \dot \imp \dot p \pp
    >
    > i.e. if p or p is true, then p is true.
    > 
    > (3) \thm \dott q \dot \imp \dot p \or q \pp
    >
    > i.e. if q is true, then p or q is true.
    >
    > (4) \thm \dott p \or q \dot \imp \dot q \or p \pp
    > 
    > i.e. if p or q is true, then q or p is true.
    >
    > (5) \thm \dott p \or ( q \or r ) \dot \imp \dot q \or (p \or r) \pp
    >
    > i.e. if either p is true or "q or r" is true, then either q is true or "p or r" is true.
    >
    > (6) \thm \dottt q \imp r \dot \imp \dott p \or q \dot \imp \dot p \or r \pp
    >
    > if q implies r, then "p or q" implies "p or r".

39. They are paraphrased as follows
    1. each item is (x, y, and z such that x is true, z conditionalizes x with y, and z is true, only if y is true)
    2. each item is (x and y such that y alternates x with x and y is true, only if x is true)
    3. each item is (x, y, and z such that x is trueand z alternates y with x, only if z is true)
    4. each item is (w, x, y, and z such that w alternates x with y, w is true, and z alternates y with x, only if z is true)
    5. each item is (s, t, u, v, x, y, and z such that u alternates x with v, u is true, v alternates y with z, s alternates y with t, and t alternates x with z, only if s is true)
    6. each item is (t, u, v, w, x, y, and z such that u conditionalizes v with w, u is true, x alternates y with v, z alternates y with w, and t conditionalizes x with z, only if t is true)

40. Note Russell's use of quotations where I strongly recommend commas, or, if familiar methods of carrying over grouping into punctuation are to be entirely dropped, full parenthetical methods.

41. The paraphrases can be broken into their grammatical and denotative parts e.g. number five can be written out as
    - each item is (s, t, u, v, x, y, and z such that u alternates x with v, v alternates y with z, s alternates y with t, and t alternates x with z, and u is true, only if s is true)

42. These paraphrases point up the versatility of these methods e.g. number five may be alternatively rendeder to look more like Russell's English paraphrase:
    - each item is (u, v, x, y and z such that u alternates y with z, v alternates x with z, and x is true or u is true, only if y is true or v is true)

43. Presumably, what Russell here calls 'primitive propositions' can be paraphrased as I have done and shown to be conclusions in a consistent theory of truth (or denotation) like that from Tarski's analysis of Quine's extension of it to denotation (where truth becomes degenerate denotation).

44. By such paraphrasing it can perhaps be shown that something like Russell's hierarchy of langauges (as in the type theory of PM or its further elaboration in "An Inquiry into Meaning and Truth") fits smoothly into Tarski's analysis.

45. Russell seems hell bent on introducing a hierarchy of predicates of grammar along with a hierarchy of predicates of denotation and perhaps he is right to do so and that the difference is made clear through the difference between the three different purported paraphrases of primitive proposition five.

## 2025 0911 1654

This begins my notes on Whitehead and Russell's 'Principia Mathematica'.

1. As much as my early history as a mathematician is involved with Bertrand Russell's writings, I never expected to be taking notes on his and Whitehead's 'Principia Mathematica'.
Happily I got to read through it as an undergraduate, but my outlook has changed significantly since then.
It is the most recent work on predicate functor logic that prompts me to begin taking notes here on Volume One.

2. Most of what goes as theories of relations can be carried over to pure logic.
Russell thought, perhaps until the end of his life, that what he had worked on in Principia Mathematica was logic.
At best it  as a logical theory of sets, better, a logical theory of classing classes.

3. It is much easier for me to know read Russell after having traced the schematic methods of Quine through the symbolic methods of Carnap.
Symbolic methods are plauged by careless designation: symbols symbolize the things they perpurt to designate.

4. Whereas predicate functors are purely grammatical constructions, e.g. the conjunction of a pair of sentences or predicates is marked by a grammatical particle 'and' without 'and' designating anything whatsoever, the propositional functions of 'Principia Mathematica' and the connectives of anything like that found in Carnap's 'Introduction to Symbolic Logic and its Applications' are wrapped up with reference and meaning, e.g. a connective designates its propositional function.

5. I shall be looking back upon Russell with the aid of hindsight.
Perhaps the most helpful distinctions are to be found in the following quotation from Quine's "From Stimulus to Science" as it distinguishes a later outlook from Russell and Frege's earlier ones:
    > "For what follows we must come squarely to terms with 'denote'. Since it is often used interchangeably with 'designate', and a singular term normally designates one and only one object, readers are apt to think of denotations as relating a predicate likewise to a single object, namely the class of all the things it is true of, or a property shared by them. in my use of 'denote', as in John Stuart Mill's ('A System of Logic' 2 volumes, London 1868, Chapter 2 Section 5), a predicate denotes rather each separate thing of which it is true. The class or property is not involved.
    >
    > For years, to obviate confusion, I avoided 'denote' altogether in favor of 'true of'; but that evasion would be impracticable in these pages, where denotation is becoming the center of action. Unlike Mill, I still withold the word from singular terms; they are well served by 'designates'.
    >
    > A word of caution is in order regarding 'predicate' too. Some logicians take a predicate as a *way* of building a sentence aroudn a singular term or, more concretely, as what Peirce called a *rheme* (volume 2 paragraph 95), a sentence wtih blanks in it, these being distinctively marked in the case of a many-place predicate. This version covers, implicitly, the potential output of predicate abstraction or predicate functors. But a predicate in my sense is always an integral wrod, phrase, or clause, grammatically a noun, adjective, or verb. some are generated from others by grammatical constructions, notably the relative clause or, formally, predicate abstraction, or predicate functors.
    >
    > Denotation, explained thus far only for one place predicates, is needed also for predicates of two ro more places. Two place predicates such as transitive verbs, may be said to denote ordered pairs, and n place predicates denote sequences of length n" pg. 60-61 "From Stimulus to Science" by Quine

6. Quine just barely mentioned properties in that quotation and only later in that book contemplates intentions, attitudes, and attributes.
For a few years, these distinctions puzzled me until I tried setting up predicate functor logic without building it atop the truth functional and quantificational logic (TQL) (sometimes unhappily called 'first order logic' (FOL)).
After great effort I have found that the following threefold distinction is key to dealing with a host of problems throughout logic and science:
    1. attributing attributes
    2. classing classes
    3. predicating predicates.

7. Reading Carnap finally filled in the gaps for me (though I do not agree with his outlook, he is able to explain himself far better than any other logicians following Frege and Russell).

8. Multiple times in his good book 'Introduction to Symbolic Logic and its Applications' (ISA) he goes over the distinction which makes working back to Russell and Frege from Quine much easier e.g. (these are elaborate quotations because this problem is so fundamental and Carnap brings together the different views without the forgone, and justified, conclusions of Quine)
    > "The theoretical treatment of any domain fo objects consists in setting up sentences concerning teh objecs of the domain (sentences ascribing certain properties and relations to the objects in question), and in establishing rules according to which other sentences can be derived from those given. .. To form sentences concerning the individuals of a given domain there must first of all be available in the language two kinds of signs: 1. names for the individuals of the domain-- we call these *individual constants*; 2. designations for the properties and relations predicated of the individuals-- we call these *predicates*.
    >
    > .. It is only when we move away from pure logic (i.e. from consideration of the skeleton language to be constructed in what follows) that we speak of the interpretation of the separate individual constants and predicates.
    >
    > .. 'P' might designate teh property Spherical. (I prefer this mode of expression to the more elaborate turn of phrase 'the property of being spherical'. Similarly, I write 'the property Prime Number', 'the property ODD', etc. Again, I use 'the class Spherical' in place of 'the class of spherical individuals'; and analogously, 'the class Blue', etc. And again, I say 'the relation Greater' rather than 'the relation that obtains between x and y when x is greater than y'; and similarly 'the relation Similar', 'the relation Father', etc.) Now suppose that, in addition to designating the property Spherical by 'P', we take 'a' to designate teh sun and 'b' to designate the moon. Then in our symbolic language we write the sentence 'P(a)' for 'the sun is sphereical'. Similarly, 'P(b)' is the translation into our symbolic language of the English sentence 'the moon is spherical'.
    >
    > ..
    > 
    > Regarding Terminology. 1. In orderinary word language there is no word which comprehends both properties and relations. Since such a word would serve a useful purpose, let us agree in what follows that the word 'attribute' shall have thsi sense. Thus a one-place attribute is a property, and a two-place (or a many-place) attribute is a relation. 2. Similarly, it is useful to havea  comprehensive term for the designations of one- and many-place attributes. For this let us follow Hilbert and use the word 'predicate'. (Heretofore, this word has been confined mostly to properties or to designations of them, and has not included many-place attriutes or predicates.) Thus a one place predicate is a sign for a one place attribute (i.e. for a property); and in general an n place predicate is a sign for an n place attribute. 3. Let us always distinguish clearly between signs and what id esignated. Failure to observ e this distinction has in the past occasioned much confusion in logic and in philosophy generally. In speaking about an expression, let us always put the expression in quotation marks or use some special designation for it, e.g. a German letter as in 21a. We make but one exception to this practice: we omit quotation marks in case the expression stands on a line either alone or with a designating number or letter; see e.g. our enumeration of formulas in T8-2. Suppose, e.g., 'Pa' is taken as a symbolic translation of 'a is old'; then we say: "P (but not: 'P') is a one-place attribute, viz. the property Old; this attribute is designated by a one-place *predicate* 'P'". Similarly, we say: "the two-place *relation* R exists between such and such persons", "the two-place *predicate* 'R' occurs in such and such a sentence". And simiarly: 'the individual a ...', "the name 'a' ...".
    >
    > 2b. Sentential constants. It is often burdensome to work with sentences that are entirely written out like 'Pa' or 'Rac', esxpecially if they are even longer or are repeated frequently in the same connection. We therefore use on occasion the letters 'A', 'B', 'C' as abbreviations for any sentences whatever of the symbolic language. These letters are called *sentential constants* (or: propositional constants). E.g. in a certain case 'A' might be taken as an abbreviation for 'Pa'; as soon as 'P' and 'a' are interpreted, 'A' is also interpreted. In our use of a sentential constant we will for the most part leave open what particular sentence it stands for as an abbreviation." pg. 4-6 ISA

9. More examples from Carnap shall be forthcoming, but first, if the distinctions being raised, e.g. that between Quine and Carnap, are not clear or do not clearly present themselves as consequential, then a much more detailed explanation of the problems and solutions has already been given in the following entry on Goldfarb's "Deductive Logic" where a use and mention mistake is made in the very section on use and mention.

10. Note, Carnap gets away with his symbolic methods in a way that Goldfarb does not: as long as Carnap does not claim anything more than symbolic logic as his subject then he is fine to carelessly deploy designations and names with impunity, but symbolic logic is not to be confused with schematic logic or pure logic (Carnap does make this confusion, but it is forgivable because it is a mistake in name and not a mixture of methods: his methods are strictly symbolic and do not rise to the schematic).

11. Logic does not presuppose attributes, classes, or predicates.
Logic may be said to be about predicates in as much as it is about validity as truth under each supplemented lexical substitution.
It is a use and mention error to take schema as sentences i.e. to take an assembly of marks deployed in the development of logic as referring or meaning, or even ranking among the compounds of some grammatical construction from lexical elements.

12. It is von Neumann 1927 "Zur Hilbertschen Beweistheorie" through Quine that grants us the utility of schematic methods in logic without anything like the trappings of signs, symbols, or formula of *theories* of logic as contemplated by Russell and Carnap and their descendant.

13. Back to Carnap whose 'predicate variables' are culprits of further errors that are made by Russell and Whitehead throughout 'Principia Mathematica'.
I shall type up the entire section because it is the crux of the mistakes made by those who have not yet navigated the distinctions made by Quine and ultimately backed up by such science as Skinner's in his "Verbal Behavior".
    > "10a. Predicate variables. According to our treatment of the universal quantifier and the existential quantifier, a sentence of the form '(x)(..x..)' is true if and only if the sentential formula '..x..' holds for every individual; and a sentence of the form '(E x)(..x)' is true if and only if the formula '..x..' holds for at least one individual.
    >
    > Now, it is easy to see taht the sentence '(x)Px only if Pa' (i.e. '(x)(Px) only if Pa') is true in every possible case, no matter what the facts are regarding the individual a and the property P. ONly two cases need to be distinguished. Case (1): the individual a has property P. In this case, 'Pa' is true; hence (by Truth-table I(4)) teh whole sentence is true. Case (2): a fails to have property P. In this case, the sentence '(x)Px' is false because it asserts that all individuals have property P; hence (again by the truth-table) the whole sentence is true. The sentence in quenstion is thus necessarily true, regardless of the facts. We may also see this immediately from the word-language version of '(x)Px only if Pa'. "If all individuals are P, then a is P". Indeed, the sentence '(x)Px only if Pa' can be included among sentences that are L-true in our technical sense, provided we extend in a suitable way the rules governing value-assignments. Let us make that extension now.
    >
    > Let us agree that free variables and descriptive signs count as value-bearing signs. (Thus, in '(x)Px only if Pa' only 'P' and 'a' are value bearing.) As values of individual signs, let us take all individuals of the domain in question; and as values of one-place predicates, let us take all classes of these individuals (i.e. all subclasses of the domain in question).
    >
    > Let us agree to regard a one-place atomic formula as true at a given value assignment if and only if the individual (assigned as the value fo the individual sign) belongs to teh class (assigned as the value of the predicate). Further, we agree to regard a universal sentence (say '(x)Px') as true at a given value-assignment provided the operand of this sentence (here 'Px') is true at each value-assignment to 'x', in view of the assignment already given to the remaining value-bearing signs (here, only 'P').
    >
    > In view of the above, it is readily seen that the sentence '(x)Px only if Pa' is true at every value-assignment to the value-bearing signs 'P' and 'a', and hence is L-true. (The argument is essentially the same as that given at the beginning of this section; we repeat it here because the formation must now be phrased in terms of value-assignments. Case (1): the value-assignment to 'P' and 'a' is such taht the individual assigned to 'a' does in fact belong to the class assigned to 'P'. At this value-assignment, 'Pa' is true; and hence the whole sentence '(x)Px only if Pa' is true. Case (2): the value-assignment to 'P' and 'a' is such that the individual assigned to 'a' does not belong to the class assigned to 'P'. At this value-assignemnt, '(x)Px' is false since 'Px' is not true at every value-assignment to 'x' (in particular 'Px' is not true if we assign to 'x' the individual presently assigned to 'a'); hence at thsi value-assignemnt the whole sentence '(x)Px only if Pa' is again true. consequently the sentence is true at every value-assignment.) Similarly, the open formula '(x)Px only if Py' is L-true; for the possible value-assignment to the free variable 'y' are identical with those of 'a'.
    >
    > It is further evident that any other sentence with teh same form as '(x)Px only if Pa', but with a different predicate in place of 'P', is true just as '(x)Px only if Pa' is. E.g. '(x)Qx only if Qa' is true. Now we saw earlier that sentential variables are useful because they facilitate the creation of open L-true formulas from which L=-true sentences can be obtained by arbitrary substitutions. Here, analogously, it is useful to introduce *predicate variables*. Let us agree to use 'F', 'G', 'H', 'K' (and other letters, as occasion demands) for predicate variables, and to count as expressions substitutable for these variables either predicate constants or other predicate variables. In making value-assignments for a sentential formula, we assign classes of individuals to one-place predicate variables and also to one-place predicate constants. Thus e.g. the open formula '(x)Fx only if Fa' is L-true, since in fact the possible value-assignments to 'F' are the same as those originally possible for 'P'. From this L-true formula our earlier L-true sentences can then be obtained by substituting 'P' for 'F', or else 'Q' for 'F'. The open formula '(x)Fx only if Fy' with both 'F' and 'y' as free variables is also L-true, and is in fact the most general formula of the form considered here; it has as substitution instances the previous L-true formulas of this section. '(x)Fx only if Fy' is a purely logical formula, devoid of descriptive constants.
    >
    > 10b. Intensions and extensions. Our practice has been to define L-concepts on the basis of value-assignments. Now let us take up several questions regarding the sorts of values we have used in such assignment. Why do we take the values of sentential variables to be truth-values and not propositions? Of course, it is simpler to work with just two truth-values than with indefinitely many propositions. But teh question is: Is this simplification justifiable? A similar question occurs in connection with one-place predicate variables: Is it justifiable to take as values of these predicate variables just classes of individuals, rather than properties?
    >
    > In order to resolve these questions we introduce here the semantic concepts of intensions and extensions. (A reader concerned chiefly to master the technique of the symbolic language, and having less interest in semantic and philosophic matters, may omit this section.)
    > 
    > A one-place predicate designates a property. (E.g. 'Book' designates the property of beign a book; 'Blue' designates the colour blue, a property of certain things.) We shall call this property the *intension* of the predicate. By the *extension* of a predicate we shall understand the class of individuals having the property designated by the predicate. (E.g. the extension of 'Book' is the class of books; and the extension of 'Blue' is teh class of blue things.) Analogously, we consider the intension of a two-place predicate to be the two-place relation designated by the predicate, and the extension of a two-place predicate to be the class of ordered pairs of individuals for which the predicate holds (i.e. the class of ordered pairs that satisfy the relation designated by the predicate). (E.G. the intension of the predicate 'Fa' is the relation of fatherhood, and the extension of this predicate is the class of paris comprising a father and one of his children.) In general, for any natural number n, n greater than or equal to two, we take the intension of an n-place predicate to be the n-place relation designated by that predicate, and its extension to be the class of ordered n-tuples for which the predicate holds.
    >
    > While not customary, it is useful to make analogous distinctions for individual constants (or, more general, for closed individual expressions). Suppose the father of Peter Brown is also may of Lexington. Then the two phrases 'the father of Peter Brown' and 'the mayor of Lexington' (more precisely, the individual expressions in our symbolic language that correspond to these two phrases; such expressionare introduced later (35) as 'descriptions') refer to the same individual. Of these two phrases therefore, we say that they have the same extension, viz. this particular individual. On the other hand, it is evident that the two phrases have different senses. By the intension of an individual expression we understand its sense. This is a concept similar to property or relation, but of a different type for which there is no established designation; we agree to use for it the term 'individual concept'. We will become aquanted later with still other such concepts, among them functions like e.g. the arithmetic sum-function designated '+'. By teh intension of such a function sig (or: functor) we understand the function designated by the sign; by its extension we understand the value-distribution of the function (a notion to be explicated later).
    > 
    > Next, suppose that the symbolic language contains variables whose substitutable expressions include the constants and closed compound expressions of some fixed kind. Following out the distinction between the intension and the extension of a constant, it is possible here to set up a similar distinction between the *value-intensions* and the *value-extensions* of a variable. Expressions substitutable for a variable have both intensions and extensions; we count all such intensions among the value-intensions of the variable, and similarly all such extensions among the value-extensions of the variable. When we think of the 'values' of a variable, we usually have its value-intensions in mind. However, in examining the L-truth of logical formulas constructed in a loanguage with so simple a structure as the symbolic languages treated in this book, ti is quite sufficient to consider the values of a variable as its value-extensions. E.g. the values (regarded as value-intensions) of the sentential variables 'p', 'q', etc. , are propositoins; as we have see however the tautological character of (say) the formula 'p or not p' can be ascertained without considering numerous (in some circumstances, infinitely numerous) propositions, but simply the two truth-values which are the value-extensions of the variable 'p'.
    >
    > So far as teh truth-value of a sentential compound is concerned, it is sufficient to consider just the value-extensions (the truth-values) of constituent sentential variables because the truth-value of this compound is uniquely determined by teh truth-values of its components; i.e. the sentential connectives used in such compounds are themselves extensional. Again, the truth-value of an atomic sentence obviously depends only on the extension of its predicate and teh extensions of its individual constants; hence, an atomic sentence is also extensional. Continuing, the truth-value of a universal sentence depends only on the extension of the property determined by teh operand of the quantifier (i.e. on whether this property attaches to all individuals, or not); thus a universal sentence is also extensional. The same remark applies to an existential sentence. INdeed, each of our symbolic langauges-- the present language A, and the languages B and C to be introduced later-- is an *extensional language* in the following sense: a sentence in any one of these languages does not change tis truth value if any expression in the sentence is replaced by another with the same extension. Consequently, it suffices for the evaluation of any formula to consider simply the possible extensions of the formula's descriptive constants and the value-extensions of the formula's variables.
    >
    > A symbolic language which, in contrast to the one treated here, also contains symbols for the so-called *logical modalities*-- i.e. such concepts as necessity, possibility, impossibility, contingency and the like-- is not extensional. (For suppose it is not raining here now. Then the sentence 'it is raining' is false, and so has the same extension (or truth-value) as the L-false sentence 'it is raining and it is not raining'. Now let this last sentence be a component in a larger modal sentence; when 'it is raining and it is not raining' is replaced by 'ti is raining', the truth-value of the whole modal sentence does not always remain unchanged. E.g. the modal sentence 'it is impossible that it is raining and it is not raining' is true, whereas the sentence 'it is impossible that it is raining' (produced therefrom by the indicated replacement) is false-- for while it is note the case that it is raining here now, this case is nevertheless logically possible. Thus symbolic languages with modality symbols are generally not extensional.) In such non-extensional symbolic languages, one must consider intensions as well as extensions as values of descriptive conastants and variables.
    >
    > Most systems of symbolic logic employ an extensional language, for the reason taht such a language has raidcally simpler structure and hence simpler constitutive rules. However, it cannot justly be said that this procedure compels a neglect of the logical modalities: these can be expressed in another way, viz. in the metalanguage, with the aid of L-concepts. Instead of saying a certain proposition (or state of affairs) is necessary-- or impossible, or possible, or contingent--, we say that a corresponding sentence (i.e. one that designates the proposition in question) is L-true-- or L-false, or not L-false, or L-indeterminate, respectively. E.g. let 'A' designate the proposition (the possible state of affairs) that it is raining here now; 'A or not A' then designates the proposition that it is raining or it is nto raining. Within a modal language containing words one would say 'it is necessary that it is raining or it is not raining'; in a symbolic modal language having the symbol 'N' for 'necessary', the sentence would appear as 'N(A or not A)'. By contrast, this sentence cannot be stated in our object language because this language is extensional; however we can formulate the corresponding sentence "the sentence 'A or not A' is L-true" in our metalanguage.
    > 
    > Intensions and extensions of the chief types of expressions:
    >   1. Expressions
    >       1. Sentence
    >       2. Individual constant
    >       3. One-place predicate
    >       4. n-place predicate (n greater than one)
    >       5. Functor
    >   2. Intensions
    >       1. Proposition
    >       2. Individual concept
    >       3. Property
    >       4. n-place relation
    >       5. Function
    >   3. Extensions
    >       1. Truth-value
    >       2. Individual
    >       3. Class of individuals
    >       4. Class of ordered n-tuples of individuals
    >       5. Value-distribution." pg. 38-42 ISA

14. Symbolic methods resort to 'expressions' because a formula is said to express (symbolically) a proposition or a state of affairs.
So, although I may speak of sentences, predicates, and functors, they are not the expressions that Carnap (or Russell) are talking about.
Carnap's use of functor in ISA corresponds to functional intensions and value-distributions.

15. My use, and Quine's, of 'functor' is purely grammatical (and more closely related to Carnap's earlier use of the term):
    > "The word 'functor', grammatical in import but logical in habitat, becomes useful at this point. A functor is a sign that attaches to one or more expressions of given grammatical kind or kinds to produce an expression of a given grammatical kind. The negation sign is a functor that attaches to a statement to produce a statement, and to a term to produce a term. The alternation sign, conditional sign, and biconditional sign are functors that join statements in pairs to produce statements and join terms in pairs to produce terms. 'some' is a functor that attaches toa  term to form a statement; so is 'each'. Finally 'includes', 'properly includes', and 'coextensive' are functors that join terms to form statements. Functors of this last kind are called *copulas*; in particular 'properly includes' means 'are'." pg. 129 Methods of Logic 4th Edition by Quine

16. Even Quine can not avoid slipping into talk of 'signs', 'expressions', and 'statements'.
B. F. Skinner is the only writer whose work in 'Verbal Behavior' entirely avoids the etymological baggage that follows from talk of words.
The key to dealing with functors is to combine Quine's work from the fourth edition of 'Methods of Logic' with his work in the second edition of 'Philosophy of Logic':
    > "The grammarian's question is, then, what strings of phonemes belong to the language? What strings, that is, ever get uttered or could get uttered in the community as normal speech? The grammarian's job is to demarcate, formally, the class of all such strings of phonemes. Formally? THis means staying within a purely mathematical theory of finite strings of phonemes. More explicitly, it means saying nothing that could not be said by means of a technical vocabulary in which, besides the usual logical particles and any desired auxiliary apparatus from pure mathematics, there are only the names of the ponemes and a symbol signifying the concatenation of phonemes.
    >
    > A mere listing of strings would already be formal, but it would not suffice, since the desired strings, though finite in lenght, are infinite in number. So the grammarian has recourse to recursion: he specifies a *lexicon*, or list of words, together with various grammatical *constructions*, or steps that lead to compound expressions from constituent ones. His job is to devise his lexicon and his constructions in such a way as to demarcate teh desired class: the class of all the strings of phonemes that could be uttered in normal speec. The strings of phonemes obtainable from the lexicon by continued use of the construction should all be capable of occurring in normal speech; and, conversely, every string capable of occurring in normal speech should be obtainable from the lexicon by the constructions (or should at least be a fragment of a string which as a whole is obtainable from the lexicon by constructions).
    > 
    > When we analyze a complex expression according to the constructions involved, we get something of the form of an inverted tree, like a genealogy. The complex expression is at the top. Below it, at the next level, are the 'immediate constituents'-- one or two or more-- from which the complex expression was got by one application of some one construction. Below each of these constituents are its immediate constituents; and so on down. Each branch of teh tree terminates downward in a word.
    >
    > ..
    > 
    > As an aid to specifying the constructions, the lexicon is classified into gramatical *categories*. For we want to be able to specify a construction by saying what operation is to be performed upon any expression of such and such category; or perhaps what operation is to be performed upon any pair of expresions, one of this cateogry and one of that. Since the compound expressions obtained by constructions are to be available as constituents under further constructions, we must also say what category each construction issues in.
    >
    > Thus a construction is specified in this vein: take any expressions, belonging respectively to such and such categories, and combine them in such and such a distinctive way; the result will belong to such and such a category. Commonly the distinctive way of combining the constituents will be marked by the insertion of a distinctive particle; examples are 'or', 'plus', 'and', 'but'. Also there are constructions that operate on single constituents, rather than combining two or more; one such is negation, which consists in prefacing the constituent with the partical 'not'.
    >
    > The constructions serve to add complex members to the categories, which had begun with word lists. A construction may even start a new category, which had no simple members; for instance, the class of sentences. Teh constructions, once specified, apply over and over, swelling the several categories ad infinitum.
    >
    > The categories are what we used to call the parts of speech, though they need no preserve the traditional lines of cleavage. One of our categories might be that of singular terms. Another might be that of copulas. Another might be that of intransitive verbs. Another might be that of adjectives. One of our constructions might be that of applying 'not' to a copula to get a complex copula. Another might be that of prefixing a copula to an adjective to get a complex intransitive verb: 'is mortal', 'is not mortal'. Another might be that of joining a singular term to an intransitive verb to get a sentence: "Tom is mortal", "Tom is not mortal". Another might be that of joining two sentences by an 'or' to get a sentence: 'Tom is mortal or Tom is not mortal'. What grammar tells us thus indirectly through its lexicon, categories, and constructions is not that this last sentence is true, but just that it is English." pg. 16-18 Philosophy of Logic Second Edition Quine

17. So, taking Quine's definition of functor from Methods of Logic 4th edition (he does give a different definition of 'functor' in the philosophy) with the grammatical distinctions in Philosophy of Logic second edition: functors are the grammatical particles of a grammatical construction on gramamtical categories.

18. It is important to distinguish grammatical particles from functors: not all gramatical particles need be functors.
I leave this open because though the logical methods I am allied to are those where the distinction between particle and functor is immaterial, I anticipate the odd grammar that may appear where some technical matter turns on the distinction between functor and particle.
For more see pages 26 thru 30 of Quine's Philosophy of Logic Second Edition.

19. All of these preliminary quotations from Quine and Carnap have been carefully presented so that upon dipping into Principia Mathematica I can make clear what are the problems and their solutions.
They are all principle concerned with the difference between logic and mathematics (and that specialization of mathematics called 'mathematical logic').

20. Already in the first sentence of the first volume of Principia Mathematica can the slurring of subjects be observed:
    > "The mathematical treatment of the principles of mathematics, which is the subject of the present work, has arisen from the conjunction of two different studies, both in the main very modern. On the one hand we have the work of analysts and geometers, in the way of formulating and systematising their axioms,a nd the work of Cantor and others on such matters as the theory of aggregates. On the other hand we have symbolic logic, which, after a necessary period of growth, has now, thanks to Peano and his followers, acquired the technical adaptability and the logical comprehensiveness that are essential to a mathematical instrument for dealing with what have hiterto been the beginnings of mathematics. From the combination fo these two studies two results emerge, namely (1) that what were formerly taken, tacitly or explicitly, as axioms, are either unnecessary or demonstrable; (2) that the same methods by which supposed axioms are demonstrated will given valuable results in regions, such as infinite number, which had formerly been regarded as inaccessible to human knowledge. Hence the scope of matheamtics is enlarged both by the addition of new subjects and by a backward extension into provinces hitherto abandoned to philosophy." Preface, v, Principia Mathematica Volume 1.

21. My claim is that it is the projection of strictly logical methods onto the methods of mathematics that produces the two results enumerated in the first paragraph of the preface (not the mathematization of the logical methods of mathematics).
It is perhaps best explained by the analogy of seperation of church and state. 

22. I must emphasize that I see no other way that humans could have achieved present logical and mathematical methods without going through something like the mistaken means amplified by Russell and Whitehead.
I submit that for better science to appear we must more carefully separate the methods of logic from the methods of the other sciences (and, hence, the application of the methods of logic to the methods of the other sciences).

## 2025 0911 1347

1. Made some tiny edits to [#2025-0908-1633](#2025-0908-1633):
    - added connected and strongly connected
    - added antisymmetric
    - changed 'total' to 'entire' to distinguish from 'total partial order' etc.

2. Almost ready to introduce order abbreviations: still not sold on 'multigraph abbreviations' as name of these.

## 2025 0910 1318

1. An technology of teaching is effective when it constructs the terminal repertoire specified.

2. A terminal repertoire is effective when it makes a person who responds appropriately to the world.

3. A response is appropriate when it contributes to the survival of the culture which aided in its construction.

4. Logical and scientific responses have contributed most to the survival of the cultures which aided in their construction.

5. 

## 2025 0908 2144

This continues notes from [#2025-0826-1635](#2025-0826-1635) on 2023 âThe Earth and Its Peoples 8th ed.â (EP) by Richard Bulliet, Pamela Crossley, Daniel Headrick, Steven Hirsch, and Lyman Johnson.

The fragments of notes from chapter 2 will come after I have finished notes on chapter one exhaustively.
I shall start at number 19 of the notes on chapter one, leaving point 20 until after it and so on.
Here it will begin anew as point 1.

1. No later than 80-50k years ago did verbal communities of humans appear. It could be much earlier because the science of behavior has yet to be effectively applied to history outside of what similarities it shares with experimental anthropology.
    - "currently approximately 7k languages spoken in the world, many of which are spoken by small numbers of people rapidly dying out" pg. 10
    - "scientists investigating the ancestors of *Homo sapiens* have to use changes in the anatomy of mouth and throat, brain size, and cultural behaviors to determine when language began" pg. 10
    - "Chomsky further argues that a genetic mutation occuring about 90k years ago in a single individual led to our ability to talk" pg. 10
        - Obviously this is inconsistent with the science of behavior.
    - "Some scholars are willing to place the origins of human language much further back in time, perhaps as far back as 2M years ago with *Homo erectus* .. Homo erectus was bipedal, made tools, cooperated wtih others in hunting large animals, traveled across the water in boats or rafts, produced simple forms of art, and organized its habitat into areas devoted to different functions .. However, homo erectus had the vocal appartus of a gorilla, so it could only make a few sounds." pg. 10
    - "earliest speech may have primarily invovled gestures, something that apes do to communicate and are capable of learning, whereas they lack the anatomical equipment to speak (and bodily gestures and facial expressions are still part of how we communicate today)." pg. 11
    - "Homo erectus was bipedal, these early hominins had their hands free to make such gestures. It is likely that this often involved miming the activity being - 'discussed' -. The next stage, freeing up the hands for other tasks like toomaking, was dependent on further anatomical development of the vocal apparatus." pg. 11
    - "While there can be no record of language until the advent of writing (arising first in Mesopotamia in the fourth millennium BCE), scholars have been able to use indirect evidence to reconstruct the spread and differentiation of the Indo-European family of languages that are spoken in large parts of Europe and Asia today (as well ast he Americas and Australia/New Zealand)." pg. 11
    - "Proto-Indo-European, the ancestral language from which all modern Indo-European langauges descend, was spoken in what si today Ukrain and souther Russia around 500BCE. By 400, peoples speaking this language began to migrate .. in an arc to the west, south, and east. The once-common langauge altered over time .. their ways of speaking differntiated into seperate languages." pg. 11
    - "The whoel Indo-European family of languages can be subdivided into major sub-groups: Celtic, Germanic, Romance (descended from Latin), Slavic, Iranian and Indian. .. English is at root a Germanic language with a significant overlay of words from the Romance family due to the Norman French conquest of England in 1066 CE." pg. 11
    - "It was noticed by teh late 18th centure CE taht many fundamental words in Greek, Latin, and Sanskrit are similar, and thus descend from a common ancestor; .. Scholars labor to use these words to reconstruct the original proto-Indo-European language" pg. 11
    - I have included so many selections from this part because I am particularly interested in the evolution of social behavior into verbal behavior.

2. Between 50-46k years ago humans migrated out of eastern and southern Africa throughout south Asia. Around 46k years ago they populated western and eastern Asia and Australia. Some 46-10k years ago they were in northeastern Asia and north and south America.

3. 1-2: Technology and Culture in the Ice Age

4. "Learned patterns fo action and expression constitute *culture*. .. Culture includes both material objects, such as dwellings, clothing, tools,a nd crafts, and nonmaterial values, beliefs, and languages. .. Among humans, instincts are less important thant he cultural traditions that each generation learns from its elders." pg. 13

5. 1-2a: Food Gathering and Stone Tools

6. "Because the tools that survive are mostly made of stone, the extensive period of history from the appearance of the first fabricated stone tools around 2.6M years ago until the appearance of metal tools around 6k years ago has been called the *Stone Age*." pg. 13

7. The name is misleading: "Early humans also made useful objects out of bone, skin, wood, plant fibers, and other materials less likely than stone to survive the ravages of Time."

8. "Early scholars recognized two phases of the Stone Age: the *Paleolithic* (pay-lee-oh-LITH-ik) (Old Stone Age), down to 8000 BCE, and the *Neolithic* (NEE-OH-LITH-IK) (New Stone Age), which is associated with the rise of agriculture. Modern scientists have developed more complex schemes with many subdivisions." pg. 13

9. "early humans depended heavily on vegetable foods such as leaves, seeds, and grasses, but during the Ice Age the consumption of highly nutritious animal flesh increased. .. increased meat eating and toolmaking-- appear to be closely linked." pg. 13

10. "Specimens of crude early tools found in the Great Rift Valley of eastern Africa reveal that *Homo habilis* made tools by chipping flakes off the edges of volcanic stones." pg. 13

11. "Lacking the skill to hunt and kill large animals, *Homo habilis probably obtained animal protein by scavenging meat from kills made by animal predators or resulting from accidents. .. used large stone 'choppers' for cracking open bones to get at the nutritious marrow .. such tools are found far from the volcanic outcrops where they were quarried.. people carried them long distances for use at kill sittes and camps." pg 13.

12. "Members of *Homo erectus* were also scavengers .. they made more tools for butchering large animals, including a hand ax formed by removing chips from both sides of a stone to produce a sharp outer edge. .. suitable for skinning and butchering animals, for scraping skins clean for use as clothing and mats, for sharpening wooden tools, and for digging up edible roots .. can also be hurled accurately for nearly 100 feet (30 meters), .. projectile to fell animals .. hunted elephants by driving into swamps .. trapped and died." pg. 13

13. "*Homo sapiens* .. array of finely made tools, they tracked and killed large animals. Sharp stone flakes chipped from carefully prepared rock cores were used in combination with other materials. Attaching a stone point to a wooden shaft made a spear. Embedding seveer sharp stone flakes in a bone handle produced a sawing tool. .. may of caused ecological crises. between 10k and 13k years ago .. giant mastodons and mamoths .. disappeared from Africa, Southeast Asia, and northern Europe. In North america around 11k years ago, three-fourths of the large mammals became extinct .. giant camels, ground sloths, stag-moose, giant cats, mastodons, and mammoths .. In australia .. similar events .. the Ice age .. distinguish .. climate change and human predation." pg. 13-14

14. "anthropologists do not believe that early humans depended primarly on meat for their food .. present-day *foragers* (hunter and food-gathering peoples) in Africa .. nourishment from wild vegetable foods, .. meat .. feasts, .. . The same .. true for Stone Age .. tools .. left few traces because they were .. perishable .. they .. used skins and mats woven from leaves for collecting fruits, berries, adn wild seeds .. dug edible roots out of the ground with wooden sticks." pg. 14-15

15. ".. meat and vegetables .. easier to digets when .. cooked .. first cooked .. by accident .. wildfires. .. setting fires .. 1.4M years ago .. maintaining hearths .. 500k years ago. .. clay cooking pots some 18k years ago in East Asia .. hard evidence of cooking." pg.15



## 2025 0908 2134

Collation of timeline from 'The Little Book of Big History' by Ian Crofton and Jeremy Black 2017

- 13.8B years ago: the big bang
4.6B years ago: the sun, Earth, and solar system form
- 4.5B years ago: the moon appears (collision between Earth and Mars size planet?)
- 4.2B years ago: oceans formed?
- 4.1-3.8B years ago: astroids pepper inner planets including Earth
- 4B years ago: appearence of rocks found to this day (proto-lifeforms e.g. metabolism or replicators?)
- 3.7B years ago: organic molecules and proto-bacteria?
- 3.4B years ago: photosynthesizing Cyanobacteria (blue-green algae)
- 2.45B years ago: oxygen in atmosphere from photosynthesis
- 600M years ago: multicellular organisms
- 542-488M years ago: Cambrian period e.g. external skeletons (trilobites and brachiopods), vertebrates (notochord as proto-spinal column)
- 488-444M years ago: Ordovician period e.g. diversity of trilobites, lamp shells, gastropods, and graptolites; appearence of sea urchins, starfishes, and ammonites; later, land plants and mass extinctions
- 444-416M years ago: Silurian period e.g. marine life (e.g. cartilaginuous, latterly boney); invertebrates on land (e.g. scorpions andwingless insects); vascular plants (e.g. club mosses)
- 416-359M years ago: Devonian period e.g. big coral reefs, ferns, early amphibians, four-legged animals, spread of land animals
- 359-299M years ago: Carboniferous period e.g. flying insects, reptiles, land plants (e.g. conifers) spread (origin of modern coal deposits)
- 299-251M years ago: Permian period e.g. diverse reptiles, mass extinction of many marine and land animals (e.g. trilobytes)
- 251-200M years ago: Triassic period e.g. dinosaurs and small mammals
- 200M-145M years ago: Jurassic period e.g. diverse dinosaurs, turltes, crocodiles, tropical forests, and birds
- 145M-66M years ago: Creteous period e.g. flower plants, grasses, extinction of dinosaurs, survival of birds and mammals
- 66-56M years ago: Palaeoscene epoch e.g. diverse mammals, primates
- 56-34M years ago: Eocene epoch e.g. mammls spread (e.g. elephants, whales, rodents, carnivores and hoofed)
- 34-23M years ago: Oligocene epoch e.g. grasslands, monkeys
- 23-5.3M years ago: Miocene epoch e.g. horses, apes, frogs, snakes, rats
- 7M years ago: common ancestor splits into ancestors of humans, chimps, and bonobos
- 6M years ago: some early humans walk on hind legs
- 5.3-2.6M years ago: Plioscene epoch e.g. mammoths, early humans walk upright
- 2.6M years ago: human tool use
- 2.6M-11.7k years ago: Pleistocene epoch e.g. ice ages and warmer interglacial periods
- 2.4M years ago: Homo habilis
- 1.9M-143k years ago: Homo erectus spreads
- 200k years ago: Homo sapien in Africa
- 150-50k years ago: language? 33 100k years ago: Homo sapien out of Africa, burials and graves
- 75k years ago: pierced shell necklaces
- 45k years ago: âfully modernâ humans in Europe ???
- 42k years ago: wood and bone flutes in Europe
- 40-35k years ago: stone and ivory carved into human-animal shapes in Europe
- 38-35k years ago: cave art
- 22k years ago: peak of latest ice age
- 19k years ago: foraging wild cereal in Middle East
- 14k years ago: domestocation of dogs from wolves, grindstones in Middle East
- 13k years ago: engraved antler in Longyn Cave of China
- 12k years ago: European glaciers retreat
- 11.7k-present: Holoscene epoch e.g. end of last ice age, extinction of many large land animals (by humans?), humans spread far and wide
- 8k years ago: cultivation of wheat and barley from Middle East to Nile valley
- 7k years ago: villages of hunters and fishers in Yangtze river delta of China cultivate rice, villages of western Europe farm cereal
- 5.5-4k BCE: Sumerian culture in Mesopotamia (now Iraq)
- 4.5k years ago: long-distance trade in South America
- 3650-1400 BCE: Minoan and early Aegean cultures
- 3100 BCE: Upper and Lower Egypt united by first pharaoh
- 2600-1900 BCE: strong Indus valley cultures
- 2580-2560 BCE: Great Pyramid of Giza
- 2070 BCE: Xia dynasty of China
- 2000 BCE: preclassic Mayan culture in Mesoamerica
- 1754 BCE: Code of Hammurabi of Babylonian empire
- 1650 BCE: Hittite kingdom of Turkie
- 1600 BCE: Shang dynasty of middle valley of Yellow River of China
- 1600-1500 BCE: Olmec culture in present day Mexico
- 1550-1077 BCE: New Empire of Egypt controls Levant to Nubia
- 1500-800 BCE: Vedic Age e.g. Hindu scripture of India
- 1070 BCE: Kush Kingdom in present day Sudan
- 1000 BCE: Strong Phoenician cultures, e.g., of Tyre and Sidon
- 911-612 BCE: Neo-Assyrian culture of Tigris valley
- 900-200 BCE: Chavin culture of present day Peru
- 800-400 BCE: Dâmt Kingdom of Ethiopia
- c. 550 BCE: Cyrus the Greatâs Achaemenid empire of Persia
- 510-323 BCE: Classical Ancient Greece
- 509 BCE: Roman Republic
- 331 BCE: Alexander the Great of Macedon strengthens Hellenistic culture by controlling from the Adriatic Sea to the Indus River e.g. introducing Roman practcies into Achamenid culture of Persia
- 321-185 BCE: Mauryan culture of India
- 300 BCE: Library of Alexandria
- 221 BCE: Qin then Han culture unites China
- 212 BCE: Edict of Caracalla
- 100 BCE: Rome is the biggest city
- 100 BCE - 750 CE: Teotihuacan evolves and dies as the once biggest city of the mericas
- 300-1200 CE: Ghana culture in present day Naurtania and Mali
- 410 CE: Roman culture extinguished by Visigoths no later than 476 CE
- 661-750 (CE) : Umayyad caliphate culture controls largest area including that of present day Georgia, Uzbekistan, Pakistan, Arabian penisula, North Africa, Spain, and Portugal
- 1055: Seljuk Turks control Baghdad
- c. 1200: Incas in Andean valley of Peru
- 1200-1400: Mississippian culture of North America is strongest
- 1206: Qutb al-Din Aibak is first controller of Delhi Sultante
- 1211: Mongols start controlling Eurasia and northern China
- 1368: Ming dynasty established in China
- 1393: Timur (aka Tamerlane) sacks Baghdad.
- 1405: Beginning of Zheng Heâs voyages in Indian Ocean
- 1438: Beginning of period of Inca conquests
- 1453: Ottoman Turks control Constantinople
- 1455: Gutenberg Bible from his printing press
- 1492: Muslim control in Spain extinguished by Christians; Columbus in West Indies
- 1498: Vasco da Gama of Portugal in India via South most tip of Africa
- 1517: Protestant Reformation of Martin Luther
- 1519-21: Spanish control Aztec culture
- 1519-22: Magellan and Del Cano circumnavigate the world
- 1526: Mughal controls India
- 1532-5: Spanish control Inca
- 1543: heliocentric theory of Copernicus
- 1571: Ottoman control of Mediterranean ended at Lepanto by Holy League
- 1607: English control Virginia
- 1644: Machus set up Qing control of China
- 1648: End of Thirty Yearsâs War
- 1652: Dutch set up Cape Colony of southern Africa
- 1683: Vienna countercontrols Turks
- 1687: Newtonâs of motion and universal gravitation
- 1763: End of Seven Yearsâs War
- 1776: US Declaration of Independence; âWealth of Nationsâ by Adam Smith
- 1783: Human flight by baloon
- 1785: Steam powered cotton mill
- 1788: British in Australia
- 1789: French Revolution
- 1792-1815: French Revolutionary and Napoleonic wars
- 1803: US buy part of North America from France
- 1808-26: Spanish control of Amercas weakens
- 1825: passenger steam railway in England, from Stockton to Darlington
- 1830: Greece countercontrols Ottoman Turks
- 1833: British culture abolishes slavery
- 1844: Morseâs telegraph
- 1848: âCommunist Manifestoâ of Marx and Engles
- 1848-9: revolutions extinguished in Europe
- 1853: US countercontrols Japan: opens trade
- 1857: India countercontrols British
- 1859: âOrigin of Speciesâ by Darwin
- 1861: Italian Unification
- 1861-5: American Civil War
- 1868: Japan strengthens
- 1869: transcontinental railway in USA; Suez Canal
- 1871: German Unification
- 1876: telephone
- 1884: Europeans control Africa at Berlin Conference
- 1895: wireless telegraphy from Marconi
- 1903: Wright Flyer
- 1905: Einstienâs special theory of relativity
- 1911: Chinese revolution
- 1914: First World War; Panama canal
- 1917: Russian Revolution
- 1918: End of First World War; Austrian, German, and Turkish cultures weaken
- 1923: non-Mliky Way galaxies
- 1928: penicillin
- 1929: Great Depression
- 1933: Naziâs strengthen
- 1937: Second Sino-Japanese War
- 1939: Second World War
- 1943: electronic computer
- 1945: atomic bomb; End of Second World War; United Nations; Cold War
- 1947: Indian Independence
- 1949: Chinese Civil War; NATO
- 1950-3: Korean War 62: 1955-75: Vietnam War
- 1957: Sputnik 1; European Economic Community
- 1960: 3B people
- 1961: Human Space Flight
- 1967: Human heart transplant
- 1969: Human on Moon
- 1976: Robot on Mars
- 1978: Human invitro fertilization
- 1979: Smallpox extinguished
- 1989: World Wide Web
- 1991: Soviet Union extinguished
- 1997: cloned mammal
- 1999: 6B people
- 2003: Human Genome Project
- 2011: 7B people
- 2012: Higgs Boson


## 2025 0908 1633

Although it can be time consuming, when I collect and collate past notes into present ones, as I shall do here, I often type them out rather than copy and paste.
It helps me to catch old errors and every so often introduces new ones.
Not all new errors are helpful, but sometimes they are.
Presumably, they are helpful enough that their tolerance and that of the difficulty of typing rather than copypasting is worth it in the end.

1. The predication abbreviations:
    1. Abstraction: '..a(..u:..uF..x:..x)..b' for '..some v is such that.. some y is such that .., v=a, .., y=b, and .. some u is such taht .. some x is such that ..uF..x, .., u=v, .., and x=y'
    2. Concretion: ..a(..u:..uF..x:..x)..b if and only if ..aF..b

2. A complete collection of primitive predicate functor abbreviations
    1. Drop: 'drop F' or 'drop (..u:..uF..x:..x)' for '(..uv:..uF..x:..x)'
    2. Hem: 'hem F' or 'hem (..uvw:..uvwFx..y:x..y)' for '(..uvw:..uvwFv..y:..y)'
    3. Huh: 'huh F G' or 'huh (..uxw:..uxwF..y:..y) (..uxw:..uxwG..y:..y)' for '(..u: some v is such that ..uxvF..y nor ..uxvG..y :x..y)' where 'v' is a new variable

3. The denotative functional abbreviations are
    1. Joint Denial: 'nor F G' for 'huh hem drop drop F hem drop drop G' i.e. '(..u:..uFx..y nor ..Gx..y:x..y)'
    2. Negation: 'not F' for 'nor F F' i.e. '(..u: not ..uF..x:..x)'
    3. Alternation: 'or F G' for 'not nor F G' i.e. '(..u:..uF..x or ..uG..x:..x)'
    4. Converse Conditional: 'if F G' for 'or F not G'
    5. Complementary Converse Conditional 'not-if F G' for 'not if F G'
    6. Complementary Conditional: 'not-only-if F G' for 'not-if G F'
    7. Conditional: 'only-if F G' for 'not not-only-if F G'
    8. Alternative Denial: 'not-and F G' for 'only-if F not G'
    9. Conjunction: 'and F G' for 'not not-and F G'
    10. Exclusive Alternation: 'or-exclusively F G' or 'orx F G'and or F G not-and F G'
    11. Biconditional: 'if-and-only-if F G' or 'iff F G' for 'not or-exclusively F G'
    12. Repeat notation
        1. '(0 ...)' for ''
        2. '(1 ...)' for '...'
        3. '(1+n ..)' for '... (n ...)'
    13. Cedents: 'cede m n ..F ..G' for 'only-if (m and) ..F (n or) ..G'

4. The remaining recombic abbreviations (in addition to 'drop' and 'hem'):
    1. Push: 'push F' for 'not huh drop F drop F' i.e. '(..u:..uxF..y:x..y)'
    2. Dush: 'dush F' for 'drop push F' i.e. '(..uv:..uFx..y:x..y)'
    3. Prop: 'prop F' for 'push drop F' i.e. '(..u:..uF..y:x..y)'
    4. Over: 'over F' for 'hem push F' i.e. '(..uvw:..uvwvF..x:..x)'
    5. Oem: 'oem F' for 'over hem F' i.e '(..uvw:..uvwvFw..x:..x)'
    6. Dup: 'dup F' for 'oem dush F' i.e. '(..uvw:..uvwwF..x:..x)'
    7. Pop: 'pop F' for 'oem drop drop F' i.e. '(..uvw:..uvFw..x:..x)'
    8. Nip
        1. 'nip n F' for '(n pop) drop (n push) F' i.e. '(..uv..wa: ..u..waF..x:..x)'
        2. 'nip F' for 'nip 1 F' i.e. '(..uvw:..uwF..x:..x)'
    9. Dig: 'dig F' for 'hem nip F' i.e. '(..uvw:..uwFv..x:..x)'
    10. Bury
        1. 'bury n F' for '(n dig) (n push) F' i.e. '(..u..vw:..uw..vF..x:..x)'
        2. 'bury F' for 'bury 1 F'
    11. Unbury
        1. 'unbury n F' for '(n bury n) F' i.e. '(..uw..va:..u..vawF..x:..x)'
        2. 'unbury F' for 'unbury 1 F'
    12. Roll (clockwise)
        1. 'roll i m F' for '(m push) bury i+m (m pop) F' i.e. '(..u..wv:..uy..wFv..x..z:..xy..z)'
        2. 'roll i F' for 'roll i 0 F'
        3. 'roll F' for 'roll 1 F'
    13. Unroll (counterclockwise):
        1. 'unroll i m F' for '(m push) unbury i+m (m pop) F' i.e. '(..uw..v:..u..vxF..yw..z:x..y..z)'
        2. 'unroll i F' for 'unroll i 0 F'
        3. 'unroll F' for 'unroll 1 F'
5. The quantificational abbreviations
    1. Universal Cropping
        1. 'each F' for 'not huh hem nip F hem nip F' i.e. '(..u: each v is such that ..uvFx..y:x..y)'
        2. 'each i F' for '(i each) F'
        3. 'each i m F' for 'each i+m (m pop) F'
    2. Existential Cropping
        1. 'some i m F' for 'not each i m not F'
        2. 'some i F' for 'some i 0 F'
        3. 'some F' for 'some 1 F'

6. The Boolean abbreviations
    1. Existential Closure: 'Some F' for 'some i m F' when 'F' is substituted for an (i,m) place predicate or schema
    2. Universal Closure: 'Each F' for 'not Some not F'
    3. Inclusion: 'includes F G' for 'Each if F G'
    4. Converse Inclusion: 'included F G' for 'includes G F'
    5. Coextension: 'coextensive F G' for 'and includes F G and included F G'
    6. Proper Inclusion: 'properly-includes F G' for 'and includes F G not coextensive F G'
    7. Proper Converse Inclusion: 'properly-included F G' for 'properly-includes G F'

7. The sameness abbreviations
    1. Simple Indiscernability
        1. 'sind 0 i m F' for 'each i m (2 unburry i+2) iff drop F nip F'
        2. 'sind 1+k i m F' for 'and sind 0 i m F sind k i m roll i m F'
        3. 'sind F' for 'sind i+m i m F' when 'F' is substituted for an i m place predicate or schema
    2. Complex Indiscernability
        1. 'ind 1 F' for 'sind F'
        2. 'ind 1+k F ..G' for 'and sind F ind k ..G'
    3. Lexical Indiscernability
        1. 'id' for the substitution in 'ind k ..F' of .., and 'F' for the k lexical predicates of the theory
        2. 'id 0 i' for '(i nip) (i nip 2) id'
        3. 'id 1+k i' for 'and id 0 i roll i id k i'
        4. 'id i' for 'id i i\*2'
    4. Lexical Discernability
        1. 'nid i' for 'not id i'
        2. 'nid' for 'nid 1' 
    5. Item
        1. 'item' for 'dup id'
        2. 'item 0 i' for '(i nip) item'
        3. 'item 1+k i' for 'and item 0 i roll i item k i'
        4. 'item i' for 'item i i'
    6. Void
        1. 'void i' for 'not item i'
        2. 'void' for 'void 0'

8. The compositional abbreviations
    1. Projection: 'proj k i m F' for 'some i m (k roll 1+i m) F'
    2. Field
        1. 'field 0 i m F' for 'proj 0 i m F'   
        2. 'field 1+k i m F' for 'or field 0 i m F field k i m roll 1+i m F'
        3. 'field i m F' for 'field i+m i m F'
        4. 'field F' for 'field i m F' when 'F' is substituted for an i m place predicate or schema
        5. 'field F 0 n' for '(n drop) field F'
        6. 'field F 1+k n' for 'and field F 0 n roll n field F k n'
        7. 'field F n' for 'field F n n'
    3. Resultant: 'res j k m F G' for 'some j (j bury j+k) and (k drop) F (m prop) G'
    4. Composition
        1. 'compose i j F G' for 'res i j 0 F (i nip j) G'
        2. 'compose i F G' for 'compose i i F G'
    5. Iteration
        1. 'iter 0 i F' for 'and id i field F i'
        2. 'iter 1 i F' for 'F'
        3. 'iter 1+k i F' for 'compose i F iter k i F'
    6. Converse
        1. 'conv i j F' for '(j roll i+j) F'
        2. 'conv i F' for 'conv i i F'
        3. 'conv F' for '(m push) conv i m (i pop) F' when 'F' is substituted for an i m place predicate or schema
    7. Inverse Iteration
        1. '-iter k i F' for 'iter k i conv i F'
        2. '-iter F' for '-iter 1 i F' when 'F' is substituted for an 2\*i m place predicate or schema
    8. Functive
        1. 'functive i j F' for 'included compose i j F conv i j F id i'
        2. 'functive F' for 'functive i m (m push) F' when 'F' is substituted for an i m place predicate or schema
    9. Injective: 'injective i j F' for 'included compose j i conv i j F F id j'
    10. One-to-one: 'one-to-one i j F' for 'and injective i j F functive i j F '
    11. One-to-many: 'one-to-many i j F' for 'and injective i j F not functive i j F'
    12. Many-to-one: 'many-to-one i j F' for 'and not injective i j F functive i j F'
    13. Many-to-many: 'many-to-many i j F' for 'and not injective i j not functive i j F'
    14. Entire: 'entire i j F' for 'each i some j F'
    15. Surjective: 'surjective i j F' for 'entire j i conv i j F'
    16. Functional: 'functional i j F' for 'and functive i j F total i j F'
    17. Injectional: 'injectional i j F' for 'and functional i j F injective i j F'
    18. Surjectional: 'surjectional i j F' for 'and functional i j F surjective i j F'
    19. Bijectional: 'bijectional i j F' for 'and and functional i j F injectional i j F surjectional i j F'

9. The multigraph abbreviations
    1. Symmetries: 'symmetric i F' for 'included F conv i F'
    2. Nonsymmetries: 'nonsymmetric i F' for 'not symmetric i F'
    3. Asymmetries: 'asymmetric i F' for 'included F not conv i F'
    4. Antisymmetric: 'antisymmetric i F' for 'included and F conv i F id i'
    5. Transitivities: 'transitive i F' for 'include iter 2 i F F'
    6. Nontransitivities: 'nontransitive i F' for 'not transitive F'
    7. Intransitivities: 'intransitive i F' for incluced iter 2 i F not F'
    8. Reflexivities: 'reflexive i F' for 'included iter 0 i F F'
    9. Nonreflexivities: 'nonreflexive i F' for 'not reflexive i F'
    10. Irreflexivities: 'irreflexive i F' for 'included F nid i'
    11. Total Reflexivities 'total-reflexive i F' for 'included id i F'
    12. Connected: 'connected i F' for 'antisymmetric i not F'
    13. Strongly Connected: 'strongly-connected i F' for 'asymmetric i not F'

## 2025 0906 2107

Fragments of notes and two selections from Durant's 'The Pleasures of Philosophy'.

1. Arguing is said to express reasoning, arguments to express reasons, and logic to study correct, appropriate, effective, faultless, proper, good, right, flawless, strict, errorless, infallible, impeccable, or accurate reasoning.

2. Here are two selections from Durant's 1929 'The Mansions of Philosophy' (reprinted in 1952 as 'The Pleasures of Philosophy', the edition from which I quote)

    > "Logic is a poor hors d'ourvre for the feast of philosophy; it dulls a thousand appetites for every one it whets. We suspect logic because we have learned that most reasoning is desire dressed in a little rationality; we pretend to be constructing edifices of impartial thought, when actually we are selecting only such facts and agreements as will give dignity to some personal or patriotic wish. We suspect logic because middle age has taught us that life is larger, surer, profounder than our syllogisms; logic is static, puffed up with 'invariable truths,' while life is fluent and changeful, and surprises all formulas. 'The number of things that reason at first refused to recognized, and yet had in the end to admit, is considerable.' Perhaps in our youth we memorized all the rules of perfect thinking, only to find that the pursuit of knowledge, the recognition of truth, and the wisdom of life, fell incalculably outside this elegantly ordered realm. How gladly we would leave to the end this lgoic that can make even philosophy dry and spiritless, rather than set it here as a barrier to problems less basic, possibly, but much more directly vital to our lives! And yet we must not; we cannot ride forth on our quest of truth without determining in advance what we are looking for, by what road we propose to seek it, and how we shall know it if we come upon it. Any other order would not be logical!" pg. 15-16

    > "Where, then, shall be the place of reason in this ridiculously plebeian logic of ours, that confirms the prejudices of the commonest man in the street? Its function here, as elsewhere, is to coordinate-- sensations into ideas, ideas into knowledge, knowledge into wisdom, purposes into personality, individuals into society, societies into peace. The role of reason in the conquest of truth is secondary but vital: it must weave the chaos and contradictions of many sens into unified and harmonious conclusions which it shall hold subject to verification or rejection by subsequent sensation. It is not half so certain as sensation; for 'in transcending what is given by actual perception we without doubt make use of an inference'; and every inferential step awa from immediate sensation lowers the probability of our truth. But this, too, is a gamble that life must make; we must attempt the reconcilliation of discordant senses and partial views; if we are to extend our understanding and our mastery. Just as Kohler's chimpanzees reasoned best when they took in the entire situation, so for ourselves reasoned truth, like philosophy and wisdom, like morality and beauty, is total perspective, the harmonious union of the part with the whole. Through sensation we stand firmly with our feet on the earth; through reason we lift the mind's eye beyond the present scope of sense, and conceive new truths which some day the senses may verify. Sensation is the test of truth, but reason is its discoverer." pg. 20


## 2025 0905 2108

1. Happened to catch an episode of Jude Judy's 'Justice on Trial' and enjoyed it quite a lot: I would enjoy seeing more mock trial shows.

2. A delightful conversation with the wonderfully talented Polyducks <https://polyducks.co.uk> prompted me to dig back into the foundations of the science of behavior.

3. One of the key paths that needs smoothing out is that from social behavior through verbal behavior to scientific and logical behavior.
I shall collect notes from work done by Skinner and others along this path and attempt to link everything together with a aim towards the construction of a logical theory.

4. There are many things which Skinner went out of his way to explain with a kind of clarity that very often surpasses Quine's keen eye for clear and exact prose.
Skinner's explanation of 'interpretation' is one such example:
    > "The use of concepts and laws derived from an experimental analysis in the interpretation of daily life is also a source of misunderstanding. An analogy from another science may be helpful. Geophysics interprets the present condition of the accessible parts of the earth in terms of presumed conditions in the mantle and core. It appeals quite freely to the physical laws derived from laboratory analyses of matter under various pressures and temperatures, even though it is merely an assumption that comparable states actually prevail in the interior of the earth. In the same way familiar facts about verbal behavior are interpreted with principles derived from the laboratory study of contingencies of reinforcement, even though the contingencies maintained by the verbal environment cannot be precisely ascertained. In both these examples principles derived from research conducted under the favorable conditions of the laboratory are used to give a plausable account of facts which are not at the moment under experimental control. Neither account can at the present time be proved, but both are to be preferred to treatments which lack the same kind of experimental support." pg. 100 Skinner 'Contingencies of Reinforcement'

5. This distinguishes interpretation from speculation, though the boundary is not as clear as that between, e.g., preformal and formal mathematics.
What Skinner emphasizes is that the vast majority of verbal behavior is not strictly logical or scientific and yet much of what passes as scientific and logic behavior is not as strict as vast majority of scientists and logicians purport.

6. As a side note: almost all (all?) of Skinner's writings are available through the B. F. Skinner foundation <https://www.bfskinner.org/>.

7. The following selection from the beginning of the fourth chapter 'The experimental analysis of behavior' in Skinner's book 'Contingencies of Reinforcement' is perhaps my favorite starting point:
    > "A natural datum in a science of behavior is the probability that a given bit of behavior will occur at a given time. An experimental analysis deals with that probability in terms of frequency or rate of responding. Like probability, rate of responding would be a meaningless concept if it were not possible to specify topography of response in such a way that separate instances of an operant can be counted. The specification is usually made with the help of a part of the apparatus-- the 'operandum'-- which senses occurrences of a response. In practice, responses so defined show a considerable uniformity as the organism moves about in a framework set by its own anatomy and the immediate environment." pg. 83.


## 2025 0904 2341

An experiment in writing without restraints.

1. It is sad but true that I have not yet written substantially about behavior in these notes.
So much of the last decade of my life and work has been dedicated to behavior and yet there is nothing here that would make that clear.
Sure, I mention B. F. Skinner more often than most, but here I have mentioned Quine more than Skinner.
To those that know me 'in real life' this is a surprise: they wait for Skinner to enter every conversation (and, unsurprisingly, they now bring Skinner up before I do).

2. I am not a disciple of Skinner.
I may call myself more of a radical behaviorist than just a regular old behaviorist, but that does not, to me at least, then imply that I am devoted to all that Skinner has said or all that has been attributed to what Skinner may or may not have said.
This must be written explicitly because there is so much about Skinner that can either be misunderstood or when understood is very unlike what I have got from reading him.

3. It is not uncommon for me to read something that so many others have read and to discover something about it that is at odds with prevailing interpretations.
This happens when I read Skinner and when I read Quine.
It happens because I have read almost everything that they have written and reread it all again.
There are always gaps in my reading: I am not a reading machine even though I may dream of one day being one.

4. The Skinner we have from his writings is not the same Skinner we have from the rest of his work.
This is true of anyone who has done work which the public has been forced to recognize because of its consequences to culture.
Little has had a more profound effect on the way we live and work than the science which sometimes goes by the peculiar name 'the experimental analysis of behavior'.

5. The phrase 'the experimental analysis of behavior' is a mouthful.
Sometimes it is interchangeable with 'the science of behavior' and sometimes it is not.
When I speak with others about behavior, especially when I speak to them about human behavior, I often have to say 'the experimental analysis of behavior' rather than 'the science of behavior' because most people are quick to separate humans from the rest of the behaving organisms the world has come to know.

6. Another reason to emphasize the experimental analysis of behavior is to point away from 'the analysis of mind' or 'psychological analysis', e.g., as in Freud's early science.
Skinner said on more than one occasion that Freud was one of the first behaviorists, and this is as much a word game as it is a concrete observation: Freud linked present behavior to past consequences.

7. As an aside, I'm writing more casually than I have up to this point because I can not stand the stark entries I've made as of late.
They are practically impenetrable to most people and I do not like that.
I'd rather write in a way that is at least accessible to someone who, unlike me, is less sensitive to precision: inaccuracy is inexcusable but imprecision is often acceptable.

8. How to start up the science of behavior is as difficult a problem as how to start up the science of logic.
There are no obvious entryways into the bits that matter most.
Everything is either obvious or obscure.

9. There is the temptation, one which has been around for centuries, to flirt with philosophy or with some other science which is in vogue e.g. cognitive science is still popular enough to command the attention of most casual readers.
A kind of watered down cognitive science is still popular among the influencers who profess a sort of mystic mastery over the minds of the masses.

10. It is popularity gives the public a feeling of familiarity: if they don't already know something about it they feel as if there is something there to be known.
It is surprising how often I have spoken either about behavior or logic and the listener has said 'I do not know what you are talking about'.
Yet, if I was to tell a fantasy story about a unicorn's search for the lost love of their youth, there is suddenly something to know.

11. It is not as if I suck at communicating, though I in no way profess to be good at writing or speaking.
I lack the rhetorical flare that comes from that false sense of certainty behind any hard won piece of proper literature (be it an essay or an encyclopedic entry).

12. I am uncertain in almost all that I do and this is the ultimate weakness in an effective writer.
Writers are tyrrants.
They take complete control of the reader and are bound by no laws, no reasons, no limits other than what they can get away with, and a good writer can get away with the world.

13. A good story well told beats any sort of scientific explanation or experiment every time.
It doesn't just beat it, it extinguishes it.
Science lives or dies by the stories that people tell: no experiment in the world can undo the damage done by a good story.
Good stories are their own death, and so often that death is delayed again and again.
People can't drop a good story even if it harms their friends and family.
The world is powerless, and the writer knows all of this, and somehow they still publish.

14. I'm glad to say that this note has now become less about the science of behavior and more about the pains caused by my subpar writing.
I'm uncertain and fallible.
Raw strength sniffs out whatever whiff of weakness there is to be found in unfamiliar words and sentences.
Those who would preach to the choir are at least assured of the interests of their audience.
Pick love, money, power, or any other of the more complex derivatives of food, sex, and water and you'll find a choir waiting for you to sing for your supper.

15. How am I to tell a good story when I dispise them so?
Can science go on without stories?
Others have told me that there is a compromise e.g. imprecision is unavoidable.
If you had to sit around all day and speak down to ever nanometer you'd never reach any part of the world beyond the tip of your nose.
Are accuracy and imprecision enough to spawn stories?
Why not just look at what others have written?

16. Writers of history have done the most to help me understand the kinds of stories that can be told which are both accurate and imprecise.
Imprecision is the order of the day in history.
But, historians are plagued by the same old problem: who cares?
So what if so-and-so won the war some two centuries ago: why are you wasting my time with this?

17. To say "Well, some people like history and some people don't." is to condemn those who do and those who don't.
They are born that way, or they were brought up that way, and there's nothing you can do about their birth or their upbringing: they are in the past and inaccessible, much less manipulable.
Science is where such misguided excuses end: the more we know about how the world works the more control we have over how the world works out.

19. This has been another strange experiment in a less restricted kind of writing.

## 2025 0904 1549

1. Sequence of links to papers that are relevant to garbarge collection of binary trees.

2. Henry Baker 1992 'Lively linear Lisp'
    - <https://www.cs.utexas.edu/~hunt/research/hash-cons/hash-cons-papers/BakerLinearLisp.pdf>

3. Henry Baker 1994 'Linear logic and permutations stacks'
    - <https://dcreager.net/pdf/Baker1994.pdf>
    - <https://plover.com/~mjd/misc/hbaker-archive/ForthStack.html>

4. Doublas W. Clark 1978 'A Fast Algorithm for Copying List Structures'
    - <https://dl.acm.org/doi/abs/10.1145/359488.359491>

5. Gary Lindstrom 1974 'copying List Structures Using Bounded Workspace'
    - <https://dl.acm.org/doi/10.1145/360924.360936>

6. K. P. Lee 1980 'A Linear Algorithm for Copying Binary Trees Using Bounded Workspace'
    - <https://dl.acm.org/doi/abs/10.1145/358826.358835>

7. Jacques Cohen 1981 'Garbage Collection of Linked Data Structures'
    - <https://www2.cs.arizona.edu/~collberg/Teaching/553/2011/Resources/gc-cohen.pdf>
    - <https://dl.acm.org/doi/10.1145/356850.356854>

## 2025 0904 1501

1. Most people learn logic for the same reason they learn any other foreign language: they love someone (or something) and wish to know them (or it) in their native tongue.

2. Those in love with ideas, items, and words are ready for logic.
Those who love this or that philosopher may only be as prepared to learn logic as they are to learn their native language, be it Latin, Greek, Gemran, French, Spanish, English, Chinese etc.

3. The language of philosophy's most well known child, science, is logic.
Contemporaries would point to something beyond logic, be it a deviation or the divinations of statisticians and probability theorists (the modern day Oracle of Delphi).

4. At least the probability theorist has the courage to put 'theorist' in their name.
Those that would dodge the dismissive of 'theory' are doomed to be found out and live in constant fear that they shall be caught by their assumptions.

5. The days where philosophy gave us a consistent way of life are gone, but the delights of philosophic practices are as alive as ever.
Those that condemn human behavior for its limited fallibility have condemned their life no matter how consistent it may seem.

6. There is no better reason for anyone to read what I've written here than there is for them to go to a pizza parlor just because I recommended it.
It is easy enough for me to admit that I hide in obscurities because I am destine to lose to anyone who professes the profound.
The richness of my methods are mundane when compared to the new.
Mine are old.
Not so old as the stories we have told since we found stories to tell, but much older than the new and much less ornate in their presentation.

7. I am told that this is a failure on my part.
Though I object, perhaps by saying this is a feature, I am at a loss to defend myself against the profound, the ambitious, or the earnest.
The performance is lost on me, my life, and my work.

8. At best I meddle in the world of words, never quite reaching the items and ideas that dazzle and confound the best and worst of the world (though to call me a nominalist would be a dire insult).

9. What logic brings to me is my love.
If your love is science, and if through it you have found a kind of sublime satisfaction, then logic is the language in which you may know your love better than you know your self.

10. Once upon a time, logic was correct reasoning.
Humans were special because they reasoned in ways that no other creature could.
The division of logic into its purportedly formal and informal parts was a grand mistake: at best they are poor names for poles between which the practices of paraphrasing are more or less effective (but do not be fooled into taking effectiveness as a criterion of logic).

11. Arguments express inferences, or so we are told: what evidence there is for either the argument or the inference is in need of closer examination.

12. An argument was then correct when its conclusions can be inferred from its premises: it is seen by some as evidence for the inference which is part of an act of reason.

13. When an argument is said (erroneously) to express an inference, the premises and conclusions of the argument are said to express what the inference is about.
The things inferences are about are called propositions: they are also the things that are said (erroneously) to be true or false.

14. When it is said that a sentence of an argument is true, it is true only because it expresses a true proposition (again this is the classical error).

15. This has been a poorly written polemic.

## 2025 0904 1346

1. My  work is largely inaccessible because of how bad I am at writing.
Almost all of the writing I've done throughout my life is aimed at a kind of self discovery: it doesn't take into account what other people are interested in or in what they tend to think.

2. While there's room for the kind of writing that leads to discovery, it can only go so far.
It might help one or two people who are already on a similar discovery expedition.


## 2025 0903 1853

1. When I work to know a thing my view of the world narrows upon it.
I am blind to what else the world has to offer and may bash myself upon an obstacle heralded no more covertly than the rising of the sun.

2. This blindness seems characteristic of any intense investigation.
In the past, metaphors made up for such mistakes.
A reader may question whether the metaphor carries the day, but may never make it back to the skirmish from which it was won.
My fear, and it is so very frequent, is that without logic I shall always be blind to the largest of obstacles: doubt.

3. To know logic has been my thing.
Beyond logic is theory and my theory of the world is bound to B. F. Skinner's radical behaviorism.

4. The basic things of the world are contingencies.
They are consequences from responses on occasions.
No more is known of them than that.

5. Theories project logic upon the world and work with the rest of our repertoires to deal with what troubles us most.

6. The premises of a theory activate the network of validities uncovered by logic, and through them reach conclusions which check the rest of the world.
Conclusions go extinct before premises, but no premise is left unthreatened by contravened conclusions.

7. I am failing to write freely.

## 2025 0903 1548

1. The wikipedia page on binary relations <https://en.wikipedia.org/wiki/Binary_relation> includes a subsection on 'Types of Binary Relations'.
These are properly taken as types of two place, or two zero place, or one one place, predicates.

2. The sum of the number of left and right places of a predicate is its degree.
The organization of predicates of any degree with respect to the types of binary relations proceeds in the same way as when the methods of integral and differential calculus are extended to their multivariable generalizations.

3. Thankfully, my names for the different kinds of funtional abbreviations of logic are consistent with those in the wikipedia article.

4. Upon reading the wikipedia article, it occurred to me to distinguish between injectiveness, perhpas just injective, and injectivity.
Maybe better, the difference between injective and injection.

5. An injection is total and injective i.e. functional and injective.

6. The difference between functive and functional just occured to me.
This would be carried on as injective and injectional, surjective and surjectional, and bijective and bijectional.

7. Another edit that must be made to my organization of predicates is to change 'graph abbreviations' to 'multigraph abbreviations'.
They were always multigraph abbreviations and it is only when the multigraph abbreviations are restricted to predicates of degree two that they become graph abbreviations.

8. Connectivity may also be better explained as the inclusion of the appropriately degreed nonidentity of the predicate with the alternation of the predicate and its inverse.

## 2025 0903 1455

1. Having got a complete collection of the 'Harvard Classics' and a partial collection of 'Great Books of the Western World' my interest in lists of great books grew.
    - <https://en.wikipedia.org/wiki/Harvard_Classics>
    - <https://en.wikipedia.org/wiki/Great_Books_of_the_Western_World>

2. A great list of great lists is found at <http://www.greaterbooks.com/>.

3. My interest is in the verbal control of human behavior e.g. the textual and vocal responses that are shaped by verbal behavior which was emitted hundreds if not thousands of years ago.

4. Books are the tip of a verbal iceberg which is forever submerged in the murky waters of unrecorded history.

5. Completely unrelated, a link to Carnap's 1950 'Empiricism, Semantics, and Ontology': <https://www.phil.cmu.edu/projects/carnap/editorial/latex_pdf/1956-ESO.pdf>

6. The list with books from the world, not just from Europe and America, is <https://lawpark.jimdofree.com/>.
Other gems can be found thereabouts.

## 2025 0902 1648

1. In [#2025-0830-1425](#2025-0830-1425) the world, camera, and image plane coordinate frames were constructed.
All that is left, to finish setting up the camera obscura problem (of predicting where points are projected from the world through the pinhole onto the back wall), is to set up the image sensor or image receptor coordinate frame.

2. In the dark room version of the camera obscura presented first in [#2025-0828-1649](#2025-0828-1649) the back wall became what is now called the image sensor.
I've suggested multiple times now that it is better named 'image receptor' because 'sensing' is a far more elaborate repertoire than 'receiving' and receptors are the basis of our contact with the world at the ends of our peripheral nervous system.

3. Whether it's called the image sensor or image receptor, there are only a few key measurements that complete the description of the intrinsic or internal parameters of the camera.

4. The image sensor is a rectangular part of the image plane.
Each of its sides are parallel to one of the basis vectors of the image plane coordinate frame and perpendicular to the other i.e. its upper and lower sides are parallel to the floor and its left and right sides are perpendicular to the floor.

5. It has a definite width and height.
Both are measured in (meter) stick units of the image plane coordinate frame.

6. In the idealized camera obscura, the principle point (the origin of the image plane coordinate plane) is at the intersection of the diagonals of the image sensor.
In reality, finding the location of the principle point relative to the image sensor coordinate frame is part of a general problem called 'camera calibration'.

7. The origin of the image sensor coordinate frame is the upper left hand corner of the image sensor (when looking through the pinhole to the image plane).
Therefore, the origin of the image sensor is designated by '(-half of the width of the image sensor, -half of the height of the image sensor)' in the image plane coordinate frame.

8. The image sensor is made of rectangular pixels and the units of the image sensor coordinate frame are pixels.
So, when talking about lengths it can either be given in the units of the image sensor coordinate frame or in the units of the image plane coordinate frame.

9. Since the pixels are rectangular and not square, three horizontal pixels may be longer, shorter, or the same length as three vertical pixels.
Thus, lengths are most often given in (meter) stick units because those are the same units as the world, camera, and image plane coordinate frames.

10. Unlike the basis vectors of the world, camera, and image plane, the basis vectors of the image sensor coordinate frame are possibly different lengths even though they are both given in pixel units.
Thus, we differentiate between horizontal and vertical pixel units.

11. A horizontal pixel unit is the width (in the world, camera, or image plane units) of a pixel, and a vertical pixel unit is the height of a pixel.
The first basis vector points from the upper left to the upper right of a single pixel and the second basis vector points from the upper left to the lower left of a single pixel.

12. Since pixels are rectangles and not points, the pixel at the origin of the image sensor coordinate frame intersects the origin at its upper left hand corner.
While it may seem obvious that the upper left corner of the image sensor is the upper left corner of a pixel, this is not always the case, and there are other ways of picking the origin that may be helpful e.g. some times it helps to pick the center of the upper left corner pixel as the origin.

13. Converting between pixel units and (meter) stick units involves two densities: horizontal and vertical pixel density.
Horizontal pixel density is the number of horizontal pixles per meter and vertical pixel density is the number of vertical pixels per meter.

14. Thus, the principle point is designated by '(the product of the horizontal pixel density with half the width of the image sensor, the product of the vertical pixel density with half the height of the image sensor)' in the image sensor coordinate frame.  

15. Now, to solve the problem of projecting a point in the world onto the image sensor, we need to go from the world coordinate frame to the camera coordinate frame to the image plane coordinate plane to the image sensor coordinate frame!


## 2025 0901 1832

1. Continuing the timeline from [#2025-0831-1507](#2025-0831-1507) of âThe Little Book of Big Historyâ by Ian Crofton and Jeremy Black 2017.

2. 1453: Ottoman Turks control Constantinople
3. 1455: Gutenberg Bible from his printing press
4. 1492: Muslim control in Spain extinguished by Christians; Columbus in West Indies
5. 1498: Vasco da Gama of Portugal in India via South most tip of Africa
6. 1517: Protestant Reformation of Martin Luther
7. 1519-21: Spanish control Aztec culture
8. 1519-22: Magellan and Del Cano circumnavigate the world
9. 1526: Mughal controls India
10. 1532-5: Spanish control Inca
11. 1543: heliocentric theory of Copernicus
12. 1571: Ottoman control of Mediterranean ended at Lepanto by Holy League
13. 1607: English control Virginia
14. 1644: Machus set up Qing control of China
15. 1648: End of Thirty Years's War
16. 1652: Dutch set up Cape Colony of southern Africa
17. 1683: Vienna countercontrols Turks
18. 1687: Newton's of motion and universal gravitation
19. 1763: End of Seven Years's War
20. 1776: US Declaration of Independence; 'Wealth of Nations' by Adam Smith
21. 1783: Human flight by baloon
22. 1785: Steam powered cotton mill
23. 1788: British in Australia
24. 1789: French Revolution
25. 1792-1815: French Revolutionary and Napoleonic wars
26. 1803: US buy part of North America from France
27. 1808-26: Spanish control of Amercas weakens
28. 1825: passenger steam railway in England, from Stockton to Darlington
29. 1830: Greece countercontrols Ottoman Turks
30. 1833: British culture abolishes slavery
31. 1844: Morse's telegraph
32. 1848: 'Communist Manifesto' of Marx and Engles
33. 1848-9: revolutions extinguished in Europe
34. 1853: US countercontrols Japan: opens trade 
35. 1857: India countercontrols British
36. 1859: 'Origin of Species' by Darwin
37. 1861: Italian Unification
38. 1861-5: American Civil War
39. 1868: Japan strengthens
40. 1869: transcontinental railway in USA; Suez Canal
41. 1871: German Unification
42. 1876: telephone
43. 1884: Europeans control Africa at Berlin Conference
44. 1895: wireless telegraphy from Marconi
45. 1903: Wright Flyer
46. 1905: Einstien's special theory of relativity
47. 1911: Chinese revolution
48. 1914: First World War; Panama canal
49. 1917: Russian Revolution
50. 1918: End of First World War; Austrian, German, and Turkish cultures weaken
51. 1923: non-Mliky Way galaxies
52. 1928: penicillin
53. 1929: Great Depression
54. 1933: Nazi's strengthen
55. 1937: Second Sino-Japanese War
56. 1939: Second World War
57. 1943: electronic computer
58. 1945: atomic bomb; End of Second World War; United Nations; Cold War
59. 1947: Indian Independence
60. 1949: Chinese Civil War; NATO
61. 1950-3: Korean War
62: 1955-75: Vietnam War
63. 1957: Sputnik 1; European Economic Community
64. 1960: 3B people
65. 1961: Human Space Flight
66. 1967: Human heart transplant
67. 1969: Human on Moon
68. 1976: Robot on Mars
69. 1978: Human invitro fertilization
70. 1979: Smallpox extinguished
71. 1989: World Wide Web
72. 1991: Soviet Union extinguished
73. 1997: cloned mammal
74. 1999: 6B people
75. 2003: Human Genome Project
76. 2011: 7B people
77. 2012: Higgs Boson

## 2025 0831 1542

A very important ascii map

```
180 150W  120W  90W   60W   30W  000   30E   60E   90E   120E  150E 180
+90N-+-----+-----+-----+-----+----+-----+-----+-----+-----+-----+-----+
|          . _..::__:  ,-"-"._       |7       ,     _,.__             |
|  _.___ _ _<_>`!(._`.`-.    /        _._     `_ ,_/  '  '-._.---.-.__|
|.{     " " `-==,',._\{  \  / {)     / _ ">_,-' `                mt-2_|
+ \_.:--.       `._ )`^-. "'      , [_/(                       __,/-' +
|'"'     \         "    _L       oD_,--'                )     /. (|   |
|         |           ,'         _)_.\\._<> 6              _,' /  '   |
|         `.         /          [_/_'` `"(                <'}  )      |
+30N       \\    .-. )          /   `-'"..' `:._          _)  '       +
|   `        \  (  `(          /         `:\  > \  ,-^.  /' '         |
|             `._,   ""        |           \`'   \|   ?_)  {\         |
|                `=.---.       `._._       ,'     "`  |' ,- '.        |
+000               |    `-._        |     /          `:`<_|h--._      +
|                  (        >       .     | ,          `=.__.`-'\     |
|                   `.     /        |     |{|              ,-.,\     .|
|                    |   ,'          \   / `'            ,"     \     |
+30S                 |  /             |_'                |  __  /     +
|                    | |                                 '-'  `-'   \.|
|                    |/                                        "    / |
|                    \.                                            '  |
+60S                                                                  +
|                     ,/           ______._.--._ _..---.---------._   |
|    ,-----"-..?----_/ )      _,-'"             "                  (  |
|.._(                  `-----'                                      `-|
+90S-+-----+-----+-----+-----+----+-----+-----+-----+-----+-----+-----+
Map 1998 Matthew Thomas. Freely usable as long as this line is included

```

There is also a program that displays maps in the terminal:
- <https://www.uninformativ.de/>
- <https://www.linuxlinks.com/asciiworld-world-map-ascii/>

## 2025 0831 1507

1. Corrected the schematic abbreviation by the predicate functor of functionality in [#2025-0830-2137](#2025-0830-2137) whose compositional component was missing 'j'.

2. Continuing the timeline from [#2025-0829-1654](#2025-0829-1654) starting on page 99 of 'The Little Book of Big History' by Ian Crofton and Jeremy Black 2017.


3. 331 BCE: Alexander the Great of Macedon strengthens Hellenistic culture by controlling from the Adriatic Sea to the Indus River e.g. introducing Roman practcies into Achamenid culture of Persia
4. 321-185 BCE: Mauryan culture of India
5. 300 BCE: Library of Alexandria
6. 221 BCE: Qin then Han culture unites China
7. 212 BCE: Edict of Caracalla
8. 100 BCE: Rome is the biggest city
9. 100 BCE - 750 CE: Teotihuacan evolves and dies as the once biggest city of the mericas
10. 300-1200 CE: Ghana culture in present day Naurtania and Mali
11. 410 CE: Roman culture extinguished by Visigoths no later than 476 CE
12. 661-750 (CE) : Umayyad caliphate culture controls largest area including that of present day Georgia, Uzbekistan, Pakistan, Arabian penisula, North Africa, Spain, and Portugal
13. 1055: Seljuk Turks control Baghdad
14. c. 1200: Incas in Andean valley of Peru
15. 1200-1400: Mississippian culture of North America is strongest
16. 1206: Qutb al-Din Aibak is first controller of Delhi Sultante
17. 1211: Mongols start controlling Eurasia and northern China
18. 1368: Ming dynasty established in China
19. 1393: Timur (aka Tamerlane) sacks Baghdad.
20. 1405: Beginning of Zheng He's voyages in Indian Ocean
21. 1438: Beginning of period of Inca conquests

## 2025 0830 2137

1. A reminder that the abbreviations of logic (and their schematic descendants) do little more than set the stage for validities (e.g. 'coex and F F F') and that validities do little more than set the stage for theories.

2. It occurred to me that philosophy as a science can be defined as that which seperates theory from logic e.g. set theory from its logic.
This would explain the back and forth between philosophers, logicians, and theoraticians (and experimentalists).

3. Having completed in [#2025-0830-1409](#2025-0830-1409) the schematic graph abbreviations of logic I turned forward to the order abbreviations.
But, before that, I looked back at what I had done thus far and found further simplifying improvements.

4. Going back to the schematic compositional abbreviations of logc set out in [#2025-0828-2145](#2025-0828-2145) a simplification can be introduced by moving from 'resultant' to an intermediate predicate functor 'composition'.
I'm unsettled on the phrase 'composition', but accept that there is room for a definition of its sort in the methods of logic as I give them (that is with a stack based outlook).

5. The definition of resultants does not change.
It is still 

    - âres j k m F Gâ for âsome j 0 (j bury j+k) and (k drop) F (m prop) Gâ.

    (I had to correct this when looking back at it: it had 'each' where it needed 'some')
    It is very unfamiliar for those not already steeped in stack based methods of programming e.g. be they in a high level language like Forth or a low level language like x86 assembly.

6. To help explain how resultants work in a stack based world for those not already familiar with them, I'll skip to unraveling the resultant:

    1. res j k m F G
    2. some j 0 (j bury j+k) and (k drop) F (m prop) G  (abbreviation)
    3. (j+0 some) (0 pop) (j bury j+k) and (k drop) F (m prop) G (abbreviation)
    4. (..u: some ..v is such that ..u..v( (0 pop) (j bury j+k) and (k drop F) (m prop) G )..x..y :..x..y) (abbreviation)
    5. (..u..w: some ..v is such that ..u..w..v( (j bury j+k) and (k drop F) (m prop) G) ..x..y : ..x..y) (abbreviation)
    6. (..u..w: some ..v is such that ..u..v..w( and (k drop) F (m prop) G )..x..y : ..x..y) (abbreviation)
    7. (..u..w: some ..v is such that ..u..v..w( (k drop) F )..x..y and ..u..v..w( (m prop) G)..x..y :..x..y) (abbreviation)
    8. (..u..w: some ..v is such that ..u..vF..x..y and ..u..v..wG..y :..x..y)

    (I still have to do this for most of the other schematic abbreviations).

7. So, âres j k m F Gâ for âsome j 0 (j bury j+k) and (k drop) F (m prop) Gâ i.e. '(..u..w: some ..v is such that ..u..vF..x..y and ..u..v..wG..y :..x..y)'.

8. This is familiar to Forth programmers in the following way:
    1. an instance of the part '..u..vF..x..y' is "1 2 3 STEP 'plus' 'times'" where 'STEP' is the predicate of a single step in the execution of a stack based language
    2. everything to the left of 'STEP' is 'on the stack' as Forth programmers say, and everything to the right is on the return stack, which, in this special case, is the execution stack i.e. the list of operations that are going to be performed (this is a deep topic and I will only stay in the shallow end of the pool e.g. the 'instruction register' is usually where 'machine codes' are listed and where one state of the machine is a step from an other)
    3. in "1 2 3 STEP 'plus' 'times'" it is seen that the predicate '(uvw: uvwPLUS :)', which is true of ((a,b,c),()) if and only if c is the addition of b to a, implies (along with the standard premises of the arithmetic of natural numbers) '1 2 3 PLUS' since three sums two with one. 
    4. now, an instance of  the part '..u..v..wG..y' is "1 2 3 6 STEP 'times'" where 'TIMES' is true of ((a,b,c),()) if and only if a multiplies b with c
    5. it is standard form when programming Forth to 'pass parameters' on the stack between the programs mentioned on the return stack i.e. "1 2 3 6 STEP 'times'" is true since '2 3 6 TIMES' is true (i.e. six multiplies three with two)
    6. now, the result of 'STEP' with 'STEP' is two steps of execution of a stack machine i.e. the following are equivalent

        1. 1 2 6 (res 3 3 1 STEP STEP) 'plus' 'times'
        2. 1 2 6 (uvw: some a is such that uvaSTEPxy and uvawSTEPy :xy) 'plus' 'times' (abbreviation)
        3. some a is such that 1 2 a STEP 'plus' 'times' and 1 2 a 6 STEP 'times' (concretion)

        the last of which follows from "1 2 3 STEP 'plus' 'times'" and "1 2 3 6 STEP 'times'".

    7. Thus, the resultant, as defined, makes it easy to analyze about what is classically called 'the composition of concatenative programs'

9. Thus the definition of resultant of a pair of predicates enforces a method of program composition at a very early stage without ever mentioning such complex theories.

10. The drawback is but momentary: the definition of resultant doesn't appear to have the features familiar to those who were erroneously trained as mathematicians or computer scientists to deal with the composition of two place relations rather than predicates and rather than predicates regimented on the expedience of stack notation.

11. The intermediate definition of composition is introduced now (it was already hidden in the iterate abbreviations):

    - 'compose i j F G' for 'res i j 0 F (i nip j) G'
    - 'compose i F G' for 'compose i i F G'


12. I still refuse to give any abbreviation what so ever for truth functional and quantificational schema like

    - some y is such that zFy and yGx

    or, better, their predicate abstract logic analogue

    - (z: some y is such that zFy and yGx :x)

    which is closest to Godel 1940 and Peano 1911 which are to be greatly preferred, as Quine strongly suggests on pages 24 and 25 of 'Set Theory and its Logic Revised Edition', to modern methods (which were the methods by which I was originally instructed when I was taught a strictly deductive mathematical logic).

13. With the predicate functor 'compose' in hand, the abbreviation of the predicate functor of iteration is much shorter (and more familiar):

    - 'iter 2+n (j+1)\*2 F' for 'compose j+1 F iter 1+n (j+1)\*2 F'

14. The predicate functor 'compose' also comes in handy when giving better abbreviations for the predicate functor of functionality:

    - 'func i j F' for 'included compose i j F (i ccw i+j) F id j\*2'

15. Note this new abbreviation brings 'func' into harmony with the other three place predicate functors 'inject', 'surject', 'biject', and 'corr'.
This is a grand improvement over the abbreviations in [#2025-0828-2145](#2025-0828-2145).

16. Oh, it seems there are also abbreviations for 'cw' and 'ccw' that I have yet to include:

    - 'cw i F' for 'cw i 0 F'
    - 'ccw i F' for 'ccw i 0 F'

    There are perhaps others of similar familial resemblence which I have accidentally omitted: this is a large part of why I distinguish between notes and memos!
    Memos set a high bar.

17. In preperation for the schematic order abbreviations of logic:
    
    - Connectivities: 'conn i\*2 F' for 'included field F i\*2 or or F iter -1 i\*2 F id i\*2'

18. I do not yet know if 'antisym' shoudl be put as an order abbreviation or a graph abbreviation.
It is likely that only by putting everything together anew that I shall sort through smoother steps.
I am frightened to think of what further consideration of schematic validities might give way to in future evolutions! 


## 2025 0830 1425

1. Having set up the world and camera coordinate frames in [#2025-0829-1227](#2025-0829-1227) there remain two coordinate frames to set up (those of the image plane and the image sensor) before solving the problem of calculating the point projected through a pinhole from the world onto the sensor, better the receptor, can be solved.

2. The third basis vector of the camera coordinate frame points into the dark room or box of our camera obscura.
All the work put into setting up the world and camera coordinate frames is collected under the phrase 'extrinsic parameters' since they deal with stuff that is outside the dark room.

3. Even though I didn't overtly say that the points of the world to be projected through the pinhole are to be outside the room, I've tacitly made descriptions of the projected points as being exterior when I spoke of the trunk of a tree being projected upside down onto the back wall of the dark room.
Now that I've overtly required that the points to be projected be outside the room, that can be added to the list of things that fall under the premises of the forward model of pinhole cameras.

4. The remaining parameters to be explained are called 'intrinsic parameters' becuase they deal with the interior of the camera obscura.

5. Just as the first measurement in the world coordinate frame was the location of the origin of the camera coordinate frame with respect to the origin of the world coordinate frame, so too is the first measurement in the camera coordinate frame the origin of the *image plane coordinate frame* the first measurement in the camera coordinate frame.

6. The line along the third basis vector of the camera coordinate frame (the one whose tail is at the pinhole and that points into the dark room and is perpendicular to the window with the pinhole in it) is called the optical axis.

7. The optical axis intersects with the back wall of the dark room.
The back wall of the dark room is called the image plane because it is the two dimensional surface on which the points projected by the pinhole are imaged.

8. The point where the optical axis intersects with the image plane is called 'the principle point' (of the camera).

9. In a standard camera (or in a standard room) the back wall is perpendicular to the wall with the pinhole in it so that the optical axis is perpendicular not only to the image plane of the back wall but also to the window with the pinhole in it.
Complex cameras can include elaborate mirrors (and even more elaborate lenses) that bounce projected points that would otherwise hit the back wall towards some other surface: nothing like that is contemplated here.

10. The distance from the pinhole to the principle point is called the 'effective focal length' of the camera, or just the focal length.

11. Since the third basis vector of the camera coordinate frame points right at the principle point from the pinhole, then the coordinate of the principle point in the camera coordinate frame is '(0, 0, the focal length)'.

12. The principle point is *the origin of the image plane coordinate frame*.

13. The image plane coordinate frame has only two basis vectors: only two measurements are needed to locate any point on the back wall with respect to the principle point i.e. with respect to the origin of the image plane coordinate frame.

14. Just as some basic conventions constrained the way the camera coordinate frame was set up, there are basic conventions when setting up the image plane coordinate frame that keep calculations nice and tidy.

15. The first and second basis vectors of the image plane coordinate frame are parallel to the first and second basis vectors of the camera coordinate frame.
So, there are no more than two ways each of them can point i.e. they can point in the same direction as the bases vectors of the camera coordinate frame or in the opposite direction of the bases vectors of the camera cordiante frame.

16. Because of how image sensors work, and because of how images are drawn on computer screens (a pixel at a time), the bases vectors of the image plane coordinate frame are made to point in the opposite directions of those of the camera coordinate frame.

17. So, since the second basis vector of the camera coordinate frame points up, then the second of the image plane coordinate frame points down, and since, when looking through the pinhole to the back wall, the first basis vector of the camera coordinate plane points left, then the first of the image plane coordinate frame points right (when peeping through the pinhole to the back wall).

18. To recap:
    1. The origin of the world coordinate frame is some point (usually in the exterior of the camera) and its basis vectors are set up by picking up a stick and putting one end at the origin, its tail, and letting its tip select the first basis vector.
Then, turn the tip of the stick through a right angle (any way will do) while keeping the tail at the origin to get the second basis vector.
The third basis vector must be perpendicular to the first two: there are two ways this can be done, clockwise or counterclockwise.
By convention, the counterclockwise orientation is picked and we check that we did it right by looking through the circle formed by the tips of the bases vectors to the origin of the world coordinate frame and seeing that going from the first to the second to the third basis vector takes us counterclockwise around the circle.
    2. The world coordinates of the pinhole of the camera pick out the origin of the camera coordinate frame.
Putting the tail of the stick at the pinhole and pointing it into the dark room so that it is perpendicular to the wall with the pinhole in it gives the third basis vector of the camera coordinate frame.
The line that goes through the pinhole in the direction of the third basis vector is called the optical axis.
Rotate the tip of the stick so that it is perpendicular to the third basis vector and the ground to get the second "up" basis vector of the camera coordinate frame.
The first basis vector is got by the same counterclockwise convention from setting up the world coordinate frame.
    3. The optical axis is perpendicular to the front and back wall and intersects the back wall at the principle point.
The distance from the pinhole to the principle point is called the (effective) focal length.
The principle point is designated by '(0, 0, the focal length)' in the camera coordinate frame.
The origin of the image plane coordinate frame is the principle point, the first basis vector points in the opposite direction of the first basis vector of the camera coordinate frame, and the same for the second basis vector.



## 2025 0830 1409

1. With the schematic compositional and functional abbreviations completed in [#2025-0828-2145](#2025-0828-2145) I move on to the schematic graph abbreviations of logic.

2. But, first, it occurred to me that I forgot to put two fundamental abbreviations in the sameness section:

    1. Item: 'item' for 'dup id'
    2. Void: 'void' for 'not item'

    Though they may seem silly in their brief insignificance, they complete a wide range of validities that are often misinterpreted as algebraic identites.

3. Symmetries: 'sym i\*2 F' for 'included F iter -1 i\*2 F'
4. Nonsymmetries: 'nonsym i F' for 'not sym i F'
5. Asymmetries: 'asym i F' for 'included F not iter -1 i\*2 F'
6. Antisymmetric: 'antisym i\*2 F' for 'included and F F iter -1 i\*2 id i\*2'
7. Transitivities: 'tran i\*2 F' for 'included iter 2 i\*2 F F'
8. Nontransitivities: 'nontran i F' for 'not tran i F'
9. Intransitivities: 'intran i F' for 'included iter 2 \i*2 F not F'
10. Reflexvities: 'refl i\*2 F' for 'included iter 0 i\*2 F F'
11. Nonreflexivities: 'nonrefl i F' for 'not refl i F'
12. Irreflexivities: 'irrefl i\*2 F' for 'included F nid i\*2'
13. Total Reflexivities: 'trefl i\*2 F' for 'included id 2\*i F'

## 2025 0829 1654

Notes on 'The Little Book of Big History' by Ian Crofton and Jeremy Black 2017

1. 'The Little Book of Big History' is broken into six parts:
    1. Setting The Scene
    2. Animal Planet
    3. Humans Start to Dominate
    4. Civilization
    5. The Rise of the West
    6. The Modern World

2. Timeline:
    1. 13.8B years ago: the big bang
    2. 4.6B years ago: the sun, Earth, and solar system form
    3. 4.5B years ago: the moon appears (collision between Earth and Mars size planet?)
    4. 4.2B years ago: oceans formed?
    5. 4.1-3.8B years ago: astroids pepper inner planets including Earth
    6. 4B years ago: appearence of rocks found to this day (proto-lifeforms e.g. metabolism or replicators?)
    7. 3.7B years ago: organic molecules and proto-bacteria?
    8. 3.4B years ago: photosynthesizing Cyanobacteria (blue-green algae)
    9. 2.45B years ago: oxygen in atmosphere from photosynthesis
    10. 600M years ago: multicellular organisms
    11. 542-488M years ago: Cambrian period e.g. external skeletons (trilobites and brachiopods), vertebrates (notochord as proto-spinal column)
    12. 488-444M years ago: Ordovician period e.g. diversity of trilobites, lamp shells, gastropods, and graptolites; appearence of sea urchins, starfishes, and ammonites; later, land plants and mass extinctions
    13. 444-416M years ago: Silurian period e.g. marine life (e.g. cartilaginuous, latterly boney); invertebrates on land (e.g. scorpions andwingless insects); vascular plants (e.g. club mosses)
    14. 416-359M years ago: Devonian period e.g. big coral reefs, ferns, early amphibians, four-legged animals, spread of land animals
    15. 359-299M years ago: Carboniferous period e.g. flying insects, reptiles, land plants (e.g. conifers) spread (origin of modern coal deposits)
    16. 299-251M years ago: Permian period e.g. diverse reptiles, mass extinction of many marine and land animals (e.g. trilobytes)
    17. 251-200M years ago: Triassic period e.g. dinosaurs and small mammals
    18. 200M-145M years ago: Jurassic period e.g. diverse dinosaurs, turltes, crocodiles, tropical forests, and birds
    19. 145M-66M years ago: Creteous period e.g. flower plants, grasses, extinction of dinosaurs, survival of birds and mammals
    20. 66-56M years ago: Palaeoscene epoch e.g. diverse mammals, primates
    21. 56-34M years ago: Eocene epoch e.g. mammls spread (e.g. elephants, whales, rodents, carnivores and hoofed)
    22. 34-23M years ago: Oligocene epoch e.g. grasslands, monkeys
    23. 23-5.3M years ago: Miocene epoch e.g. horses, apes, frogs, snakes, rats
    24. 7M years ago: common ancestor splits into ancestors of humans, chimps, and bonobos
    25. 6M years ago: some early humans walk on hind legs
    26. 5.3-2.6M years ago: Plioscene epoch e.g. mammoths, early humans walk upright
    27. 2.6M years ago: human tool use
    28. 2.6M-11.7k years ago: Pleistocene epoch e.g. ice ages and warmer interglacial periods
    29. 2.4M years ago: *Homo habilis*
    30. 1.9M-143k years ago: *Homo erectus* spreads
    31. 200k years ago: *Homo sapien* in Africa
    32. 150-50k years ago: language?
    33  100k years ago: *Homo sapien* out of Africa, burials and graves
    34. 75k years ago: pierced shell necklaces
    35. 45k years ago: "fully modern" humans in Europe ???
    36. 42k years ago: wood and bone flutes in Europe
    37. 40-35k years ago: stone and ivory carved into human-animal shapes in Europe
    38. 38-35k years ago: cave art
    39. 22k years ago: peak of latest ice age
    40. 19k years ago: foraging wild cereal in Middle East
    41. 14k years ago: domestocation of dogs from wolves, grindstones in Middle East
    42. 13k years ago: engraved antler in Longyn Cave of China
    43. 12k years ago: European glaciers retreat
    44. 11.7k-present: Holoscene epoch e.g. end of last ice age, extinction of many large land animals (by humans?), humans spread far and wide
    45. 8k years ago: cultivation of wheat and barley from Middle East to Nile valley
    46. 7k years ago: villages of hunters and fishers in Yangtze river delta of China cultivate rice, villages of western Europe farm cereal
    47. 5.5-4k BCE: Sumerian culture in Mesopotamia (now Iraq)    
    48.  4.5k years ago: long-distance trade in South America
    49. 3650-1400 BCE: Minoan and early Aegean cultures
    50. 3100 BCE: Upper and Lower Egypt united by first pharaoh
    51. 2600-1900 BCE: strong Indus valley cultures
    52. 2580-2560 BCE: Great Pyramid of Giza
    53. 2070 BCE: Xia dynasty of China
    54. 2000 BCE: preclassic Mayan culture in Mesoamerica
    55. 1754 BCE: Code of Hammurabi of Babylonian empire
    56. 1650 BCE: Hittite kingdom of Turkie
    57. 1600 BCE: Shang dynasty of middle valley of Yellow River of China
    58. 1600-1500 BCE: Olmec culture in present day Mexico
    59. 1550-1077 BCE: New Empire of Egypt controls Levant to Nubia
    60. 1500-800 BCE: Vedic Age e.g. Hindu scripture of India
    61. 1070 BCE: Kush Kingdom in present day Sudan
    62. 1000 BCE: Strong Phoenician cultures, e.g., of Tyre and Sidon
    63. 911-612 BCE: Neo-Assyrian culture of Tigris valley
    64. 900-200 BCE: Chavin culture of present day Peru
    65. 800-400 BCE: D'mt Kingdom of Ethiopia
    66. c. 550 BCE: Cyrus the Great's Achaemenid empire of Persia
    67. 510-323 BCE: Classical Ancient Greece
    68. 509 BCE: Roman Republic
    69. ended on page 99

## 2025 0829 1423

Notes on 'Geography' by Crash Course.

1. When organizing my reading, writing, and thinking on my history of the world, I have found geography to be extremely helpful e.g. its divisions of the planet into the litho-, hydro-, bio-, and atomo- spheres.
It is funny, in a good way, that the division of the world, e.g. by Aristotle, into earth, water, air, and fire lives a ghostly life through this broad organizational scheme of geography.

2. I do not mean to imply that the division of the world into its four spheres is the one that I shall settle on.
For me, the world is still to be organized around its contingencies where a contingency is a consequence from a response on an occasion e.g. a door opens from a push on a lever, a heart beats faster from a kiss on a cheek, and mountains rise from shifts on tectonic plates.

3. While my aim is on organizing the contingencies of the world around the methods of logic, I must start with what is already available to me e.g. the delightful presentations from The Crash Course YouTube Playlist on Geography:
    - <https://www.youtube.com/playlist?list=PL8dPuuaLjXtO85Sl24rSiVQ93q7vcntNF>

4. The first four vides in the playlist are a gentle preamble to geography and paint a big picture of its origins and aims.
My notes start on the fourth video in the series.
    - <https://youtu.be/vlVVaZhRAEA?si=dHlhI--5QFOl4hdy>

    I'm disinclined to fixate on the 'space', 'place', etc. principles of geography because they mismanage the social contingencies which are better handled on the side of the science of behavior (though, perhaps this conclusion will be contravened by contemplations recorded here).

4. The methods of geography are divided into two nonexclusive and nonexhaustive parts: physical and human.
Human geography discovers, predicts, and, not so often, controls changes in social behaviors as a result of changes in the litho, hydro, bio, and atmo spheres.
Physical geography uncovers the chnages above adn below the surface of the earth.
    > "Physical geograph is all about recognizing the characteristics of teh environment and the processes that create, modify, and destroy those environments" (1:35 'What is physical geography? Crash Course Geography #4')
    >

5. Geo-ecosphere is the collection of all systems of the planet (Earth).
It is divided into four subsystems:
    1. Atmosphere: clouds, weather, ozone layer, air we breath
    2. Hydrosphere: water underground, oceans, lakes, ice caps, water in plants and animals, water molecules in the atmosphere
    3. Lithosphere: outermost layer of earth, continents, line the bottom of the oceans
    4. Biosphere: where life can occur

6. Ecosystems are composed of the interconnected systems of the atmo, hydro, litho, and bio spheres.

7. What causes these systemst to interact? The sun and its insolation (incoming solar radiation).

8. Topography: shape of the land.

9. Geomorphology: the origin and evolution of the shape of the Earth's surface e.g. weathering, glaciers, streams.

10. Pedology: study of soil types and how they form.

11. Hydrology: how water is moved managed and distributed above and below the Earth's surface.

12. Climatology: study of atmospheric weather patterns over time

13. Oceanography: study of the past present and future features of the oceans

14. Meteorology: study of atmospheric processes and phenomena

15. Biogeography: distribution of plants and animals (fauna and flora) in an area

16. Ecologists study the biosphere: what physically happens as different species interact across the landscape

17. When geographers study the biosphere they focus on how ecological processes are distributed across space, and how species move and change over time.
They focus on spatial patterns in the landscapes.

18. Notes from episode 5 'How does the Earth move?'
    - <https://youtu.be/ljjLV-5Sa98?si=5DgM6W2NT8DO6Kc4>

19. 13.7B years ago the universe began!

20. 4.5B years ago in the Milkyway Galaxy a solar nebula collapsed under its own gravity: most of its mass went to the center as what we now call the sun, and the rest, a very tiny amount, spun out and formed the rest of the solar system.

21. That tiny amount spun out the Earth and other planets: they rotate counterclockwise because they are all gravitationally condensed parts of that tiny bit of the collapsed solar nebula that made our sun.

22. As a consequence of the solar nebula collapse, the earth rotates around the sun and rotates around a central axis which passes through the north and south poles.
A full rotation occurs once about every 24 hours.

23. At the equator the Earth spins at 1.6k kilometers per hour (km/h) which is about 13 times faster than a Cheetah can run at top speed.

24. In St. Petersburg, at 60 degrees North latitude, the spin speed is half that at the equator i.e. about 800 km/h.

25. At the poles, it is 0 km/h.

26. The sun moves in the sky because of the relative motion of an observer on the surface of the Earth.
From the moon it is easy to see the Earth rotating because the moon rotates on its axis at the exact same rate it orbits Earth i.e. on Earth we always see the same side of the moon.

27. The Circle of Illumination: the dividing line that seperates the half of Earth that's currently receiving light and solar energy from the other half that's in total darkness.

28. Earth wobbles: Milankovitch cycles select Earth's climate
    1. on a 26k year cycle called precession where the central axis wobbles predictably like a spinningtop
    2. melting ice changes the distribution of water in the sea, also changing distribution of land mass, and causing unpredicted changes in the wobble
    3. redistribution of ground water (because of human use) at 45 degrees North and South latitude amplifies unpredicted wobble because of spherical harmonics

29. The path of the Earth around the sun is called an orbit: a revolution (traveling from one point on the orbit and back to it) takes about 365 days.

30. The orbit is eliptical:
    1. Perihelion: where on the orbit the Earth is closest to the sun, January, about 147M km from the sun
    2. Aphelion: where on the orbit the Earth is farthest from the sun, July, 152M km from the sun
    3. Our distance from the sun does not change enough in our orbit to play a functional part in our climate

31. Plane of the Ecliptic: the imaginary plane that contains Earth's orbit

32. The Earth is tilted 23.5 degrees from a line perpendicular to the plane of the ecliptic through the center of the Earth (probably happend when something hit Earth billions of years ago).

33. Insolation: incoming solar radiation. It's how the different Earth systems.

34. How the progression of seasons works:
    1. December Solstice (December 21): 
        1. the south pole is tilted towards the sun so that everything south of 66.5 degrees South Latitude (the Antarctic Circle) receives 24 hours of daylight.
        2. At noon, sunrays are perpendicular to the surface of the Earth at 23.5 degrees south (the Tropic of Capricorn).
        3. The 66.5 degrees North latitude (the Artic Circle) receives 24 hours of darkness.
        4. The Northern Hemisphere is in winter and the Southern Hemisphere is in summer.
    2. March Equinox (near March 21st):
        1. Earth's axis isn't tilted toward or away from the sun.
        2. The circle of illumination passes through both the North and South poles: each place on Earth has equal hours of day and night.
        3. Spring in the Northern Hemispher and Fall in the Southern Hemisphere.
    3. June Solstice (around June 21st):
        1. Earth's axis tilts towards the sun.
        2. Opposite conditions from December solstice: Artic Circle gets 24 hours daylight, Antartic Circle gets 24 hours darkness.
        3. At noon, surays are perpendicular to the surface of the Earth at 23.5 degrees North (the Tropic of Cancer).
    4. September Equinox (near September 21st):
        1. Everything is like the March Equinox except its fall in the Northern Hemisphere and Spring in the southern Hemisphere.

35. Obviously, the seasons select the behavior of organisms e.g. what food is grown where, what organisms survive where and when, etc.



## 2025 0829 1227

1. In note [#2025-0828-1649](#2025-0828-1649) we set up the world coordinate frame with a stick we found next to the tree whose upside down projection first presented the problem of modeling a pinhole camera.
With the counterclockwise bases vectors we were able to measure where points are in the world coordinate frame.

2. The task is still to calculate the point projected through the pinhole in the window from the tree onto the back wall of the dark room.

3. The first point measured from the origin of the world is the location of the pinhole i.e. the world coordinates of the camera.

4. The world coordinates of the camera are also the world coordinates of the origin of the camera coordinate frame!
So, all together there are two origins: the origin of the world coordinate frame and the origin of the camera coordinate frame.

5. The origin of the camera coordinate frame also comes with three basis vectors.
Unlike with the bases vectors of the world coordinate frame, the bases vectors of the camera coordinate frame come with some important constraints i.e. we can't just pick up a stick and set up a counterclockwise coordinate frame by turning it at right angles around the origin willy nilly.

6. The most important basis vector of the camera coordinate frame is the third one: it points perpendicular to the window with the pinhole in it and it points *into* the dark room.
The line through the third basis vector of the camera coordinate frame is called 'the optical axis'.

7. The second most important basis vector is, surprise, the second one.
It's often called 'the up vector' because when you're using a camera without twisting or turning it in interesting, and perhaps even artistic ways, it is perpendicular to the optical axis and perpendicular to the ground.
I've also seen it called 'the top vector' because it is perpendicular to the top plate of a camera.

8. The least important basis vector is the first and it is set up by requiring that the camera coordinate frame be a counterclockwise one i.e. it is perpendicular to the third basis vector (the optical one) and the top or up vector and if you go from the first to the second and then the third basis vector it makes a counterclockwise path along the cricle through their tips when you look through that circle at the origin.

9. To recap:
    1. The world coordinate frame is set up by picking a world origin point and grabbing a stick and putting one end at the origin and letting the other end be the tip of the first basis vector.
The next basis vector is got by turning the tip of the stick through a right angle.
The last is perpendicular to both, but since there are two ways to do that, we pick one, the counterclockwise way, which we check by drawing a circle through the tips of the bases vectors and looking at the origin through the circle to see if going from the first to the second to the third basis vector takes us around the circle clockwise or counterclockwise.
    2. The camera coordinate frame is set up by measuring where the pinhole of the camera is in the world coordinate frame and picking that point as the camera origin.
The third basis vector is made by putting one end of the stick at the camera origin and pointing the other end so that the stick is perpendicular to the window with the pinhole in it and points intside the dark room.
The line through the third basis vector is called the optical axis.
The second basis vector is perpendicular to the third and perpendicular to the ground: it's called the up or top vector.
The first basis vector of the camera coordinate frame is perpendicular to the other two and oriented counterclockwise.

## 2025 0828 2145

1. With the schematic sameness abbreviations of logic written out in [#2025-0826-1442](#2025-0826-1442), I move onto the schematic compositional abbreviations.

2. Projections: 'proj 1+i m k F' for 'each i m (k cw i+1 m) F'
3. Fields:
    1. 'field 1+i m 0 F' for 'proj 1+i m 1 F'
    2. 'field 1+i m 1+k F' for 'or proj 1+i m 1+k F field 1+i m k F'
    3. 'field 1+i m F' for 'field 1+i m 1+i+m F'
    4. 'field F' for 'field 1+i m F' where 'F' is substituted for an i m place predicate or schema
    5. 'field F 0 1+n' for '(n drop) field F'
    6. 'field F 1+k 1+n' for 'and field F 0 1+n cw 1+n 0 field F k 1+n'
    7. 'field F 1+n' for 'field F 1+n 1+n'
4. Resultants: 'res j k m F G' for 'some j 0 (j bury j+k) and (k drop) F (m prop) G'
5. Iterates:
    1. 'iter 0 (j+1)\*2 F' for 'and id (i+1)\*2 field F (i+1)\*2'
    2. 'iter 1 (j+1)\*2 F' for 'F'
    3. 'iter 2+n (j+1)\*2 F' for 'res j+1 j+1 0 F (j+1 nip (j+1)\*2) iter 1+n (j+1)\*2 F'
    4. 'iter -n (j+1)\*2 F' for 'iter n (j+1)\*2 (j+1 bury (1+j)\*2) F'

6. Now for the functional abbreviations.

7. Functionalities: 'func i F' for 'included id i\*2 and (i drop) F (i nip i) F'
8. Totalities: 'total i F' for 'Each some i 0 F'
9. Partialities: 'partial i F' for 'not total i F'
10. Injectivities: 'inject i j F' for 'included id i\*2 and (i nip j) F (i nip i+j) F'
11. Surjectivities: 'surject i j F' for 'Each some i 0 (j cw i+j 0) F'
12. Bijectivities: 'biject i j F' for 'and inject i j F surject i j F'
13. Correlations: 'corr i j F' for 'and func i F inject i j F'


## 2025 0828 1649

Work on a memo on how to program your own virtual worlds from scratch.

1. Light bounces off a tree and passes through a tiny pinhole in a very dark room and is projected onto the back wall.
Surprisingly, the image of the tree on the wall is upside down.

2. You can try this yourself:
    1. on a bright and sunny day
    2. cover up the windows in a room
    3. make a very tiny hole for light to come through
    4. turn off the lights and close the door
    5. wait for your eyes to adjust to the low light
    6. look on the walls for upside down images of the world outside your room
    7. if your walls are darker colors you can put a piece of white poster board (the less glossy the better) on the wall across from the window.

3. If someone covers up the pinhole and carefully measures, e.g. in meters, where a point on the tree is relative to the hole in the window, can you mark on the back wall exactly where that point will be projected when the pinhole is uncovered?

3. Setting up and solving this problem produces the forward imaging model of a pinhole camera: how to go from a point in the world to a point on an image sensor, e.g. the back wall of the dark room.

4. When you try to go from a point on the image sensor, e.g. the back wall, to a point in the world you get the backward imaging model of a pinhole camera. Interestingly, at least two *camera obscura* (the fancy word for 'pinhole camera') are needed to calculate the location of a point in the world!

5. Technically, the forward imaging model transforms the world coordinates of a point into the image sensor coordinates of its projected point.

6. I'll do my best to explain everything as concretely as possible and first walk through each of the steps that lead from the simple camera obscura experiment to the simple geometry and algebra that goes into drawing virtual worlds on a screen and controlling them with your mouse and keyboard.

7. This little project is one of the better ways to see how the methods of linear algebra solve all sorts of practical problems using only a handfull of vector and matrix methods.
But, I'm not going to focus on the linear algebra at all, because it isn't actually needed to navigate the steps that make up the forward imaging model of a pinhole camera.

8. In fact, the forward imaging model is not even linear at first!
It can be turned into a linear problem by the beautifully general methods of homogenous coordinates. 
No need to worry about that now though.

9. For those who aren't scared off by seeing technical terms they do not yet know feel free to read any sentence that starts with 'technically'.
Otherwise, feel free to skip over any such sentence.

10. Technically, the forward imaging model can be made linear with homogeneous coordinates and reduces a four step calculation into a single 'projection' matrix operation.
The projection matrix can be factored into two parts:
    1. Intrinsic Matrix: internal parameters (pixel densities, principle point, and focal length) that give the geometry of the camera
    2. Extrinsic Matrix: external parameters (position and orientation) that place the camera in the world.

11. The first step in setting up the pinhole camera problem is to pick a point in the world and call it the *origin*.
More than one origin turns up as we go along, so this one will be called 'the world origin' or 'the origin of the world', or, technically, *the origin of the world coordinate frame*.

12. The world coordinate frame is a fancy way of mentioning how we're going to measure things from the origin (of the world!) e.g. how we're going to figure out where a point on the tree is at in the world with respect to the origin.

13. Pick up a stick and put one end at the origin and the other end of that stick now becomes the end point of our first *basis vector*.
Vectors are no more complicated than that: one point is their tip and the other point is their tail.

14. What makes the origin and the other end of the stick into a *basis* vector is that we're going to use the length of the stick as our basic or basis unit.
Everything else we do will be in basic stick units.

15. If we happen to use a meter stick, then other people with a meter stick can more easily recreate all the measurements we'll need to make.
There is nothing more special to a meter than that. 

16. Technically, the meter is special in that it is convenient when making physical calculations.
Even more technically, there are stacks of papers on how we arrived at the standard units of measurement used throughout the sciences and you can read more by searching for them.

17. The line that goes through the first basis vector of the world coordinate frame is called 'the x-axis', or, more precisely, 'the world x-axis', or, technically, 'the x-axis of the world coordinate frame'.

18. The word 'coordinate' in the phrase 'the world coordinate frame' refers to the triple of numbers that we'll use to write down our measurements once we've set up our world coordinate frame e.g. '(2,3,4)' designates a triple whose first coordinate tells us to go two stick units in the direction of the first basis vector (and along the x-axis) as the first step in finding the point with coordinate (2,3,4) in the world coordinate frame.

19. By keeping the tail of our stick at the origin of the world and rotating it at a right angle from the first basis vector, we get the point at the tip of the second basis vector.
The line that goes through the second basis vector is called the 'y-axis of the world coordinate frame'.

20. Constructing a right angle is one of those delightful things that you learn when doing geometry with straight edge and compass.
A great place to pick up a lot of the tricks of the trade from geometry is by watching the wonderfully instructive YouTube videos of Chris from ClickSpring.
    - <https://www.youtube.com/channel/UCworsKCR-Sx6R6-BnIjS2MA>

    Especially the series "Antikythera Fragments - Ancient Tool Technology"
    - <https://youtube.com/playlist?list=PLZioPDnFPNsGnUXuZScwn6Ackf6LGILCa&si=LR38jL7H_KMwqHFK>

    But, if you just want to see how geometric constructions work, then its best to look for YouTube videos on drafting techniques or those on geometric constructions outright e.g.
    - <https://youtube.com/playlist?list=PLXSXBztZD71NulqG4PDC1BsYCpR0Us9IN&si=zg8ksBlhWKDCiZuU>

21. To get the third basis vector we move the stick so that it is perpendicular to the first and second basis vector.
Sadly, there are two ways that you can do this!
One is called the clockwise way and the other is called the counterclockwise way.

22. To figure out whether the third basis vector makes a clockwise or counterclockwise coordinate frame you have to draw a circle through the three points at the tips of each basis vector and then look at the origin through the circle and see if when you go from the first to the second to the third basis vector you go clockwise or counterclockwise around the circle.

23. We want a counterclockwise frame.
It is most often known as a 'right handed frame' becuase if you put out your thumb, pointer finger, and middle finger on your right hand as if you were making them into the first, second, and third basis vector then you make a counterclockwise path around the points at their tips.

24. The three triples '(1,0,0)', '(0,1,0)', and '(0,0,1)' designate the tips of the first, second, and third basis vector.
Any point in the world can be reached, in only *one* way, by going out along the first axis, then the second, then the third and writing out the number of steps along each basis vector in a triple e.g. '(3,-4,5)' designates the point at three units out from the origin in the direction of the first basis vector, then negative four units out from the origin in the direction of the second basis vector (i.e. four units out from the origin in the *opposite* direction of the second basis vector), and five units out in the direction of the third basis vector.

25. These triples of numbers are all that you get when you're calculating where the point designated by such triples is projected on the back wall through the pinhole.
They can be more than just integers e.g. '(3.5, -4.32, 1.6)'.

26. The first world coordinate we get is that of the origin of the camera coordinate frame i.e. the location of the pinhole in the world.


## 2025 0828 1623

1. Paul Graham is reading the 1721 edition of "The Works of the Right Hounourable Joseph Addison, Esq" a four volume work on the cofounder of The Spectator and whose books were banned by the Catholic Church until 1966.

2. Here are the links to the 1721 editions at the internet archive:
    1. <https://archive.org/details/bim_eighteenth-century_the-works-of-the-right-h_addison-joseph_1721_1_0>
    2. <https://archive.org/details/bim_eighteenth-century_the-works-of-the-right-h_addison-joseph_1721_2>
    3. <https://archive.org/details/bim_eighteenth-century_the-works-of-the-right-h_addison-joseph_1721_3>
    4. <https://archive.org/details/bim_eighteenth-century_the-works-of-the-right-h_addison-joseph_1721_4>

3. I originally linked to the 1761 editions when I wrote up a tweet giving the full name of 'Addision' as 'Joseph Addison' because I didn't know of him.
While I had a weak response to 'The Spectator' I'm pretty sure it was just that there is a modern magizine called 'The Spectator'.

4. Links to the relevant wikipedia articles
    1. <https://en.wikipedia.org/wiki/Joseph_Addison>
    2. <https://en.wikipedia.org/wiki/The_Spectator_(1711)>
    3. <https://en.wikipedia.org/wiki/Richard_Steele>

5. The reason that Paul is likely reading this is that it shaped the thinking and writing of the world, e.g. the 'founding fathers', and continues to control much of what goes as wit and wisdom to this day.

## 2025 0826 1635

1. Notes on Chapter 1 'Nature, Humanity, and History, to 3500 BCE' from 2023 âThe Earth and Its Peoples 8th ed.â (EP) by Richard Bulliet, Pamela Crossley, Daniel Headrick, Steven Hirsch, and Lyman Johnson.

2. Geological Epochs (EP pg. 7)
    1. 2M-9k BCE Pleistocene (Great Ice Age)
    2. 8k BCE-present Holocene

3. Technology (EP pg. 7)
    1. 2.6M BCE Earliest stone tools; foraging
    2. 2M-8k BCE Paleolithic (Old Stone Age)
    3. 400k BCE Fire
    4. 30k BCE Cave paintings
    5. 8-2k BCE Neolithich (New Stone Age); agriculture

4. Species and Migrations (EP pg. 7)
    1. 7M BCE Earliest hominids
    2. 4.5M BCE Australopithecines
    3. 2.3M BCE Early *Homo habilis*
    4. 1.8M-350k BCE *Homo Erectus*
    5. 200-100k BCE Anatomically modern *Homo sapiens* in Africa
    6. 80-50k BCE Behaviorally modern *Homo sapiens*; migrations to Eurasia
    7. 46k BCE Modern humans in Australia
    8. 18k BCE Modern humans in Americas 

5. In 1856 in the Neander Valley of Germany the 40,000 year old 'Neanderthal 1' fossil of Homo neanderthalensis was found:
    > "A creature with a body much like that of modern humans but with a face that had heavy brown ridges and a low forehead, like the faces of apes." EP pg. 5

6. In 1871 Charles Darwin (1809â1882) published 'The Descent of Man and Selection in Relation to Sex' which extended his 1859 book 'On the Origin of Species by Means of Natural Selection, or the Preservation of Favoured Races in the Struggle for Life' to the evolution of humans and predicted that humans first appeared in Africa because of their similarities to African apes.

7. In 1891 in the Indonesian island of Java Eugene Duboi's team found the 700,000 and 1,490,000 year old skullcap of 'Java man' (Homo erectus erectus).

8. In 1924 "Raymond Dar found the skull of a creature he named *Australopithecus africanus* (aw-strah-loh-PITH-uh-kuhs ah-frih-KAH-nuhs) (African southern ape), which he argued was transitional between apes and early humans" (EP pg. 6).

9. In 1929 near Peking (old Beijing) China, W. C. Pei found the skullcap of 'Peking man' and it was similar to the one found by Duboi.

10. "Since 1950, Louis and Mary Leaky and their son, Richard, along with many others have discovered a wealth of early human fossils in the exposed sediments of the Great Rift Valley of eastern Africa." (EP pg. 6)

11. "Biologists classify *australopithescines* (aw-strah-loh-PITH-uh-seen) and humans as members of a family of primates known as *hominids* (HOM-uh-nid) .. The first hominids are now dated about 7M years ago." (EP pg. 6)

12. "Three traits distinguish humans from apes and other primates"
    1. "bipedalism (walking upright on two legs). Being upright frees the forelimbs from any role in locomotion and enhances an older primate trait: a hand with a long, opposable thumb that can work with the fingers to manipulate objects skillfully."
    2. "very large brain."
    3. "The location fo the *larynx* (voice box). In humans it lies much lower in the neck than in any other primate" (EP pg 6) 

13. "Beginning approximately 4.5M years ago, several species of australopithecines evolved in southern and eastern Africa."
    1. In 1974 northern Ehtiopia, Donald Johanson found 24 year old female skeleton "Lucy"
    2. In 1977 Tanzania, Mary Leaky found fossilized footprints: "visual evidence that australopithecines walked on two legs" (EP pg. 7)

14. Between 2M and 3M years ago, climate selected genus *Homo* (to which modern human belong).

15. In 1960s Olduvai (ol-DOO-vy) Gorge of northern Tanzania, Louis Leakey found fossilized *Homo habilis* (HOH-moh HAB-uh-luhs) (handy human) whose brain was about 50\% larger than that of australopithecines: "Seeds adn other fossilized remains found in ancient *Homo habilis* camps indicate that the new species ate a greater variety of more nutritious foods than did australopithecines." (EP pg. 7)

16. "By 1M years ago, *Homo habilis* and all the australopithecines had become extinct" (EP pg. 7) and "*Homo erectus* (HOH-moh ee-REK-tuhs) (upright human)" took their place having appeared in eastern Africa some 1.8M years ago.
    1. Brains were a third larger than *Homo habilis*.
    2. In 1984 Lake Turkana of Kenya, Richard Leaky found complete fossil of male *Homo erectus*: similar to modern humans from the neck down.
    3. May have had language, 1-2M years ago.

17. In 2015 Gauteng of South Africa, Rick Hunter and Steven Tucker found about 1k bones of *Homo naledi*.
In 2017, Dirks et al. dated the bones to 335â236k years ago i.e. in the Middle Pleistocene.
They had small brains and experts are presently revising the part that brain size plays in theories of human evolution because of this.
    - Dirks, P.H.G.M.; Roberts, E.M.; Hilbert-Wolf, H.; Kramers, J.D.; Hawks, J.; et al. (2017). "The age of Homo naledi and associated sediments in the Rising Star Cave, South Africa"
    -  Berger, L.R.; Hawks, J.D.; Dirks, P.H.G.M.; Elliott, M.; Roberts, E.M. (2017). "Homo naledi and Pleistocene hominin evolution in subequatorial Africa"

18. Between 200k and 100k years ago *Homo sapiens* (HOH-moh SAY-pee-enz) (wise human) appeared:
    1. probably in Eastern and southern Africa
    2. brain a third larger than *Homo erectus*.
    2. anatomically similar to present day humans

19. No later than 80-50k years ago did verbal communities of humans appear.
It could be much earlier because the science of behavior has yet to be effectively applied to history outside of what similarities it shares with experimental anthropology.

20. Between 50-46k years ago humans migrated out of eastern and southern Africa throughout south Asia.
Around 46k years ago they populated western and eastern Asia and Australia.
Some 46-10k years ago they were in northeastern Asia and north and south America.


Notes on chapter 2 'The first River Valley Civilizations, 3500-1500 BCE' of 

1. Before 2000 BCE, the 'Epic of Gilgamesh' defined 'civilization' as the people of Mesopotamia.
    > "Certain political, social, economic, and technological traits are usually seen as indicators of *civilization*:
    > 1. cities as administrative centers,
    > 2. a political system based on control of a defined territory rather than kinship connections,
    > 3. many people engaged in specialized, nonfood-producing activities,
    > 4. status distinctions based largely on accumulations of substantial wealth by some groups,
    > 5. monumental buildings,
    > 6. a system for keeping permanent records,
    > 7. long-distance trade,
    > 8. major advances in science and the arts.
    >
    > The earliest societies exhibiting these traits developed in the floodplains of great rivers: the Tigris (TIE-gris) and Euphrates (you-FRAY-teez) in Iraq, the Indus in Pakistan, the Yellow or Huang He (hwang huh) in China, and the Nile in Egypt." EP pg. 28-29

2. 'Mesopotamia' is Greek for 'the land between rivers' i.e. the alluvial plains (now Iraq), north of the Syrian and Arabian deserts and southwest of the Zagros Mountains, split by the Euphrates and Tigris rivers which flow southeast from the Taurus Mountains of Anatolia (now Turkiye) to the Persian Gulf.



## 2025 0826 1442

1. With a few simple edits to the abbreviations laid out in [#2025-0825-1745](#2025-0825-1745) I can more easily reach universal and existential closures while also making way for indiscernibility.
The introduction of the abbreviations for indiscernibility are unhappy, but I'd rather get on with it than labor over notation.

2. 'each i m F' for '(i+m each) (m pop) F'
3. 'each F' for 'each 1 0 F'
4. 'some i m F' for 'not each i m not F'
5. 'some F' for 'some 1 0 F'
4. 'Each F' for 'each i m F' where 'F' is substituted for an i m place predicate or schema
5. 'Some F' for 'not Some not F'

6. That completes the new methods of introducing the Boolean abbreviations.
Now for the sameness abbreviations of indiscernibility and (lexical) identity:

6. 'indi 0 i+2 m F' for 'each i m (2 unbury i+2) iff drop F nip F'
7. 'indi 1+k i+2 m F' for 'and indi 0 i+2 m F indi k i+2 m cw i m F'
8. 'indi F' for 'indi i+2+m i+2 m F' where 'F' is substituted for an i m place predicate or schema
9. 'ide ()' or 'ide 0' for ''
10. 'ide (F ..G)' or 'ide 1+n F .. G' for 'and indi F ide n ..G'
11. 'id' for the substitution in 'ide (..F)' of ..F for the lexical predicates
12. 'id 0 (i+1)\*2' for '(i nip) (i nip 2) id'
13. 'id 1+k (i+1)\*2' for 'and id 0 (i+1)\*2 cw (i+1)\*2 0 id 1+k (i+1)\*2'
14. 'id (i+1)\*2' for 'id i (i+1)\*2'
15. 'id 0' and 'id i\*2+1' for 'over drop' i.e. no-op
16. 'nid i' for 'not id i'

12. Now is a good time to recall that the abbreviations of logic are inert: they do little more than hint at the validities in which they play a part.
Validities are equally inert in that they merely hint at the theories in which they play a part.


## 2025 0825 2104

1. The 30 books that I got for $25 were 30 of the 54 volumes in the 1952 series 'Great Books of the Western World'.
    - <https://en.wikipedia.org/wiki/Great_Books_of_the_Western_World>

2. I have all but one or two books after the 24th and none before that one.
While I may buy the missing books online, I may also just leave them as little quests. 

3. One thing I know for sure is that this series contains many of the texts that I've either read online or have wanted to read e.g. I already dove into Spinoza's 1677 'Ethics' (written from 1661-1675) and Descarte's 1701 'Rules for the Direction of the Mind' (written from 1628).

4. I also got the entire compendium of 'Harvard Classics' for $30.

5. There should be more series like this that are less encyclopedic but more exaustive.

6. The methods of logic have yet to be adequatly applied to history.

7. The compendiums are woefully inadequate when compared with what is presently known about the history of the world.

## 2025 0825 1745

1. Went to the bookfar again: bought over 30 books for $25.

2. A grammatically simpler and uniform prefix notation shall be adapted as a result of the notational exploration done in the previous note.

3. First a summary of the results derived yesterday in [#2025-0824-1844](#2025-0824-1844) (with the old notation).
    1. Predicate Abstraction: '..a(..u: ..uH..x :..x)..b' for '.. some v is such that .. some y is such that .., v=a, .., y=b, and .. some u is such .. some x is such that that ..uH..x, .., u=v, .., and x=y'
    2. Concretion: ..a (..u: ..uF..x: ..x)..b if and only if ..aF..b
    3. Basic Predicate Functors:
        1. Drop: 'drop F' or 'drop (..u: ..uF..x :..x)' for '(..uv: ..uF..x :..x)'
        2. Hem: 'hem F' or 'hem (..uvw: ..uvwFx..y :x..y)' for '(..uvw: ..uvwFv..y :..y)'
        3. Huh: 'F huh G' or '(..uxw: ..uxwF..y :..y) huh  (..uxw: ..uxwG..y :..y)' for '(..u: some v is such that ..uxvF..y nor ..uxvG..y :x..y)' (technically: 'v' must be replaced by some varaible fresh to the context)
    4. Joint Denial: 'F nor G' for '(hem drop drop F) huh (hem drop drop G)' i.e. '(..u: ..uFx..y nor ..uGx..y :x..y)'
    5. Negation: 'not F' for 'F nor F' i.e. '(..u: not ..uF..x :..x)'
    6. Alternation: 'F or G' for 'not (F nor G)' i.e. '(..u: ..uF..x or ..uG..x :..x)'
    7. Converse Conditional: 'F if G' for 'F or not G'
    8. Complementary Converse Conditional: 'F not if G' for 'not (F if G)'
    9. Complementary Conditional: 'F not only if G' for 'G not if F'
    10. Conditional: 'F only if G' for 'not (F not only if G)'
    11. Alternative Denial: 'F not and G' for 'F only if not G'
    12. Conjunction: 'F and G' for 'not (F not and G)'
    13. Exclusive Alternation: 'F or exclusively G' for '(F or G) and (F not and G))'
    14. Biconditionals: 'F if and only if G' for 'not (F or exclusively G)'
    15. Cedents: '..F cede ..G' for '(.., and F) only if (.., or G)
    16. Push: 'push F' for 'not ((drop F) huh (drop F))' i.e. '(..u: ..uxF..y :x..y)'
    17. Dush: 'dush F' for 'drop push F' i.e. '(..uv: ..uxF..y :x..y)'
    18. Prop: 'prop F' for 'push drop F' i.e. '(..u: ..uF..y :x..y)'
    19. Over: 'over F' for 'hem push F' i.e. '(..uvw: ..uvwvF..x :..x)'
    20. Oem: 'oem F' for 'over hem F' i.e. '(..uvw: ..uvwvFw..x :..x)'
    21. Dup: 'dup F' for 'oem dush F' i.e. '(..uvw: ..uvwwF..x :..x)'
    22. Pop: 'pop F' for 'oem drop drop F' i.e. '(..uvw: ..uvFw..x :..x)'
    23. Nip: 'nip F' for 'pop drop push F' i.e. '(..uvw: ..uwF..x :..x)'
    24. Repeat Notation:
        1. '(0 \~\~\~)' for ''
        2. '(1 \~\~\~)' for '\~\~\~'
        3. '(1+n \~\~\~)' for '\~\~\~ (n \~\~\~)'
    25. Nip n: 'nip 1+n F' for '(1+n pop) drop (1+n push) F' i.e. '(..uv..wa: ..u..waF..x :..x)'
    26. Dig: 'dig F' for 'hem nip F' i.e. '(..uvw: ..uwFv..x :..x)'
    27. Bury: 'bury F' for 'dig push F' i.e. '(..uvw: ..uwvF..x :..x)'
    28. Bury n: 'bury 1+n F' for '(1+n dig) (1+n push) F' i.e. '(..u..vw: ..uw..vF..x :..x)'
    29. Unbury: 'unbury F' for 'bury F' i.e. '(..uvw: ..uwvF..x :..x)'
    30. Unbury n: 'unbury 1+n F' for '(1+n bury 1+n) F' i.e. '(..uw..va: ..u..vawF..x :..x)'
    31. Clockwise: 'clockwise i m F' or 'cw i m F' for '(m push) bury i+m (m pop) F' i.e. '(..u..wv: ..uy..wFv..x..z :..xy..z)'
    32. Counterclockwise: 'counterclockwise i m F' or 'ccw i m F' for '(m push) unbury i+m (m pop) F) i.e. '(..uw..v: ..u..vxF..yw..z :x..y..z)'
    33. Universal Cropping: 'each F' for 'not ( (hem nip F) huh (hem nip F))' i.e. '(..u: each v is such that ..uvFx..y :x..y)'
    34. Existential Cropping: 'some F' for 'not each not F' i.e. '(..u: some v is such that ..uvFx..y :x..y)'
    35. Universal Closure: 'Each F' for '(i+m each) (m pop) (..u: ..uF..x :..x)' i.e. '.. each u is such that .. each x is such that ..uF..x'
    36. Existential Closure: 'Some F' for 'not Each not F'
    37. Inclusion: 'F includes G' for 'Each (F if G)'
    38. Converse Inclusion: 'F is included in G' for 'G includes F'
    39. Coextension: 'F is coextensive with G' for '(F includes G) and (F is included in G)'
    40. Proper Inclusion: 'F properly includes G' for 'F includes G and not F is coextensive with G'
    41. Proper Converse Inclusion: 'F is properly included in G' for 'G properly includes F'

4. The uniform grammar enforces a prefix notation on composing predicate functors.
It eliminates a lot but not all parentheses but they can always be included to guide the eye even when they are otherwise redundant.
Each of the predicate functor abbreviations given above are given below with the new notation.

5. Each predicate functor has a number of places (just like each predicate has a number of left places and a number of right places).
For example, huh is a two place predicate functor (just as 'fathered' is a 1 1 place predciate since it has one left place and one right place, and 'pairs' is a 1 2 place predicate since it has one left place and two right places).
Knowing the number of places of a predicate functor makes chaining them together require no parentheses e.g. 'huh huh F G H' is equivalent to 'huh (huh F G) H' (remember that most parentheses are there to help your eye) and 'huh huh F G huh H F' is equivalent to 'huh (huh F G) (huh H F)'.

6. I shall continue to keep the infix notation of familiar truth functional (better, denotative functional) and quantificational logic e.g. 'huh F G' is equivalent to '(..u: some v is such that ..uxvF..y nor ..uxvG..y :x..y)' just as it was with the old predicate functor notation.

7. The new abbreviations are
    1. The Basics:
        1. Predicate Abstraction: '..a(..u: ..uH..x :..x)..b' for '.. some v is such that .. some y is such that .., v=a, .., y=b, and .. some u is such .. some x is such that that ..uH..x, .., u=v, .., and x=y'
        2. Concretion: ..a (..u: ..uF..x: ..x)..b if and only if ..aF..b
        3. Drop: 'drop F' or 'drop (..u: ..uF..x :..x)' for '(..uv: ..uF..x :..x)'
        4. Hem: 'hem F' or 'hem (..uvw: ..uvwFx..y :x..y)' for '(..uvw: ..uvwFv..y :..y)'
        5. Huh: 'huh F G' or 'huh (..uxw: ..uxwF..y :..y) (..uxw: ..uxwG..y :..y)' for '(..u: some v is such that ..uxvF..y nor ..uxvG..y :x..y)' (technically: 'v' must be replaced by some varaible fresh to the context)
    2. Joint Denial: 'nor F G' for 'huh (hem drop drop F)  (hem drop drop G)' i.e. '(..u: ..uFx..y nor ..uGx..y :x..y)'
    3. Negation: 'not F' for 'nor F F' i.e. '(..u: not ..uF..x :..x)'
    4. Alternation: 'or F G' for 'not (nor F G)' i.e. '(..u: ..uF..x or ..uG..x :..x)'
    5. Converse Conditional: 'if F G' for 'or F (not G)'
    6. Complementary Converse Conditional: 'not-if F G' for 'not (if F G)'
        > The complementary converse conditional is one of the first examples where proper notational methods make special names superflurous e.g. there is never a need to write 'not-if F G' since it is equivalent to 'not if F G'.
        > This convenience is familiar to any Forth programmer.
        > If you're a Forth programmer then you can think of predicate functors as defining words like colon and semicolon.
    7. Complementary Conditional: 'not-only-if F G' for 'not-if G F'
    8. Conditional: 'only-if F G' for 'not (not-only-if F G)'
        > 'not-only-if F G' and 'only-if F G' can both be written as 'not only if F G' and 'only if F G' by taking 'only' as a predicate functor functor, but I shall not do that at this time.
        > Not only does it confuse this prefix notation (which, with appropriate parentheses, is familiar to LISP programmers), it also prompts erroneous talk of 'higher order predicate functor logics' which are as bad as talk of 'higher order truth functional and quantificational logics'.
    9. Alternative Denial: 'not-and F G' for 'only-if F (not G)'
    10. Conjunction: 'and F G' for 'not not-and F G'
    11. Exclusive Alternation: 'or-exclusively F  G' or 'orx F G' for 'and or F G not and F G' i.e. 'and (or F G) (not-and F G)'
    12. Biconditionals: 'if-and-only-if F G' or 'iff F G' for 'not or-exclusively F G'
    13. Repeat notation (this needs a better name):
        1. '(0 \~\~\~)' for ''
        2. '(1 \~\~\~)' for '\~\~\~'
        3. '(1+n \~\~\~)' for '\~\~\~ (n \~\~\~)'
    14. Cedents: 'cede (..F) (..G)' or 'cede m n ..F ..G' for 'only-if (m and) ..F (n or) ..G'
    15. Push: 'push F' for 'not huh drop F drop F' i.e. '(..u: ..uxF..y :x..y)'
    16. Dush: 'dush F' for 'drop push F' i.e. '(..uv: ..uxF..y :x..y)'
    17. Prop: 'prop F' for 'push drop F' i.e. '(..u: ..uF..y :x..y)'
    18. Over: 'over F' for 'hem push F' i.e. '(..uvw: ..uvwvF..x :..x)'
    19. Oem: 'oem F' for 'over hem F' i.e. '(..uvw: ..uvwvFw..x :..x)'
    20. Dup: 'dup F' for 'oem dush F' i.e. '(..uvw: ..uvwwF..x :..x)'
    21. Pop: 'pop F' for 'oem drop drop F' i.e. '(..uvw: ..uvFw..x :..x)'
    22. Nip n: 'nip n F' for '(n pop) drop (n push) F' i.e. '(..uv..wa: ..u..waF..x :..x)'
    23. Nip: 'nip F' for 'nip 1 F' i.e. '(..uvw: ..uwF..x :..x)'
    24. Dig: 'dig F' for 'hem nip F' i.e. '(..uvw: ..uwFv..x :..x)'
    25. Bury n: 'bury n F' for '(n dig) (n push) F' i.e. '(..u..vw: ..uw..vF..x :..x)'
    26. Bury: 'bury F' for 'bury 1 F' i.e. '(..uvw: ..uwvF..x :..x)'
    28. Unbury n: 'unbury n F' for '(n bury n) F' i.e. '(..uw..va: ..u..vawF..x :..x)'
    27. Unbury: 'unbury F' for 'unbury 1 F' i.e. '(..uvw: ..uwvF..x :..x)'
    29. Clockwise: 'clockwise i m F' or 'cw i m F' for '(m push) bury i+m (m pop) F' i.e. '(..u..wv: ..uy..wFv..x..z :..xy..z)'
    30. Counterclockwise: 'counterclockwise i m F' or 'ccw i m F' for '(m push) unbury i+m (m pop) F) i.e. '(..uw..v: ..u..vxF..yw..z :x..y..z)'
    31. Universal Cropping: 'each F' for 'not huh hem nip F hem nip F' i.e. '(..u: each v is such that ..uvFx..y :x..y)'
    32. Existential Cropping: 'some F' for 'not each not F' i.e. '(..u: some v is such that ..uvFx..y :x..y)'
    33. Universal Closure: 'Each F' for '(i+m each) (m pop) (..u: ..uF..x :..x)' i.e. '.. each u is such that .. each x is such that ..uF..x'
    34. Existential Closure: 'Some F' for 'not Each not F'
    35. Inclusion: 'includes F G' for 'Each if F G'
    36. Converse Inclusion: 'included F G' for 'includes G F'
    37. Coextension: 'coextensive F G' or 'coex F G' for 'and includes F G included F G'
    38. Proper Inclusion: 'properly-includes F G' or 'pincludes F G' for 'and includes F G not coex F G'
    39. Proper Converse Inclusion: 'properly-included F G' or 'pincluded F G' for 'pincludes G F'


## 2025 0824 1844

1. Went to a bookfair, bought over 60 books for $50.

2. Anyone reading this note is assumed to have read the slower and gentler explanations in [#2025-0823-1417](#2025-0823-1417) on the evolution of predicate functor logic from truth-functional and quantification logic thru predicate abstracts and the schematic premise of abstraction and concretion.

3. âa(u: uFx :x)bâ for âsome v is such some y is such that v=a, y=b, and some u is such some x is such that that uFx, u=v, and x=yâ
    1. In general, '..a{..u: ..uH..x :..x}..b' for '.. some v is such that .. some y is such that .., v=a, .., y=b, and .. some u is such .. some x is such that that ..uH..x, .., u=v, .., and x=y'
    2. Predicate abstraction and its various parts are explained as the logical import of the relative clause.
    3. The English relative clause 'who loves Dick' and the pidgin 'x such that x loves Dick' are uniformly paraphrased by the *predicate abstract* '{x: x loves Dick}'.
    4. It *abstracts* 'Tom' from 'Tom loves Dick' by *binding* the *free* occurrence of 'x' in the *free* sentence 'x loves Dick' with the prefix 'x:'.

4. Wangâs schematic premise of identity theory âFx if and only if some y is such that Fy and x=yâ implies the schematic biconditionals of concretion

    1. a (u: uFx :x) b if and only if aFb
    2. ..a (..u: ..uF..x: ..x)..b if and only if ..aF..b

    and round out the role of relative clause
    3. the predication â{x:x loves Dick}Tomâ concretes to âTom loves Dickâ, and
    4. predicate abstraction and concretion bestow predicational completeness upon truth-functional and quantificational logic: what can be said of a thing can be said by predicating a predicate of its name.

5. All of predicate logic rest upon at most three predicate functors:
    1. 'drop (..u: ..uF..x :..x)' for '(..uv: ..uF..x :..x)'
    2. 'hem (..uvw: ..uvwFx..y :x..y)' for '(..uvw: ..uvwFv..y :..y)'
    3. '(..uxw: ..uxwF..y :..y) huh  (..uxw: ..uxwG..y :..y)' for '(..u: some v is such that ..uxvF..y nor ..uxvG..y :x..y)'    
    > Technically, 'v' must be replaced by some varaible fresh to the context.

    i.e.
    1. 'drop F' for '(..uv: ..uF..x :..x)'
    2. 'hem F' for '(..uvw: ..uvwFv..x :..x)'
    3. 'F huh G' for '(..u: some v is such that ..uxvF..y nor ..uxvG..y :x..y)'

6. 'F nor G' for '(hem drop drop F) huh (hem drop drop G)' i.e.
    1. (..u: some v is such that ..uxv(hem drop drop F)..y nor uxv(hem drop drop G)..y :x..y) (abbreviation)
    2. (..u: some v is such that..uxv(..abc: ..abc(drop drop F)b..f :..f)..y nor ..uxv(..abc: ..abc(drop drop G)b..f :..f)..y :x..y) (abbreviation)
    3. (..u: some v is such that ..uxv(drop drop F)x..y nor ..uxv(drop drop G)x..y :x..y) (concretion)
    4.  (..u: some v is such that ..uxv(..abc: ..ab(drop F)f..g :f..g)x..y nor ..uxv(..abc: ..ab(drop G)f..g :f..g)x..y :x..y) (abbreviation)
    5. (..u: some v is such that ..ux(drop F)x..y nor ..ux(drop G)x..y :x..y) (concretion)
    6. (..u: ..ux(drop F)x..y nor ..ux(drop G)x..y :x..y) (interchange of equivalents â(each v is such that p) if and only if pâ)
    7. (..u: ..ux(..ab: ..aFf..g :f..g)x..y nor ..ux(..ab: ..aGf..g :f..g)x..y :x..y) (abbreviation)
    8. (..u: ..uFx..y nor ..uGx..y :x..y) (concretion).

7. 'not F' for 'F nor F' i.e.
    1. (..u: ..uF..x nor ..uF..x :..x) (abbreviation)
    2. (..u: not ..uF..x :..x) (interchange of equivalents 'p nor p, if and only if not p')

8. 'F or G' for 'not (F nor G)' i.e.
    1. (..u: not ..u(F nor G)..x :..x) (abbreviation)
    2. (..u: not ..u(..v: ..vF..y nor ..vG..y :..y)..x :..x) (abbreviation)
    3. (..u: not (..uF..x nor ..uG..x) :..x) (concretion)
    4. (..u: ..uF..x or ..uG..x :..x) (interchange of equivalents 'p or q, if and only if not, p or q')

9. The rest of the logical abbreviations work out just as 'not' and 'or' do from 'nor' and they are
    1. 'F if G' for 'F or not G'
    2. 'F not if G' for 'not (F if G)'
    3. 'F not only if G' for 'G not if F'
    4. 'F only if G' for 'not (F not only if G)'
    5. 'F not and G' for 'F only if not G'
    6. 'F and G' for 'not (F not and G)'
    7. 'F or exclusively G' for '(F or G) and (F not and G))'
    8. 'F if and only if G' for 'not (F or exclusively G)'
    9. '..F cede ..G' for '(.., and F) only if (.., or G)

10. Happily, in composing 9.9 I stumbled upon the name 'cedent' for what I and most have called 'sequents' and it yielded a beautiful way of reading cedents alound as 'cede'.
Happy linguistic accidents are often the best.

11. 'push F' for 'not ((drop F) huh (drop F))' i.e.
    1. not (..u: some v is such that ..uxv(drop F)..y nor ..uxv(drop F)..y :x..y) (abbreviation)
    2. not (..u: some v is such that ..uxF..y nor ..uxF..y :x..y) (interchange of equivalents '..uv(drop F)..x if and only if ..uF..x')
    3. not (..u: ..uxF..y nor ..uxF..y :x..y) (interchange of equivalents 'p if and only if some v is such that p')
    4. not (..u: not ..uxF..y :x..y) (interchange of equivalents 'p nor p, if and only if not p')
    5. (..u: not not ..uxF..y :x..y) (abbreviation)
    6. (..u: ..uxF..y :x..y) (interchange of equivalents 'p if and only if not not p')

12. The valid schema '..uv(drop F)..x if and only if ..uF..x' invoked in step 2 of item 10 is characteristic of recombic functors in that such biconditionals shorten the intermediate equivalents established by abbreviation and concretion e.g. the following are equivalent
    1. ..uv(drop F)..x
    2. ..uv(..ab: ..aF..f :..f)..x (abbreviation)
    3. ..uF..x (concretion).

13. 'dush F' for 'drop push F' i.e.
    1. (..uv: ..u(push F)x..y :x..y) (abbreviation)
    2. (..uv: ..uxF..y :x..y) (interchnage of equivalents '..u(push F)x..y if and only if ..uxF..y').

14. 'prop F' for 'push drop F' i.e.
    1. (..u: ..ux(drop F)..y :x..y) (abbreviation)
    2. (..u: ..uF..y :x..y) (interchange of equivalents '..uv(drop F)..x if and only if ..uF..x')

15. 'over F' for 'hem push F' i.e.
    1. (..uvw: ..uvw(push F)v..x :..x)
    2. (..uvw: ..uvwvF..x :..x)

16. 'oem F' for 'over hem F' i.e.
    1. (..uvw: ..uvwv(hem F)..x :..x) 
    2. (..uvw: ..uvwvFw..x :..x)

17. 'dup F' for 'oem dush F' i.e.
    1. (..uvw: ..uvwv(dush F)w..x :..x)
    2. (..uvw: ..uvwwF..x :..x)

18. 'pop F' for 'oem drop drop F' i.e.
    1. (..uvw: ..uvwv(drop drop F)w..x :..x)
    2. (..uvw: ..uvw(drop F)w..x :..x)
    3. (..uvw: ..uvFw..x :..x)

19. 'nip F' for 'pop drop push F' i.e.
    1. (..uvw: ..uv(drop push F)w..x :..x)
    2. (..uvw: ..u(push F)w..x :..x)
    3. (..uvw: ..uwF..x :..x)

20. Repeat notation is technically a step up to predicate functor functors, but rather than take that step formally, I'll simply employ this imprecise yet accurate abbreviation (grouping parentheses are dropped when convenient):
    1. '.0.( \~\~\~ )' for ''    
    2. '.1.( \~\~\~ )' for '\~\~\~'
    3. '.1+n.( \~\~\~ )' for '\~\~\~ .n.( \~\~\~ )'
    > This dot notation is experimental and may be replaced by something like 
    > - '(1+n)( \~\~\~ )' for '\~\~\~ (n)( \~\~\~)'
    > - '(1+n \~\~\~)' for '\~\~\~ (n \~\~\~)'

21. Based on the comment to 20 the repeat notation is very experimental and so much so that I'm already changing it to:
    1. '(0 \~\~\~)' for ''
    2. '(1 \~\~\~)' for '\~\~\~'
    3. '(1+n \~\~\~)' for '\~\~\~ (n \~\~\~)'

22. 'nip 1+n F' for '(1+n pop) drop (1+n push) F' i.e.
    1. (..uv..wa: ..uv..w((n pop) drop (1+n push) F)a..x :..x)
    2. (..uv..wa: ..uv(drop (1+n push) F)..wa..x :..x)
    3. (..uv..wa: ..u((1+n push) F)..wa..x :..x)
    4. (..uv..wa: ..u..waF..x :..x)

23. 'dig F' for 'hem nip F' i.e.
    1. (..uvw: ..uvwFv..x :..x)
    2. (..uvw: ..uwFv..x :..x)

24. 'bury F' for 'dig push F' i.e.
    1. (..uvw: ..uw(push F)v..x :..x)
    2. (..uvw: ..uwvF..x :..x)

25. 'bury 1+n F' for '(1+n dig) (1+n push) F' i.e.
    1. (..u..vw: ..uw((1+n push) F)..v..x :..x)
    2. (..u..vw: ..uw..vF..x :..x)

26. 'unbury F' for 'bury F' i.e.
    1. (..uvw: ..uwvF..x :..x)

27. 'unbury 1+n F' for '(1+n bury 1+n) F' i.e.
    1. (..uw..va: ..uaw..v((n bury 1+n) F)..x :..x)
    2. (..uw..va: ..u..vawF..x :..x)

28. 'clockwise i m F' or 'cw i m F' for '(m push) bury i+m (m pop) F' i.e.
    1. (..u..wv: ..u..wv..xy(bury i+m (m pop) F)..z :..xy..z) 
    2. (..u..wv: ..uy..wv..x((m pop) F)..z :..xy..z) 
    3. (..u..wv: ..uy..wFv..x..z :..xy..z)

29. 'counterclockwise i m F' or 'ccw i m F' for '(m push) unbury i+m (m pop) F) i.e.
    1. (..uw..v: ..uw..vx..y(unbury i+m (m pop) F)..z :x..y..z)
    2. (..uw..v: ..u..vx..yw((m pop) F)..z :x..y..z)
    3. (..uw..v: ..u..vxF..yw..z :x..y..z)

30. Doing each of the recombic functors above made me notice that I had written the order of predicate functors in the recombic abbreviations backwards!
This is something that happens from time to time because there is a bit of confusion when working from the inside out and the outside in with predicate functors and then going over and working with the functional predicates that carry over the recombic functors like the remainder of the quotient of arithmetic operations divided by two do for the truth functional functors (better, denotative functional functors).

31. 'each F' for 'not ( (hem nip F) huh (hem nip F))' i.e.
    1. not (..u: some v is such that ..uxv(hem nip F)..y nor ..uxv(hem nip F)..y :x..y)
    2. not (..u: some v is such that not ..uxv(hem nip F)..y :x..y)
    3. not (..u: some v is such that not ..uxv(nip F)x..y :x..y)
    4. not (..u: some v is such that not ..uvFx..y :x..y)
    5. (..u: not some v is such that not ..uvFx..y :x..y)
    6. (..u: each v is such that ..uvFx..y :x..y)

32. 'some F' for 'not each not F'
    1. not (..u: each v is such that ..uv(not F)x..y :x..y)
    2. (..u: not each v is such that ..uv(not F)x..y :x..y)
    3. (..u: not each v is such that not ..uvFx..y :x..y)
    4. (..u: some v is such that ..uvFx..y :x..y)

33. 'Each F' for '(i+m each) (m pop) (..u: ..uF..x :..x)' i.e.
    1. (i+m each) (..u..x: ..uF..x :)
    2. (: .. each u is such that .. each x is such that ..uF..x :)
    3. .. each u is such that .. each x is such that ..uF..x

34. The rest of the schematic Boolean abbreviations are
    1. 'Some F' for 'not Each not F'
    2. 'F includes G' for 'Each (F if G)'
    3. 'F is included in G' for 'G includes F'
    4. 'F is coextensive with G' for '(F includes G) and (F is included in G)'
    5. 'F properly includes G' for 'F includes G and not F is coextensive with G'
    6. 'F is properly included in G' for 'G properly includes F'

40. That completes the schematic denotative functional, Boolean, quantificational, and recombic abbreviations of logic.

41. Soon I will give the schematic sameness, compositional, functional, and graphical of logic (and then much more).

## 2025 0823 1534

1. In an attempt to write out the name of the chapter mentioned in the the 19th item of the previous note, I was thrown into reading a number of selections from Russell's "An Inquiry into Meaning and Truth" (IMT) as a result of having read and made notes in [#2025-0817-1523](#2025-0817-1523) on  âRussellâs Logicism and Theory of Coherenceâ by Conor Mayo-Wilson in Russell: The Journal of Bertrand Russell Studies. Vol. 31. Summer, 2011. pp. 89-106.

2. I turned to page 400 of IMT, the first page of chapter 23 titled 'Warrented Assertability' and read the second sentence
    > "The third, that of coherence, was discussed and rejected in Chapter X." pg. 400

3. From there it occurred to me that Mayo-Wilson was claiming something else, that Russel was primarly concerned with coherence.
Here is a selection from the abstract to Mayo-Wilson's paper:
    > "This raises the question: what did Russell understand to be the philosophical importance of logicism? Building on recent work by Andrew Irvine and Martin Godwyn, I argue that Russell thought a systematic reduction of mathematics increases the certainty of known mathematical theorems (even basic arithmetic) facts by showing mathematical knowledge to be *coherently* organized. The paper outlines Russell's theory of coherence, and discusses its relevance to logicism and teh certainty attributed to mathematics." pg. 1

4. Next I flipped to Chapter 21 titled 'Truth and Verification' and read the following selection:
    > IN recent philosophy we may distinguish four main types of theory as to 'truth' or as to its replacement by some concept which is thought preferable. These four theories are:
    > 1. The theory which substitues 'warrented assertability' for 'truth'. This theory is advocated by Dr. Dewey and his school.
    > 2. The theory which substitutes 'probability' for 'truth'. This theory is advocated by Professor Reichenbach.
    > 3. The theory which defines 'truth' as 'coherence'. This theory is advocated by Hegelians and certain lgoical positivits.
    > 4. The correspondence theory of truth, according to which the truth of basic propositions depends upon their relation to some occurrence, and the truth of other propositions depends upon their syntactical relations to basic propositions.
    > 
    > For my part I adhere firmly to this last theory. It has, however, two forms between which the decision is not easy. In one form, the basic propositions must be derived from experience, and therefore propositions which cannot be sutably related to experience are neither true nor false. In the other form, the basic propositions need note be related to experience, but only to 'fact,' though if they are not related to experience they cannot be known. Thus the two forms of the correspondence theory differ as to the relation of 'truth' to 'knowledge'.
    > 
    > Of the above four theories, I have discussed the third in chapter X;" pg. 362-363 Russell IMT

5. Upon reading that last sentence I turned to chapter ten titled 'Basic Propositions' and sought out where Russell wrote 'coherence'.
It is a long and meandering chapter, and for all that seems to hinge upon it, it is weakly assembled and frequently supported by the kind of personalized convictions which pervade Russell's literary style e.g.
    > "But what do I know?
    > 1. What I see when I look at them
    > 2. What I hear when others read them aloud
    > 3. What I see when others quote them in print
    > 4. What I see when I compare two copies of the same book." pg. 180

6. The experimental analyst of behavior has spent a great deal of time and energy removing such blind spots i.e. in making clear that such naive certainty is extinguished the moment the behavior of organisms is examined in a laboratory environment and is reinforced by vistigial literary environments found in prescientific libraries.

7. This particular quote from the tenth chapter is also an indication of Russell's outlook: that which is certain is that with which he is in direct contact.

8. I can not keep everything together without simply quoting selections that I have read:    
    > "Assuming, as I shall do henceforth, that there are basic propositions, it seems to me that, for theory of knowledge, 'basic propositions' may be alternatively defined as 'those propositions about particular occurrences which, after a critical scrutiny, we still believe independently of any extraneous evidence in their favour.'
    >
    > Let us consider the clauses of this definition, and let us begin at the end. There may be evidence in favoour of a basic proposition, but it is not this evidence *alone* that causes our belief. You may wake up in the morning and see that it is daylight, and you may see from your watch that it is daylight, and you may see from your watch that it must be daylight. But even if your watch pointed to midnight, you would not doubt that it is daylight. in my scientific system, a number of propositions based on observations support each other, but each is capable of commanding belief on its own account. Moreover mutual support among basic propositions is only possible on the basis of some theory.
    >
    > There are cases, however-chiefly where memory is concerned-- in which our belief, though not inferential, is more or less uncertain. In such cases, a system composed of such beliefs wins more acceptance than any one of them singly. I think Mr. Z. invited me to dinner on Thursday; I look in my diary, and find an entry to that effect. Both my memory adn my diary are fallible, but when tehy agree I think it unlikely that they are both wrong. I will return to this kind fo case later; for the present, i wish to exclude it. It is to be observed, meantime, that a non-inferential belief need not be either certain or indubitable." 187-188

    > "We thus arrive at the momentary perception as the least questionable thing in our experience, and as therefore the criterion and touchstone of all other certainties and pseudocertainties.
    > 
    > But for theory fo knolwedge it is not sufficient that we should perceive something; it is necessary that we should express what we perceive in words. now most objectwords are condensed inductions; this is true of teh word 'dog,' as we have already had occasion to notice. We must avoid such words, if we wish to be merely recording what we perceive. To do this is very difficult, and requires a special vocabulary. We have seen that this vocabulary includes words such as 'red,' and relation-words such as 'precedes,' but not names of persons or physical objects or classes of such terms.
    >
    > We have considered the subject of 'basic propositions' or 'Protokollsatze,' and tried to show that empirical knowledge is impossible without them. it will be remembered that we defined a 'basic proposition' by two characteristics:
    > 1. It arises on occasion of a perception, which is the evidence for its truth;
    > 2. It has a form such that no two propositions having the form can be mutually inconsistent if derived from different percepts.   
    >
    > A proposition having these two characteristics cannot be disproved, but it would be rash to say that it *must* be true.
    >
    > Perhpas no actual proposition quite rigidly fulfils the definition. But pure perceptive propositions remain a limit to which we can approach indefinitely, and the nearer we approach the smaller is the risk of error.
    > 
    > Empirical knowledge requires, however, other premises asserting matters of fact, in addition to pure perceptive propositions. I shall give the name 'factual premiss' to any uninferred proposition which asserts something having a date and which I believe after a critical scrutiny. I do not mean that the date is part of theassertion, but merely that soem kind of temporal occurrence is what is involved in teh truth of the assertion.
    >
    > Factual premisses are not alone sufficient for empirical knowledge, since most of it is inferred. We require, in addition, the premisses necessary for deduction, and those other premisses, whatever they may be, that are necessary for the non-demonstrative inferences upon which science depends." 189-190 IMT ch. 'factual premises'

    > "It must be held, on logical grounds, that no occurrence gives *demonstrative* grounds in favor of belief in any other occurrence. But the grounds are often such as we cannot fail to accept as giving *practical* certainty. We saw taht there can be no reason for disbelieving the proposition 'that is red' when made in the presence of a red percept; it must, however, be admitted that belief in this proposition is logically possible in teh absence of a red percept. Such grounds as exist for supposing that thsi does not occur are derived from causal laws as to teh occurrence of language. We can, however, in theory, distinguish two cases in relation to a judgement such as 'taht is red': one, when it is cause by what it asserts, adn the other, when words or images enter into its causation. In the formercase, it must be true; in the latter, it may be false." pg. 199-200 ch. 'factual premises'

    > "It must be remembered that a factual premiss need not be indubitable, even subjectively; it need only command a certain degree of credence. It can therefore always be reinforced if it is found to harmonize with other factual premisses. What characterizes a factual premiss is not indubitabilitiy, but the fact that it commands a greater or less degree of belief on its own account, independently of its relations to other propositions. We are then led to a combination of self-evidence with coherence: sometimes one factor is very much more important than the other, but in theory coherence always plays some part. The coherence required, however, is not strict logical coherence, for factual premisses can and should be so stated as to be deductively independent of each other. The kind of coherence invovled is a matter which I hope to consider in a later work." pg. 201-202 ch. 'factual premises', Russell 'Inquiry into Meaning and Truth'

9. There is still much to be done to better understand where Mayo-Wilson is coming from and where Russell is coming from in IMT.

## 2025 0823 1417

Work on potential memo 'Stack Notation for Predicate Functor Logic' set to strengthen [#a-stack-notation-for-predicate-functor-logic-2025-0414-1626](#a-stack-notation-for-predicate-functor-logic-2025-0414-1626).

1. Stack notation for predicate functor logic combines the utility of the Forth and J programming languages with Quine's elimination of variables from truth-functional and quantificational logic via the additional methods of predicate abstraction and concretion as the logical import of the grammarian's relative clause.

2. Instead of a predicate having a certain number of crossreferenced places wherein given names are predicated of it, each predicate has a number of left and a number of right places.

3. For example, the classic two place predicates 'father of', 'less than', and 'member of' each have one left place and one right place and are therefore called '(1,1) place predicates'.

4. Predications of (i,m) place predicate letters are depicted schematically by prepending i variables and appending m variables to a predicate letter e.g. 'uFx', 'uvGxyz', and 'uvwHxyz'.

5. Thus, a predication such as '*a* pairs *b* with *c*' is rendered as '*a*PAIRS*bc*'.

6. Quine's predicate abstracts can be introduced as an abbreviation of truth-functional and quantificational logic of an identity theory (technically, the predicate abstract is introduced as part of an abbreviated predication and it is shown that the predicate abstract part is coextensive with the predicate of the abstracted predication):
    - '{u: Fu}x' for 'some a is such that x=a and some u is such that Fu and u=a'
    - '{uv: Guv}xy' for 'some a is such that some b is such that x=a, y=b, and some u is such that some v is such that Guv, u=a, and v=b'
    - '{..u: H..u}..x' for '.. some a is such that .., x=a, and .. some u is such that H..u, .., and u=a'.

7. Predicate abstraction and its various parts are explained as the logical import of the relative clause:
    1. The English relative clause 'who loves Dick' and the pidgin 'x such that x loves Dick' are uniformly paraphrased by the *predicate abstract* '{x: x loves Dick}'.
    2. It *abstracts* 'Tom' from 'Tom loves Dick' by *binding* the *free* occurrence of 'x' in the *free* sentence 'x loves Dick' with the prefix 'x:'.

8. What I call 'free sentences' are called by Quine, and most others, as 'open sentences'.

9. The shift in terminology is justified by noting that what once went as an open sentence, e.g. 'x is a member of y', was either taken as a closed sentence where the free variables 'x' and 'y' were playing the part of names (singular terms) purporting to designate one and only one item throughout a given logical demonstration, or were properly paraphrased as predicates by supplmenting a suppresed prefix of predicate abstraction, e.g. 'x and y are such that x is a member of y'.

10. Thus, a free sentence is free to be the component of a predicate abstract or to stand alone as a proper (closed) sentence containing proper names (singular terms).

11. When predicate abstracts are abbreviations of sentences of truth-functional and quantificational logic the schematic biconditionals of concretion (which has its origins in Frege and Russell) can be proved from  instances of Wang's schematic premise of identity theory 'Fx if and only if some y is such that Fy and x=y' e.g.
    1. Fu if and only if some u is such that Fu and u=x 
    2. some u is such that Fu and u=x, iff some a is such that a=x and some u is such that u=a and Fu

12. The schematic biconditionals of concretion are then
    1. Fx if and only if {u: Fu}x
    2. Fxy if and only if {uv: Fuv}xy
    3. F..x if and only if {..u: F..u}..x

13. The schematic biconditionals of concretion can themselves be given as the starting point of a truth functional, predicate abstract, and predicate functor logic, but that shall not be shown here.

14. Rounding out the role of relative clause
    1. the predication â{x:x loves Dick}Tomâ concretes to âTom loves Dickâ, and
    2. predicate abstraction and concretion bestow predicational completeness upon truth-functional and quantificational logic: what can be said of a thing can be said by predicating a predicate of its name.

15. Concretion makes each predicate coextensive with each predicate abstract of its predications so that the shapes of predicates and predicate letters in a stack based language can be depicted by a predicate abstract of a predication like '*a*PAIRS*bc*' e.g. '{xyz: xPAIRSyz}'.

16. But, the predicate abstract '{xyz: xPAIRSyz}' is not only misleading to those who see the curly brackets as indicating what is most commonly called 'set builder notation', but also does not preserve the shape of the predicate shown i.e. '*a*PAIRS*bc* if and only if {xyz: xPAIRSyz}*abc*' suggests that the predicate abstract '{xyz: xPAIRSyz}' has the shape (0,3) whereas its purportedly coextensive predicate 'PAIRS' has shape (1,2).

17. In languages where variables abound, this confusion over shape is obviously eliminated by further and familiar regimentation: 'xPAIRSyz' is regimented as 'PAIRSxyz' or, with an eye towards the methods of stack based langauges, 'xyzPAIRS'.

18. When it comes to eliminating variables and perhaps even introducing them from a variable free predicate functor logic, the methods of stack based languages do pay their way, but to those unfamiliar with overtly stack based langauges, e.g. Charles H. Moore's Forth programming language or Iverson and Hui's J programming language, or with the covertly stack based langauges, e.g. John McCarthy and Steve Russell's LISP programming language, there may appear as if there is little to recommend with such divisions of predicates into their left and right parts.

19. Interestingly, it was Russell in "An Inquiry into Meaning and Truth" and "Human Knolwedge: its scope and limits" where the problems of analysis, e.g. the logic of a predicate 'part of', and individuation are brought down to a division of the world, or at least what can be seen by an eye, into its left and right parts.

20. Variations on the curly bracket notation abound e.g.

    1. (xyz: xFyz)
    2. (xyz such that xFyz)
    3. ((x, yz) such that xFyz)
    4. (x with yz such that xFyz)

    and such grander deviations as
    
    5. {x: xFyz :yz}
    6. (x: xFyz :yz)
    7. (x such that xFyz that for yz)

    or such pedantic (and wild) deviations as

    8. {(((), x), (y, (z, ())))): xFyz)
    9. ( ((), x) with (y, (z, ())) such that xFyz)
    10. ( ((), x) such that xFyz that for (y, (z, ())) ).

21. Somewhere among all these parentheses, the notation of LISP recommends itself if the lambda calculus retcon is ignored (lambda abstraction and application suffer from the same unfortunate kinship to predicate abstraction and concretion that membership does to predication) e.g.
    1. ( ( (u v w) (x y) ) uvwFxy)
    2. ( ( () (x y)) Fxy)

22. A variation on the LISP notation which draws from the tacit methods of the J programming language yields:
    1. ( (u) uFxyz (x y z) )
    2. ( (u v w) uvwF () )

23. Such is the notation best suited to my full generalization of concretion to its destructuring expedient e.g. 
    - (a (b c)) ( (u v) (v (u u) v)F((x y) (x y z)) ((x y) z) ) ((f g) h) if and only if ((b c d) (a a))F((f g) (f g h)).

24. But, thankfully, such generalization doesn't fit the simple aim sought here, and the notation which shows some affinity to Grassmann's inner product notation and Dirac's Bra-ket notation shall be finally adopted e.g.
    1. (uvw: vuuwFxxxy :xyz)
    2. (u..vw: ..vu..vF..x..zy :..xy..z).

25. I do sincerely hope that all this talk fo alternative notations justifies my inclination to avoid notation all togehter for well factored and carefully compounding abbreviations.

26. Having rolled out the red carpet there is nothing left to do but construct the entirety of predicate logic and its profoundly powerful reach into traditionally mathematical realms from three predicate functors:
    1. 'drop F' for '(..uv: ..uF..x :..x)'
    2. 'hem F' for '(..uvw: ..uvwFv..x :..x)'
    3. 'huh F G' for '(..u: some v is such that ..uxvF..y nor ..uxvG..y :x..y)'
    > Technically, 'v' must be replaced by some varaible fresh to the context.

27. For those wishing to see predicate abstracts in the abbreviation as well as the abbreviated phrase, and who are considerate of the further restrictions imposed on substitutions in and for predicate abstracts, the following may be instructive:
    1. 'drop (..u: ..uF..x :..x)' for '(..uv: ..uF..x :..x)'
    2. 'hem (..uvw: ..uvwFx..y :x..y)' for '(..uvw: ..uvwFv..y :..y)'
    3. 'huh (..uxw: ..uxwF..y :..y) (..uxw: ..uxwG..y :..y)' for '(..u: some v is such that ..uxvF..y nor ..uxvG..y :x..y)'
    > with the same technical stipulation on 'v'.

28. Warning! The predicate functor 'huh' is an amalgam of what is otherwise presented as three seperate predicate functors: 'push', 'some', and 'nor'.
Following the changes in shape when they are shown in both parts of the abbreviation can be confusing and can otherwise be worked out, with some effort, from the work just shown.

29. Technically: given that the reader is careful about the additional constraints on where and when a given variable can be introduced. Such constraints can be formally deduced by going all the way back to the truth functional and quantificational abbreviation of predicate abstracts and the relevant restraints on substutitutions of variables between truth functional and quantificational schema.

30. Such technicalities are avoided by assuming only what has already been weakly at work with notation such as '..x' i.e. the various parts are to be filled out into the appropriate shapes based on context clues.

31. The following abbreviation continues the play on words of confusion from 'huh' because both are overtly confusing for the sake of showing that logic can be got down to at most three predicate functors:
    - 'uh F' for 'huh F F' i.e. '(..u: some v is such that ..uxvF..y nor uxvF..y :x..y)'.

32. While I've gone ahead and expanded 'huh F F' into the predicate abstract it abbreviates '(..u: some v is such that ..uxvF..y nor uxvF..y :x..y)' so that 'uh F' can be written directly as the predicate abstract it abbreviates, I may not always do so, and when I don't it is appropriate to work it out for yourself.

33. Finally, a familiar alethic predicate functor and one which is familiar to those who work with Forth and other stack based programming languages:
    - 'not F' for 'uh hem drop drop F'
    - 'push F' for 'not uh drop F'

34. There is one step in unraveling these abbreviations that I have not overtly shown and so I shall go through the steps of first getting down to the predicate abstract abbreviated by 'not F' and then the one abbreviated by 'push F'.

35. Unraveling 'not F':
    1. uh hem drop drop F
    2. huh (hem drop drop F) (hem drop drop F) (abbreviation)
    3. (..u: some v is such that ..uxv(hem drop drop F)..y nor uxv(hem drop drop F)..y :x..y) (abbreviation)
    4. (..u: some v is such that not ..uxv(hem hem drop F)..y :x..y) (interchange of equivalents 'p nor p, if and only if not p')
    5. (..u: not each v is such that ..uxv(..abc: ..abc(drop drop F)b..f :..f)..y :x..y) (rule of passage)
    6. (..u: not each v is such that ..uxv(drop drop F)x..y :x..y) (concretion)
    7. (..u: not each v is such that ..uxv(..abc: ..ab(drop F)f..g :f..g)x..y :x..y) (abbreviation)
    8. (..u: not each v is such that ..ux(drop F)x..y :x..y) (concretion)
    9. (..u: not each v is such that ..ux(..ab: ..aFf..g :f..g)x..y :x..y) (abbreviation)
    10. (..u: not each v is such that ..uFx..y :x..y) (concretion)
    11. (..u: not ..uFx..y :x..y) (interchange of equivalents '(each v is such that p) if and only if p')

36. This unraveling explains to the native speaker of truth functional and quantificational logic (with its abbreviation for predicate abstracts) what it is that the predicate functor logician is talking about when they use the predicate functor of negation as opposed to the truth function of negation.

37. All in all, the above work on 'not F' may be abbreviated as follows:
    - 'not F' for 'uh hem drop drop F' i.e. '(..u: not ..uF..x :..x)'.

38. Now for the unraveling of 'push F'
    1. not uh drop F
    2. (..u: not ..u(uh drop F)..x :..x) (abbreviation)
    3. (..u: not ..u(huh (drop F) (drop F))..x :..x) (abbreviation)
    4. (..u: not ..u(..a: some b is such that ..afb(drop F)..g nor ufv(drop F)..g :f..g)..x :..x) (abbreviation)
    5. (..u: not some b is such that ..uyb(drop F)..z nor ..uyb(drop F)..z :y..z) (concretion, but note 'y..z' and '..x' have the same length and corresponding variables i.e. the double dot notation dodges some tedium that is explained in greater detail elsewhere)
    6. (..u: not some b is such that not ..uyb(drop F)..z :y..z) (interchange of equivalents 'p nor p, if and only if not p')
    7. (..u: each b is such that ..uyb(drop F)..z :y..z) (interchange of equivalents 'not some b is such that not Gb, if and only if each b is such that Gb')
    8. (..u: each b is such that ..uyb(..ac: ..aF..f :..f)..z :y..z) (abbreviation)
    9. (..u: each b is such that ..uyF..z :y..z) (concretion)
    10. (..u: ..uyF..z :y..z) (interchange of equivalents 'each b is such that p, if and only if p)

39. Thus, 'push F' for 'not uh drop F' i.e. '(..u: ..uxF..y :x..y)'.

40. Tomorrow I shall start with the three basic predicate functors 'drop', 'hem', and 'huh', and shorten all that has been shown here in the last few notes, and then get on with the rest of the abbreviations of logic as described in the hint [#how-the-abbreviations-of-logic-work](#how-the-abbreviations-of-logic-work) (though there they are described without notation, and a total of six predicate functors are actually used as a starting point in [#how-logic-works](#how-logic-works)).
    


## 2025 0822 1810

1. The notes in [#2025-0821-2007](#2025-0821-2007) on the first volume of the "story of civilization" by the Durants sparked the collection of the references to be collated into my history of the world.

2. The center piece, for better or for worse, is 2023 âThe Earth and Its Peoples 8th ed.â by Richard Bulliet, Pamela Crossley, Daniel Headrick, Steven Hirsch, and Lyman Johnson.

3. This is largerly because I have access to Richard Bulliet's lectures on an earlier edition of this book where he makes specific comments about its composition and the various conflicts between members of the commity assembled to construct it.
The economic, political, religious, and academic influences are emphasized throughout Bulliet's lectures and aid in my own interpretation of the text as it relates to the often older sources available to me.
    - <https://youtube.com/playlist?list=PLXxM47ZxXvkaODXkQBO5RrUJQh5mA0N6l&si=QcW61mPKNdVEZ8jL>

4. Another video source is the Crash Course on World History designed around the 2012 AP World History curriculum presented in many small chunks by John Green:
    - <https://www.youtube.com/playlist?list=PLBDA2E52FB1EF80C9>

5. A major task is collating the chronology.

6. But first a proper list of secondary sources that I have at hand (in no particular order):
    1.  Bulliet, Richard, et al. The Earth and Its Peoples: A Global History, 8th ed., Cengage Learning, 2024
    2. Durant, Will. Our Oriental Heritage; The Story of Civilization. New York: Simon and Schuster, 1954.
    3. Bauer, Susan Wise. The History of the Ancient World: From the Earliest Accounts to the Fall of Rome. 2007.
    4. Christian, David. Origin Story: A Big History of Everything. New York: Little, Brown and Company, 2018.
    5. Crofton, Ian, and Jeremy Black. The Little Book of Big History: The Story of Life, the Universe and Everything. Michael O'Mara Books, 2016.
    6. Wiesner-Hanks, Merry E. A Concise History of the World. Cambridge University Press, 2015.
    7. Harman, Chris. A People's History of the World. Verso, 2017.
    8. McNeil, Ian, ed. An Encyclopedia of the History of Technology. Routledge, 1990.
    9. Harris, Marvin. Cannibals and Kings: The Origins of Cultures. New York: Random House, 1977.
    10. Roberts, J. M., & Westad, O. A. (2013). The Penguin History of the World. Oxford University Press.
    11. Roberts, J. M. (1993). History of the World. Oxford University Press.

7. I have the remaining books in the Durants's volumes, and the other two history texts by Susan Wise Bauer.
I also have a few texts on the history of philosophy, political philosophy, and histories of economics that must also get folded into my history.

8. There is no good place to start when writing the kind of history I have in my sights, but start I shall, and start I have.

## 2025 0822 1541

1. I shall start composing rather than just improvising at the piano.
This is largely due to a conversation that I had with a friend.

2. Continuing work on what I'm now calling "the graph abbreviations" for the memo on predicate logic.

3. The graph abbreviations are
    1. i-symmetrics are inclusions of their component with the first (i\*2)-inverse iterate of their component
    2. i-nonsymmetrics are negations of i-symmetrics
    3. i-asymmetrics are inclusions of their component with the negation fo the first (i\*2)-inverse iterate of their component
    4. i-transitivities are inclusions of the second (i\*2)-iterate of their component with their component,
    5. i-nontransitivities are negations of i-transitivities,
    6. i-intransitivities are inclusions of the second (i\*2)-iterate of their component with the negation of their component,
    7. i-reflexivities are inclusions of the zeroth i-iterate of their component with their component
    8. i-nonreflexivities are negations of i-reflexivities
    9. i-irreflexivities are inclusions of their component with i-nonidentities.
    10. i-totalreflexivities are inclusions of i-identities with their component

4. Each of these abbreviations is in need of further review: compare and contrast with the corresponding abbreviations in Quine's "Set Theory and its Logic" in the section on virtual relations (which are later taken in Quine's "Methods of Logic 4th edition" as proper predicate functors), and Tarski's 1941 "Calculus of Relations" and Givant's 2006 "The Calculus of Relations as a Foundation for Mathematics".

## 2025 0821 2007

1. Before I began itemizing my notes and flattening entries down to the hour (if you scroll down far enough you'll find entries broken into days and then days into entries identified down to the minute), I was writing long paragraphs of notes on the things that I was reading.
I stopped doing that because it was unsustainable, and made navigating such entries almost impossible.

2. I begin again with a new book.

3. Will and Ariel Durant's 1936 "The Story of Civilization: Part I Our Oriental Heritage".

4. The first six chapters compose an unnamed 'Book Zero':
    1. the conditions of civilization
    2. the economic elements of civilization
    3. the political elements of civilization
    4. the moral elements of civilization
    5. the mental elements of civilization
    6. the prehistoric beginnings of civilizations

5. In American English, 'oriental' can happily be replaced with 'asian'.

6. Since the science of behavior only appeared less than a century ago, there is little to no history of individual behavior which comes close to that which is necessary for a theoretical analysis.

7. At best, the histories which have been assembled since reports of events began being recorded textually can provide some foundation for a theoretical analysis of cultural scales of change.

8. The social practices which composed the cultures of the world throughout history are know to us by their often deferred consequences.
The past is remote and histories are our weak link to them.

9. The contingencies which select and maintain social behavior compose a social environment and the shape of the social environment is often mentioned by talk of cultural or social practices e.g. the economic, political, moral, and mental elements mentioned in the Durants's volumes.

10. There is no room in psychology as a science of behavior for mental attributes and, thankfully, the sections of chatper five "the mental elements of civilization" are
    1. letters: language, its animal background, its human origins, its development, its results, education, initiation, writing, poetry
    2. science: origins, mathematics astronomy, medicine, surgery
    3. art: the meaning of beauty, of art, the primitive sense of beauty, the painting of the body, cosmetics, tattooing, scarification, clothing, ornaments, pottery, painting, sculpture, architecture, the dance, music, summary of the primitive preparation for civilization.

11. None of these sections or the items in their description raise any problems which demand mentalistic explanations (thankfully) e.g.
    > "After fifty thousand years of art men still dispute as to its sources in instinct and in history. What is beauty?-- why do we admire it?-- why do we endeavor to create it? Since this is no place for psychological discourse we shall answer, briefly and precariously, that beauty is any quality by which an object or a form pleases a beholder. Primarily and originally the object does not please the beholder because it is beautiful, but rather he calls it beautiful because it pleases him. Any object that satisfies desire will seem beautiful: food is beautiful-- Thais is not beautiful-- to a starving man." pg. 82

12. The final sentence, 'food is beautiful to a starving man' is the crux of any effective theoretical analysis of cultural practices: as far as we can identify the contingencies of reinforcement (or punishment) that prevailed during a given time, most often by examining the artifacts which helped to maintain social contingencies in the absense of an other organism medaiting their consequences, is as far as we can take whatever history has to offer us.

13. I hope that as the science of behavior supplants the prescience of mind (and its modern reincarnation in cognitive, information, and systems theories) there will be more that the history of today can give to the people and cultures of tomorrow.

14. Until then, I have only the assembled histories available to me e.g. the volumes from the Durants and 2023 âThe Earth and Its Peoples 8th ed.â by Richard Bulliet, Pamela Crossley, Daniel Headrick, Steven Hirsch, and Lyman Johnson and 2014 âHistory of the World 6th ed.â by J. M. Roberts and Odd Arne Westad.

15. It is surprising to many people that science and technology change the control we have over history.
The popular story is that history has happened, and that there is some correspondence between the past and its history.
Whatever connection there is is established by science in the same way that any other records of events are connected to other records of events.

16. It is better for me to begin with what textual records there are than to speculate on the scope and limits of such records: such theoretical speculation is for a much later time.

17. Book One 'the near east' is divided into the following chapters:
    1. Sumeria
    2. Egypt
    3. Babylonia
    4. Assyria
    5. a motley of nations
    6. Judea
    7. Persia

18. Book Two 'India and her Neighbors' is divided into
    1. The foundations of India
    2. Buddha
    3. From Alexander to Aurangzer
    4. the life of the people
    5. the paradise of the gods
    6. the life of the mind
    7. the literature of India
    8. Indian art
    9. a Christian epilogue

19. Book Three 'The Far East' is broken into two parts: China and Japan.

20. Part A 'China' is broken into:
    1. the age of the philosophers
    2. the age of the poets
    3. the age of the artists
    4. the people and the state
    5. revolutions and renewal

21. Part B 'Japan' is broken into:
    1. the makers of Japan
    2. the political and moral foundations
    3. the mind adn art of old Japan
    4. the new Japan

22. This particular edition doesn't have the printed pictures within the text: it does have black and white images at the front of the book.
The back and front covers have endsheets that display simple maps of the larger regions dealt with in the text: there is not a one-to-one match between places mentioned in the text and places labeled on the maps.

23. Since this history is a little less than a century old, there is an additional problem of collating its reports with those made by modern texts, such as the eigth edition of 'The Earth and its Peoples'.

24. The first chapter is titled "The conditions of Civilization":
    1. definition
    2. geological conditions
    3. geographical
    4. economic
    5. racial
    6. psychological
    7. causes of the decay of civilizations

25. The term 'civilization' has the following etymology:
    > <https://www.etymonline.com/word/civilization>
    > 
    > civilization(n.)
    > 
    > 1704, in a now-obsolete sense "law which makes a criminal process civil," from civil + -ization.
    > 
    > The meaning "civilized condition, state of being reclaimed from the rudeness of savage life" is recorded by 1772, probably from French civilisation, serving as an opposite to barbarity and a distinct word from civility, as if from civilize + -ation.
    > 
    > The sense of "a particular human society in a civilized condition, considered as a whole over time," is from 1857. Related: Civilizational.
    > 
    > Civility, formerly the substantive of both civil and civilizeâthe latter of which it was not likely to suggest, except by help of its context,âwas judiciously relieved of one of its meanings, by civilization. [Fitzedward Hall, "Modern English," 1873]
    >

    > <https://www.etymonline.com/word/civil>
    >
    > civil(adj.)
    >
    > late 14c., "relating to civil law or life; pertaining to the internal affairs of a state," from Old French civil "civil, relating to civil law" (13c.) and directly from Latin civilis "relating to a society, pertaining to public life, relating to the civic order, befitting a citizen," hence by extension "popular, affable, courteous;" alternative adjectival derivative of civis "townsman" (see city).
    > 
    > Meaning "not barbarous, civilized" is from 1550s. Specifically "relating to the commonwealth as secularly organized" (as opposed to military or ecclesiastical) by 1610s. Meaning "relating to the citizen in his relation to the commonwealth or to fellow citizens" also is from 1610s.
    > 
    > The word civil has about twelve different meanings; it is applied to all manner of objects, which are perfectly disparate. As opposed to criminal, it means all law not criminal. As opposed to ecclesiastical, it means all law not ecclesiastical: as opposed to military, it means all law not military, and so on. [John Austin, "Lectures on Jurisprudence," 1873]
The sense of "polite" was in classical Latin, but English did not pick up this nuance of the word until late 16c., and it has tended to descend in meaning to "meeting minimum standards of courtesy." "Courteous is thus more commonly said of superiors, civil of inferiors, since it implies or suggests the possibility of incivility or rudeness" [OED].
    > 
    > Civil, literally, applies to one who fulfills the duty of a citizen; It may mean simply not rude, or observant of the external courtesies of intercourse, or quick to do and say gratifying and complimentary things. ... Courteous, literally, expresses that style of politeness which belongs to courts: a courteous man is one who is gracefully respectful in his address and manner â one who exhibits a union of dignified complaisance and kindness. The word applies to all sincere kindness and attention. [Century Dictionary, 1895]
Civil case (as opposed to criminal) is recorded from 1610s. Civil liberty "natural liberty restrained by law only so far as is necessary for the public good" is by 1640s.

    > <https://www.etymonline.com/word/city>
    >
    > city(n.)
    >
    > c. 1200, from Old French cite "town, city" (10c., Modern French citÃ©), from earlier citet, from Latin civitatem (nominative civitas; in Late Latin sometimes citatem) originally "citizenship, condition or rights of a citizen, membership in the community," later "community of citizens, state, commonwealth" (used, for instance of the Gaulish tribes), from civis "townsman," from PIE root \*kei- (1) "to lie," also forming words for "bed, couch," and with a secondary sense of "beloved, dear."
    > 
    > Now "a large and important town," but originally in early Middle English a walled town, a capital or cathedral town. Distinction from town is early 14c. OED calls it "Not a native designation, but app[arently] at first a somewhat grandiose title, used instead of the OE. burh" (see borough).
    > 
    > Between Latin and English the sense was transferred from the inhabitants to the place. The Latin word for "city" was urbs, but a resident was civis. Civitas seems to have replaced urbs as Rome (the ultimate urbs) lost its prestige. Loss of Latin -v- is regular in French in some situations (compare alleger from alleviare; neige from nivea; jeune from juvenis. A different sound evolution from the Latin word yielded Italian citta, Catalan ciutat, Spanish ciudad, Portuguese cidade.
    > 
    > London is the city from 1550s. As an adjective, "pertaining to a city, urban," from c. 1300. City hall "chief municipal offices" is first recorded 1670s; to fight city hall is 1913, American English. City slicker "a smart and plausible rogue, of a kind usu. found in cities" [OED] is first recorded 1916 (see slick (adj.)). City limits is from 1825.
    > 
    > The newspaper city-editor, who superintends the collection and publication of local news, is from 1834, American English; hence city desk attested from 1878. Inner city first attested 1968.

    > <https://www.etymonline.com/word/*kei->
    >
    > \*kei-(1)
    >
    > Proto-Indo-European root meaning "to lie," also forming words for "bed, couch," and with a secondary sense of "beloved, dear."
    > 
    > It might form all or part of: ceilidh; cemetery; city; civic; civil; civilian; civilization; civilize; hide (n.2) measure of land; incivility; incunabula; Siva.
    > 
    > It might also be the source of: Sanskrit Sivah "propitious, gracious;" Greek keisthai "to lie, lie asleep;" Latin cunae "a cradle;" Old Church Slavonic semija "family, domestic servants;" Lithuanian Å¡eima "domestic servants," Lettish sieva "wife;" Old English hiwan "members of a household."

26. Following B. F. Skinner's hint 'Etymology is the archeology of thought.' and digging in and around 'civilization' to the PIE root '\*kei-' it is found that the origins of civilization are beloved beds.
It is this much wider sense (with all of its metaphorical trappings) that I shall take as 'civilization' and I shall leave behind civilization as something opposed to barbarity.

27. But not before digging up the etymology of 'barbarity':
    > <https://www.etymonline.com/word/barbarity> 
    >
    > barbarity(n.)
    >
    > 1560s, "want of civilization," from Latin barbarus (see barbarian (n.)) + -ity. The meaning "savage cruelty, inhuman conduct" is recorded from 1680s.

    > <https://www.etymonline.com/word/barbarian>
    >
    > barbarian(adj.)
    > 
    > mid-14c., "foreign, of another nation or culture," from Medieval Latin barbarinus (see barbarian (n.)). The meaning "of or pertaining to savages, rude, uncivilized" is from 1590s.
    > 
    > barbarian(n.)
    >
    > early 15c., in reference to classical history, "a non-Roman or non-Greek," earlier barbar (late 14c.) "non-Roman or non-Greek person; non-Christian; person speaking a language different from one's own," from Medieval Latin barbarinus (source of Old French barbarin "Berber, pagan, Saracen, barbarian"), from Latin barbarus "strange, foreign, barbarous," from Greek barbaros "foreign, strange; ignorant," from PIE root \*barbar- echoic of unintelligible speech of foreigners (compare Sanskrit barbara- "stammering," also "non-Aryan," Latin balbus "stammering," Czech blblati "to stammer").
    > 
    > Greek barbaroi (plural noun) meant "all that are not Greek," but especially the Medes and Persians; originally it was not entirely pejorative, but its sense became moreso after the Persian wars. The Romans (technically themselves barbaroi) took up the word and applied it to tribes or nations which had no Greek or Roman accomplishments.
    > 
    > Also in Middle English (c. 1400) "native of the Barbary coast;" meaning "rude, wild person" is from 1610s. Occasionally in 19c. English distinguished from savage (n.) as being a step closer to civilization. Sometimes, in reference to Renaissance Italy, "a non-Italian." It also was used to translate the usual Chinese word of contempt for foreigners.
    > 
    > Barbarian applies to whatever pertains to the life of an uncivilized people, without special reference to its moral aspects. Barbarous properly expresses the bad side of barbarian life and character, especially its inhumanity or cruelty: as, a barbarous act. Barbaric expresses the characteristic love of barbarians for adornment, magnificence, noise, etc., but it is not commonly applied to persons: it implies the lack of cultivated taste .... [Century Dictionary, 1889]

28. So it is that 'barbarity' descends from the PIE '\*barbar-' "echoic of unintelligible speech of foreigners".
It is here also that 'civilization' clearly ceases to be of any help in separating scientifically significant elements of social environments from irrelevant ones.

29. Yet, the Durants (I will continue throughout to speak of both husband and wife even though Ariel was only later added as author of later volumes) define civilization as follows:
    > "Civilization is social order promoting cultural creation. Four elements constitute it: economic provision, political organization, moral traditions, and the pursuit of knowledge and the arts. It begins when chaos and insecurity end. For when fear is overcome, curiosity and constructiveness are free, and man passes by natural impulse towards the understanding and embellishments of life." pg. 1

30. Here we find the developmentalistic metaphors of those like Piaget who speak as if organisms are promoted through distinct stages of progress or perhaps fail to take the step from one stage to the next because of some punitive intervention or lack of innate potential.
It is as if one culture creates the next with the first culture coming from some unknown creator.

31. Cultures are no more created than species are.
Social behavior is selected by consequences from variations on past social and nonsocial behavior and, hence, cultural practices as social contingences in a social environment evolve like biological organisms and their behaviors do.

32. There is no more to natural impulse than what has been selected by consequences from variations on past impulses and nonimpulses (whatever these may be).
Impulses and impedence point to the push and pull of forces having conspicuous causes and effects or occasions and responses: the selecting consequences are left unmentioned.

33. Just as the opposite of reinforcement is not punishment (it is extinction), so too are pairs like 'encourage' and 'discourage' unfavorable to a theoretical analysis of history.

34. Thankfully, the marks of selection by consequences from variation are in records made whether they are so described or not.
    > "Certain factors condition civilization, and may encourage or impeded it. First, geological conditions. Civilization is an interlude between ice ages: at any tiem the current of glaciation may rise again, cover with ice and stone the works of man, and reduce life to some narrow segment of the earth. Or the demon of earthquake, by whose leave we build our cities, may shrug his shoulders and consume us indifferently." pg.1

35. Recent work on "Big History" has brought some more attention to the problem of how chemical changes appear from physical ones, how biological changes appear from chemical ones, how behavioral changes appear from biological ones, and how social changes appear from behavioral ones.

36. The conditions of geology and geography are seen as far more dynamic than they once were: the environments in which social behaviors occur are less a stage on which bits of behaving biology play, and more a cumulative selector.

37. geological conditions:
    1. ice ages
    2. glaciation
    3. ice and stone
    4. earth
    5. earth quake

38. There are more, and they are to be found in notes that have yet to appear on the science of geology.

39. geographical conditions:
    1. heat
    2. rain
    3. soil
    4. river
    5. lake
    6. ocean

40. The modern science of geography which is closest to studying conditions like those listed above are nonexclusive divisions of what is called physical geography:
    1. geomorphology: what consequences select the form of the surfaces above or below seal level (bathymetric or hypsometric surfaces of the major distribution of liquid water) on the planet?
    2. hydrology: what consequences select the distribution of liquid water above sea level on the planet?
    3. oceanography: what consequences select the distribution of liquid water below sea level on the planet?
    3. glaciology: what consequences select the distribution of solid water (above sea level) on the planet?
    4. climatology: what consequences select the distribution of gaseous water (above sea level) on the planet?
    5. soil geography: what consequences select the composition of the surfaces of the planet?



## 2025 0821 1706

1. Added hint "HOW TO COOK IT": use a theremometer.

2. Most people over cook or under cook their meats (and other foodstuffs).
Cooking foods to the temperatures given by those who have spent their life cooking is a better place to start than cooking to taste.

3. Cook to taste after cooking to temperature.
You are likely to find that cooking to temperature unlocks flavors and textures that can be startling in their surprise to the tongue and palate.

## 2025 0820 1850

1. Today I got Godlfarb's "Deductive Logic" 2003.

2. When I got to page 56 I almost threw the book across the room.

3. It occurs in section 12 "Use and Mention".

4. It reveals clearly and exactly that Goldfarb, like Carnap, does not know the difference between a schematic letter and a pronoun or variable.

5. The selection is as follows:
    > "Thus in talking about a schema we use a name of that schema, most usually the name obtained by surrounding the schema with quotation marks. What then do we use to talk abotu schemata generally? Naturally, we use expressions such as 'every schema' and the like; but for some purposes we must also use variables that range over schemata, which are called *syntactic variables*. Just as we might use 'x' as a numerical variable and say
    > 
    > (4) A number x is odd if and only if x^2 is odd,
    >
    > we can use 'X' and 'Y' as syntactic variables and say
    >
    > (5) A schema X implies a schema Y if and only if the conditional with antecedent X and consequent Y is valid.
    >
    > Particular instances of (4) are obtained by replacing the variable 'x' with the name of a number, for example, '3 is odd if and only if 3^2 is odd'. So too particular instances of (5) are obtained by preplacing teh variables 'X' and 'Y' with names of schemata, for example,
    >
    > (6) '-p' implies '-(p.q)' if and only if the condition with antecdent '-p' and consequent '-(p.q)' is valid.
    >
    > Note that the last ten words of (6) constitute a complex name of the schemata '-P only if -(p.q)'." pg. 56-57 Goldfarb "Deductive Logic".

6. At best, Goldfarb has weakly presented a substitutional theory of quantification without going out of his way to alert the reader to this fact and to its limitations.
I will get to its limitations directly by later quoting a later part of Quine's "Philosophy of Logic Second Edition" (POL) of 1986.

7. In short, the substitutional theory of quantification is the result of a hasty generalization that goes back to Wittgenstein and his introduction of the phrase "Tautologous".
The inspiration for a substitutional theory of quantification comes from the following logical equivalence: in a universe of discourse whose items are only and solely designated by the names 'Fred', 'Tim', and 'Joe' the following schema are equivalent:
    1. each item is (x such that Fx)
    2. F(Fred), F(Tim), and F(Joe).

    Also, the following are equivalent:

    1. some item is (x such that Fx)
    2. F(Fred), F(Tim), or F(Joe).

8. It is possible to avoid the parenthesis around 'Fred', 'Tim' and 'Joe' if the lowercase letters 'a', 'b', 'c' designate only and solely the items in the universe of discourse:
    1. 'each item is (x such that Fx)' is equivalent to 'Fa, Fb, and Fc'
    2. 'some item is (x such that Fx)' is equivalent to 'Fa, Fb, or Fc'.

9. Some people have trouble with the phrase 'each item is' followed by a parenthetically enclosed relative clause, e.g. 'x such that Fx', and crave the awkward notation that prevails throughout mathematics and logic.
I've avoided similar notation as often as possible for two reasons:
    1. it permits exactly the kinds of use and mention errors that I am here explaining, those that are mostly decendent from the early mistake of confusing logic with something called 'symbolic logic' (at best symbolic logic is another name for mathematical logic which is most often identified with a poorly specified set theory or, as is more popular in computer programming circles, a type theory or formal calculus, or, very rarely, as Quine's theory of protosyntax), and 
    2. It weakens logic as a language by making it hard to read aloud, hard to write with a limited alphabet, and harder to collate with translations or paraphrases.

10. The entire issue is dealt with in POL starting on page 66:
    > "This tendency to see set theory as logic has depended early and late on overestimating the kinship between membership and predication. An intermediate notion, attribution of attributes, insinuates itself and heightens the illusion of continuity.
    >
    > In the innocent 'Fx' of the logic of quantification the schematic letter 'F' stands in place of a predicate. Or, more explicitly, the combination 'Fx' stands in the place of an open sentence in 'x'; whether the sentence has 'x' on one side and an isolated predicate on the other is of no moment. What is important is that in writing 'F' and 'Fx' we are just schematically simulating sentences and their parts; we are not *referring* to predicates or other strings of signs, nor are we referring to attributes or sets. Some logicians, however, have taken a contray line, reading 'F' as an attribute variable and 'Fx' as 'x has F'. Some, fond of attributes, have done this with their eyes open; others have been seduced into it by a confusion.
    >
    > The confusion begins as a confusion of sign and object; a confusion between mentioning a sign and using it. Instead of seeing 'F' steadfastly as *standing in place of* an unspecified predicate, our confused logician sees it half the time as *naming* an unspecified predicate. Thus 'F' gains noun status, enabling him to read 'Fx' as 'x has F' without offend his grammatical ear. Having got this far, he can round out his confusion by calling F an attribute. This attunes his usage to that of the unconfused but prodigal logician who embraces attributes with his eyes open.
    >
    > The prodigal logician is identifiable with Frege. The confused logician could be Russell, dispite his great contributions." pg. 66

11. I make the distinction Quine makes in the second to last paragraph of the quote in 10 by talking about the difference between a schematic letter and a dangling pronoun.
Schematic letters are replaced in the mechanical operation of substitution (von Neumann went so far as to say that substitution is executed on a schema in order to further distinguish between the schema and its sentences), but pronouns which occur in a schema (such as 'x', 'y', or 'it') also occur in the substitutional instances of such a schema.

12. I have gone out of my way when designing my memo on predicate logic to not only avoid pronouns (which is made possible by Quine's predicate functors) but also to avoid schema.
I have gone so far as to avoid notation generally.
The price is high, but the pros far outweight the cons: there is no access point for any of the confusion caused over the past century by, e.g., mistaking predicating a predicate for attributing an atribute or classing a class.

13. It is still my mission to provide a perfectly smooth explanation of predicate logic from the logic of truth functions and quantifications (TQL) or, unhappily, "first-order logic".
This can be accomplished by successive abbreviations through Quine's method of predicate abstraction and concretion (which is the logical import of the grammarians relative clause).

15. In fact, TQL can be seen as an abbreviation for predicate (functor) logic, and that is too be shown e.g. Wang's schematic premise of identity is 'Fx if and only if some item is (y such that Fy and x=y)' which is equivalent to 'Fx if and only if (crop (F and {x}))' where '{x}' is short for the predicate (abstract) 'y such that x=y' and 'crop F' is short for the predicate abstract '.., and x such that some item is y such that Fy..x' and where the predicate abstract '(x such that Fx)y' is short for 'some u is such that u=y and some x is such that u=x and Fx', which, all together, provides a biconditional whose left component is a predication of TQL and whose right component is a predicate functor of component predicates.
    > For those unable to deal with the appearence of 'x' in '{x}' it is advised that they introduce a one place predicate 'X' to their lexicon (perhaps with appropriate adornments to distinguish it from some other unforunately conflicting predicate) along with the premises 1) 'some u is such that Xu' and 2) 'each u is such that each v is such that, Xu and Xv, only if u=v'.
    > In general, what is 'learned by ostention' in a predicate functor community are predicates: names are excommunicated.
    > Said another way, designation and truth are degenerates of denotation.

16. All that being said, back to "the heart of the matter":
    > "The quantifiers 'some F is such that' and 'each F is such that' are at the heart of the matter. I already deplored this sort of quantification somewhat after the middle of Chapter 2. I think it worth while now to develop my objections in more detail.
    > 
    > Consider first some ordinary quantifications "some x is such that (x walks)', 'each x is such that (x walks)', 'some x is such that (x is prime)'. The open sentence after the quantifier shows 'x' in a position where a name could stand; a name of a walker, for instance, or of a prime number. The quantifications do not mean that names walk or are prime; what are said to walk or to be prime are the things that could be named *by* names in those positions. To put the predicate letter 'F' in a quantifier, then, is to treat predicate positions suddenly as name positions, and hence to treat predicates as names of entities fo some sort. The quantifier 'some F is such that' or 'each F is such that' says not that some or all predicates are thus and so, but that some or all entitles of the sort named by predicates are thus and so. The logician who grasps this point, and still quantifies 'F', may say that these entities are attributes; attributes are for him the values 'F', the things over which 'F' ranges. The more confused logician, on the other hand, may say that these entities, the values of 'F', are predicates. He fails to appreciate the difference between schematically *simulating* predicates and quantificationally talking *about* predicates, let alone about attributes.
    >
    > Even the first logician's line here is to be deplored. I urged in Chapter 1 that propositions are undesirable; and the same goes for attributes. Attributes are to predicates, or open sentences, as propositions are to closed sentences. Attributes are like propositions in the inadequacy of their individuation. Sets are well individuated by the *law of extensionality*, which identifies sets whose members are the same; but this law fails for attributes, save as the wrod 'attribute' is ill-applied and 'set' would serve better. Open sentences that are true of just the same things never determine two sets, but may determine two attributes. What is further required for sameness of attributes is synonymy, in some sense, of the open sentences; and in Chapter 1 we despaired of making satisfactory sense of such synonymy.
    >
    > Some logicians, for this reason, view the values of 'F' as sets. But I deplore the use of predicate letters as quantified variables, even when the values are sets. Predicates have attributes as their 'intensions' or meanings (or would if there were attributes), and they have sets as their extensions; but they are names of neither. Variables eligible for quantification therefore do not belong in predicate positions. They belong in name positions.
    > 
    > To put the point another way: even one who admits attributes should not read 'Fx' as 'x has F', with 'F' thus in a name position; rather let him write 'x has y', or, if he prefers distinctive variables for attributes, 'x has \#'. Likewise, if someone wants to admit sets as values of quantifiable variables, let him write 'x in y'; or, if he prefers distinctive variables for sets 'x in \#'. Let him switch explicitly to what I called in Chapter 4 the set-theoretic analogue. The predicate letter 'F', like the sentence letter 'p', is not a value-taking variable at all, but just a substitution-taking schematic letter." pg. 66-67

17. The only thing Quine forgot to mention overtly in these paragraphs (and the next ones I shall further quote to cover just one of the consequent errors which follows from not taking such advice to heart) is that it is only and solely in predicate logic as strictly maintained by Quine in his later work that the definitions of validity in terms of structure, substitution, models, proof, and grammar are equivalent in so weak a theory as protosyntax or what can be made equivalent to a theory of numeric place value notation.
One example of the supremacy of this equivalence can be seen in LindstrÃ¶m's theorem.

18. For those that would throw out such methods there is nothing more to ask of them but to produce a better alternative.
Quine awaited such a day and now so do I.

19. The remaining quote from the section on "Set Theory in Sheep's Clothing" traces the historic origins of mixing up predicating predicates with attributing attributes and classing classes:
    > "If anyone thought attributes a more congenial assumption than sets, he could assume quantification over attributes and then introduce quantification over sets, or a reasonable fascimile, by a certain scheme of contextual definition. Russell took this line. the point of the definition is just to secure the law of extensionality for sets without assuming it for attributes; for this law is the only difference between the two domains. But why would Russell find attributes a more congenial assumption than sets? It was a case rather of not appreciating where elementary logic, in its innocent simulation of predicates, gave way to talk about attributes. The phrase 'propositional function', adapted from Frege, cloaked the confusion; Russell used it to refer sometimes to predicates and sometimes to attributes. As a result it was thought by some that Russell had derived set theory, and therewith mathematics generally, from narrowly logical beginnings.
    >
    > Followers of Hilbert have continued to quantify predicate letters, obtaining what they call a higher-order predicate calculus. The values of these variables are in effect sets; and this way of presenting set theory gives it a deceptive resemblance to logic. One is apt to feel that no abrupt addition to ordinary logic of quantification has been made; just some more quantifiers, governing predicate letters already present. In order to appreciate how deceptive this line can be consider the hypothese 'some y is such that each x is such that (x in y if and only if Fx)'. It assumes a set {x: Fx} determined by an open sentence in the role of 'Fx'. This is the central hypothesis of set theory, and the one that has to be restrained in one way or another to avoid the paradoxes. This hypothesis itself falls out of sight in the so-called higher-order predicate calculus. We get 'some G is such that each x is such that (Gx if and only Fx)', which evidently follows from the genuinely logical triviality 'each x is such that (Fx if and only Fx)' by an elemntary logical inference. There is no actual risk of paradox as long as the ranges of values of 'x' and 'G' are kept apart, but still a fair bit of set theory has slipped in unheralded." pg. 67-68

20. In the past I have not made a proper distinction between 'each item is x such that Fx' and 'each item x is such that Fx' because I've made no effort to present logical theories of logic and have kept to my native tongue of English as much as possible (an extension of my avoidance of notation most likely).
To explain myself here, at least, I quote Quine from page 31 of "From Stimulus to Science"
    > "Instead of reading .. '(something x)(is such that Fx)', we take to read it as '(something is)(x such that Fx)'.
    >
    > Making this shift means recognizing an operator 'x such that' of *predicate abstraction*, in Peano's notation .., for encapsulating in a self-contained complex predicate all that a sentence affirms of an object. Just substitution 'x' for 'Tom' in the sentence, if Tom is the object in question, and prefix 'x such that' to the resulting open sentence. The complex predicate thus formed, when predicate of anyone, will say of him what the original sentence said of Tom. This is our familiar 'such that' idiom, which is matheamtical pidgin English for our indigenous relative clause."

21. Then, rather than taking the indefinite singular terms 'some thing' or 'every thing' I took 'some item' and 'each item'.
I then froze those into 'some item is such that' and 'each item is such that' as canonical quantifiers, from which the problems of cross reference are relieved by the suite of quantifiers 'some item x is such that' and 'each item x is such that', and then the operation of quantification is factored from the operation of predicate abstraction by switching to 'some item is' and 'each item is' with the suite of predicate abstractions 'x such that Fx', 'y such that Fy', and so on.
This is all to pave the way for elimination of variables entirely, recognizing 'some' as a predicate functor of existential cropping and 'each' as its universal analogue and homogenizing and eliminating the variables bound by predicate abstractions via the recombic predicate functors.

22. There is a subtle reason for preferring 'each' over 'every' and 'all' and it has to do with section 2 of chapter 5 of John Stuart Mill's 1868 "A System of Logic" where a predicate is said to denote 'each seperate thing of which it is true' according to Quine on page 60 of 'From Stimulus to Science', or, directly from page 101 of Mill
    > "The only propositions of which HObbes' principle is a sufficient account, are that limited and unimportant class in which both the predicate ant eh subject are proper names. For, as has already been remarked, proper names have strictly no meaning; they are mere marks for individual objects: and when a proper name is predicated of another proper name, all the signification conveyed is, that botht eh names are marks for the same object. But this is precisely what Hobbes produces as a theory of predication in general. His doctrine is full explanation of such predications as these: Hyde was Clarendon, or, Tully is Cicero. It exhausts the meaning of those propositions. But it is sadly inadequate theory of any others. That it should ever have been thought of as such, can be accounted for only by the fact, that Hobbes, in common with other Nominalists, bestowed little or no attention upon the *connotation* of words: and sought for their meaning exclusively in what they *denote*: as if all names had been (what none but proper names really are) marks put upon individuals; and as if there were no difference between a proper and general name, except that the first denotes only one individual, adn the last a greater number."
    - <https://archive.org/details/asystemoflogic01milluoft>
    - <https://archive.org/details/asystemoflogic02milluoft>
    - <https://archive.org/details/john-stuart-mill-a-system-of-logic>
    
23. The word 'designates' (as opposed to 'denotes') is then reserved for singular (as opposed to general) terms which purport to name one and only one item in the purportedly nonempty universe of discourse.

24. But for the hindesight of history, there would be an irony to Mill making this distinction between designation and denotation in reaction to Hobbes by reference to propositions and attributes.

25. Finally I come to the selection from Quine's "Philosophy of Logic Second Edition" where substitutional quantification is confronted and connected to all that has thus far been mentioned in this note.
    > "We reflected lately on the connection between existential quantification and alternation. If all the objects are named and finite in number, the quantification is of course dispensable in favor of alternation, and can be viewed as a mere abbreviation. If the objects are infinite in number, on the other hand, this expansion would require an infinitely long alternation. Midway in Chapter 4 we arrived at a view of expressions as finite sequences, in a mathematical sense; and the further step to infinite sequences is in no way audacious. It would, however, be distinctly a departure from all writings on grammar and most writings on logic, including this book, to invoke infinite expressions. Existential quantification over an infinite universe is not dispensable in favor of our alternation notation: finite alternation.
    >
    > But existential quantification over an infinite universe still admits of an attractive semantical explanation of truth conditions, even though not of elimination, so long as all the objects have names. The quantification is true if adn only if at least one of its instances-- got by dropping the quantifier and putting a name for the variable-- is true.
    >
    > All this about existential quantification holds equally, *mutatis mutandis*, for universal quantification. As long as all objects have names, a universal quantification is true if and only if all its instances are true.
    >
    > Our standard logical grammar, at its strictest, admitted no names (Chapter 2). Names can be simulated by contextual definitions; this was why we could do without names in our strictest standard grammar. This way of providing for names does not go very well with these semantical explanations of quantification, however, since quantification is needed already in the contextual definition. So we will do well for the space of the present discussion to think in terms rather of standard logical grammar in the broader sense, admitting names.
    >
    > These truth conditions for quantification, based on substitution of names and variables, have been favored by Ruth Marcus and others. They compare and contrast curiously with the Tarskian one seen in (5) of Chapter 3. They do not share the sort of circularity that was already remarked on midway in Chapter 3: the existential quantifcation is true if *some* instance is true, and the universal quantification is true if *every* instance is true. But the great contrast is that (5) of Chapter 3 speaks only of values of variables and makes no appeal to names. For this, (5) pays a price in complexity.
    >
    > Thus far, no deviation; just different characterizations of the same quantification, so long as everything has a name. But now it must be remarked that this last is a very restrictive condition, even given our provisional new rediness to admit names. In a generous universe there are more things than can be named even with an infiniteude of names. For, let us recall again one of the twin discrepancies noted in Chapter 4 between sets and open sentences. We saw that some of the sets are not determined by any of the sentences. But these ests will lack names; for if a set has a name, say 'a', then the set is determined by the sentece 'x in a'.
    >
    > A more customary argument to the same purpose makes reference to a classical theorem of set theory, which says taht the irrational numbers cannot all be assigned distinct integers. In contrast, all names can be assigned distinct integers, for instance in Godel's way. It follows that the irrational numbers can not all be assigned distinct integers.
    >
    > We saw in Chapter 4 that the substitutional definitions of logical truth came out coextensive with teh model-theoretic definition of logical trut, so long as the object language was rich enough in vocabulary. We now see that an oppostie sort of situation holds for quantification: the substitutional characterization fo quantification is not coextensive witht eh characterization in terms of objects, or values of variables, if we we assume a rich universe. An existential quantification could turn out false when substitutionally construed and true when objectually construed, because of there being objects of the purported kind but only nameless ones. A universal quantification could turn out false when objectually construed and true when substitutionally construed, because of there being objects to the contrary but only nameless ones. And no lvaishness with names can prevent there beign nameless objects in a generous universe. Substitutional quantification is deviant if the universe is rich.
    >
    > *Its strengths*. The deviation just now noted consists in leaving certain objects out of account, namely, those without names. But substitutional quantification can deviate also in an opposite way: the substituted expressions can fail to name. The truth condition that we formulated for substitutional quantification spoke expressly fo the substitutino of names, but it would work for any other grammatical category as well as for the category of names. If the category is finite, then the quantifications are again of course eliminable in favor of alternations and conjunctions. We sawa  case of precisely this already in teh altter part of Chapter 5, in teh elimination of quantification of the 'alpha', 'beta', etc., in the positions of a hundred one-place predicates. It is interesting to note now an extension of that eliminable brand of quantification, in the form of an uneliminable brand of substitutional quantification. It builds upon the virtual theory fo classes (Chapter 5), and the names that it draws upon in substitution for its variables 'alpha', 'beta', 'etc. are contextually defined abstracts of the virtual theory. If the resulting quantification over simulated classes is seen as cause for rejoicing, then let me remind you in sorrow that these quantifiers are no mere manner of speaking. They are nicely explained, but they are not eliminably defined. Moreover, because vritual class abstracts and corresponding variables come only after 'in', the domain of classes afforded is insufficient foundation for any appreciable matheamtics.
    >
    > When soem new brand of quantification is introduced by definition, and thus eliminable, it of course does nto commit us really to recognizing any objects as values of the variables. It is simulated quantification with a simulated ontological commitment. Our real commitment rests rather with the real quantifiers in the standard language that backs up these false fronts of contextual definition. And note the importance of this stiuplation that the grammar be standard. If modalities or other constructions are admitted in addition to truth functions and quantifiers, they add to the strength or content of the theories in ways that are incommensurable with what mgiht be got by enlaring the universe; incommensurable, that is, except relative to some translation of the whole into standard grammar.
    >
    > Substitution quantification, on the ohter hand, is neither an eliminable simulation nor a genuine objectual quantification (unless, of course, all things have names). It is not a way, then, of getting along with a null ontology, an empty universe; it is a non-standard idiom, rather, foreign to the langauge in which we talk fo what there is and of values of variables. IF one does still happen to wonder what would be an adequate universe for some theory that comes to him in this non-standard idiom, the thing for him to do is seek one or another reasonable-looking paraphrase of the theory into a standard form in which quantification is objectually construed. Then he can assess the universe of this theory-- though the various passable translationas may well call for different universes. An unimaginative way of those translating substitution quantification is to translate it into a metalanguage in which we talk fo strings of signsa nd conctenation and substitution and truth-- the sort of language touched on midway in Chapter 3. Identifying the strings with numbers as Godel did, we end up with the positive integers as universe." pg. 91-94.

## 2025 0820 1436

note: I discover that 3-6 are wrong in 7 and present the correction in 8 with reflection on what went wrong in 9.

1. Changed 'open sentence' to 'free sentence' in hint on how predicate abstracts work.

2. I submit that the practice of speaking of 'open sentences' is incomplete and that what was once called an open sentence is either a poorly paraphrased relative clause, i.e. a predicate, or simply a sentence with dangling pronouns or poorly paraphrased proper nouns.

3. 'F transitively closes G with respect to H' for 'Transitive H, and H includes G, only if H includes F'

4. Relative transitive closures are conditionals of conjunctions of a transitivity of a first component with the inclusion of the first with a second, with the inclusion of the first with a third.

5. Premises of transitive closures of one component with respect to an other are relative transitive closures of the one with the other with yet some other.

6. 'F transitively closes G with respect to F' implies 'Transitive F, and F includes G': 'F includes F' is valid and 'Transitive F, and F includes G, only if F includes F' with 'F includes F' implies 'Transitive F, and F includes G'.

7. NO, 6 is wrong, and hence the rest of 3-6 do not work out as I said.

8. There is no getting around the premises of transitive closure of indefinite predicate F* of the other indefinite predicate G* as
    1. Transitive F* and F* includes G*
    2. each instance of 'Transitive F and G* includes F, only if F* includes F' for the schmematic predicate letter 'F'.

9. This is a great example of not being governed by the laws of implication: predicates imply validities.

10. I appear to have taken the converse conditional associated with the implication instead of the conditional.

## 2025 0820 1416

More work on logic memo.

1. The sameness abbreviations are
    1. (i+1,m)-indiscernabilities are i, universal croppings of bury 2s of, m, universal croppings of pops of, conjunctions of each biconditional of, drops of with nip 1s of, the 1st thru (i+m)th (i,m)-CWs of their component (Leibniz, Quine).
    2. Identity is the conjunction of each indiscernability of a lexical atom(Leibniz, Quine).
    3. ((i+1)\*2)-identities are conjunctions of each i nip 2s of i nip 1s of the 1st thru ith (i,0)-CWs of their component. 
    4. 2-nonidentities are negations of identities, and (i+3)-nonidenties are conjunctions of i+2 dupes of ((i+3)\*2)-identities, with the drop of (i+2)-nonidentity

2. The compositional abbreviations are
    1. (i,m)-projections are i, existential croppings of bury 1s of, m, existential croppings of pops,
    2. (i,m)-fields are alternations fo each (i,m)-projection of the first thru (i+m)th (i,m)-CWs,    
    3. (j,k,m)-resultants are j existential croppings of j bury (j+k)s of the conjunction of k drops with m props.
    4. first (j\*2)-iterates are their component, and  (n+2)th (j\*2)-iterates are (j,j,0)-resultants of their component with j nip (j\*2)s of (n+1)th (j\*2)-iterates of their component,
    5. (n+1)th (j\*2)-inverse iterates are (n+1)th (j\*2)-iterates of j bury (j\*2)s.
    6. zeroth ((j+1)\*2)-iterates are conjunctions of ((j+1)\*2)-identities with each first thru ((j+1)\*2)-CW of (j\*2 +1) drops of the ((j+1)\*2,0)-field of their component.

3. updating the hint with the above.

## 2025 0819 1911

1. There are two parts to the problem posed by what are sometimes called 'free logics', but which may better be called "vacuous logic":
    1. the problem of vacuous universes of discourse
    2. the problem of vacuous singular terms.

2. The phrase 'universe of discourse' comes from DeMorgan 1846 "On the Structure of the Syllogism"
    - <https://api.pageplace.de/preview/DT0400.9780429511394_A38476725/preview-9780429511394_A38476725.pdf>

3. Sadly that's as far as this note goes.


## 2025 0818 2248

1. Updating hints from

```
HOW THE ABBREVIATIONS OF LOGIC WORK

1. The truth-functional, better 'denotative-functional', abbreviations are
    1. negations are self joint denials,
    2. alternations are negations of joint denials,
    3. converse conditionals are alternations of their consequent with the negation of their antecedent,
    4. complementary converse conditionals are negations of converse conditionals,
    5. complementary conditionals are swapped complementary converse conditionals,
    6. conditionals are negations of complementary conditionals,
    7. alternative denials are conditionals with negated consequents,
    8. conjunctions are negations of alternative denials,
    9. exclusive alternations are conjunctions of alternative denials with matching alternations,
    10. biconditionals are negations of exclusive alternations, and 
    11. sequents are conditionals of conjunctions with alternations.
2. The Boolean abbreviations are
    1. universal closures are negations of existential closures of negations,
    2. inclusions are universal closures of conditionals,
    3. converse inclusions are swapped inclusions,
    4. proper inclusions are conjunctions of inclusions with the negation of  their converse, and
    5. coextensions are conjunctions of inclusions with their converse.
3. The quantificational abbreviation is
    - universal croppings are negations of existential croppings of negations.
4. The recombic abbreviations are
    1. dushes are pushes of drops,
    2. props are drops of pushes,
    3. overs are pushes of hems,
    4. oems are hems of overs,
    5. dupes are dushes of oems,
    6. pops are two drops of oems,
    7. nip ns are n pushes of drops of n pops,
    8. digs are nips of hems,
    9. bury ns are n pushes of n digs,
    10. unbury ns are n bury ns,
    11. a (i,m)-clockwise turn (CW) is m pops of a bury (i+m) of m pushes, and
    12. a (i,m)-counterclockwise turn (CCW) is m pops of a unbury (i+m) of m pushes. 
5. (j,k,m)-resultants are j existential croppings of j bury (j+k)s of the conjunction of k drops with m props.
6. First (j\*2)-iterates are their component, and  (n+2)th (j\*2)-iterates are (j,j,0)-resultants of their component with j nip (j\*2)s of (n+1)th (j\*2)-iterates of their component.
7. (n+1)th (j\*2)-inverse iterates are (n+1)th (j\*2)-iterates of j bury (j\*2)s.
8. (i+1,m)-indiscernabilities are i, universal croppings of bury 2s of, m, universal croppings of pops of, conjunctions of each biconditional of, drops of with nip 1s of, the 1st thru (i+m)th (i,m)-CWs of their component (Leibniz, Quine).
9. Identities are conjunctions of lexical indiscernabilities (Leibniz, Quine).
10. (i,m)-projections are i, existential croppings of bury 1s of, m, existential croppings of pops.
11. (i,m)-fields are alternations of each (i,m)-projection of the 1st thru (i+m)th (i,m)-CWs of their component.

```

2. to this

```
HOW THE ABBREVIATIONS OF LOGIC WORK

1. The truth-functional, better 'denotative-functional', abbreviations are
    1. negations are self joint denials,
    2. alternations are negations of joint denials,
    3. converse conditionals are alternations of their consequent with the negation of their antecedent,
    4. complementary converse conditionals are negations of converse conditionals,
    5. complementary conditionals are swapped complementary converse conditionals,
    6. conditionals are negations of complementary conditionals,
    7. alternative denials are conditionals with negated consequents,
    8. conjunctions are negations of alternative denials,
    9. exclusive alternations are conjunctions of alternative denials with matching alternations,
    10. biconditionals are negations of exclusive alternations, and 
    11. sequents are conditionals of conjunctions with alternations.
2. The Boolean abbreviations are
    1. universal closures are negations of existential closures of negations,
    2. inclusions are universal closures of conditionals,
    3. converse inclusions are swapped inclusions,
    4. proper inclusions are conjunctions of inclusions with the negation of  their converse, and
    5. coextensions are conjunctions of inclusions with their converse.
3. The quantificational abbreviation is
    - universal croppings are negations of existential croppings of negations.
4. The recombic abbreviations are
    1. dushes are pushes of drops,
    2. props are drops of pushes,
    3. overs are pushes of hems,
    4. oems are hems of overs,
    5. dupes are dushes of oems,
    6. pops are two drops of oems,
    7. nip ns are n pushes of drops of n pops,
    8. digs are nips of hems,
    9. bury ns are n pushes of n digs,
    10. unbury ns are n bury ns,
    11. a (i,m)-clockwise turn (CW) is m pops of a bury (i+m) of m pushes, and
    12. a (i,m)-counterclockwise turn (CCW) is m pops of a unbury (i+m) of m pushes. 

5. The sameness abbreviations are:
    1. (i+1,m)-indiscernabilities are i, universal croppings of bury 2s of, m, universal croppings of pops of, conjunctions of each biconditional of, drops of with nip 1s of, the 1st thru (i+m)th (i,m)-CWs of their component (Leibniz, Quine).
    2. Identity is the conjunction of each indiscernability of a lexical atom(Leibniz, Quine).
    3. ((i+1)\*2)-identities are conjunctions of each i nip 2s of i nip 1s of the 1st thru ith (i,0)-CWs of their component. 

6. The compositional abbreviations are:
    1. (j,k,m)-resultants are j existential croppings of j bury (j+k)s of the conjunction of k drops with m props.
    2. First (j\*2)-iterates are their component, and  (n+2)th (j\*2)-iterates are (j,j,0)-resultants of their component with j nip (j\*2)s of (n+1)th (j\*2)-iterates of their component.
    3. (n+1)th (j\*2)-inverse iterates are (n+1)th (j\*2)-iterates of j bury (j\*2)s.

7. The functional abbreviations are
    1. i-functionalities are inclusions of ((1+i)\*2)-identities with the conjunction of i drops with i nip i's of their component
    2. i-totalities are universal closures of i existential croppings
    3. i-partialities are negations of i-totalities
    4. (i,j)-injectivities are inclusions of ((1+i)\*2)-identities with the conjunction of i nip js with i nip (i+j)s of their component
    5. (i,j)-surjectivities are universal closures of i existential croppings of j (i+j,0)-CWs of their component
    6. (i,j)-bijectivities are the conjunction of the (i,j)-injectivity with the (i,j)-surjectivity of its component
    7. (i,j)-correlations are the conjunction of the i-functionality with the (i,j)-injectivity of their component

8. The graph abbreviations are
    1. i-symmetrics are inclusions of their component with the first (i\*2)-inverse iterate of their component
    2. i-nonsymmetrics are negations of i-symmetrics
    3. i-asymmetrics are inclusions of their component with the negation fo the first (i\*2)-inverse iterate of their component
    4. i-transitivities are inclusions of the second (i\*2)-iterate of their component with their component,
    5. i-nontransitivities are negations of i-transitivities,
    6. i-intransitivities are inclusions of the second (i\*2)-iterate of their component with the negation of their component,

```

## 2025 0818 1824

1. The graph abbreviations are
    1. i-symmetrics are inclusions of their component with the first (i\*2)-inverse iterate of their component
    2. i-nonsymmetrics are negations of i-symmetrics
    3. i-asymmetrics are inclusions of their component with the negation fo the first (i\*2)-inverse iterate of their component
    4. i-transitivities are inclusions of the second (i\*2)-iterate of their component with their component,
    5. i-nontransitivities are negations of i-transitivities,
    6. i-intransitivities are inclusions of the second (i\*2)-iterate of their component with the negation of their component,

2. Not done with these, but that's all I can finish for now.

## 2025 0817 2213

1. Time to organize some more of the defined predicate functors.

2. The sameness abbreviations are:
    1. (i+1,m)-indiscernabilities are i, universal croppings of bury 2s of, m, universal croppings of pops of, conjunctions of each biconditional of, drops of with nip 1s of, the 1st thru (i+m)th (i,m)-CWs of their component (Leibniz, Quine).
    2. Identity is the conjunction of each indiscernability of a lexical atom(Leibniz, Quine).
    3. ((i+1)\*2)-identities are conjunctions of each i nip 2s of i nip 1s of the 1st thru ith (i,0)-CWs of their component. 

3. The compositional abbreviations are:
    1. (j,k,m)-resultants are j existential croppings of j bury (j+k)s of the conjunction of k drops with m props.
    2. First (j\*2)-iterates are their component, and  (n+2)th (j\*2)-iterates are (j,j,0)-resultants of their component with j nip (j\*2)s of (n+1)th (j\*2)-iterates of their component.
    3. (n+1)th (j\*2)-inverse iterates are (n+1)th (j\*2)-iterates of j bury (j\*2)s.

4. It is late and I have to get up early tomorrow, but I am reminded of the semisignificant saying of Dr. Larson "A small fish is better than an empty dish."
I can at least be happy that I got out a first draft definition of ((i+1)\*2)-identities.
I will have to make up a list of translations between the predicate functor, the predicate abstract, and the truth-functional and quantificational sentences for each of these things so that nonnatives can go.

5. The functional abbreviations are
    1. i-functionalities are inclusions of ((1+i)\*2)-identities with the conjunction of i drops with i nip i's of their component
    2. i-totalities are universal closures of i existential croppings
    3. i-partialities are negations of i-totalities
    4. (i,j)-injectivities are inclusions of ((1+i)\*2)-identities with the conjunction of i nip js with i nip (i+j)s of their component
    5. (i,j)-surjectivities are universal closures of i existential croppings of j (i+j,0)-CWs of their component
    6. (i,j)-bijectivities are the conjunction of the (i,j)-injectivity with the (i,j)-surjectivity of its component
    7. (i,j)-correlations are the conjunction of the i-functionality with the (i,j)-injectivity of their component

## 2025 0817 1523

Notes on "Russell's Logicism and Theory of Coherence" by Conor Mayo-Wilson in 
Russell: The Journal of Bertrand Russell Studies. Vol. 31. Summer, 2011. pp. 89-106.

1. There is an error in the first footnote of Mayo-Wilson's paper:
    > "Also, Frege is not mentioned in the essay, which suggests that Quine is thinking primarily of Russell's logicist program."

2. Frege is mentioned at bottom of page 72 of Quine's 1969 "Epistomology Naturalized":
    > "This idea of contextual definition, or recognition of the sentence as the primary vehicle of meaning, was indispensable to the ensuing developments in teh foundations of mathematics. It was explicit in Frege, and it attained its full flower in Russell's doctrine of singula rdescriptions as incomplete symbols." pg. 72

3. There may be a further error in that the paragraph quoted by Mayo from the bottom of page 69 to the top of page 70 as a caricature of Russell's outlook on logicism is followed by the following two paragraph:
    > "The two ideals are linked. For, if you define all the concepts by use of some favored subset of them, you thereby show how to translate all theorems into these favored terms. The clearer these terms are, the likelier it is that the truths couched in them will be obviously true, or derivable from bovious truths. If in particular th concepts of mathematics were all reducible to the clear terms of logic, then all the truths of mathematics would go over into truths of logic; and surely the truths of logic are all obvious or at least potentially obvious, i.e., derivable from obvious truths by individually obvious steps.
    >
    > This particular outcome is in fact denied us, howerever, since matheamtics reduces only to set theory and not to logic proper. Such reduction still enhances clarity, but only because of the interrelations that emerge and not because the end terms of the analysis are clearer than others. As for the end truths, the axiosm of set theory, these have less obviousness and certainty to recommend them than do most of the mathematical theorems that we would derive from them. Moreover, we know from Godel's work that no consistent axiom system can cover matheamtics even when we renounce self-evidence. Reduction in the foundations of mathematics remains matheamtical and philosophically fascinating, but it does not do what the epistemologist would like of it: it does not reveal the ground of matheamtical knowledge, it does not show how mathematical certainty is possible." pg. 70 of "Epistomology Naturalized".

4. Quine covers the same ground as in "Epistemology Naturalized" in the first chapter of "From Stimulus to Science" titled "Days of Yore" and leading up to the second chapter "Naturalism".
Here he emphasizes that he in no way had the myopic view of Russell, and that, in fact, he embraced overtly the contribution of what Mayo-Wilson calls 'coherence' from Russell (and Whitehead's) works.
Quine goes out of the way to emphasize that it is this coherence which is of enduring value to philosophy.
    >  "A philosophical concern that motivated Whitehead and Russell in part, and Frege before them, was as to the nature of mathematical knowledge and the basis of mathematical truth. The conclusion they drew was that mathematics is translatable into pure logic. They counted membership as logic. So mathematical truth is logical truth, they reasoned, and hence all of it must be logically deducible from self-evident logical truths. This is wrong, as transpires in part from Kurt Godels' paper of 1931 and in part from finidings by Russell himself away back in 1902.
    >
    > This forlorn hope was not the only point of Russell and Whitehead's great undertaking. Its other objective, and its enduring value, was simply a deeper understanding of the central concepts of mathematics and their basic laws and interrelations. Their total translatability into just elementary logic and a single familiar two-place predicate, membership, is of itself a philosophic sensation. Modern logic was indispensable to this achievement. An essential factor, of a piece with modern logic, was contextual definition.
    >
    > Buoyed by their achievement, Russell reflected in 1914 on realizing the dream of empiricist epistemologist: the explicit construction of the external world, or a reasonable facsimilie, from sense impressions, hence from simple ideas. He adumbrated it in *Our Knowledge of the External World*, and a dozen years later Rudolf Carnap was undertaking to carry it out. Carnp's effort found expression in *Der Logische Aufbau der Welt* (1928)." pg. 9-10 of "From Stimulus to Science"
    >
    > "Carnap's *Aufbau* was the culmination of the phenomenalism that evolved through Hobbes, Locke, Berkeley, Hume and had had its roots in Descarte's doubts adn in the ancient perplexity over knowledge and error. Yet Carnap's motivation was not this traditional quest for certainty. Rather, his goal was just a systematic integration-- what he called a "constitution system"-- of our scientific concepts of mind and nature. Whitehead and Russell gave us the construction of mathematicsl from minimal beginnings: Carnap went on from there, accepting logic and mathematics as a finished tool for use in his further constructions. His choice of experiences rather than physical objects as his foundation was, he assures us, just a matter of strategy.
    > 
    > Even so, Carnap's inspiration was only in part *Principia Mathematica*. It was more emphatically Russell's frankly epistemological *Our Knowledge of the External World*. Thus it is that his constitution system took the form of a "rational reconstruction"-- Carnap's phrase again-- of man's conceptual development. If by 1928 he was seeing this rational reconstruction as merely a good strategy for a constitution system of global science, still there is no mistaking the epistemological predilection that led him to it." pg. 13-14
    > 
    > "The idea of a self-sufficient sensory language as a foundation for science loses its lustre when we reflect that systematization of our sensory intake is the very business that science itself is engaged in. The memories that link our past experiences with present ones and induce our expectations are themselves mostly memories not of sensory intake but of essentially scientific posits, namely things and events in the physical world. It was perhaps appreciation of this point that led Otto Neurath, Carnap's colleague in Vienna, to persuade Carnap to give up his methodological phenomenalism in favor of physicalism." pg. 15

5. In answer to Mayo-Wilson's question "does logicism have important consequences to *mathematical* epistomology?" Quine answers clearly when he divides the epistomological problems of mathematics into the conceptual and the doctrinal.
    > "Studies in the foundations of matheamtics divide symmetrically into two sorts, conceptual and doctrinal. The conceptual studies are concerned with meaning, the doctrinal with truth. The conceptual studies are concerned with clarifying concepts by defining them, some in terms of others. The doctrinal studies are concerned wtih establishing laws and proving them some on the basis of others." pg. 69-70 "Epistemology Naturalized"
    >
    >... "The two ideals are linked. For, if you define all the concepts by use of some favored subset of them, you thereby show how to translate all theorems into these favored terms. The clearer these terms are, the likelier it is that the truths couched in them will be obviously true, or derivable from obvious truths." pg. 70
    >
    > ..."mathematics reduces only to set theory and not to logic proper. Such reduction still enhances clarity, but only because of the interrelations that emerge and not because the end terms of the analysis are clearer than others." pg. 70
    >
    > ... "Still there remains a helpful thought, regardign epistemology generally, in that duality of structure which was especially conspicuous in teh foundations of mathematics. I refer to the bifurcation into a theory of concepts, or meaning, and a theory of doctrine, or truth; for this applies to teh epistemology of natural knowledge no less than to the foundations of mathematics."
    > ... "On the doctrinal side, I do not see that we are farther along today than where Hume left us. The Humean predicament is the human predicament. But on the conceptual side there has been progress. ... It was made by Bentham in his theory of fictions. Bentham's step was the recognition of contextual definition or what he called paraphrasis. He recognized that to explain a term we do not need to specify an object for it to refer to, nor even specify a synonymous word or phrase; we need only show, by whatever means, how to translate all the whole sentences in which the term is to be used." pg. 72
    > ... "This idea of contextual definition, or recognition of the sentence as teh primary vehicle of meaning, was indispensable to the ensuing developments in teh foundations of mathematics. It was explicit in Frege, and it attained its full flower in Russell's doctrin of singular descriptions as incomplete symbols.
    >
    > Contextual definition was one of two resorts that could be expected to have a liberating effect upon the conceptual side of the epistemology of natural knowledge. The other is resort to the resources of set theory as auxiliary concepts." pg. 72-73
    > 
    > .. "The two resorts are very unequal in epistemological status. Contextual definition is unassailable. Sentences that have been given meaning as wholes are undeiably meaningful, and the use they make of their component terms is therefore meaningful, regardless of whether any translations are offered for those terms in isolation. ... Recourse to sets, on the other hand, is a drastic ontological move, a retreat from the austere ontology of impressions. ... these sets, which amount, afterall, to the whole abstract ontology of mathematics." pg. 73
    >
    > .. "The issue has not always been clear, however, owing to deceptive hints of continuity between elementary logic and set theory. This is why mathematics was once believed to reduce to logic, that is, to an innocent and unquestional logic, and to inherit these qualities. And this si probably why Russell was content to resort to sets as well as to contextual definition when in *Our Knowledge of the External World* and elsewhere he addressed himself to the epistemology of natural knowledge, on its conceptual side.
    >
    > To account for the external world as a logical construct of sense data-- such, in Russell's terms, was the program. It was Carnap, in his *Der logische Aufbau der Welt* of 1928, who came nearest to executing it." pg. 73-74
    >
    >.. "The Cartesian quest for certainty had been the remote motivation of epistemology, both on its conceptual and its doctrinal side; but the quest was seen as a lost cause. To endow the truths of nature with the full authority of immediate experience was as forlorn a hope as hoping to endow the truths of mathematics with the potential obviousness of elementary logic.
    >
    > "What then could have motivated Carnap's heroic efforts on the conceptual side of epistemology, when hope of certainty on the doctrinal side was abandoned? There were good reasons still. one was that such constructions could be expected to elicit and clarify the sensory evidence for science, even if the inferential steps between sensory evidence and scientific doctrine must fall short of certainty. The other reason was that such constructions would deepen our understanding of our discourse about the world, even apart from questions of evidence; it would make all cognitive discourse as clear as observation terms and logic and, I must regretfully add, set theory." pg. 74-75
    >
    > "Two cardinal tenents of empiricism remained unassailable, however, and so remain to this day. One is that whatever evidence there *is* for science *is* sensorty evidence. The other, to which I shall recur, is that all inculcation of meanings of words must rest ultimately on sensory evidence. Hence the continuing attractiveness of the idea of a *logischer Aufbau* in which the sensorty content of discourse would stand forth explicitly." pg. 75
    >
    > .. "But why all this creative reconstruction, all this make believe? The stimulation of his sensory receptors is all the evidence anybody has had to go on, ultimately, in arriving at his picture of the world. Why not just see how this construction really proceeds? Why not settle for psychology? Such a surrender of the epistemological burden to psychology is a move that was disallowed in earlier times as circular reasoning. If the epistemologist's goal is validation of the grounds of empirical science, he defeats his purpose by using psychology or other empirical science in validation. However, such scruples against circulatrity have little point once we have stopped dreaming of deducing science from observations. If we are out simply to understand the link between observation and science, we are well advised to use any available information, including that provided by the very sciences whose link with observation we are seeking to understand." pg. 7
    > 
    > "Now that we are permitted to appeal to physical stimulation, the problem dissolves; A is epistemologically prior to B if A is causally nearer than B to the sensory receptors. Or, what is in some ways better, just talk explicitly in terms of causal proximity to sensory receptors and drop the talk of epistemological priority."pg. 85
    >
    > "an observation sentence is one on which all speakers of the language give the same verdict when given the same concurrent stimulation. To put the point negatively, an observation sentence is one that is not sensitive to differences in past experience within the speech community." pg. 86-87
    >
    > "The old tendency to associate observation sentences with a subjective sensory subject matter is rather an irony when we reflect that observation sentences are also meant to be the intersubjective tribunal of scientific hypotheses. The old tendency was due to the drive to base science on something firmer and prior in the subject's experience; but we dropped that project.
    >
    > "Clarification of the notion of observation sentence is a good thing for the notion is fundamental in two connections. These two correspond to the duality I remarked upon early in this lecture: teh duality between concept and doctrine, between knowing what a sentence means and knowing whether it is true. The observation sentence is basic to both enterprises. Its relation to doctrine, to our knowledge of what is true, is very much the traditional one: observation sentences are the repository of evidence for scientific hypotheses. Its relation to meaning is fundamental too, since observation sentences are the ones we are in a position to learn to understand first, both as children and as field linguists. For observation sentences are precisely the ones that we can correlate with observable circumstances of the occasion of utterance or assent, independently of variations in the past histories of individual informants. They afford the only entry to a language." pg. 88-89
    >
    > "It is no shock to the preconceptions fo old Vienna to say that epistemology now becomes semantics. For epistemology remains centered as always on evidence, and meaning remains centered as always on verification; and evidence is verification. What is likelier to shock preconceptions is that meaning, once we get beyond observation sentences, ceases in general to have any clear applicability to single sentences; also that epistemology merges with psychology, as well as with linguistics." pg. 89-90

## 2025 0815 2200

1. I may soon have to write for those who are already entrenched in their most cherished methods of argument and debate rather than for those who are already familiar with, e.g., the methods of logic as taught by Quine in "Methods of Logic 4th edtiion".

2. The challenge is the same as it has been for about the whole history of philosophy: many things that go without saying don't.

3. I've been reading some of Conor Mayo-Wilson's papers, e.g. "Russell's Logicism and Theory of Coherence" from 2011, and find myself at a loss to explain myself to the writer of such papers.
    - <http://faculty.washington.edu/conormw/Papers.htm>

4. This is largely because there is a modern canon of philosophy which is under the control of cognitive science and probabilistic and statistical methods.

5. Said another way, there is a looming influence from information theories that make it appear as if propositions are back in play throughout philosophy, even though there is very little to say of them in the methods of logic or their application.

6. Another strange practice that turns up in recent readings I've done of philosophy: the authors try to understand what the philosopher meant by this or that.
As if what was written is just expression of some deeper idea that can somehow be guessed at by the analytic reader who is properly in tune with the state of the writer across their own time period all the way up to ours.

7. The challenge is met, at least in the beginning, by going straight to the experimental analysis of behavior.
But, even here there is a great divide between where modern reading and writing is and the work of those like Skinner and Forester in "Schedules of Reinforcement".

8. In modern philosophy, philosophers reason, logicians reason, people reason, and they frequently do so statistically.
How am I to square this with the behavior that is actually investigated in experimental laboratories where the schedules of reinforcement are carefully controlled and rates of response are selected by the contingencies connecting various operanda?

9. Here is a sentence from Mayo's paper "Qualitative Birnbaum" that poses a problem to solve for me as an unknown:
    > "Everyone agrees that data should sometimes change our beliefs."

10. Another sentence, from the end of the same paragraph from which 9 came,
    > "Equipped with probabilism, one can then easily explain how the probabilistic structure of data should incorporated into a rational agentâs beliefs, as the latter also have probabilistic structure."

11. What 'rational agents'?

12. There may be some bridge to build by explaining Nelsons "Radically Elementary Probability Theory" within the evolving predicate functor logic.


## 2025 0815 2047

Typing up handwritten notes from 2025 0723 1609.
I hesitate to type these up because the notation used in them is so much more like traditional notation that it may suggest to the reader that there is some enduring value in such methods.
I can only emphasize that however familiar are these notational methods, they are superceeded in almost every way by the stack based notation found elsewhere (this same comment applies to the last note 2025 0815 2028).

I write down as many different kinds of notation as I can find in the hopes that something better appears from the work of those who are so obviously better than me.
If I recall accurately, these notes are a paraphrase of selections from Quine's "Methods of Logic 4th edition" (even though I would now see them more clearly as Quine's paraphrase of Carnap's "1958 âIntroduction to Symbolic Logic and its Applicationsâ (ISLA)).

1. 'Symmetric F^(2+n)' for 'each {xy..z: Fxy..z only if Fyx..z}'
2. 'Asymmetric F^(2+n)' for 'each {xy..z: Fxy..z only if not Fyx..z}'
3. 'Transitive F^(2+n)' for 'each {xyz..u: Fxy..u and Fyz..u, only if Fxz..u}'
4. 'Intransitive F^(2+n)' for 'each {xyz..u: Fxy..u and Fyz..u, only if not Fxz..u}'
5. 'Reflexive F^(2+n)' for 'each {xy..u: Fxy..u only if Fxx..u and Fyy..u}'
6. 'Total Reflexive F^(2+n)' for 'each {xy..y: Fxx..y}'
7. 'Irreflexive F^(2+n)' for 'each {xy..u: Fxy..u only if not Fxx..u and not Fyy..u}'
8. 'Total Irreflexive F^(2+n)' for 'each {x..y: not Fxx..y}'
9. Irreflexives and Total Irrelfexives of the same predicate are equivalent.
10. 'Cartesian Product of F^m with G^n' or 'F^m * G^n' for '{..x..y: F..x and G..y}'
11. 'k-Resultant of F^(m+k) with G^(n+k)' or 'F^(m+k) /k G^(n+k)' for '{..x..y: some {..z: F..x..z and G..z..x}}'
12. 'Image of F^(m+n) with G^n' or 'F^(m+n) " G^n' for 'F^(m+n) /n G^n'
13. 'crop F^(n+1)' for '{..x: some item is {y: Fy..x}}'
14. 'Negation of F' or 'not F' for '{..x: not F..x}'
15. 'Conjunction of F with G' or 'F and G' for '{..x: F..x and G..x}'
16. 'dup F^(n+2)' for '{x..y: Fxx..y}'
17. 'drop F^n' for '{x..y: F..y}'

> Note that 'dup' and 'drop' above are not like those of the stack based methods elsewhere presented. It is impossible for me to sort through all the different kinds of notation that might be used to work with these things, and this is in fact one of the main reasons for presenting how logic works the way that I do: without notation.

18. 'swap F^(2+n)' for '{xy..z: Fyx..z}'
19. 'cw F^(n+1)' for '{x..y: F..yx}'
20. 'ccw F^(n+1)' for '{..xy: Fy..x}'
21. 'Homoginization of F and ..alpha with respect to G and ..beta' where '..alpha' and '..beta' designate lists of variables that may occur more than once, e.g. 'xyzx'.
    - I have no definition here, I was going to write something like the homoginization algorithm presented by Quine when translating from truth functional and quantificational logic to predicate functor logic.
22. 'item^n' for '{..x: .., and x=x}'
23. 'void^n' for '{..x: .., and not x=x}'
24. '{x}' for '{y: y=x}'
25. '{..x}' for '{y: .., or y=x}' i.e. '.., or {x}'
26. '{(x,y)}' for '{uv: u=x and v=y}'
27. '{.., (..x)}' for '{..u: .., or ..u=..x}'
28. 'some 0 F' for 'not some {..x: F..x}'
29. 'some n+1 F' for 'some {..x: F..x and some n {..y: F..y and not ..x=..y}}'
30. 'Collective Relate of F with x' for '{..y: Fx..y}'
31. 'Collective Relate of F with ..x' for '{..y: F..x..y}'
32. 'F^(2\*n) transmits G^n' for 'each {..x..y: G..x and F..y..x, only if G..y}'

> This is a note from the John who is typing up these handwritten notes.
> I'm very happy to have done this because I now have much better names for the predicate functors that correspond to the above and below schematically given predicate functors e.g. 'symmetrics' instead of 'symmetries', 'asymmetrics' instead of 'asymmetries', 'transitives' instead of 'transitivities', and 'intransitives' instead of 'intransitivities', etc. 

## 2025 0815 2028

Typing up handwritten notes from 2025 0723 1329

1. Write 'S' for '{xy: x immediately succeedes y}'.
2. each {xyz: xSy and zSy, only if x=z} (functional)
3. each {x: some {y: y Sx}}
4. some {x: each {y: not xSy}}
5. each {xyz: xSy and ySz, only if xSz} (transitive)
6. each {xy: xSy only if not each {z: xSz and zSy}}
7. each {xy: xSy only if not ySx}
8. each {x: not xSx}
9. each {x: some {y: y = the {z: zSx}}
10. '0' for 'the {x: each {y: not xSy}}'
11. 's(x)' for 'the {y: ySx}'
12. 'z(x)' for 'the {y: x=x and y=0}'
13. 'z(..x)' for 'the {y: .., and x=x and y=0}'
14. each {x: xAx0}
15. each {xyz: some {u: xAyu and uSz} if and only if some {v: vAyz and vSx}}
16. each {xy: xAxy if and only if each {z: not ySz}}
17. Self-verifying theories
18. Discretely ordered Semiring axiomatization of Peano arithmetic

## 2025 0815 2006

1. First to update the hints from:

```

##HOW LOGIC WORKS

1. Predicates do or don't denote (are true or false of) where they occur, constructions compound them with connectives, and lexicons itemize atomic ones.
2. Grammars generate categories from recurrent constructions on lexicons.
3. Logic uncovers validities whose supplemented lexical substitutions (SLS) denote everywhere (Quine).
4. Constructions inherit where they denote from their components (Tarski):
5. alethically, i.e.
    1. joint denials denote where and only where (waow) each of their components don't,
    2. existential closures denote waow there is some where denoted by their component, and
    3. components of existential croppings denote the left of waow their compound denotes with some item with the right of the same whereabouts, and
6. recombically, i.e.
    4. drops denote the left of the left of waow the compound does with the right of the same whereabouts,
    5. pushes denote the left of waow the compound does, with the left of the right of the same whereabouts, with the right of the right of the same whereabouts, and
    6. hems denote the left of waow the compound does, with the right of the left of the left of the same whereabouts with the right of the same whereabouts.

## HOW THE ABBREVIATIONS OF LOGIC WORK

1. The truth-functional, better 'denotative-functional', abbreviations are
    1. negations are joint denials of their component with itself,
    2. alternations are negations of joint denials of their (two) components,
    3. converse conditionals are alternations of their left component (consequent) with the negation of their right component (antecedent),
    4. complementary converse conditionals are negations of the converse conditional of their components,
    5. complementary conditionals are complementary converse conditionals of their right component with their left component,
    6. conditionals are negations of complementary conditionals of their components,
    7. alternative denials are conditionals of their left component with the negation of their right component,
    8. conjunctions are negations of alternative denials of their components,
    9. exclusive alternations are conjunctions of alternative denials of their components with alternations of their components,
    10. biconditionals are negations of exclusive alternations of their (two) components, and 
    11. sequents are conditionals of the conjunction of its antecedent components with the alternation of its consequent components.

2. The Boolean abbreviations are
    1. universal closures are negations of existential closures of negations of their component
    2. inclusions are universal closures of conditionals of their components,
    3. converse inclusions are universal closures of converse conditionals of theirs,
    4. proper inclusions are conjunctions of inclusions of theirs with the negation of their converse inclusion, and
    5. coextensions are conjunctions of inclusions with their converse inclusion.

4. The quantificational abbreviation is
    - universal croppings are negations of existential croppings of negations of their component.

5. The recombic abbreviations are
    1. overs are pushes of hems of their component,
    2. ohems are hems of overs of theirs,
    3. dushes are pushes of drops of theirs,
    4. props are drops of pushes of theirs,
    5. dupes are dushes of ohems of theirs,
    6. pops are drops of ohems of theirs,
    7. nips are dushes of hems of theirs,
    8. digs are nips of hems of theirs,
    9. buries are pushes of digs of theirs,
    10. bury ns are n pushes of n digs of theirs,
    11. unburies are two buries of theirs, and    
    12. unbury ns are n bury ns of theirs.

6. An (i,j,k)-resultant is j existential croppings of k bury (j+k)s of the conjunction of k drops of the left component with i props of the right component.

## HOW INDISCERNIBILITY WORKS

1. For the predicate letter 'F' write '=F' for '{xy: each {u..v: Fxu..v if and only if Fyu..v, .., and Fu..vx if and only if Fu..vy}'.
2. Write '=(..F,G)', or just '=', for '{xy: (=(..F))xy and (=G)xy}'.
3. Identity and indiscernibility (with respect to the lexicon of predicates of a standard theory) are coextensive.
```

2. To: 

```
## HOW LOGIC WORKS

1. Predicates denote or don't (are true or false of) where they occur (Aristotle); 
2. grammars generate categories from recurrent connective construction on itemized atomic lexicons (Frege, Hilbert, von Neumann);
3. supplemented lexical substitutions (SLS) of validities denote everywhere (Quine); and 
4. denotata are inherited (Tarski) alethically, i.e.
    1. joint denials denote where and only where (waow) each of their components don't (Peirce, Sheffer),
    2. existential closure denote waow there is some where denoted by their component (Boole, Quine, Hailperin), and
    3. existential croppings denote the left part of waow their compound does with some item, with the right part of the same whereabouts (Frege, Russell, Quine), and
5. recombically (Russell, SchÃ¶nfinkel, Curry, Bernays, Tarski, Quine, Charles H. Moore), i.e.
    1. components of drops denote the left part of the left part of waow their comopund does with the right part of the same whereabouts,
    2. components of pushes denote the left part of waow their comopund does with the left part of the right part of the same whereabouts, with the right part of the right part of the same whereabouts, and
    3. components of hems denote the left part of waow their compound does, with the right part of the left part of the left part of the same whereabouts with the right part of the same whereabouts.

## HOW THE ABBREVIATIONS OF LOGIC WORK

1. The truth-functional, better 'denotative-functional', abbreviations are
    1. negations are self joint denials,
    2. alternations are negations of joint denials,
    3. converse conditionals are alternations of their consequent with the negation of their antecedent,
    4. complementary converse conditionals are negations of converse conditionals,
    5. complementary conditionals are swapped complementary converse conditionals,
    6. conditionals are negations of complementary conditionals,
    7. alternative denials are conditionals with negated consequents,
    8. conjunctions are negations of alternative denials,
    9. exclusive alternations are conjunctions of alternative denials with matching alternations,
    10. biconditionals are negations of exclusive alternations, and 
    11. sequents are conditionals of conjunctions with alternations.
2. The Boolean abbreviations are
    1. universal closures are negations of existential closures of negations,
    2. inclusions are universal closures of conditionals,
    3. converse inclusions are swapped inclusions,
    4. proper inclusions are conjunctions of inclusions with the negation of  their converse, and
    5. coextensions are conjunctions of inclusions with their converse.
3. The quantificational abbreviation is
    - universal croppings are negations of existential croppings of negations.
4. The recombic abbreviations are
    1. dushes are pushes of drops,
    2. props are drops of pushes,
    3. overs are pushes of hems,
    4. oems are hems of overs,
    5. dupes are dushes of oems,
    6. pops are two drops of oems,
    7. nip ns are n pushes of drops of n pops,
    8. digs are nips of hems,
    9. bury ns are n pushes of n digs,
    10. unbury ns are n bury ns,
    11. a (i,m)-clockwise turn (CW) is m pops of a bury (i+m) of m pushes, and
    12. a (i,m)-counterclockwise turn (CCW) is m pops of a unbury (i+m) of m pushes. 
5. (j,k,m)-resultants are j existential croppings of j bury (j+k)s of the conjunction of k drops with m props.
6. First (j\*2)-iterates are their component, and  (n+2)th (j\*2)-iterates are (j,j,0)-resultants of their component with j nip (j\*2)s of (n+1)th (j\*2)-iterates of their component.
7. (n+1)th (j\*2)-inverse iterates are (n+1)th (j\*2)-iterates of j bury (j\*2)s.
8. (i+1,m)-indiscernabilities are i, universal croppings of bury 2s of, m, universal croppings of pops of, conjunctions of each biconditional of, drops of with nip 1s of, the 1st thru (i+m)th (i,m)-CWs of their component (Leibniz, Quine).
9. Identities are conjunctions of lexical indiscernabilities (Leibniz, Quine).
10. (i,m)-projections are i, existential croppings of bury 1s of, m, existential croppings of pops.
11. (i,m)-fields are alternations of each (i,m)-projection of the 1st thru (i+m)th (i,m)-CWs of their component.
```

3. and retiring the 'tie up' section:

```

## TIE UP
(this has fallen out of sync with all that I'm doing, and I may have to just drop this section because I skip right over it almost every time I have a look at recent notes)

- Predicate (Functor) Logic Memo
    - [2025 0608 1515](#2025-0608-1515)
    - [2025 0625 2043](#2025-0625-2043)
    - [2025 0625 2333](#2025-0625-2333)
    - [2025 0627 1410](#2025-0627-1410)
    - [2025 0627 1904](#2025-0627-1904)
- Earmuff Experiment
    - [2025 0625 1537](#2025-0625-1537)
    - [2025 0625 2259](#2025-0625-2259)
    - [2025 0629 1518](#2025-0629-1518)
- LISP
- FORTH
- "A first-order axiomatization of the theory of finite trees" by Backofen, Rogers, and Vijay-Shanker 1995
    - [2025 0718 1745](#2025-0718-1745)
```

4. Some edits and tiny corrections were made in bringing the latest work on the predicate logic memo to the hints section e.g. the bicategoricals of the recombic connectives were missing an initial phrase 'the components of', and the wordy sentence about the inheritance of denotata has been replaced with just 'denotata are inherited'.

5. No longer is a distinction made about the direction of the inheritance of denotata e.g. Quine emphasizes chasing truth up and down the tree of grammar.
Proofs are another way, as Quine also demonstrated, although it was much more remote than may be thought upon reading "Philosophy of Logic 2nd Ed." (POL) where it is merely mentioned that his main method can be applied outright to sentences rather than to the truth functional and quantificational schema that turn up in his "Methods of Logic 4th ed." (MOL).


## 2025 0815 1354

1. Hailperin's 1981 "Boole's Algebra Isn't Boolean Algebra" is the only other palce other than Quine's "Methods of Logic 4th edition" (MOL) where I've found an accurate report on the actual methods presented in Boole's 1854 "An Investigation of the Laws of Thought: on Which are Founded the Mathematical Theories of Logic and Probabilities" (aka "Laws of Thought').

2. For Quine, and perhaps in part for Hailperin, Boole's contribution to logic covers the logic of existential closures and denotative functions.

3. Each item of the grammatical category of denotative functions is equivalent to one in the grammatical category of recurrent joint denial constructions which enforce a uniformity of shape among its non-denotative-functional subcomponents e.g. the joint denial of a pair of predicates with shape (2,2) inherits the shape (2,2).

4. The existential closure of a denotative functional compound is called by Quine a "Boolean Existence Sentence" schema (on page 116 of MOL Quine was speaking technically of "Boolean Existence Schema" each instance of which can be called a "Boolean Existence Sentence", though Quine was still wed to 'statement' when he wrote MOL and would later drop it for 'sentence' to avoid any confusion between the consequences of a verbal response and the verbal response itself).

5. Denotative functions of existential closures were then called "Boolean sentences": these have a delightful decision procedure called by Quine "Existential Conditionals".

6. The strict correspondence between Boole's work and Quine's method of "Existential Conditionals" is sketched out in chapter 20 of MOL "Some Boolean Incidentals" (a sorta catch all for the utilities of Boolean abbreviations).

7. In Quine's "Logic Papers" the method of "Existential Conditionals" is obscured by picking up Boole's equational methods earlier than he does in MOL.
There, as in Boole's "Laws of Thought", Quine begins by writing "There does not exist" as "0=".

8. In MOL Quine almost comes out and says that Booles methods are not those of Boolean algebra, but he merely hints at the past association between an algebra of classes and Boole's equations, quickly leaving such potential misinterpretations for an abbreviated notation close to that of Boole's which is equivalent to the method of existential conditionals.

9. Quine does not take the further step to deal with existential closures of Boolean sentences.
It is clear enough upon further reflection that there is strictly nothing added by repeatedly construction existential closures of existential closures, but free wheeling construction of complex existential closures and denotative functions fits with the methods familiar in truth functional and quantificational logic.

10. Quine also fails to make a distinction between existential closures and repeated existential croppings: when are they equivalent, are they?

11. In MOL the closest he comes to confronting this problem is when he introduces the predicate functor of existential cropping itself.
There he first contemplates the construction of an existential closure of a predicate abstract which is made equivalent to a sequence of quantifiers each variable of which is one bound by the predicate abstract (in the order given by the binding phrase) and these are themselves existential closures of the strict kind contemplated by Quine in the chapters on Boolean logic which are actually only about one place predicates.

12. The logic of existential closures is, as mentioned in 3, accomodating of predicates of any number of places, but a closer examination of the case Quine excluded, mentioned above in 9, reveals that Quine's predicate abstracts cover up the evolution of a 'partial existential closure' into 'existential cropping' proper.

13. This evolution is primed by predicate functors which are called for if the rules of passage which can be shown with Boolean sentences (and, specifically, Boolean existence sentences) are to carry over to situations described as in 9 e.g. Existential closures of Boolean sentences.

14. Actually, 13 and 9 do not quite reach the problems hinted at in 12 and 13: it is when Boolean terms (which are here called denotative functional compounds) are mixed with Boolean sentences and Boolean Closures.
Some definitions will help make the distinctions clear.

15. What Quine called Boolean terms are now simply called denotative functions (as shown in 3).
Boolean compounds now come to be existential closures or denotative functions of lexical predicates or Boolean compounds.
Then, Boolean schema are existential closures or denotative functions of predicate letters or Boolen schema.

16. By definition the existential closure of a predicate has the shape of a sentence (which, in the stack notation embraced here, means a shape of (0,0)).
The constraints on the shape of denotative compounds from the shapes of their components (they must all be the same) makes it so that a Boolean schema which conjoins a predicate letter to an existential closure fixes the shape of the predicate letter to (0,0).

17. But, how then to preserve the rule of passage which takes the conjunction of predicate letter with an existential closure to the existential closure of the predicate letter with the component of the earlier existential closure?

18. Quine, as much as Carnap in his 1958 âIntroduction to Symbolic Logic and its Applicationsâ (ISLA) gives sentence letters that may end up being purportededly open or closed.
The open case is made clear by me when we take a sentence 'x loves y' as closed (where 'x' and 'y' are as much like 'John' and 'Dick' and are no more like dangling pronouns than 'John' and 'Dick' are) and 'x and y such that x loves y' as the predicate which open sentences are so often taken as (that is, an open sentence is not open at all, it is simply a relative clause whose bound parts have not been properly indicated).

19. With 18 in mind, answering 17 becomes clearer.
First a closer look at the rule of passage in its schematic form from page 142 chapter 23 "Rules of Passage. Monadic Schemata".
    > "In the schemata in which we have illustrated quantification thus far, the scope of each occurrence of a quantifier has been a truth function of 'Fx', 'Gx', etc. But we can be more liberal, allowing some components of the scope to be devoid of 'x' and represented simply as 'p' and 'q'.
    >
    > It makes good sense, e.g., to write 'some item is (x such that p and Fx)'. This should come out true under just those interpretations of 'p' and F' that make 'p and Fx' true of at least one thing x. Those, of course, are just the interpretations that make 'p' true and 'F' true of something; so teh two schemata 'each item is (x such that p and Fx)' and 'p and each item is (x such that Fx)' are equivalent." pg.142 Quine MOL

20. In a pure predicate functor logic the equivalence of 19, which is given by Quine with variables and quantifiers (the quantifiers themselves been explained by the construction of an existential closure of a one place predicate abstract), is that between the existential cropping of a conjunction of a drop of a predicate on the left with a predicate on the right, and a conjunction of the predicate on the left with the existential cropping of the predicate on the right.

21. But, note in 20, that existential croppings appear and not existential closures.

22. Starting instead with a conjunction with a left predicate and an existential closure of a right predicate, the shape of the right predicate dictates an equivalence between the existential closure of the right predicate and, say, (i+m) existential croppings of a recombic combination of the right predciate (whose shape is now obviously (i,m)).

23. Supposing we leave out the part about the recombic combination of the right predicate (by just supposing we have a lexical predicate of appropraite shape or that we do not look beyond the presently unknown recombic functors as part of our official logical language) we are introduced to exactly one recombic functor by way of maintaining the rule of passage.

24. Specifically, (i+m) existential croppings of the conjunction of (i+m) drops of the left predicate with the right predicate, is equivalent to the conjunction of the left predicate with (i+m) existential croppings of the right predicate.

25. Moving from repeated existential croppings to existential closures, there appears to be a 'drop closure' such that an existential closure of the conjunction of a drop closure of a left predicate with a right predicate, is equivalent to the conjunction of the left predicate with the existential closure of the right predicate.

26. This 'big drop' is then later seen to be equivalent to appropriately repeated drops.

27. I must end here, but must also mention that the big problem looming here is that pointed up at the end of Russell's "An Inquiry into Meaning and Truth" in the chapter on "analysis" where the problem of the predicate 'is a part of' appears as it relates to the methods of logic.

28. As is very clear from the other notes I've made here, I find it all but impossible to talk about logic without mentioning ordered pairs, and it may only be in semantic ascent that the problem of 'is a part of' occurs.
Here, the orderd pair is always as Quine noticed at the end of "Word and Object": a paradigm of philosophic analysis.

29. On the other hand, the abreviation "{(a,b)}" for the predicate abstract '{xy: x=a and y=b}' found in Carnap's ISLA, along with some side comments by Herbrand (I think it was in Herbrand's later paper or his thesis).


## 2025 0814 1547

Predicate memo work.

1. Predicates denote or don't (are true or false of) where they occur (Aristotle); grammars generate categories from recurrent connective construction on itemized atomic lexicons (Frege, Hilbert, von Neumann); supplemented lexical substitutions (SLS) of validities denote everywhere (Quine); and constructed compounds inherit their denotata from connected components (Tarski) alethically, i.e.
    1. joint denials denote where and only where (waow) each of their components don't (Peirce, Sheffer),
    2. existential closure denote waow there is some where denoted by their component (Boole, Quine, Hailperin), and
    3. existential croppings denote the left part of waow their compound does with some item, with the right part of the same whereabouts (Frege, Russell, Quine),

    and recombically (Russell, SchÃ¶nfinkel, Curry, Bernays, Tarski, Quine, Charles H. Moore)

    4. drops denote the left part of the left part of waow their comopund does with the right part of the same whereabouts,
    5. pushes denote the left part of waow their comopund does with the left part of the right part of the same whereabouts, with the right part of the right part of the same whereabouts, and
    6. hems denote the left part of waow their compound does, with the right part of the left part of the left part of the same whereabouts with the right part of the same whereabouts.

2. The truth-functional, better 'denotative-functional', abbreviations are
    1. negations are self joint denials,
    2. alternations are negations of joint denials,
    3. converse conditionals are alternations of their consequent with the negation of their antecedent,
    4. complementary converse conditionals are negations of converse conditionals,
    5. complementary conditionals are swapped complementary converse conditionals,
    6. conditionals are negations of complementary conditionals,
    7. alternative denials are conditionals with negated consequents,
    8. conjunctions are negations of alternative denials,
    9. exclusive alternations are conjunctions of alternative denials with matching alternations,
    10. biconditionals are negations of exclusive alternations, and 
    11. sequents are conditionals of conjunctions with alternations.

3. The Boolean abbreviations are
    1. universal closures are negations of existential closures of negations,
    2. inclusions are universal closures of conditionals,
    3. converse inclusions are swapped inclusions,
    4. proper inclusions are conjunctions of inclusions with the negation of  their converse, and
    5. coextensions are conjunctions of inclusions with their converse.

4. The quantificational abbreviation is
    - universal croppings are negations of existential croppings of negations.

5. The recombic abbreviations are
    1. dushes are pushes of drops,
    2. props are drops of pushes,
    3. overs are pushes of hems,
    4. oems are hems of overs,
    5. dupes are dushes of oems,
    6. pops are two drops of oems,
    7. nip ns are n pushes of drops of n pops,
    8. digs are nips of hems,
    9. bury ns are n pushes of n digs,
    10. unbury ns are n bury ns,
    11. a (i,m)-clockwise turn (CW) is m pops of a bury (i+m) of m pushes, and
    12. a (i,m)-counterclockwise turn (CCW) is m pops of a unbury (i+m) of m pushes. 

6. (j,k,m)-resultants are j existential croppings of j bury (j+k)s of the conjunction of k drops with m props.

7. First (j\*2)-iterates are their component, and  (n+2)th (j\*2)-iterates are (j,j,0)-resultants of their component with j nip (j\*2)s of (n+1)th (j\*2)-iterates of their component.

8. (n+1)th (j\*2)-inverse iterates are (n+1)th (j\*2)-iterates of j bury (j\*2)s.

9. (i+1,m)-indiscernabilities are i, universal croppings of bury 2s of, m, universal croppings of pops of, conjunctions of each biconditional of, drops of with nip 1s of, the 1st, .., thru (i+m)th (i,m)-CWs of their component (Leibniz, Quine).

10. Identities are conjunctions of lexical indiscernabilities (Leibniz, Quine).

11. (i,m)-projections are i, existential croppings of bury 1s of, m, existential croppings of pops.

12. (i,m)-fields are alternations of each (i,m)-projection of the 1st, .., thru (i+m)th (i,m)-CWs of their component.

## 2025 0812 1501

The memo and the hint on predicate (functor) logic may soon diverge.
There is a lot that can be said in even less space with unfamiliar three letter words along with terse sentences that have longer explanations in the appropriate memos.

1. Predicates do or don't denote (are true or false of) where they occur, grammars recursively construct categories with connectives from lexicon, and validities's supplemented lexical substitutions (SLS) denote everywhere (Quine).

2. Constructions inherit where they denote (Tarski): alethically, i.e.
    1. joint denials denote where and only where (waow) each of their components don't,
    2. existential closures denote waow there is some where denoted by their component, and
    3. components of existential croppings denote the left of waow their compound denotes with some item with the right of the same whereabouts, and

    recombically, i.e.

    4. drops denote the left of the left of waow the compound does with the right of the same whereabouts,
    5. pushes denote the left of waow the compound does, with the left of the right of the same whereabouts, with the right of the right of the same whereabouts, and
    6. hems denote the left of waow the compound does, with the right of the left of the left of the same whereabouts with the right of the same whereabouts.

5. The truth-functional, better 'denotative-functional', abbreviations are
    1. negations are self joint denials,
    2. alternations are negations of joint denials,
    3. converse conditionals are alternations of their consequent with the negation of their antecedent,
    4. complementary converse conditionals are negations of converse conditionals,
    5. complementary conditionals are swapped complementary converse conditionals,
    6. conditionals are negations of complementary conditionals,
    7. alternative denials are conditionals with negated consequents,
    8. conjunctions are negations of alternative denials,
    9. exclusive alternations are conjunctions of alternative denials with matching alternations,
    10. biconditionals are negations of exclusive alternations, and 
    11. sequents are conditionals of conjunctions with alternations.

6. The Boolean abbreviations are
    1. universal closures are negations of existential closures of negations,
    2. inclusions are universal closures of conditionals,
    3. converse inclusions are swapped inclusions,
    4. proper inclusions are conjunctions of inclusions with the negation of  their converse, and
    5. coextensions are conjunctions of inclusions with their converse.

7. The quantificational abbreviation is
    - universal croppings are negations of existential croppings of negations.

8. The recombic abbreviations are
    1. dushes are pushes of drops,
    2. props are drops of pushes,
    3. overs are pushes of hems,
    4. nops are drops of overs,
    5. oems are hems of overs,
    6. dupes are dushes of oems,
    7. pops are two drops of oems,
    8. nip ns are n pushes of drops of n pops,
    9. dips are nips of drops,
    10. digs are nips of hems,
    11. bury ns are n pushes of n digs, and
    12. unbury ns are n bury ns.

9. A (j,k,m)-resultant is j existential croppings of k bury (j+k)s of the conjunction of k drops with m props.

10. The first iterate of a predicate whose left shape is (j\*2) is itself and its (n+1)st iterate is the (j,j,0)-resultant of itself with the j nip (j\*2) of its n-th iterate.

11. A (i,m)-counterclockwise (CCW) turn is m pushes of a bury (i+m) of m pops.

12. Indiscernibility with respect to a predicate with left shape (i+1) and right shape m is i (existential croppings of bury 2s) of m (existential croppings of pops) of the conjunction of each biconditional of the drop of one of the up to i+m (i,m)-CCW turns with the nip of the same.

## 2025 0810 1739

More work on logic memo.

1. Predicates do or don't denote (are true or false of) where they occur, constructions compound them with connectives, and lexicons itemize atomic ones.

2. Grammars generate categories from recurrent constructions on lexicons.

3. Logic uncovers validities whose supplemented lexical substitutions (SLS) denote everywhere (Quine).

4. Constructions inherit where they denote from their components (Tarski):

5. alethically, i.e.
    1. joint denials denote where and only where (waow) each of their components don't,
    2. existential closures denote waow there is some where denoted by their component, and
    3. components of existential croppings denote the left of waow their compound denotes with some item with the right of the same whereabouts, and

6. recombically, i.e.
    4. drops denote the left of the left of waow the compound does with the right of the same whereabouts,
    5. pushes denote the left of waow the compound does, with the left of the right of the same whereabouts, with the right of the right of the same whereabouts, and
    6. hems denote the left of waow the compound does, with the right of the left of the left of the same whereabouts with the right of the same whereabouts.

7. The truth-functional, better 'denotative-functional', abbreviations are
    1. negations are joint denials of their component with itself,
    2. alternations are negations of joint denials of their (two) components,
    3. converse conditionals are alternations of their left component (consequent) with the negation of their right component (antecedent),
    4. complementary converse conditionals are negations of the converse conditional of their components,
    5. complementary conditionals are complementary converse conditionals of their right component with their left component,
    6. conditionals are negations of complementary conditionals of their components,
    7. alternative denials are conditionals of their left component with the negation of their right component,
    8. conjunctions are negations of alternative denials of their components,
    9. exclusive alternations are conjunctions of alternative denials of their components with alternations of their components,
    10. biconditionals are negations of exclusive alternations of their (two) components, and 
    11. sequents are conditionals of the conjunction of its antecedent components with the alternation of its consequent components.

8. The Boolean abbreviations are
    1. universal closures are negations of existential closures of negations of their component
    2. inclusions are universal closures of conditionals of their components,
    3. converse inclusions are universal closures of converse conditionals of theirs,
    4. proper inclusions are conjunctions of inclusions of theirs with the negation of their converse inclusion, and
    5. coextensions are conjunctions of inclusions with their converse inclusion.

9. The quantificational abbreviation is
    - universal croppings are negations of existential croppings of negations of their component.

10. The recombic abbreviations are
    1. overs are pushes of hems of their component,
    2. ohems are hems of overs of theirs,
    3. dushes are pushes of drops of theirs,
    4. props are drops of pushes of theirs,
    5. dupes are dushes of ohems of theirs,
    6. pops are drops of ohems of theirs,
    7. nips are dushes of hems of theirs,
    8. digs are nips of hems of theirs,
    9. buries are pushes of digs of theirs,
    10. bury ns are n pushes of n digs of theirs,
    11. unburies are two buries of theirs, and    
    12. unbury ns are n bury ns of theirs.

11. Pushes, pops, dupes, drops, buries, and unburies are recombically complete: any arrangement of any items can be set up from them.

12. An (i,j,k)-resultant is j existential croppings of k bury (j+k)s of the conjunction of k drops of the left component with i props of the right component.

13. An (m,n)-resultant of a pair of components where the right part of the shape of the left component and the left part of the shape of the right component are both k is k existential croppings of the conjunction of n props of k pops of the left component with n bury ks of n drops of the right component.

14. I originally thought that 13 would be simpler to write out than 12, but it is clear that 12 is simpler: this was all but bound to happen because this is precisely the definition which encapsulates the composition of stack operations and notation.

15. Problems to solve next:
    1. define (m,n)-resultants in terms of (i,j,k)-resultants
    2. define n iterates in terms of (i,j,k)-resultants
    3. the rest!

16. I've updated the hint [#how-logic-works](#how-logic-works) in accordance with work in this entry:
    > HOW LOGIC WORKS
    > 
    > 1. Predicates do or don't denote (are true or false of) where they occur, constructions connect them, and lexicons list atomic ones.
    > 2. Grammars generate categories from recurrent constructions on lexicons.
    > 3. Logic uncovers validities whose supplemented lexical substitutions (SLS) denote everywhere (Quine).
    > 4. Constructions inherit where they denote from their components (Tarski):
    > 5. alethically, i.e.
    >     1. joint denials denote where and only where (waow) each of their components don't,
    >     2. existential closures denote waow there is some where denoted by their component, and
    >     3. components of existential croppings denote the left of waow their compound denotes with some item with the right of the same whereabouts, and
    > 6. recombically, i.e.
    >     4. drops denote the left of the left of waow the compound does with the right of the same whereabouts,
    >     5. pushes denote the left of waow the compound does, with the left of the right of the same whereabouts, with the right of the right of the same whereabouts, and
    >     6. hems denote the left of waow the compound does, with the right of the left of the left of the same whereabouts with the right of the same whereabouts.

## 2025 0810 1522
More work on logic memo.

1. Predicates do or don't denote (are true or false of) where they occur, constructions compound them with connectives, and lexicons itemize atomic ones.

2. Grammars generate categories from recurrent constructions on lexicons.

3. Logic uncovers validities whose supplemented lexical substitutions (SLS) denote everywhere (Quine).

4. Constructions inherit where they denote from their components (Tarski):

5. alethically, i.e.
    1. joint denials denote where and only where (waow) each of their components don't,
    2. existential closures denote waow there is some where denoted by their component, and
    3. components of existential croppings denote the left of waow their compound denotes with some item with the right of the same whereabouts, and

6. recombically, i.e.
    4. drops denote the left of the left of waow the compound does with the right of the same whereabouts,
    5. pushes denote the left of waow the compound does, with the left of the right of the same whereabouts, with the right of the right of the same whereabouts, and
    6. hems denote the left of waow the compound does, with the right of the left of the left of the same whereabouts with the right of the same whereabouts.

7. The truth-functional, better 'denotative-functional', abbreviations are
    1. negations are joint denials of their component with itself,
    2. alternations are negations of joint denials of their (two) components,
    3. converse conditionals are alternations of their left component (consequent) with the negation of their right component (antecedent),
    4. complementary converse conditionals are negations of the converse conditional of their components,
    5. complementary conditionals are complementary converse conditionals of their right component with their left component,
    6. conditionals are negations of complementary conditionals of their components,
    7. alternative denials are conditionals of their left component with the negation of their right component,
    8. conjunctions are negations of alternative denials of their components,
    9. exclusive alternations are conjunctions of alternative denials of their components with alternations of their components,
    10. biconditionals are negations of exclusive alternations of their (two) components, and 
    11. sequents are conditionals of the conjunction of its antecedent components with the alternation of its consequent components.

8. The Boolean abbreviation is
    - universal closures are negations of existential closures of negations of their component.

9. The quantificational abbreviation is
    - universal croppings are negations of existential croppings of negations of their component.

10. The recombic abbreviations are
    1. overs are pushes of hems of their component,
    2. over2s are overs of overs of theirs,
    3. ohems are hems of overs of theirs,
    4. dushes are pushes of drops of theirs,
    5. dupes are dushes of ohems of theirs,
    6. drop2s are drops of drops of theirs,
    7. pops are drops of ohems of theirs,
    8. nips are dushes of hems of theirs,
    9. digs are nips of hems of theirs,
    10. dig2s are digs of digs of theirs,
    11. burys are pushes of digs of theirs,
    12. push2s are pushes of pushes of theirs,
    13. 2burys are push2s of dig2s of theirs,
    14. 2unburys are 2burys of 2burys of theirs,

11. The wording for 10 of burrying and unburrying has to be fixed to accomodate the standard way of speaking of iterated constructions by predciate functors e.g. 'bury 6' is short for '6 pushes of 6 digs' where '1 push' is 'push of', '2 pushes of' is 'push of push of', and '1+n pushes' is 'push of n pushes' and so on.

12. Furthering 11, '4 bury 6' is short for 'bury 6 of 3 bury 6' etc.

13. The phrase 'bury 6' can be seen in two ways:
    1. the top six items on the stack are being put under the seventh item, or
    2. the seventh item is being unburried.

14. This correspondence has caused me confusion
    

## 2025 0809 1606

1. Happily, I received my copy of Rudolf Carnap's 1958 "Introduction to Symbolic Logic and its Applications".

2. I shall write up my notes on it here as it occurs to me.

3. Importantly, the book can be bought from Dover and is hence not nearly as expensive as other technical books of this caliber are.

4. As with all of Carnap's technical writings, there are ample divisions of his writing from parts all the way down to subsections.

5. The book begins with Part One "System of Symbolic Logic" Chapter A "The Simple Language A" Section One "The Problem of Symbolic Logic" subsection 1a "The purpose of symbolic language".
    > "Symbolic logic (also called mathematical logic or logistic) is the modern form of logic developed in the last hundred years." page 1.

6. It is in this first sentence that there is already a hint at where Carnap goes wrong and where he goes right.
He goes wrong in that he does not distinguish between the methods of logic and those of mathematical logic (or what he also calls logistic).
He goes write in that he presents much of what logic has contributed to mathematics over the hundred years prior to his publication.

7. The nail in the coffin is the third sentence:
    > "Such a system is not a theory (i.e. a system of assertions about objects), but a language (i.e. a system of signs and of rules for their use)."

8. It is interesting that he says this even though he directly references Quine's "Mathematical Logic" in the sources provided.
There, Quine goes out of his way in the last chapter to present his theory of protosyntax and to explain the ways in which it comes to inclusively cover the methods that so often fly under the flag of 'formal' or 'finite' or, perhaps more restrictively, 'constructive'.

9. That is, Quine shows that, in fact, formal languages are as much theories as geometries or algebras.

10. Any doubts about where Carnap is likely to go wrong are further dashed by the next sentence:
    > "We will so construct this symbolic language that into it can be translated the sentences of any given theory about any objects whatever, provided only that some signs of the language have received determinate interpretations such that the signs serve to designate the basic concepts of the theory in question. So long as we remain in the domain of pure logic (i.e. so long as we are concerned with building this language, and not with its application and interpretation respecting a given theory), the signs of our language remain uninterpreted. Strictly speaking, what we construct is not a language but a schema or skeleton of a language: out of this schema we can produce at need a proper language (conceived as an instrument of communication) by interpretation of certain signs."

11. The use of 'symbols', 'signs', and, especially, 'designates' is the end of the schematic methods of logic: logic is not about anything in particular and its schematic letters no more symbolize, signal, or designate things than do our native sentences mean their meaning, inform their information, or propose their proposition.

12. Theories of meaning, informing, and proposing, along with theories of attributing and classing are either logical or not, and it is those methods that make up the logic of a theory which are properly dealt with in the two step process of schematics introduced by von Neumann in 1927 âZur Hilbertschen Beweistheorieâ.
Though, there he uses "Zeichen" whose PIE root is *deik-
    > <https://www.etymonline.com/word/*deik->
    >
    > *deik-
    > 
    > Proto-Indo-European root meaning "to show," also "pronounce solemnly," "also in derivatives referring to the directing of words or objects" [Watkins].
    >
    > It might form all or part of: abdicate; abdication; addict; adjudge; apodictic; avenge; benediction; betoken; condition; contradict; contradiction; dedicate; deictic; deixis; dictate; diction; dictionary; dictum; digit; disk; ditto; ditty; edict; Eurydice; index; indicate; indication; indict; indiction; indictive; indite; interdict; judge; judicial; juridical; jurisdiction; malediction; malison; paradigm; policy (n.2) "written insurance agreement;" preach; predicament; predicate; predict; prejudice; revenge; soi-disant; syndic; teach; tetchy; theodicy; toe; token; valediction; vendetta; verdict; veridical; vindicate; vindication; voir dire.
    > 
    > It might also be the source of: Sanskrit dic- "point out, show;" Greek deiknynai "to show, to prove," dikÄ "custom, usage;" Latin dicere "speak, tell, say," digitus "finger," Old High German zeigon, German zeigen "to show," Old English teon "to accuse," tÃ¦can "to teach."

13. 'Zeichen' can be read as 'sign', 'symbol', 'token', 'mark', or even 'character'.
It is the last reading 'character' that comes out in schematic methods as 'letters', and all that the marks mark, signs signal, or symbols symbolize is recurrence of place in quotation upon which the operation of substitution (as given by von Neumann) is to be 'executed'.
The german is 'ausfÃ¼hren' as in 'einen Plan ausfÃ¼hren' ('to carry out a plan') or 'ein Programm ausfÃ¼hren' ('to execute a program').

14. No matter what I may say, the second paragraph makes clear the problem with Carnap's approach by stating his approach clearly and exactly:
    > "In the present Chapter A, the first of three chapters comprising Part One, we describe a simple symbolic language A containing the following sorts of signs (to be explained later): sentential constants and variables, individual constants and variables, predicate constants and variables of various levels and types, functor constants and variables, sentential connectives, and quantifiers." page 1.

15. Predicate letters in truth function and quantification schema are neither constants nor variables.
Variables are pronouns (perhaps bound as in a relative clause) and they occur as much in schema as they do in instances of the same.
Not even von Neumann made this distinction, and many are prepared to use a new set of letter forms to speak of 'variable letters' as dummy variables!

16. Free variables can even come to play the part of names or constants or, best, singular terms, but in the schematic method, they are at best dummy singular terms, and can perhaps better be called 'pronoun letters' if we follow the dummy variable path, or, perhaps better, pronouns which no more dangle than proper nouns do to those inadept at a language.

17. All of this is to say that to take predicate letters as variables is to misunderstand the power of schematic methods and to mix theory with logic.

18. With these warnings in mind, a delightful distinction characteristic of Carnap's sharp wit, occurs on page 2:
    > "In the course of constructing our symbolic language systems, it frequently happens that a new precisely-defined concept is introduced in place of one which is familiar but insufficiently precise. Such a new concept is called an *explication* of the old one, and its introduction an *explication*. (The concept to be explicated is sometimes called the *explicandum*.) ... (FOr a more complete exposition of the methods of explication and the requirements an adequate explicatum must meet, see Carnap (Probability), Chapter I.)."

19. So it is that Carnap has put yet another book on my reading radar.

20. The question of 'explanation' and 'explication' is one dealt with by Quine, e.g. in "From Stimulus to Science" (FSTS), as
    > "The reduction of the mental to the physical, or indeed of arithmetic to set theory, can be characterized in either of two ways: as *explaining* or as explaining *away*. There is no difference, but the first phrasing has a gentler ring. To have *rupidiated* the life of the mind seems harsher than to have *explained* it in physical terms.
    >
    > In some domains the harsher phrasing is better, notably in the various ways of defining natural numbers in set theory. For Frege, twelve was (nearly enough) the class of all dozens; for von Neumann it was the class fo the first twelve natural numbers, from zero through eleven. It cannot be both. We can resolve the dilemma by not identifying twelve with either. We can do without the natural numbers; their work can be done by Frege's classes or, equally, by von Neumann's. We proceed to use one or the other set of surrogates, or some thrid, and conveniently callt eh surrogates by the newly vacated numerical names. We have solved the puzzle by speaking of *alternative eliminations* rather than *conflicting explanations*. But there is no real difference between the two characterizations." pg. 86-87 Quine "From Stimulus to Science".

## 2025 0808 2217

1. Going over Carnap's logical methods in his language C.

2. Definition D28-1 of section 28 "Compound Predicate Expressions" occurs on page 107 of "Introduction to Symbolic Logic and its Applications" (ISLA):
    > To legitimatize this use of connectives in building up compound predicate expressions we introduce the following definitions for predicate expressions of the simplest type. Analogous definitions are understood to hold for each other type of predicate variable.
    >
    > D28-1.
    > 1. (not F)x if and only if not (Fx)
    > 2. (F or G)x if and only if (Fx or Gx)
    > 3. (F and G)x if and only if (Fx and Gx)
    > 4. (F only if G)x if and only if (Fx only if Gx)
    > 5. (F if and only if G)x if and only if (Fx if and only if Gx).

3. As Carnap has set up his language, such "open sentences" as those in D28-1 are read as if they were ammended by a universal closure.

4. The expedients of D28-1 are employed to great effect by Quine in "Methods of Logic 4th edition" in the chapter named "Boolean Schemata".
Quine adds all that comes with distinguishing between schematisms and symbolisms.

5. Carnap introduces similar notational expedients in his "Logical Syntax of Language".
I recall reading it and know that it is less than half way through the book, but I do not recall the actual page or chapter.

6. I have not ventured into Carnap's methods of typing, but give the following selection from two paragraphs prior to that containing D28-1 and on page 106:
    > "The new compounds are as follows: '(P or Q)a' is counted as an abbreviation of the sentences 'Pa or Qa', and 'P or Q' treated as a predicate expression of the same type as 'P', viz. a one-place predicate expression of the first level and of type (0);"

7. Typing, as far as I am concerned, is a matter of shaping up predicates in the same way that the J programming language shapes up arrays and extends operations with its clever little rank operation.

8. Note, Carnap's book appeared before Gerhard Gentzen's introduction of the upside down 'A' as the mark of universal quantification from his works in 1935 'Untersuchungen Ã¼ber das logische SchlieÃen', in English 'Investigations into Logical Deduction I and II", had spread.
I do not use anything like such notation and have gone ahead and translated the obscure marks into their pidgin English.

9. Carnap introduces universal closures (as they are now called) rather than existental closures (as I tend to start with when presenting the Boolean part of logic following the pattern of Quine, but with particular emphasis on the proper distinction between existential closures and existential croppings which is of great significance for those who, like Russell in "An Inquiry into Meaning and Truth" and "Human Knowledge: Its Scope and Limits", rightfully struggle with the problems of individuation, mereology, and a few other phrases that I have fogotten but which are mixed up with this ancient problem of forests and trees):
    > "28b. Universality. A property of individuals is called universal provided every individual has this property. Correspondingly, in the terminology of classes: a class of individuals is universal provided every individual belongs to th is class. Generally, a class of any type is said to be universal if each entity of that type belongs to this class. Our symbol for universality is 'U'; and 'the class (or property) P is universal' is rendered 'U(P)'. Since 'U(P)' is clearly synonymous with '(x)Px', the following definition is natural:
    > 
    > D28-2. U(F) if and only if (x)Fx.
    > 
    > Analogous definitions are understood to hold for predicates of any other type, be they one-place or many-place. E.g. '(x)(y)Rxy' can be abbreviated 'U(R)'."

10. The last sentence of the quote in 9 indicates that what is there defined as 'universal' is what we now call 'universal closure'.

11. The quote in 9 is also presented as part of the problem with the frequent attempt to mix schematism with symbolism as if predicates name their extension or attribute/property.

## 2025 0808 1614

1. Carnap's methods in "Introduction to Symbolic Logic and its Applications" (ISLA) are the closest thing to the elaborate evolution of predicate functor logic that I've had in my written notes for some time.
Today I'll be going through the parts that are relevant to me.

2. I shall be receiving a print copy of ISLA over the next few days (perhaps even tomorrow): it is much easier for me to read a book than it is for me to read a screen, especially when it comes to cross referencing entries in mathematical or scientific texts.

3. Carnap introduces "Compound Predicate Expressiosn" in section 28 under the chapter "The Extended Language C".

4. Carnap does not distinguish between schematic methods, with its dummy sentence and predicate letters, from symbolic methods, with its designating sentence and predicate names.
The presupposition that predicates are names of properties or extensions is, as Quine repeatedly emphasized and which was first made crystal clear through von Neuman's schematic methods in 1927 "Zur Hilbertschen Beweistheorie", is resolved by presenting a logical theory of properties or extensions which purport to carry over the logical work of predicates to the items of their respective theories.

5. There is a historic problem that has persisted since the earliest days of philosophical record: attributing properties to things.

6. Sadly, attributing a property to a thing has been conflated with predicating a predicate of it.
Properties are sometimes even defined as that which can be predicated of an object.

7. It helps to distinguish between 'attributing attributes' and 'predicating predicates' and 'classing classes'.

8. Historically, all three of these methods have been dealt with as if they were one and the same or as if passage from one to the other goes without saying.

9. Classes are extensions of predicates, each item classed to a class is so clased because its name predicated to the predicate of which the class is its extension is true.

10. In the past, it did not make sense, e.g. to Bertrand Russell, that the integral phrases that make up a predicate and a subject of a sentence might carry the load once carried by properties and concepts, better, attributes and classes.

11. There was the problem of meaning: how could a sentence have ever appeared without the stuff which the sentence purports to be about?

12. Such stuff was to be attributes in Russell and Whitehead's "Principia Mathematica" and from there, the classes were constructed through what I am here calling 'predicates' (following John Stuart Mill through Quine, as integral words or phrases) and names (or what are often called 'singular terms').

13. For Russell and Whitehead, what I call 'predicates' were 'propositional functions'.

14. Propositions are the attributes (or properties) of logic, according to those like Russell: they are what are true or false and sentences simply express this propositional property.

15. It was Frege who pointed the way through predicates (as I take them) from attributes to classes: predicates, as the notation of Frege's Begriffsschrift, were part of the logic of truth-functions and quantifications and hence subject to the extensionality of such a logic, and thus Russell could round up the attributes (which he thought he knew better than he knew predicates) through propositions, as attributes of attributing attributes of things, and propositional functions, as functions which when applied to things attributes an atribute of it i.e. metaphorically takes a thing and returns an attribute (in this case a proposition).

16. Looking back it can be hard to see how this path was preferrable e.g. Russel's own paradox prohibits such free wheeling shifts between attributes, predicates, and classes.

17. All of this confusion remains to this day, and is present in Carnap's ISLA.


## 2025 0807 2259

1. I have the great delight of finally running into Carnap's 1958 "Introduction to Symbolic Logic and its Applications" (ISLA)!
    - <https://archive.org/details/rudolf-carnap-introduction-to-symbolic-logic-and-its-applications>

2. So much of what Carnap accomplished with language C of ISLA falls squarely under the more restricted methods of predicate functor logic as given, e.g., in my last note.

3. It is extremely exciting to find someone else who went to all the trouble to collate the many results that bring together the methods of truth-functional, quantificational, predicate abstraction, and predicate functor logic.

4. What I am beginning to see with greater clarity is that the methods pointed up, e.g., in my last note bring together the methods of relation algebras uncovered by Tarski and the methods of logic uncovered by Carnap, through the careful methods of logic collated by Quine.

5. So much of Quine follows Carnap that it is tempting to point past Quine to Carnap and leave it there: this would be a huge mistake.

6. Carnap hastily generalized, as did Church and his followers, the power of lambda abstraction far beyond anything permitted by the methods of logic: such full blown assemblies of set theoretic powers continue to go unntoiced by modern mathematicians.

7. The method of predicate abstraction as the logical import of the grammarians relative clause clears up the problems caused by taking lambda abstraction as something that may be carelessly carried over beyond a given theory of lambda abstraction and application.

8. For all the sensitivity that Carnap had to the hierarchy of languages, and for all that he promoted with his existentional thesis, he was too quick to take his notation for a ride.

9. I have just this day come to see that perhaps the primary importance of the methods of logic, e.g. those started in the last note, is that notation is entirely avoided.

10. There is a cost and there is a benefit; the cost is that nonnative speakers of the language of predicate logic I've introduced are unlikely to follow the later parts without first going over the earlier parts, the benefit is that no single notation is going to mislead me, or any other reader, into seeing metaphors that are not there.

11. The classic example of a miscarriage of metaphor in the methods of logic is the introduction of quantifiers which purport to bind predicate letters in a schema of predicate logic: this is simply incoherent and reveals that those inclined to bind predicate letters have yet to master the schematic methods so clearly and exactly set out by von Neumann's 1927 "Zur Hilbertschen Beweistheorie".

12. As I reach further out from these new beginnings I hope to take a closer look at Ernst SchrÃ¶der's work.

13. SchrÃ¶der, E., 1890â1905. Vorlesungen Ã¼ber die Algebra der Logik, 3 vols. Leipzig: B.G. Teubner. Reprints: 1966, Chelsea; 2000, Thoemmes Press.
    - Ernst SchrÃ¶der 1890 Vorlesungen Ã¼ber die Algebra der Logik (Exakte Logik), Volume 1,
        - <http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN717192873>
    - Ernst SchrÃ¶der 1891 Vorlesungen Ã¼ber die Algebra der Logik (Exakte Logik), Volume 2, Abt. 1
        - <http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN717193640>
    - Ernst SchrÃ¶der 1905 Vorlesungen Ã¼ber die Algebra der Logik (Exakte Logik), Volume 2, Abt. 2
        - <http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN717194256>
    - Ernst SchrÃ¶der 1905 Algebra und Logik der Relative, der Vorlesungen Ã¼ber die Algebra der Logik 3, Volume 3, Abt. 1
        - <http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN717195317>

14. I have read that much of Russell and Whitehead's work in Principia Mathematica was drawn from SchrÃ¶der's work and would like to see that for myself.

15. It remains a great sadness that the works of those like Hilbert and SchrÃ¶der are still not yet translated into English.
It is quite a strange thing to live in a world which professes its advanced methods without having any of the evidence one would reasonably expect from such purported advancements.

16. Another text to triangulate with is Tarski's "On the Calculus of Relations"
    - <https://www.cl.cam.ac.uk/teaching//1415/Databases/Tarski_1941.pdf>

17.  Vaughn Pratt The Origins of the Calculus of Relations suggests Peirce
    > "Collected Papers of Charles Sanders Peirce edited by Hartshorne
and Weiss and published by Harvard University Press
in the 1930â"

18. Additionally, Steven Givant's 2006 "The Calculus of Relations as a Foundation for Mathematics"
    - <https://sci-hub.st/10.1007/s10817-006-9062-x>

19. I've read some of SchrÃ¶der 1905 Volume 3 before, but it fits together with this emerging story better than it once did (I peeked at it and saw a host of visulaizations of relations that I have seen before).

## 2025 0807 1405

1. Predicates do or don't denote (are true or false of) where they occur, constructions connect them, and lexicons list atomic ones.
2. Grammars generate categories from recurrent constructions on lexicons.
3. Logic uncovers validities whose supplemented lexical substitutions (SLS) denote everywhere (Quine).
4. Constructions inheret where they denote from their components (Tarski):
5. alethically, i.e.
    1. joint denials denote where and only where (waow) each of their components don't,
    2. existential closures denote waow there is some where denoted by their component, and
    3. components of existential croppings denote the left of waow their compound denotes with some item with the right of the same whereabouts, and
6. recombically, i.e.
    4. drops denote the left of the left of waow the compound does with the right of the same whereabouts,
    5. pushes denote the left of waow the compound does, with the left of the right of the same whereabouts, with the right of the right of the same whereabouts, and
    6. hems denote the left of waow the compound does, with the right of the left of the left of the same whereabouts with the right of the same whereabouts.

7. The truth-functional, better 'denotative-functional', abbreviations are
    1. negations are joint denials of their component with itself,
    2. alternations are negations of joint denials of their (two) components,
    3. converse conditionals are alternations of their left component (consequent) with the negation of their right component (antecedent),
    4. complementary converse conditionals are negations of the converse conditional of their components,
    5. complementary conditionals are complementary converse conditionals of their right component with their left component,
    6. conditionals are negations of complementary conditionals of their components,
    7. alternative denials are conditionals of their left component with the negation of their right component,
    8. conjunctions are negations of alternative denials of their components,
    9. exclusive alternations are conjunctions of alternative denials of their components with alternations of their components,
    10. biconditionals are negations of exclusive alternations of their (two) components, and 
    11. sequents are conditionals of the conjunction of its antecedent components with the alternation of its consequent components.

8. The Boolean abbreviation is
    - universal closures are negations of existential closures of negations of their component.

9. The quantificational abbreviation is
    - universal croppings are negations of existential croppings of negations of their component.

10. The recombic abbreviations are
    1. overs are pushes of hems of their component,
    2. over2s are overs of overs of theirs,
    3. ohems are hems of overs of theirs,
    4. dushes are pushes of drops of theirs,
    5. dupes are dushes of ohems of theirs,
    6. drop2s are drops of drops of theirs,
    7. pops are drops of ohems of theirs,
    8. nips are dushes of hems of theirs,
    9. digs are nips of hems of theirs,
    10. dig2s are digs of digs of theirs,
    11. burys are pushes of digs of theirs,
    12. push2s are pushes of pushes of theirs,
    13. 2burys are push2s of dig2s of theirs, and
    14. 2unburys are 2burys of 2burys of theirs.

11. I've updated [#how-logic-works](#how-logic-works) in accordance with the updates here.

## 2025 0806 2002

1. The problem of closures and croppings has not yet been effectively resolved.

2. Existential closures denote where and only where (waow) there is somewhere denoted by their component.

3. Traditionally, closures are defined from croppings as iterated croppings which "close" the shape of their component predicate.
Such closed predicates are most often called closed sentences, but in predicate (functor) logic there is no longer a principle role for sentences (be they open or closed) and we are left speaking of predicates alone rather than dealing with the trouble of the grammatical construction called predication or the grammatical category of variables (thus there is also no confusion over the problem of a finite lexicon which contains a finite list of variables and constructs the remaining category of variables by way of, e.g., accentuation).

3. Components of existential croppings denote the left of with some item with the right of waow the compound does.

4. Note, the definition of croppings involves the complex singular descriptions 'the right of' and 'the left of' which are ultimately compounded, in 3, with 'where' as in 'the left of where and only where the compound does with some item with the right of the same whereabouts'.

5. An alternative to 3 is this: components of existential croppings denote the of waow the compound denotes with some item with the right of the same whereabouts.

6. Further alternatives may rely on the introduction of 'predicate' as that which does or doesn't denote where it occurs by giving the definition of 'predicate' rather as that which does or doesn't denote a given occasion.

7. The introduction of occasions is misleading though, as the problem of individuation is bound up with spatiotemporal whereabouts.

## 2025 0806 1439

1. A suite of abbreviations slowly factors out the methods of logic which follow from
    > HOW LOGIC WORKS
    > 
    > 1. Predicates do or don't denote (are true or false of) where they occur, constructions connect them, and lexicons list atomic ones.
    > 2. Grammars generate categories from recurrent constructions on lexicons.
    > 3. Logic uncovers validities whose supplemented lexical substitutions (SLS) denote everywhere (Quine).
    > 4. Constructions inheret where they denote from their components (Tarski):
    > 5. recombically, i.e. components of
    >    - drops denote teh left of the left of with teh right of where and only where (waow) their compound does,
    >    - pushes denote the left of with the left of the right of with the right of the right of waow the compound does,
    >    - hems denote the left of with the right of the left of with the right of waow the compound does;
    > 6. or alethically, i.e.
    >    - each component of existential joint denials doesn't denote the left of with some same item with the right of waow the compound does.

2. The recombic abbreviations are
    1. overs are hems of pushes of their component,
    2. over2s are overs of overse of theirs,
    3. ohems are overse of hems of theirs,
    4. dushes are drops of pushes of theirs,
    5. dupes are ohems of dushes of theirs,
    6. drop2s are drops of drops of theirs,
    7. pops are ohems of drops of theirs,
    8. nips are hems of dushes of theirs,
    9. hips are hems of nips of theirs,
    10. hip2s are hips of hips of theirs,
    11. swaps are hips of pushes of theirs,
    12. push2s are pushes of pushes of theirs,
    13. tors are hip2s of push2s of theirs, and 
    14. rotes are tors of tors of theirs.

3. The truth-functional, better 'denotative-functional', abbreviations are
    1. joint denials are existential joint denials of drops of their (two) components,
    2. negations are joint denials of their component with itself,
    3. alternations are negations of joint denials of their (two) components,
    4. converse conditionals are alternations of their left component (consequent) with the negation of their right component (antecedent),
    5. complementary converse conditionals are negations of the converse conditional of their components,
    6. complementary conditionals are complementary converse conditionals of their right component with their left component,
    7. conditionals are negations of complementary conditionals of their components,
    8. alternative denials are conditionals of their left component with the negation of their right component,
    9. conjunctions are negations of alternative denials of their components,
    10. exclusive alternations are conjunctions of alternative denials of their components with alternations of their components,
    10. biconditionals are negations of exclusive alternations of their (two) components, and 
    11. sequents are conditionals of the conjunction of its antecedent components with the alternation of its consequent components.

5. The quantificational abbreviations are
    1. universal croppings are negations of existential joint denials of their component with itself, and
    2. existential croppings are negations of universal croppings of negations of their component.

6. I am confronted with the problem of closed quantifications.
In the past I have made it clear that the truth functional, or what is better called the denotative functional, connectives combine with a predicate functor of existential closure, as explained by Quine in "Methods of Logic 4th edition" chapter 18 titled "Boolean Schemata".

7. The steps in this slower development are closer to each of the problems which have come up in the history of mathematics and its logic e.g. mereolgical problems, collections, assemblies, wholes, parts, individuation.

8. Do not forget S2S and Rabin's 1969 "Decidability of Second-order Theories and Automata on infinite trees".

 
## 2025 0805 1421

1. I finally succeeded in assembling the meat of predicate functor logic independent of truth functional and quantificational logic.

2. Once upon a time, a woman taught a man to speak: she held up an apple, said 'red', he held it up, said 'red', she smiled, he held up another, said 'red', she frowned, held it up, said 'green', etc.

3. Predicates do or don't denote (are true or false of) where they occur, constructions connect them, and lexicons list atomic ones.

4. Grammars generate categories from recurrent constructions on lexicons.

5. Logic uncovers validities whose supplemented lexical substitutions (SLS) denote everywhere (Quine).

6. Constructions inheret where they denote from their components (Tarski): recombically, i.e.
    1. components of drops denote teh left of the left of with teh right of where and only where (waow) their compound does,
    2. components of pushes denote the left of with the left of the right of with the right of the right of waow the compound does,
    3. components of hems denote the left of with the right of the left of with the right of waow the compound does;

    or alethically, i.e.
    4. each component of existential joint denials doesn't denote the left of with some same item with teh right of waow the compound does.

7. This will now be added to a hint.

8. I'm editing the hint "how to make it logical"
    > HOW TO MAKE IT LOGICAL
    > 1. List the nouns, adjectives, and verbs used to talk about it.
    > 2. Define as many from as few as you can.
    > 3. List true sentences made from those few.
    > 4. Conclude as many from as few as you can.
    > 5. Go to 1.

9. It now reads
    > HOW TO MAKE IT LOGICAL
    > 
    > 1. List the words used to talk about it.
    > 2. Define as many from as few as you can.
    > 3. List true sentences made from those few.
    > 4. Conclude as many from as few as you can.
    > 5. Go to 1.

10. I have added this hint
    > HOW LOGIC WORKS
    > 
    > 1. Predicates do or don't denote (are true or false of) where they occur, constructions connect them, and lexicons list atomic ones.
    > 2. Grammars generate categories from recurrent constructions on lexicons.
    > 3. Logic uncovers validities whose supplemented lexical substitutions (SLS) denote everywhere (Quine).
    > 4. Constructions inheret where they denote from their components (Tarski):
    > 5. recombically, i.e. components of
    >    - drops denote teh left of the left of with teh right of where and only where (waow) their compound does,
    >    - pushes denote the left of with the left of the right of with the right of the right of waow the compound does,
    >    - hems denote the left of with the right of the left of with the right of waow the compound does;
    > 6. or alethically, i.e.
    >    - each component of existential joint denials doesn't denote the left of with some same item with the right of waow the compound does.

## 2025 0804 1726

1. Validity can be defined in purely grammatical terms as transformable into a certain grammatical category with respect to some finite list of transformations.

2. These are frequently paraphrased as theories of calculi, e.g. syntactic rules of inference, better, of concatenation, e.g. binary strings, or, better still, trees, e.g. binary trees.

3. Truth functional inconsistency is established by full alternational or conjunctive normal form.

4. Quantificational inconsistency is established by teh chance construction of a truth-functionally inconsistent conjunction via Quine's main method (a sequence of transformations which takes a conjunction to endlessly longer ones).

5. It is important to distinguish between these grammatical definitions and the methods of proof that are so often confused with such grammatical methods in the same way that the predicate of implication is confused with the grammatical particle of conditional compound sentences 'only if'.

6. Why analyze validity into grammar and denotation?
Why not stick with proof, or with the related definition relative to special grammatical constructions as above?

7. To show that validity is as transcendent (or immanent) as grammar and denotation are.

8. The above line of thinking is what led me to ask: does the purported predicate 'grammatical' fall prey to Grelling's paradox?

9. The answer is no, but perhaps only with respect to one analysis of 'is grammatical' as defined from the predicate of lexicality and the predicates of grammatical construction e.g. 'is lexical', 'conjoins', 'negates', 'existential 

## 2025 0802 1540

1. This note records my work on solving a problem of substitution: how to deal with notation such as '\-\-\-x\-\-\-' and 'Fx' where 'F' is a schematic predicate letter and 'x' is a pronoun (otherwise called a variable).

2. This is all relative to the methods of logic given by Quine in "Methods of Logic 4th edition" (MOL).

3. The first place where a notation like '\-\-\-x\-\-\-' is used in that book is actually when 'p', 'q', and 'r' are used as dummy closed sentences or schematic closed sentence letters e.g. '\-\-\-p\-\-\-'.

4. It first appears on page 63:
    > "But there are useful laws of interchange, the least of which is the *first law of interchange*: Think of '. . . p . . .' as any schema containing 'p', and of '. . . q . . .' as formed from '. . . p . . .' by putting 'q' for one or more occurrence of 'p'; then 'p if and only if q' implies '. . . p . . . if and only if . . . q . . .'." pg. 63-64 MOL

5. The schema mentioned in the quotation in 4 is only truth-functional.

6. The letters 'p' and 'q' are first introduced as part of the methods of logic in a way that is distinct from how they are dealt with in the chapter quoted from in 4.

7. It is in the third paragraph of the first chapter "Negation, Conjunction and Alternation" that single letters are introduced as notational expedients in the following way (the whole paragraph is given because it also gives a key part of the method of paraphrasing or regimenting characteristic of logic):
    > "In logical studies it is convenient to adopt a single sign of negation, consisting of the prefix '-', applied to statements as wholes. Thus '-(Jones is away)' means 'Jones is not away'; the parentheses here serve to group, in a single whole, the statement to which '-' is applied. The sign '-' might be translated into words as 'it is not the case that'; briefly it may be pronounced 'not'. When a statement is represented as a single letter 'p', as is commonly done in logical discussion, the sign of negation will be placed above instead of in front; thus we shall write '???' instead of '-p' for the negation of 'p'." pg. 9 MOL

8. What are here called 'statements' are later mentioned as 'sentences' by Quine because 'statement' still has the haze of meaning or proposition about it.
We know this from the second page of Quine's "Philosophy of Logic 2nd Edition" (POL):
    > "Some philosophers, commendably diffident about positing propositions in this bold sense, have taken refuge in teh word 'statement'. The opening question of this chapter illustrates this evasive use. my inveterate use of 'statement' in earlier books does not; I there used the word merely to refer to declarative sentences, and said so. Later I gave up the word in the face of the growing tendency at Oxford to use the word for acts that we perform in uttering declarative sentences. Now by appealing to statements in such a sense, instead of to propositions, certainly no clarity is gained. I shall say no more about statements, but will go on about propositions." pg. 2 POL

9. The etymology of 'statement' goes through the PIE \*sta\- as from <https://www.etymonline.com/word/*sta->
    > *stÄ-, Proto-Indo-European root meaning "to stand, set down, make or be firm," with derivatives meaning "place or thing that is standing."
    > 
    > It might form all or part of: Afghanistan; Anastasia; apostasy; apostate; armistice; arrest; assist; astatic; astatine; Baluchistan; bedstead; circumstance; consist; constable; constant; constitute; contrast; cost; desist; destination; destine; destitute; diastase; distance; distant; ecstasy; epistasis; epistemology; establish; estaminet; estate; etagere; existence; extant; Hindustan; histidine; histo-; histogram; histology; histone; hypostasis; insist; instant; instauration; institute; interstice; isostasy; isostatic; Kazakhstan; metastasis; obstacle; obstetric; obstinate; oust; Pakistan; peristyle; persist; post (n.1) "timber set upright;" press (v.2) "force into service;" presto; prostate; prostitute; resist; rest (v.2) "to be left, remain;" restitution; restive; restore; shtetl; solstice; stable (adj.) "secure against falling;" stable (n.) "building for domestic animals;" stage; stalag; stalwart; stamen; -stan; stance; stanchion; stand; standard; stanza; stapes; starboard; stare decisis; stasis; -stat; stat; state (n.1) "circumstances, conditions;" stater; static; station; statistics; stator; statue; stature; status; statute; staunch; (adj.) "strong, substantial;" stay (v.1) "come to a halt, remain in place;" stay (n.2) "strong rope which supports a ship's mast;" stead; steed; steer (n.) "male beef cattle;" steer (v.) "guide the course of a vehicle;" stem (n.) "trunk of a plant;" stern (n.) "hind part of a ship;" stet; stoa; stoic; stool; store; stound; stow; stud (n.1) "nailhead, knob;" stud (n.2) "horse kept for breeding;" stylite; subsist; substance; substitute; substitution; superstition; system; Taurus; understand.
    > 
    > It might also be the source of: Sanskrit tisthati "stands;" Avestan histaiti "to stand;" Persian -stan "country," literally "where one stands;" Greek histÄmi "put, place, cause to stand; weigh," stasis "a standing still," statos "placed," stylos "pillar;" Latin sistere "stand still, stop, make stand, place, produce in court," status "manner, position, condition, attitude," stare "to stand," statio "station, post;" Lithuanian stojuos "I place myself," statau "I place;" Old Church Slavonic staja "place myself," stanu "position;" Gothic standan, Old English standan "to stand," stede "place;" Old Norse steÃ°i "anvil;" Old Irish sessam "the act of standing.""


10. The etymology of 'sentence' goes through 'sense' to \*sent\- which I already dealt with in [#2025-0526-2141-the-first-two-words-of-paul-grahams-paper-good-writing](#2025-0526-2141-the-first-two-words-of-paul-grahams-paper-good-writing)

11. Neither 'statement' nor 'sentence' are effective, and as much is clear from Skinner's "Verbal Behavior" (VB).

12. Like later Quine, I avoid 'statement' and if I am to speak of 'utterences' then I am already close enough to behavior to switch over entirely from the philosophic, logic, and gramamtical traditions to that of the radical behaviorists e.g. 'form of response', 'topography of response', or, better, 'operanda'.
Perhaps 'operanda' is the best: it points up the problem of describing responses, recording responses, and all else that comes up when comparing one occurrence with another by switching from the effectors of an organism to the effect on the laboratory environment.

13. This is not a note on behavior and I shall abandon this path of exploration.

14. I tend to use 'sentence', but 'quotation' is probably better.
Let me look at its etymology.

15. First we have <https://www.etymonline.com/word/quote>
    >quote(v.) late 14c., coten, "to mark or annotate (a book) with chapter numbers or marginal references" (a sense now obsolete), from Old French coter and directly from Medieval Latin quotare "distinguish by numbers, mark off into chapters and verses," from Latin quotus "which in order? what number (in sequence)?," from quot "how many," from PIE *kwo-ti-, from pronominal root *kwo-.

16. Then the PIE <https://www.etymonline.com/word/*kwo->
    > *kwo- also *kwi-, Proto-Indo-European root, stem of relative and interrogative pronouns.
    > 
    > It might form all or part of: cheese (n.2) "a big thing;" cue (n.1) "stage direction;" either; hidalgo; how; kickshaw; neither; neuter; qua; quality; quandary; quantity; quasar; quasi; quasi-; query; quibble; quiddity; quidnunc; quip; quodlibet; quondam; quorum; quote; quotidian; quotient; ubi; ubiquity; what; when; whence; where; whether; which; whither; who; whoever; whom; whose; why.
    > 
    > It might also be the source of: Sanskrit kah "who, which;" Avestan ko, Hittite kuish "who;" Latin quis/quid "in what respect, to what extent; how, why," qua "where, which way," qui/quae/quod "who, which;" Lithuanian kas "who;" Old Church Slavonic kuto, Russian kto "who;" Old Irish ce, Welsh pwy "who;" Old English hwa, hwÃ¦t, hwÃ¦r, etc.

16. From which it is clear that quotations spell things out e.g. morphemes, responses, etc.

17. Spelling and concatenating and listing are close to quotations.

18. The etymology of 'spell' <https://www.etymonline.com/word/spell>
    > spell(v.1) early 14c., spellen, "read letter by letter, write or say the letters of;" c. 1400, "form words by means of letters," said in most etymology sources to be from Anglo-French espeller, Old French espelir "to mean, signify; explain, interpret," also "spell out letters, pronounce, recite."
    > 
    > This French word is from Frankish *spellon "to tell" or some other Germanic source, from Proto-Germanic *spellam (source also of Dutch spellen, Old High German spellon "to tell," Old Norse spjalla, Gothic spillon "to talk, tell").
    > 
    > The native cognate word is Old English spellian, spillian "to tell, talk, speak, discourse." Only Barnhart seems to allow that the modern English word is partially from the Old English one, due to the difference in sense.
    > 
    > Klein's sources suggest a possible origin for this Germanic group in a PIE root *spel- (2) "to say aloud, recite, speak with emphasis" and cognates in Greek apeilÄ "threat" among other words, but Beekes finds the suggestion "rather far-fetched."
    > 
    > Also in early use speldren, from Old French espeldre, a variant of espelir. Related: Spelled; spelling.
    > 
    > In early Middle English still "to speak, preach, talk, tell," hence such expressions as hear spell "hear (something) told or talked about," spell the wind "talk in vain" (both 15c.). The meaning "form words with proper letters" is from 1580s.
    > 
    > Spell out "explain step-by-step" is recorded from 1940, American English. Shakespeare has spell (someone) backwards "reverse the character of, explain in a contrary sense, portray with determined negativity."

19. In "From Stimulus to Science" (FSS) Quine describes quotations as spelling out the letters of a sentence:
    > "The quotation is just a name of the depicted sequence of letters and spaces, dissociated grammatically and semantically from the outlying text. The dissociation becomes still more graphic when we analyze the quotation explicity into names of the individual characters and spaces, linked by concatenation signs. ... 
    > 
    > We have a familiar word for this analysis of quotation: it is *spelling*. Spelled out, the words disappear and so do questions of substitutivity. Substitutivity within quotation is confusion of use with mention." pg. 94-95 FSS

7. In chapter 21 "The Bound Variable" a similar notation is used 

## 2025 0801 1103

1. Ambiguous abbreviations are not a proper problem of any theory, but rather of the logic applied to it.

2. Russell's contextual elimination of descriptions is ambiguous abbreviation: as much as such was noticed when he bound the context of a description in the same way a variable was bound by a quantifier.

3. Relative to a finite lexicon, contextual elimination of descriptions by minimal context is unambiguous in the theory, but still ambiguous in the logical methods e.g. schematic methods.

4. Quine follows Peano by introducing descriptional notation via the schematic premise of descriptions in the context of identity (being itself dependent upon, e.g., Wang's schematic premise of identity):
    > x is identical to (the F) if and only if some item is (y such that x is identical to y if and only if Fy).

5. Properly, a schematic premise is not a premise at all: each of its instances are.

6. Still, even here, 'G(the F)' abbreviates 'some item is (x such that Gx and x is identical to (the F))'.

7. Such an abbreviation is eliminated by a further schematic premise of descriptions in a context other than identity:
    > G\-\-\-the F\-\-\- if and only if some item is (x such that G\-\-\-x\-\-\- and x is identical to (the F).

8. Combining the two schematic premises from 4 and 7 into one schematic premise of description gives
    > \-\-\-the F\-\-\- if and only if some item is (x such that \-\-\-x\-\-\- and each item is (y such that x is identical to y if and only if Fy).

9. Note, Quine failed to give this full schematic premise when channeling Peano in "Free Logic, Description, and Virtual Classes".
He also failed to give first the schematic premise of predicate abstraction and concretion
    > ..u{..v: \-\-\-..v\-\-\-..x\-\-\- :..x}..y if and only if \-\-\-..u\-\-\-..y\-\-\-

    before then introducing the premise of exstentionality for classes (and sets, sets are classes that belong to some class)
    > each item is (x, y, and z such that each item is (u such that u in x if and only if u in y) and x in z, only if y in z)

    and before introducing the schematic premise of class abstraction
    > x in the class of {y such that Fy} if and only if Fx

    and he never demonstrates the relation between class abstraction and descriptions in any theory with a premise of extensionality for classes:
    > the class of {x such that Fx} is identical to the (y such that each item is (x such that x in y if and only if Fx))

    and, perhaps worse of all, in none of his texts, not even Mathematical Logic 4th edition (MOL), does he deal with the double life of BOTH predicate abstracts as class abstracts AND the copula of predication with the predicate 'in' of class membership (which I shall attempt to do here).

10. Much of the work required to bring predicate abstracts and class abstracts together, so that following Quine we can let predicate abstracts "moonlight" as names of their extensionality as classes, is found in "Set Theory and its Logic Revised Edition" (STL) by Quine.

11. He mentioned the mistakes he made in STL in MOL chapter 21 "The Bound Variable":
    > "In formulating set theory in 1940 Godel appealed informally to fictious supplementary objects taht he called *notions*. They were like classes but were not values of variables. They were just a manner of speaking avoidable by cirumlocution. In my Brazlil lectures of 1942 I pressed this fiction. I presented what I called the virtual theory of classes and relations, in which I used the notation fo set theory as far as I could without assuming classes. See *O Sentido da nova logica* section 51. Martin was urging the idea concurrently. In *Set Theory and Its Logic* (1963, 1967) I made extensive use of the virtual theory of classes as an auxiliary to the real theory of classes. It proved to be a valuable axuiliary, but I persisted in presenting it as a mere prolegomenon to set theory and as a sly partial simulation thereof, failing, as we all did, to appreciate that it should stand squarely in elementary logic as a regimentation of a fundamental feature of language, the relative clause." pg. 136 MOL

12. In recent notes, I've tended to use 'is' as the copula of predication and 'in' as a predicate present in schematic premises of extensionality of classes.



## 2025 0730 1943

1. Predicate abstracts and functors render naive set theory as neither naive nor set theory.
It becomes pure logic.
The missing link between Frege and this surprising conclusion is von Neumann's schematic methods.

2. There is no more to communication than effective action.

## 2025 0729 2237

1. The schematic premise of predicate abstraction and concretion is
    > ..x{..y:  \-\-\-..y\-\-\-..u\-\-\-  :..u}..v if and only if \-\-\-..x\-\-\-..v\-\-\-.

2. In addition to committing to quantifiers when specifying the restrictions on substitution for predicate letters in valid schema to generate valid schema, as noted in the last entry, Quine also forgot to mention the problem of overlapping bondage in netsted abstracts with the same bound variables e.g. '{x: Fx and x{x: Gx only if xHy :y}x}'.

3. The example in 2 introduces a new abbreviation: '{..x: \-\-\-}' for '{..x: \-\-\-:}'.
Whether this shall be later advised is unknown.

4. Quine deals with the problem of overlapping bondage in "Mathematical Logic Revised Edition" in the way given by Hilbert and Ackermann: the inner most occurrence of bondage prevails over the others.

5. There remains the problem of an abstract whose binding prefix or postfix contains no variable free in its component.
There also remains the problem of iterated abstracts e.g. '{x: {y: xFy}}'.

6. Neither the problem solved in 4 or the problems brought up in 5 are dealt with in "Methods of Logic 4th Edition".
It appears as if the methods from "Mathematical Logic Revised Edition" can be carried over in part, but there the distinction between 'matrix', or what is later called 'open sentence', and a closed sentence with what is otherwise taken as one composed by dangling pronouns needs to be dealt with well when it comes to their appearence in free logic i.e. when dealing with vacuous singular terms. 

## 2025 0729 2020

1. For now I adopt the two sided definition of predicate abstraction that most closely follows the patterns of development I uncovered in [#a-stack-notation-for-predicate-functor-logic-2025-0414-1626](#a-stack-notation-for-predicate-functor-logic-2025-0414-1626).

2. The schematic premise of concretion is
    > ..x{..y:  \-\-\-..y\-\-\-..u\-\-\-  :..u}..v if and only if \-\-\-..x\-\-\-..v\-\-\-.

3. It is given with such strange notation as '\-\-\-' to indicate that the schematic premise itself is in fact an additional schematism which, though it can be given a more careful grammatical construction, is distracting at this moment.

4. It is indispensible though for predicate abstracts themselves are the tool upon which substitution for predicate letters is achieved.
The rule is as follows:
    > "We may sum up the two restrictions symmetrically thus: Quantifiers of the substituted abstract must not capture variables of the schema in which the substitution takes place, and variables of the substituted abstract must not be captured by quantifiers of the schema in which the substitution takes place. These restrictions simply ward off confusions of variables which, if allowed, would cause substitution to deviate from tis intended purpose of interpreting term letters." pg. 162 Quine, Methods of Logic 4th Edition (MOL)

5. Alas, this rule as given by Quine is needlessly tied up with quantifiers in large part so that fewer edits had to be made to weave in predicate abstraction as the logical importa of the grammarian's relative clause without disrupting too much of the other text.

6. The symmetric rules can be translated as follows into the language of pure predicate abstract logic:
    > We may sum up the two restrictions symmetrically thus: abstracts of the substituted abstract must not capture variables of the schema in which the substitution takes place, and variables of the substituted abstract must not be captured by the abstracts of the schema in which the substitution takes place.

7. The main reason to promote the restrictions on substitution to pure predicate abstracts rather than to entertain quantifiers is to extinguish the mistake made by Frege when he introduced quantifiers and entangled them with the bondage and freedom of variables.

8. It also allows those who are interested in other predicate functors that might not conform to the old quantificational outlook to explore how deviant ones combine with the bound variables of predicate abstraction.
For example, the whole problem of scope in the practices of computer programming are a consequence of the methods of predicate abstraction.

9. Note, the schematic premise of concretion introduces predicate abstracts, such as '{u: u pairs x with y :xy}', as basic notation i.e. it ushers in a new gramamtical category by a new gramamtical construction and does so, by including the schematic premise of concretion in the contemplated theory, without enlarging the logic.
Concretion assures us that anything said with abstracts can be said without them by eliminating each abstract for its concretion via the appropriate premise.

10. But, there is some care that must be taken and which I have not taken myself: predicate abstracts can be introduced as abbreviations for quantificational sentences, does the abbreviation correspond to the notation introduced by the schematic premise of concretion?

11. Wang's schematic premise of identity guarentees it.

## 2025 0729 2018

My repeated comments that Quine was not as careful with the problem of vacuous singular terms as he was with vacuous validity in [#2025-0727-2102](#2025-0727-2102) was wrong: he makes repeated mention in, e.g. the chpater on identity, as he ramps up to his method presented in the chapter on the elimination of singular terms.

## 2025 0729 1947

1. Among von Neumann's many accomplishments (including the method of stored programs), perhaps his most concequential technique is his schematic method from his 1927 "Zur Hilbertschen Beweistheorie".

2. In section two "Die Formder Axiomenregel" he introduces the full technique of schematic methods summarized in this sentence:
    > "Es werden gewisse Schemata angegeben, d. h. gewisse Kombinationen von einfachen Zeichen, mit den Zeichen a,b,c,.. und xp, xq, .. sowie Subst derart, dass wenn wir fur a,b,c,... Formeln einsetzen und fur xp, xq,... Variable (unter Einhaltung der fur a,b,c,..; xp,xq,.. jeweils zu formulierenden Bedingungen) und dann die Subst ausfuhren, eine Normalformel entsteht.
    >
    > Diese im folgenden anzugebenden Schemata sind die Axiomenschemata. Die Normalformeln, die so entstehen, dass isn einem Axiomenschema a,b,c,..; xp,xq,.. auf die oben beschriebene Art ersetzt und die Substausgefuhrt werden, sind die Axiome." pg. 13-14 Mathematische Zeitschrift. 1927

3. Which purportedly translates to:
    > "Certain schemata are given, i.e., certain combinations of simple symbols, with the symbols a, b, c,... and xp, xq, .. as well as substs such that if we substitute formulas for a, b, c,... and variables for xp, xq,... (subject to the conditions to be formulated for a, b, c,...; xp, xq,...) and then execute the substs, a normal formula results.
    >
    > These schemata, to be given below, are the axiom schemata. The normal formulas that result from substituting a, b, c,...; xp, xq,... in an axiom schema in the manner described above and executing the substs, are the axioms."

4. The distinction between schema and sentences (or, worse, sense) has raised a host of problems for the modern logician navigating a world awash with mathematical logic without any hint of its history and hurtles.

5. A schematic premise is no premise at all but rather a template from which premises may be constructed or otherwise identified.

## 2025 0728 1847

1. 'x is {y: Fy}' for 'some item is (z such that z=x and some item is (y such that y=z and Fy))' (from pg. 17 of "Set Theory and its Logic Revised Edition" (STL) by Quine)

2. The abbreviation shown in 1 is from STL and is the full analysis of the various parts that variables play in a logic with predicate abstracts as primitive notation without actually introducing such basic notation.

3. Predicate abstracts can be introduced as primitive notation by taking the parts of the above abbreviation and casting them into the parts of a biconditional:
    - x is {y: Fy} if and only if some item is (z such that z=x and some item is (y such that y=z and Fy))

4. When such premises are contemplated, it is expedient to present rather the following named schematic premise of concretion:
    - x is {y: Fy} if and only if Fx

5. This schematic premise of concretion is in the same vein as Peano's schematic premise of description:
    - x=the F if and only if each item is (y such that x=y if and only if Fy)

6. Though it wasn't until von Neumann 1927

## 2025 0727 2308

Don't forget to go through all of the canonical forms of truth functional compounds e.g. <https://en.wikipedia.org/wiki/Boolean_function#Representation>.

## 2025 0727 2102

1. Here is a fresh attempt to gather together the methods of predicate abstracts and predicate functors from truth-functional and quantificational logic.

2. Perhaps unsurprisingly, the elimination of singular terms by singular descriptions is tied up with Wang's single schematic premise for a theory of identity.
I mention this to at least present some link between this note and the last one.
There are many wonderful things occurring at once as a result of multiple mounds of work merging towards a single mountain of progress.

3. A quick note on terminology.
Predicate functors break predicate logic into two grammatical categories: the recombic category and the alethic category.

4. Another note: all of predicate logic can be reduced to three predicate functors.
I do not yet know if it is possible for fewer than three predicate functors to suffice for all of predicate logic.
A reason to suspect this is the case is that Quine's theory of abstraction and inclusion, like Church's theory of abstraction and application, does the work of a theory of mathematical logic (in Quine's case the theory is that of sets).

5. Here a clear distinction must be made between logic and mathematical logic. the grammatical particales 'and' and 'or' combine with component sentences in a grammatical construction that builds conjunctions and alternations as compound sentences.
The unhappily named "boolean functions" designated by 'and' and 'or' are functions, they are singular terms, and they combine with other singular terms, such as those designated traditionally by 'truth' and 'falsity' as the so called 'truth values', to designate via a compound term like 'truth and truth' a truth value, in this example truth.

6. Inability to distinguish between 'and' as a grammatical particle and 'and' as a singular term designating a function lead to insufferable errors when arguments from predicate logic (where 'and' is a grammatical particle) are confalted with arguments from mathematical logic (where 'and' tends to designate a boolean function).

7. Surprisingly, much of truth-functional logic remains wedded to arguments which properly belong to mathematical logic e.g. whenever truth tables, trees, diagrams, or maps are invoked to establish the validity of a truth functional schemata.
The logical alternative has been available for some time: full alternational normal form and full conjunctive normal form are both accessible through an argument by equivalents: truth functional validity is established outright by direct inspection.

8. Wang's schematic premise of identity is "Fx if and only if some item is (y such that x is identical to y and Fy)".

9. Note,
    1. This biconditional was used by Quine in "Philosophy of Logic 2nd Edition" (POL) to argue for the elimination of singular terms, aka names, to demonstrate that singular terms occur only in the context of identity i.e. "Fx" is equivalent to "some item is (y such that x= y and Fy)" so that "x=" can be replaced by a one place predicate 'X' so that 'Fx' is equivalent to 'some item is (y such that Xy and Fy)'
    2. Two applications of Wang's schematic premise of identity are required to set up the full abbreviation of predicate abstraction in truth-functional and quantificational logic!

10. A third point is far more important and also related to observations made in the last note (and actually in the last few notes) is that the following schema are to be kept distinct if logic is to deal well with empty singular terms:
    1. Fx if and only if some item is (y such that x is identical to y and Fy)
    2. x such that Fx if and only if some item is (y such that x is identical to y and Fy)
    3. each item is (x such that Fx if and only if some item is (y such that x is identical to y and Fy))

11. These most recent notes have mostly been born from discovery expedititions into Quine's analysis of 1. singular terms, 2. identity and indiscernability, 3. descriptions, 4. predicate abstraction and concretion, 5. predication, and 6. predicate functors among other things.

12. It is 10.3 which stands out as the appropriate schema to go by the name "Wang's schematic premise of identity", a fact which is obliquely mentioned on pages 272 and 273 of Quine's "Methods of Logic 4th edition" (MOL).
It is oblique because Wang's schematic premise of identity is only hinted at as equivalent to Godel's schematic premises of identity in exercise 2 on page 273, and the last sentence on 273 is
    > "It is convenient sometimes to cite the axiom with its quantifier or quantifiers and soemtimes without, as the above examples shown."

13. The problem is, contrary to Quine's expectation, more than a matter of convenience as he unearths in his later paper "Free Logic, Description, and Virtual Classes" of 1994:
    > "For a more broadly applicable treatment of descriptions, then, I am turned back from Russell to Peano, who first brought descriptions into logic. It was Peano who introduced iota as well as epsilon into logic, both inverted and upright. For him singular description was premitive notation, and my definition of 'the F = x' was for him rather an axiom schema:
    > > (the F)=x if and only if each item is (y such that Fy if and only if y=x)
    >
    > Peano expressed the existence condition as 'some item is (x such that (the F)=x)' even as we, and invoked it as a condition whenever he used a singular description. David Hilbert followed him in this.
    > 
    > We can join company with Peano and Hilbert and Still adhere to my account of all singular terms as descriptions and my convention to use 'a', 'b', etc. as parameters for them all. Formally the resulting logic is the same, I believe, as Leblanc and Theodore Hailperin's logic of free terms, provided that like those authors we extend the logical axioms of identity unconditionally to singular terms." pg. 281-282 "Selected Logic Papers Enlarged Edition"

14. The extension hinted at in that last sentence is where the subitems of 10 most clearly come into play.
It is the halmark of the schematic methods of logic in application to the analysis of singular terms to let free variables play the part of dummy singular terms in the same way that sentence letters play the part of dummy sentences and predicate letters play the part of dummy predicates.
But, additional care must be taken when setting up terminology that deals with potentially empty singular terms i.e. the dummies must be managed in such a way that some interpretation may leave them without a designata!

15. This care was not quite taken by Quine overtly in MOL the same way that some care was taken overtly in MOL with the case of what I have called trivial validity which entertains the possibility of an empty universe by marking each existential as false and each universal as true and resolving the whole sentence by truth functional analysis to the truth value in the empty case.

16. Specifically, it is not until the third part of MOL that the problem of empty singular terms is even mentioned (in the first chapter/section of that part called "singular terms").
Then, unhappily still, it is not even hinted at as resolvable until three chapters later in "Elimination of Singular Terms" by a method of singular description to be further described here.

17. To link up MOL with Quine's later paper on "Free Logics" I must give a bit of MOL and link what I give up with what I gave from that paper.

18. I'll start backwards so as to "cut to the chase".
Just as checking the empty universe was the result of taking existentials as false and universals as true and resolving by truth functional analysis, so too is a similar method of 1. taking simple contexts as overtly true or false, 2. resolving by familiar contextual clues.
    > "Let us take up the problem, which has been looming for some time, of the truth value of such statements as 'Cerberus barks'. Falsity, as a sweeping answer covering all statements containing 'Cerberus', would be over-hasty: first because the statement 'There is no such thing as Cerberus', at least, is true; and second, because whatever statements we adjudge false must admit of compounds, e.g., their negations, whcih will be true. Truth, as a sweeping answer, would encounter parallel difficulties.
    >
    > Our deductive methods for singular terms throw no light on the question: for we already assume that the singular term names an object when we represent the singular term by a free varaiable, and we make the same assumption again when we adopt a descriptive premise for a description. Failing a named object, our methods show nothing, for what they purport to show rests then on a contrary-to-fact assumption. Common usage, moreover, likewise leaves us in the dark; for excepting such contexts as 'There is no such thing as Cerberus', a singular term is ordinarily used only when the speaker believes or cares to pretend that the object exists." pg. 278 MOL

19. This being the first time that the problem is fully mentioned and that the naive solutions are given and dismissed is an unhappiness.
Mostly because, in practice, the student of logic is troubled by such validities as
    1. Fx only if some item is (y such that Fy)
    2. each item is (x such that Fx) only if Fy

20. But, 19.1 and 19.2 come to be validities only in the crude way that what I have called trivial validities are explained away by Quine as having extralogical interest but yet also a logical resolution.

21. The next quote is much more important than I once expected having read it a dozen times or so before: it demonstrates the ways by which logic and science cross polinate each other above and beyond applications.
    > "Under ordinary usage, we saw (Chapter 3), truth values attach not to indicative conditionals as wholes but only to the consequents conditionally upon truth of the antecedents. Analogously, under ordinary usage truth values attach to contexts of singular terms for the most part only conditionally upon existence of the objects. But if we are to have a smooth logical theory we must fill such gaps, even though arbitrarily, in such a way that every statement comes to have a truth value. Thus it was that we conventionally extended teh concept of the conditionaly, in Chapter 3, so as to allow truth values generally to conditionals as wholes. An extension in the same spirit is needed now on the score of singular terms that do not name." pg. 278 -279

22. Here I must note that the previous chapter on descriptions already introduced and eliminated a tricky problem that isn't quite effectively distinguished from that mentioned in 18 and dealt with in spirit in 21.

23. It is the problem of descriptive (or described?) singular terms and nondescriptive (or undescribable?) singular terms as oppossed to the difference between empty and nonempty singular terms.

24. I am glad that I've written through these things because I had not made that distinction clear to myself before.

25. A singular term 'a' can be said to be described by a one place predicate 'F' if 'Fa and each item is (x such that a=x if and only if Fx)'.
As a predicate functor:
    - 'F describes x' for 'Fx and each item is (y such that x=y if and only if Fy)'

26. The definition in 25 can be simplified by noting that 'each item is (y such that x=y if and only if Fy)' implies 'x=x if and only if Fx' which is equivalent to 'Fx' so that when 'F describes x' for 'each item is (y such that x=y if and only if Fy)' then 'F describes x' implies 'Fx'.

27. But note!!! The argument in 26 depends on a definition of validity (and consequently of implication) that presupposes each free variable designates some item!!! That is, it will be seen that Quine's method of eliminating singular terms in MOL has to yield expected results.

28. Jumping ahead, predicates which belong to the lexicon when predicated of an empty singular term are taken as false.
So, in an empty universe, the universal 'each item is (x such that Fx)' is false for any predicate substituting the predicate letter 'F' and 'Fy' when 'F' is substituted by a predicate of the lexicon is false, but when 'F' is substituted by a compound predicate ...

29. I'm too sleepy to finish this now.


## 2025 0726 1710

1. There are two sets of notes to type up here: 1. the use of predicate abstracts in explaining free logics, and 2. a fuller and more compact exposition of the power of predicate functors to take over what was once relegated to (naive) set theory.

2. The first step from logic to set theory is the premise of extensionality of sets i.e. sets with the same members are the same.

3. But, before getting to logic, it's time to deal with the problem of open sentences and free logics.

4. The phrase "free logic" comes through Hugues Leblanc and Rolf Schock according to Quine's March 1994 paper "Free Logic, Description, and Virtual Classes" given on the occasion of Lebanc's seventieth birthday in Montreal.

5. Free logic is contrasted with logic in two points (both of which are to be shown to not actually distinguish the methods of free logic from those of logic itself) :
    1. Free logics do not presuppose that some item exists
    2. Free logics do not presuppose that each singular term designates some item and only that item.

6. It is said, e.g. by Leblanc, that the methods of logic technically include 
    1. a premise asserting the existence of some item e.g. 'some item is (x such that Fx only if Fx)'
    2. premises asserting that each singular term designates some item i.e. that each singular term (in modern terminology, which draws almost entirely from mathematical logic, these are logical constants and they are sometimes relegated to zero degree function terms).

7. Free logic is then contrasted with logic where it is (errnoeously) said to include such premises as describe in 6.1 nad 6.2.

8. The problem of the empty universe is dealt with in standard predicate logic by the common method of "checking the empty universe" by marking each existential as false and each universal as true and resolving the whole by truth functional analysis.

9. Thus, logic retains the methods of prenexing and purifying by the rules of passage, and addresses the free logicians concerns about an empty universe.

10. I take this distinction a bit further than Quine and simply distinguish between trivial validity and nontrivial validity where "validity" comes to be short for "nontrivial validitiy".
A sentence is trivially valid if true from each interpretation in an empty universe e.g. by the method described in 8 of checking the empty universe.
Then it is (nontrivially) valid if true from each interpretation in a nonempty universe i.e. by the familiar methods which embrace the rules of passage which otherwise are trivially nonvalid.

11. In "Methods of Logic 4th Edition" Quine gives a definition of validity outright relative to nonempty universes.
I find my method far more inclusive in terminology which also emphasizes the outlook of, e.g., the free logician.
So, if the free logician wishes to emphasize their outlook, they simply assert a sentence is trivially and nontrivially valid, where as the logician who is not particularly interested in trivial validity can stick with validities knowing that all free logicians are to read 'validity' as 'nontrivially valid'.

12. Note, it is here where I find myself siding with those who criticize Quine for his tight restriction on what to count as methods of logic i.e. Quine left out the empty case and accomodated it by external invocation of truth-functional methods to "checking the empty universe", but I keep it in logic as trivial validity.
 
13. The problem with singular terms which purport to designate is far more significant i.e. point 5.2 and its paraphrase in 6.2 are an opportunity for the methods of logic to shine in their accommodation of the free logicians concerns.

13. Singular terms which purport to designate but which don't in a given theory are sometimes called 'empty terms' in parallel with 'empty universes'.
The classic example is 'Pegasus' or some other mythical creature which is presumably an empty singular term in any theory which purports to deal with this culturally significant singular term.

14. Quine's analysis of singular terms is simple: they are all to be regimented as discriptions.
Consequently, so called 'free variables' are to stand for some unspecified description.
A description is formed from a descriptive predicate functor 'the' and is combined with a predicate abstract e.g. 'the (x such that x is F)' (where 'is' is taken as the copula of predication and NOT as a predicate e.g. not identity, not membership).

15. A step by step explanation for the general method by which singular terms can be analyzed into singular descriptions starts with a selection from "Philosophy of Logic 2nd edition" (POL)
    > "Think of 'a' as a name, and think of 'Fa' as any sentence containing it. But clearly 'Fa' is equivalent to 'some item is (x such that a=x and Fx)'. We see from this consideration that 'a' needs never occur except in the context 'a='. But we can as well render 'a=' always as a simple predicate 'A', thus abandoning the name 'a'. 'Fa' gives way thus to 'some item is (x such that Ax and Fx)', where the predicate 'A' is true solely of the object a.
    >
    > It may be objected that this paraphrase deprives us of an assurance of uniqueness that the name has afforded. It is understood that the name applies to only one object, whereas the predicate 'A' supposes no such condition. However, we lose nothing by this, since we can always stipulate by further sentences, when we wish, that 'A' is true of one and only one thing:
    > 1. some item is (x such that Ax)
    > 2. not some item is (x such that Ax, Ay, and not x is identical to y).
    >
    > (The identity sign 'is identical to' here would either count as one of the simple predicates of the language or be paraphrased in terms of them).
    >
    > The notation without names talks still of a and other objects, for they are the values of the quantified variables. An object can also be specified uniquely, still, by presenting some open sentence (in one variable) which that object uniquely satisfies. 'Ax' is such a sentence for teh object a. And the names can even be resotred at pleasure, as a convenient redundancy, by a convention of abbreviation. This convention would be simply the converse of the procedure by which we just now eliminated names. Each predication, let us say 'Fa', containing the name 'a', would be explained as an abbreviation of the quantification 'some item is (x such that Ax and Fx)'. In effect this si somewhat the idea behind Russell's theory of singular descriptions." pg. 26 POL.

16. Quine hints at the next step but it is not yet clear from this explanation except that Russell's method of eliminating singular descriptions is hinted at.
Before moving onto the next step, I must note that Quine's observation is profound: the method of abbreviating 'some item is (x such that Ax and Fx)' as 'Fa' is of grand consequence beyond it as an apparent intermediate step between singular terms (also called "names" in the quotation) and elimination of singular descriptions by Russell's contextual method.

17. I shall linger here because Quine did not make the full connection in the quotation between 'Fa' as an abbreviation for 'some item is (x such that Ax and Fx)' and compound predicates '{xy: Fx only if Fy}' paraphrasing predicate functors through the following steps:
    1. {xy: (Nip F)xy only if (Drop F)xy}
    2. {xy: ((Nip F) only if (Drop F))xy}
    3. (Nip F) only if (Drop F)

18. A bit more on notation that Quine did not get to: '..x is F to ..y' is shortened to '..xF..y' and predicate abstracts can be given "two sides" as in '{..x: ..xF..y :..y}' which (I am now very excited) showed me that my method of using predicate abstracts to distinguish between variables as singular terms and variables as bound quantified colocaters is a key step to effective homogenization! (note, this insight does not require the left and right stack notation which I originally introduced here in [Stack Notation for Predicate Functor Logic](#a-stack-notation-for-predicate-functor-logic-2025-0414-1626)).

19. phew! let me take a slow step back because multiple things came together all at once there.

20. First, variables are used to abbreviate complex compound predicate functors in the following way.

21. They occur only and solely within a predicate abstract.

22. The steps in 17 are to be entirely discarded and replaced by the more carefully assembled predicate functors from here [Stack Notation for Predicate Functor Logic](#a-stack-notation-for-predicate-functor-logic-2025-0414-1626).

23. By way of example, the predicate functor schema "(Drop F) only if (Push Drop F)" has a principle configuration of (1,1) since the principle configuration of a conditional is the max of left parts of the principle configurations of its components paired with teh max of the right parts of the principle configurations of its components, and the principle configuration of a drop is one plus the left part of the principle configuration of its component paired with the right part of the principle configuration of its component, and the principle configuration of a push is the left part of the principle configuration of its component paired with one plus the right part of its principle configuration.
Finally, predicate letters have a principle configuration of zero paired with zero (unless they are given an explicit configuration by what I plan on introducing as a 'rank' operation like that of the J programming language but of a tree form).

24. A much easier way to find the principle configuration of a compound predicate functor schema is along the same pattern that I am about to demonstrate that goes from a compoud predicate functor schema to its abbreviation with variables.

25. Since (1,1) is the principle configuration of "(Drop F) only if (Push Drop F)" then the abbreviation by variables proceeds as follows:
    1. {x: x((Drop F) only if (Push Drop F))y :y}
    2. {x: x(Drop F)y only if x(Push Drop F)y :y}
    3. {x: Fy only if xy(Drop F) :y}
    4. {x: Fy only if xyF :y}

26. The predicate abstract must be kept, since 'xF only if yF' turns into '(some (X and F)) only if (some (Y and F))', which reduces to where it came from by exactly the method proposed by Quine that he says harkens back to Russell's method of singular description as follows:
    1. (some {u: u(X and F)}) only if (some {v: v(Y and F)})
    2. (some {u: uX and uF}) only if (some {v: vY and vF})
    3. (some item is (u such that uX and uF)) only if (some item is v such that vY and vF)
    4. xF only if yF

27. Although the methods in 26 leave much to be desired, they point out the direction of more elaborate methods.
They also suggest that singular terms really do not fit the rubric they are said to by those, who like Russell, suggest there are designata beyond denotation. 



## 2025 0724 1617

1. For those familiar with truth-functional and quantification logic it is easier to explain first how predicate abstracts work and then how predicate functors work than it is to explain outright how predicate functors work.

2. This is because variables come to be bound by predicate abstracts and not by quantifiers in such a way that compound and complex predicates are easily constructed from sentences which only become open *within* a predicate abstract.

3. It also just occurred to me that predicate abstracts make it surprisingly obvious what the difference is between so called 'free variables' as singular terms and the variables bound by a predicate abstract.

4. It also occurs to me that Quine did not notice this distinction could be made with predicate abstracts.

5. An example: the sentence 'x loves y' is said, in classical truth functional and quantificational logic, to be open, and consequently, is often dealt with not as a sentence with singular terms 'x' and 'y', but rather as a *predicate* and not a sentence at all: 'x loves y' is said to be true of this paired off with that if and only if this loves that.

6. The accident here is of not recognizing the power of predicate abstracts!

7. The sentence 'Tom loves Dick' is as much an open sentence as 'x loves y', but because we are taught that such a sentence does not appear with respect to any old Tom or Dick, but with respect to some Tom and Dick that are otherwise uncovered by that all encompassing method of "context clues", it is hard to see that 'Tom' and 'Dick' are variables just as 'x' and 'y' are.

8. This is made clear by the method of predicate abstraction where 'Tom loves Dick' and 'x loves y' are the same (they are complete *sentences* in that 'x' and 'y' are as much like 'Tom' and 'Dick' in that we're not talking about any old x or y) and both are different from the predicates '{Tom,Dick: Tom Loves Dick}' and '{x,y: x loves y}' (which are in fact coextensive and hence quantificationally equivalent).

9. What's more, when we look at the origin of the predicate abstract in the grammarians relative clause, we find that there are even in our native tongue a difference between 'x loves y' which is a perfectly fine closed sentence with items purportedly designated by the singular terms 'x' and 'y' and the *incomplete* sentence 'x and y such that x loves y'.

10. In other words, predicate abstracts point up that what once went on as open sentences are actually incomplete sentences which have not been fully rendered as the relative clause which they purport to stand for.

11. This is far more profound than I had ever expected.


## 2025 0724 1604

I am removing the following from [hints][#hints]:

> ** HOW TO CONTROL YOUR BEHAVIOR **
> 1. Consequences select behavior.
> 2. Control consequences to control behavior.
> 3. Build new behavior from old behavior.
> 4. Build new controls from old controls.
> 
> ** HOW TO DEAL WITH ANHEDONIA AND UNCERTAINTY **
> 1. Read Dalio's "Life and Work Principles" when you feel anhedonia.
> 2. Read Quine's "Methods of Logic 4th Ed." when you feel uncertain.
> 3. Go to 1.

and I am removing 4 of 

> ** HOW TO FIND A GOOD PSYCHOLOGIST **
> 1. **Never** settle for a bad one.
> 2. Good ones are exceptional people who happen to be psychologists.
> 3. They are cultural liasons.
> 4. You're more likely to find a good psychologist through a friend than you are through a phycisian.

because I doubt its accuracy e.g. there may be families which point people to others who will only and solely reinforce their familial control and that this could be far more damaging than finding a psychologist through a phycisian.

I've made these edits because those hints do not help in the same way, e.g., that the hints on how prediate abstracts work or how indiscernibiliy works.

There are better hints to be made than these.
The material contained within them will not be entirely dropped.

Another edit.

In [HOW TO MAKE IT LOGICAL][#how-to-make-it-logical] point 3 is stronger when given as 'List true conclusions made from those few'.
It prompts me to write point 4 as 'Conclude as many from as few (premises) as you can.'
This introduces the important difference between premises and conclusions in the actual construction of a logical theory: everything that 'goes into' the construction is classed as a conclusion up to that point, it is only once the theory is assembled that the distinction between conclusions, premises, and truths.

Perhaps an even better way of doing this is to leave 3 as is "List ture sentences made from those few" and only add the parenthetical "(premises)" into point 4.
I'll try that out first: it follows the principle of minimal mutilation.

## 2025 0722 2115

I just discovered that using backticks gives an inline quote under the rules of markdown.
To get a literal back tick (for, e.g., use in writing out a javascript template literal) you must prepend a backslash to the backtick!

## 2025 0722 1740

1. Instead of sectioning notes into days and days into entries timestamped down to the minute, I'll try flattening everything down to the minute timestamped entries: I wasn't very often making more than one entry a day except for right at the beginning and anything that can be done to flatten the organization seems like it is more likely to help than to hurt, but time will tell.

2. Last entry left off right as I was about to go over how what Quine calls 'substitutivity of the biconditional' or 'substitutivity of covalence' (and what is distinguishable from what he has called 'subsitutivity of coextension') gives a logical basis for the premises of a theory of identity.

3. I left off exactly at the moment that it occurred to me that in "Methods of Logic 4th edition" (MOL4) Quine made the distinction between 'substitutivity of the biconditional' and 'substitutivity of the coextensional' with the word 'interchange' and that he did so relative to truth-functional, monadic, and quantificational equivalence.

4. What is called 'quantificational equivalence' in MOL4 is elsewhere called 'logical equivalence', e.g. in 'Philosophy of Logic 2nd Edition' (POL2). 

5. Most people are familiar with 'logical equivalence' as the counterpart to 'logical implication' (which is the most familiar) and, sadly, with 'logical truth' which, following the pattern from 'equivalence' and 'implication', is more helpfully called 'logical validity'.
In each case, the prefixed word 'logical' is dispensible if not just because there is no validity but that which is logical (a controvertial claim I am told) and it leaves room for distinguishing between different grades of validity e.g. truth-functional, monadic, quantificational, functorial, etc.

6. I'll make this note here, though I may have already made it in the last entry but it can only help to repeat it, that the distinctions Quine makes in MOL4 are some of the most helpful that I've found in any of the books on math, logic, and science that I've read or been taught from.

7. This has largely to do with the fact that each sentence in MOL4 has evolved as a consequence of its context in the rest of the book.

8. Quine is highly adept at smoothly cross referencing forward and backwards within a text so as to tie together technical terminology into a critical mass of consequential distinctions.

9. It is only in "From Stimulus to Science" FSTS that there are perhaps more such distinctions: as difficult a text as it is, I can not recommend that anyone go on reading early Quine without cross referencing his past terminology with that in FSTS.
Especially when navigating his nonlogical work (i.e. any work done outside the boundaries set up by MOL4).

10. The reason for this is that he drops certain distinctions that were once important in the evolution of his writing, but which were too sharp when literally applied.
Subsequent revisions to terminological distinctions dulled and broadened the boundaries which softly separate concerns.

11. Perhaps the most important example is the change in terms from 'Word and Object" (WAO) to FSTS: the later use of occasion, observation, and standing sentences is divorced more from their earlier commitment to what he then called 'stimulus meaning'.
In WAO he leaned too heavily on a crude grasp of verbal behavior as operant behavior which rendered it almost indistinguishable from the ancient analysis of verbal behavior under the stimulus-response psychology of Watson.
Still, even in FSTS, Quine's references to 'dispositions' and 'global stimuli' are an inadequate analysis of, e.g., Skinner's "Verbal Behavior" in light of such work as in Skinner and Ferster's "Schedules of Reinforcement".

12. All that praise of Quine aside, the distinction he made in MOL4 was given as 'Laws of Interchange' rather than as 'substitutivity of the biconditional' or 'substitutivity of the coextensional'.

13. The laws of interchange are given first in the chapter 9 "equivalence" pg. 63 of MOL4, where the equivalence of the chapter is specifically called truth-functional equivalence and contrasted with monadic and quantificational equivalence as already mentioned in 3 above.

14. The laws of interchange are contrasted with the laws of substitution which are given in paragraphs prior to the introduction of the term 'interchange'.
    > "Substitution consists always in putting schemata for single letters, and for all recurrences of teh letters. When these restrictions are not met, the putting of one schema for another will be called not substitution but *interchange*. Thus interchange consists in putting one schema for another which need not be a single letter, and which need not be supplanted in all its recurrences." pg. 63 MOL4

15. Though the laws of interchange are presented as seperate laws, they are perhaps better explained as three steps to the final grand law of interchange:
    > "Interchange of equivalents preserves validity, implication, equivalence, and inconsistency, and unlike substitution for letters, it even preserves consistency, nonvalidity, nonimplication, and nonequivalence." pg. 64 MOL4

16. Here is another note I must make now: this just quoted sentence is an example of Quine using the terminology that I favor 'nonvalidity', 'nonimplication' and 'nonequivalence', but he mixes it with 'inconsistency' where, if he was to keep that method of prefixing, 'invalidity' can replace 'nonvalidity'.
I have settled on 'nonconsistency' and 'non' everything, because the definition of each such technical term is never given by me in my presentation of logic: their definition is implicit in taking 'non' as 'not' in any exposition where such technical terms occur.

17. What Quine calls "the third law of interchange" is what, following the other patterns he established with the phrases containing 'substitution', e.g. 'substitution of the bicondtiional', is either what I call "THE law of interchange" or "Interchange of Equivalents".

18. Interchange of Equivalents covers two parts of Quine's definition of 'extensional contexts' in FSTS: substitution of covalence (aka substitution of the biconditional) and substitution of coextensiveness (aka substitution of the coestensional).
BUT, only when "interchange of equivalents" includes "Interchange of Truth-Functional Equivalents" and "Interchange of Quantificational Equivalents".

19. The coextensional of a pair of predicates is the universal closure of their biconditional.

20. Quantificational equivalence is validity of the coextensional.

21. So, interchange of equivalents, specifically interchange of quantificational equivalents, is what Quine calls "Substitution of the biconditional" in "Set Theory and its Logic Revised Edition" (STL):
    > "Students of logic are familiar also with certain second-order generalizations regarding the validity of quantificational schemata. Conspicuous among these is the *substitutivity of the biconditional*, which may be represented thus:
    > > each item is (..x, and y such that A if and only B) and CA, only if CB.
    > 
    > Here 'A', 'B', 'CA', and 'CB' represent any quantificational schemata such that the last two are alike except for containing as corresponding parts the respective schemata represented by 'A' and 'B'; and ..'x', and 'y' represent all variables , free in the schemata represented by 'A' and 'B', that are captured by quantifiers in the schemata represented by 'CA' and CB' (See for example my *Mathematical Logic* section 18)." pg. 12 STL

22. The reference to section 18 of "Mathematical Logic Revised Edition" (ML) is another example of Quine's deft cross referencing.

23. It is also a concrete example of how and when later and earlier writings of Quine can be interpreted with respect to the other.
This is so important, and also this particular example is directly relevant to the general problem of establishing a logical basis for the premises of theories of identity (and establishing such links as there may be wtih theories of indiscernability) that I shall say mroe about section 18 of ML.

24. Section 18 of ML is titled "Substitutivity of the Biconditional" and it is from here that he borrows the same phrase in STL.
The first paragraph gives what is there called "a principle" but what is in STL called "a second-order generalization".
In ML the principle is ultimately give as what is there called 'a metatheorem'.
Such invocations of the prefix 'meta-' were dropped by Quine for the good reason that there it suggests a special status which is inappropriate e.g. it is still common for people to speak of 'metatheories', or perhaps even 'metalogic', as if they are some how above and beyond the reach of theory and logic. They are not in that they are just theories which purport to include some of the apparatuses of some theories as their items.
Confusion on this issue is so great that an appendix had to be added by Quine to ML explaining the difference between a 'metatheorem' and a 'theorem': this appendix along with other problems involving the so called "higher order logics" entirely extinguished Quine's use of such terminology in, e.g., MOL4.

25. The first paragraph of section 13 of ML (which gives the principle of substitutivity of the biconditional) is
    > "In section 13 a restriction was imposed according to which one formula can occur in another only in a context of quantification or truth-functional composition. This restriction gives rise, it will be found, to the following convenient *substitutivity principle*: if A and B are statements agreeing in truth value, then B can be substituted for any occurrences of A in any statement CA without affecting the truth value of CA. In other words, if the statements CA and CB are alike except that CB contains the statement B in places where CA contains the statement A, then CA and CB are alike in truth value if A and B are. In other words, (I) *any statement of the form* \`${A} if and only if ${B}, only if ${CA} if and only if ${CB}\` *is true, where* CB *is like* CA *except for containing* B *in places where* CA *contains* A. E.g., the conditional:
    > > Smith met Jones if and only if Jones was in Omaha, only if each item is (x such that Smith met Jones or not Smith sold x to Jones) if and only if each item is (x such that Jones was in Omaha or not smith sold x to Jones)
    >
    > is true.

26. Two things about notatoin in my quotation of the first paragraph of sectoin 13 of ML in 25:
    1. I have replaced phi by 'A', primed phi by 'B', psi by 'CA' and primed ps by 'CB' to bring the paragraph closer to the notation used to paraphrase the quote in 21 from STL
    2. I have, against my better judgement, used javascript template literal notation to give an example of exactly how Quine's method of quasiquotation is inhereted by javascripts method of template literals (aka template strings)

27. I have been sitting for far too long i.e. since I started until now which is 2025 0722 2102.

28. I need a book holder.

29. I'm going to stand up, make food, eat, and maybe come back here.
If not later today, then later tomorrow.


## 2025 0721 1657

1. There are a cluster of methods of logic that are almost unique to Quine but which are worthy of a braod inclusion in any presentation purporting to introduce a student to the study of logic.

2. The cluster of methods appears to crystalize around indiscernability.

3. It may also be said to crystalize around identity.

4. Between indiscernability and identity we find Leibniz and his analysis of both in terms of the other e.g. in, so called and sadly so, 'second order logic' we call the 'second order sentence'
    > each item is (x and y such that x is identical to y if and only if each item is (F such that x is F if and only if y is F))

    by the name 'Leibniz's Law', and sometimes 'Leibniz's Biconditoinal Law' when emphasizing that the connective between identity and the biconditional purporting to define 'x is indiscernable from y' (as the second order sentence 'each item is (F such that x is F if and only if y is F) is itself the bicondtiional and not a conditional, with grammatical partical 'only if', or a converse conditional, with grammatical partical 'if'.

5. Second order logic fails for the well known reasons, e.g. [LindstrÃ¶m's theorem](https://en.wikipedia.org/wiki/Lindstr%C3%B6m%27s_theorem), and must be abandoned wherever predicate logic prevails.

6. For more on the problems with putting predicates in positions suited for variables, see Quine's chapter on "Deviant Logics" in "Philosophy of Logic 2nd Edition" (note, there are circumstances where a substitutional definition of quantification appears to carry out what n-order logics purport to deal with, but note that such a theory of logic is explained with predicate logic for exactly those reasons that primed Frege's enterprise: extensionality).

7. Leibniz's Law is charitably taken as submitting, in one direction, that each instance of the schema
    > each item is (x and y such that x is identical to y only if, x is F if and only if y is F)

    is a premise of a theory subject to this law for (schematic) predicate letter 'F'.

8. Two things:
    1. This schematic half, the 'only if' part of Leibniz's Law, is properly called "the indiscernibility of identicals", and
    2. the compound 'x is F' is now firmly in the grammatical category of predications in that 'is' is a grammatical partical, called a copula, whose construction takes a variable and a predicate and affixes one to the other, most often, as 'Fx' and only sometimes as 'x is F', "x F's", etc.

9. One more thing: since 'F' is not a schematic predicate letter to which only one variable is attached in the schema given in 7, the consequent of the universal conditional no longer carries the purported definition of indiscernability given by an allegidly second order sentence.

10. When the schematic letter 'F' is predicated of only one variable, then the predicates which are permitted to replace 'F', when forming an instance of a schema that contains it as a predicate letter, are only and solely one place predicates.

11. Indiscernability demands more than that each biconditional of a one place predicate with respect to one variable in the antecedent and another variable in the consequent be true.
That is, it is not enough to somehow conclude 'x is indiscernable from y' from having shown that each instance of 'Fx only if Fy' is true (though, this is a line which has been taken in the past by those who are unable to distinguish between a predicate and its purported extension, or, worse, its perported property, attribute, essence, concept, quality, proposition, propositional function, or, in general, its intension as a conspicuously nonextensional substance or kind).

12. Note, to conclude 'x is indiscernable from y' from having shown each instance of 'Fx only if Fy' is true involves a theory in which predicates of truth, quotation, and substitution (or some other items which fascilitate the general method which Quine has called 'semantic ascent', but note that semantic ascent only involves quotation or its reflection, disquotation, and substitution may not be directly involved, though it appears unavoidably so, and to uncover this fully would require a digression from this digression so I shall not indulge it now).

13. Concretely, the conditional which paraphrases the conclusion of 'x is indiscernable from y' from the premise "each instance of 'Fx only if Fy' is true" is give as

    > each item is (x and y such that each item is (z such that z quotes a predicate only if the substitution of z for 'F' throughout 'Fx only if Fy' is true of (x,y)) only if x is indiscernable from y)

    where 'is true of' is defined as in the consistent theory of predicates of 'denotes' from Quine's chapter "Denotation and Truth" in "From Stimulus to Science" adapted to the careful method of extending 'satisfaction' to sequences in Quine's chapter "Truth" from his "Philosophy of Logic".

14. Thankfully, the method sketched in 13 is both like what must be done to paraphrase the traditional definition of 'indiscernibility' as 'qualitative samness' or 'have all properties in common' contemplated in the second order paraphrase of Leibniz's law, and entirely obviated by a resolution of the problem pointed up in 11.

15. From supplementary definitions of the predicate functors (a grammatical construction which permits ingredients from any list of grammatical categories e.g. variables and one place predicates)
    1. 'x is indiscernable from y with respect to the m place predicate F in the n-th place' for 'each item is (..u..v such that F..ux..v if and only if F..uy..v)'
    2. 'x is indiscernable from y with respect to the m place predicate F' for 'x is indiscernable from y with respect to the m place predicate F in the 0-th place, .., and x is indiscernable from y with respect to the m place predicate F in the m-th place'

    indiscernability is defined relative to the lexicon of a standard theory whose finite list of m, .., and n place predicates F, .., and G (respectively) as
    3. 'x is indiscernable from y' for 'x is indiscernable from y with respect to the m place predicate F, .., and x is indiscernable from y with respect to the n place predicate G'

    and also seen not to depend on any particular feature of the natural numbers (though I admit to relying heavily on their convenience in pointing the way to systematically unraveling the abbreviations by '..').

16. The full demonstration that this definition of indiscernability is adequate builds on the thurough demonstration by Quine in the first section of the revised edition of "Set Theory and its Logic".

17. The demonstration there does not do full just to the problem posed by 11 here, which to reaffirm its relevance in this context, can be described as the problem of constructing an extensional paraphrase of Leibniz's law.

18. Though I will not repeat Quine's demonstration, I shall build upon it and emphasize the pieces relevant to the problem restated as in 17.

19. First, though Quine is not particularly interested in the larger problem of an extensional paraphrase of Leibniz's law, the order of his presentation is surprisingly parallel to some arguments that have been presented to address this problem in the past.

20. Before I forget, identity and indiscernability are not to be confused with equality (which purports to be the extension of identity as a special kind of relation) and equivalence (as a predicate functor which when attached to, e.g., a two place predicate abbreviates the grammatical construction of the conjunction of attaching that same predicate to the predicate functors of reflexivity, symmetry, and transitivity, each of which should themselves not be confused for the narrower definitions of reflexive relation, symmetric relation, or transitive relation).

21. Sadly, I know of no good explanation of all the distinctions from 20 and the others used here: it is part because of that sad fact that I am even writing this now.

22. As Quine so deftly begins on page 12 of "set theory and its logic revised edition' there is a method of logic which not only contributes greatly to the reach of such methods, but which also can be seen as independent inspiration from the methods of logic itself for appropriate constraints on identity and indiscernability: what Quine calls there "substitutivity of the biconditional", but which he later calls "substitutivity of coextensiveness" as part of his definition of 'extensional context' on page 90 of "From Stimulus to Science".
Schematically:
    > each item is (x, .., and y such that Fx..y if and only if Gx..y) and ...F..., only if ...G...

23. A footnote explains that the full demonstration of this can be found in the 13th section of Quine's Mathematical Logic the revised edition (I can not overemphasize how important it is to get the latest edition of Quine's books because they evolved in often drastic ways that change the consequences of his work from trivial to nontrivial).

24. Now, in Quine's "Methods of Logic 4th edition", there is a distinction made between substitutivity of the biconditional and substitutivity of the coextenstional (where the coextensional of a pair of predicates is the universal closure of their biconditoinal; it was originally introduced by Peano who attached what we now call the universally quantified variables to the mark of his biconditional (which was a three parallel equally lengthed lines stratled by its components, like an equal sign with one more line)).

25. Interestingly, in Quine's definition of 'exentional context' in "from stimulus to science" he calls "substitutivity of covalence" what he can, and perhaps should, call "substitutivity of biconditional"!
This is an interesting example of a common problem when it comes to dealing with logic: we are not the first to have arrived in such wilds and hence not the first to talk about all these beautiful and sometimes dangerous things).

26. Substitutivity of the biconditional, in Quines "Methods of Logic 4th edition", is first established for truth-functional logic in chapter 9 on "equivalence", and I am glad I went and hunted for it because I need to switch up my technical terms in order to avoid some deep pit falls.

27. The main distintion I've messed up is that between 'substitution' and 'interchange'.
The word 'substitution' is usually contrasted with the word 'replacement', and, often enough, defined with respect to it as "replace each occurrence of this in that with it".
Note, "replacement of occurrences" and the more carefully given "replacement of nonoverlapping occurrences" are precisely the problems that Quine addressed when he gave the first (surprising I know) definition of "substitution" in the 1936 paper (again I know that it is hard for people to accept that there was not a definition of "substitution" prior to 1936) "Definition of Substitution" to the "society of fellows, harvard university".

28. Outside "Methods of Logic 4th edition" Quine picked the word 'substitution' rather than 'interchange' or 'replacement' because it is far more familiar to the audience of his books: 'interchange' is unfamiliar outside Quine's "Methods of Logic" and that is why it is so valuable.

29. What I have come to call "substitutivity of the biconditional" and "substitutivity of the coextensional" are cast by Quine as "Laws of Interchange".

30. Interchange comes up as distinct from substitution (and more suggestive than replacement) as a result of examining methods of generating validities from valid schema.

31. Sadly, I've been writing for a little under three hours straight (it's 202507211944) and must stop here: hopefully I've laid enough foundation to pick up where I left off and get on to the cluster of methods from Quine (whcih include, identity as indiscernability, definition of predicate abstraction in a truth-functional and quantificational logic with a predicate of identity, and the elimination of singular terms as singular descriptions and the elimination of singular descriptions either by Russell's contextual definition or by Peano's premises, and the relation of those methods to the ones presented in methods of logic, where "descriptional premises" are overtly mentioned as part of theories which deal with items purportedly designated by singular descriptions, and the role of singular descriptions in the introduction of variables into a predicate functor theory and how they are related to predicate abstracts, and now it should be obvious why there is so much work done here and yet to be done here).

32. I really wish there was something I could read that dealt with all these problems so that I didn't have to do all this.

## 2025 0720

### 2025 0720 1732

1. Don't play the piano too much: it gets in the way of typing things up and the thinking that comes along with that.

2. I have not returned fully to my work on primitive recursive number theory, primitive recursive calculus, and primitive recursive analysis since I fully familiarized myself with Quine's methods of logic.
I'm returning to it fully now.

3. As Quine repeatedly showed throughout his later works: identity is coextensive with indiscernibility in a standard logic.
A standard logic is one with a finite lexicon i.e. a finite list of basic predicates from which all grammatical categories are constructed via predication (with variables, which are also classed as part of the lexicon under technical complications, involving accentuation in the simplest case, but which are entirely dropped when predicate functors are taken as the only grammatical particles and predicates are the only items of teh lexicon).

4. There are many things that I finally wrote up that I have not copied here.
Giving the definition of indiscernibility relative to a lexicon of predicates is one such thing.

5. In a truth functional and quantification logic, indiscernibility can be given as a predicate functor which takes a finite list of predicates and a pair of variables and constructs the relevant compound sentence.

6. Schematically, for predicate letters 'F', ..G, and 'H' of f, ..g, and h places (the number of places of a predicate letter tells us how many variables are to attach to any predicate which comes to substitute it: recall that each predicate has a shape, and, traditionally, it is given as a length from counting the number of variable letters attached to it e.g. 'x parented y' shows that 'parented' is a two place predicate).
I use '..G,' as a further schematism to avoid the trouble of introducing some scheme of subscripts, superscripts, or other indexical marks that tend to clutter methods presented for an unspecified but finite list of things (those who doubt there are definite methods of so indexing can start with predicate letters 'F0', 'F1', 'F2', ..., 'F' concatenated with the decimal numeral of some number designated by 'N', but note that this tends to make it appear as if an argument or construction is the result of some number theory even though it is not; the only method by which all such doubts can be eliminated is to present a logical theory of logic but that is precisely what I am trying to avoid).

7. Write 'x equals y with respect to F' for 'each item is (u,..v, and w such that Fxu..vw if and only if Fyu..vw, Fux..vw if and only if Fuy..vw, ..., Fu..vxw if and only if Fu..vyw, and Fu..vwx if and only if Fu..vwy)' where, all together, the list of distinct variable letters 'u..vwx' is of length f.

8. Write 'x equals y with respect to F, ..G, H' for 'x equals y with respect to F, ..G, and each item is (u,..v, and w such that Hxu..vw if and only if Hyu..vw, Hux..vw if and only if Huy..vw, ..., Hu..vxw if and only if Hu..vyw, and Hu..vwx if and only if Hu..vwy)' where, all together, the list of distinct variable letters 'u..vwx' is of length h.

9. Other than by Quine's method of quasiquotation, the mechanical production of the sentence abbreviated by 'x equals y with respect to F, ..G, H' can be got by the now standard string manipulation functions of modern computer programming languages.
Note, it is strange to see Quine's method of quasiquotation presented as 'templates' or 'macros' without directly mentioning their origins in Quine's "Mathematical Logic".
The reason this is so strange, is that they were introduced for a specific reason, to make it all but impossible for a careless reader to falal into the use-mention trap, and yet that reason seems to go unheeded by many designers of programming languages and many programmers of such languages.

10. I must go out of my way to mention that Quine's method of quansiquotation would save anyone from 'what color is your function', among many other strange methods that have cropped up recently in programming languages.
Though, I must also go out of my way to mention that literally coloring functions, as in colorForth, uhdForth, or my own programming language is supported by Quine's methods, so that might put people off because his methods are too liberal.
To those so put off, I would have them restrict Quine's method of quasiquotation rather than toss it out entirely as some ancient and useless relic of a bygone era.

11. A better way of introducing indiscernibility occurred to me while I was thinking to myself of better ways of getting it into a form amenable to inclusion in [hints](#hints).

12. For the predicate letter 'F' write '=F' for '{xy: each {u..v: Fxu..v if and only if Fyu..v, .., and Fu..vx if and only if Fu..vy}' and '=(F,..,G)', or just '=', for '{xy: (=F)xy, ..., and (=G)xy}'.

13. That method does depend on mastery of predicate abstracts, but there is no excuse for not mastering them when you belong to a culture that insists on the conspicuity of variables.

14. This note was supposed to deal with primitive recursive arithmetic, but it ended up making a new hint: [how indicernibility works](#how-indiscernibility-works).

## 2025 0718

### 2025 0718 1745

1. Notes on "A first-order axiomatization of the theory of finite trees" by Backofen, Rogers, and Vijay-Shanker 1995

2. This paper is related to my work on the theory of ordered pairs.
The basic predicate in such a theory is 'x pairs y with z' and it is written 'xPyz' for short (technically, 'P' is short for 'x, y, and z such that x pairs y with z', or using Quine's predicate abstract notation, '{xyz: x pairs y with z}'

3. Extensionality of binary trees is given by the following sentence:
    - each item is (q, r, s, t, x, and y such that each item is (u and v such that xPuv if and only if yPuv), qPxr, and sPtx, only if qPyr and sPty)

4. Extensionality as given is a far cry from what most are used to when dealing with the basic principle of ordered pairs (as it is so often called) e.g. '(x,y)=(u,v) if and only if x=u and y=v'.

5. But, it corresponds directly to the more familiar principle of extensionality of sets e.g. 'each item is (x such that x in y if adn only if x in z) and y in w, only if z in w'.

6. Even this is not so often noticed as the principle of extensionality of sets.
In English "sets with the same members are equal" i.e. each item is (x such that x in y if and only if x in z) only if y equals z.

7. The parallel principle for orderd pairs is 'each item is (u and v such that xPuv if and only yPuv) only if x=y'.

8. Shall come back to this later.


### 2025 0718 1429

Old version of hint for "what to do" which was itself a newer version of the hint "what I must do before I die".

> **WHAT TO DO**
> 1. Discover, predict, and control changes
> <br> *in counts, rates, and accelerations*
> 2. as selections from variations
> <br> *on physical, chemical, biological, behavioral, and cultural scales*
> 3. by making and maintaining strong practices
> <br> *mediated by strong people marked by strong principles*
> 4. from the sciences of
> <br> **logic** *denotative, Boolean, functor*
> <br> **mathematics** *calculi, collections, categories*
> <br> **physics** *quantum, thermodynamic, gravitional*
> <br> **chemistry** *phyiscal, biophysical, biological*
> <br> **biology** *oranelles, organisms, environments*
> <br> **behavior** *biological, biosocial, social*
> <br> **culture** *history, science, technology*

## 2025 0707

### 2025 0707 1949

1. I have not worked on my stack based programming language for a while.

2. It is written in javascript: there's no other language that seems as well known or as easy to know (and play with) than it.
It will eventually be given as a logic program with Quine's main method.

3. The garabage collector implements "A Linear Algorithm for Copying Binary Trees using Bounded Workspace" by K. P. Lee from March 1980.

4. In the past I took pictures of the code and posted it to Twitter.
Now I shall do more to explain myself from the beginning.

5. First, we allot a lot of cells, 65536 to be exact, and label them with numbers from 0 to 65535.
We call the allotment 'at' because you store and retrieve what is in a cell *at* a number.
Allotments are called 'arrays' in Javascript.
```
let at = new Array(65536);
```

6. The allotment has to be big because everything, even numbers, are treated as (ordered) pairs.
We make it a convention to address a pair by the address of its left part, that is, with an even number (zero is even), and to always put its right part at the successor of the address of its left part i.e. the n-th pair is at n*2, its left part is there too, and its right part is at n*2+1.
```
L = n => at[n*2];
R = n => at[n*2+1];
X = (n,L,R) => (at[n*2]=L, at[n*2+1]=R, n);
```

7. What happens if 

## 2025 0706

### 2025 0706 1353

1. The definition of croppings in the draft of the the memo on logic from the last note is wrong: it fails to fully describe where the component denotes with respect to the compound.
    > components of *croppings* denote when some item is the right of the left with the right of waow the compound denotes
    >
    > WRONG!

2. work on a fix
    1. components of *croppings* denote when the left with some item, with the right of waow the compound denotes
    2. components of *croppings* denote when some item is such that the left with it, with the right of waow the compound denotes
    3. *croppings* denote waow the left of it with some item, with the right of it is denoted by the component
    4. *croppings* denote waow some item is such that the left of where the compound denotes with it, with the right of where the compound denotes, is where the component denotes
    5. there is some item such that it is the right of the left part of waow the component denotes and the left of the left with the right of waow the compound denotes
    6. A cropping denotes waow there is some item such that it is the right of the left part of waow the component denotes
    7. when there is some item such that it is the right part of the left of where the component denotes, the compound denotes the left of the left with the right of the matching occasion
    8. a cropping denotes waow some item is x such that the component denotes the left of the left of it with x, with the right of it

3. I'm failing to avoid a switch to my outlook on croppings that comes from Quine's sublime definition of the truth of existentials in "Philosophy of Logic second edition":
    > An existential quantification consists of some sentence preceded by an existential quantifier whose variable is, say, the ith variable of the alphabet. This quantification, then is satisfied by a given sequence if and only if the constituent sentence is satisfied by some sequence that matches the given one except perhaps in its ith place. POL2 pg.39

4. This would change the definition of croppings as follows: croppings denote waow there is somewhere denoted by the component that matches it except perhaps at the right of the left of it.

5. The problem with this change is unfortunate because it does not make the compound a smaller shape than the component: it makes it the same shape.
There is something important about making everything the same shape, and ultimately this may be the only option available to me because of precisely the problems Quine navigates with subtle talk of satisfaction by sequences rather than satisfaction by sequences of a given length on page 37 and 38.

6. But a good definition may have just occurred to me:
    1. Croppings denote the left of the left with the right of waow some item is the right of the left of it and it is denoted by the component.
    2. Components of croppings denote the left with some item, with the right of waow the compound does
    3. When some item is such that 
    4. Croppings denote when some item is such that their component denotes the left with it, with the right of waow they denote.
    5. Croppings denote when some item is such that the component of the cropping denotes the left with it, with the right of waow the compound denotes.

7. For now I'm settling on the somewhat cryptic 'Components of *croppings* denote the left with some item, with the right of waow the compound does.'

8. It is cryptic because it is complete yet, perhaps, incompressible.

9. The rest of the basic recombic functors are not yet given in the draft of the memo.
It's unclear where they can best be introduced, but it is clear that the details of their introduction must be given in full so that others do not fumble around like I did when looking for a cumulative straight line to them.
    - 'OVER F' for 'HEM PUSH F'
    - 'OVER2 F' for 'OVER OVER F'
    - 'OEM F' for 'OVER HEM F'
    - 'DSH F' for 'DROP PUSH F'
    - 'DUP F' for 'OEM DSH F'
    - 'DROP2 F' for 'DROP DROP F'
    - 'POP F' for 'OEM DROP2 F'
    - 'NIP F' for 'POP DSH F'
    - 'HIP F' for 'HEM NIP F'
    - 'HIP2 F' for 'HIP HIP F'
    - 'SWAP F' for 'HIP PUSH F'
    - 'PUSH2 F' for 'PUSH PUSH F'
    - 'TOR F' for 'HIP2 PUSH2 F'
    - 'ROT F' for 'TOR TOR F'

10. Press shift and space to scroll up a page.

11. The logical connectives 
    - Alternative Denial) 'F nand G' for '{..x: not(F..x and G..x)}'
    - Complement) 'comp F' or 'not F' or '-F' for 'F nand F'
    - Alternation) 'F or G' for '(not F) nand (not G)'
    - Joint Denial) 'F nor G' for 'not(F or G)'
    - Complementary Conditional) 'F not only if G' for 'F nor not G'
    - Conditional) 'F only if G' for 'not (F not only if G)'
    - Converse Conditional) 'F if G' for 'G only if F'
    - Complementary Converse Conditional) 'F not if G' for 'not(F if G)'
    - Conjunction) 'F and G' for 'F not if (not G)'
    - Biconditional) 'F iff G' for '(F only if G) and (F if G)'
    - Exclusive Alternation) 'F xor G' for 'not(F iff G)'
    - Minor Existential) 'some F' for '{..x:some item is y such that Fy..x}'
    - Major Existential) 'Some F' for 'some^n F' where 'F' is an n place predicate and for a predicate funtor 'f' the iterates are 'f^1' for 'f' and 'f^(n+1)' for 'f^n f'
    - Minor Universal) 'each F' for 'not some not F'
    - Major Universal) 'Each F' for 'not Some not F'
    - Inclusion) 'F => G' for 'Each(F only if G)'
    - Converse Inclusion) 'F <= G' for 'G => F'
    - Proper Inclusion) 'F > G' for '(F => G) and not (F <= G)'
    - Converse Proper Inclusion) 'F < G' for 'G > F'
    - Coextension) 'F <=> G' for '(F <= G) and (F => G)'.

12. there are two paths to define the 'logical' functions: from nand and from nor.
One of them works better when introducing a boolean arithmetic prior to talk of the arithmetic of remainders of division by two.


## 2025 0705

### 2025 0705 2243

1. I delight in the evolution of the memo on logic.
The following is a better ordering of the major catogories from most to least familiar.
It also includes those parts of the earlier drafts that carry over directly to this new definition of validity.
    > *Once upon a time, a man grew up all alone. One day, a woman taught him to speak: she held up an apple, said 'red', he held it up, said 'red', she smiled, he held up another, said 'red', she frowned, held it up, said 'green', and so on.*
    >
    > *Predicates* do or don't denote (are true or false of) where they occur.
    > *Constructions* connect predicates, *lexicons* list atomic predicates, and *grammars* generate *categories* from recurrent constructions on lexicons.
    > *Logic* reveals *validities* whose supplemented lexical substitutions (SLS) denote everywhere (Quine).
    >
    > Constructions inheret where they denote from their components (Tarski) e.g. functionally, e.g.
    >
    > 1. *joint denials* denote waow each of their components don't,
    >
    > 2. *negations* (self joint denials) denote waow their component does not,
    >
    > 3. *alternations* (negations of joint denials) denote waow some of their components do,
    >
    > 4. *conjunctions* (joint denials of negations) denote waow each of their components do,
    >
    > 5. *alternative denials* (alternations of negations) denote waow some of their components don't; and 
    > 
    > determinatively, e.g.
    >
    > 6. *closures* denote waow there is somewhere denoted by their component, and 
    >
    > 7. components of *croppings* denote when some item is the right of the left with the right of waow the compound denotes; and
    >
    > recombically, e.g. components of
    >
    > 8. *drops* denote the left of the left with the right,
    >    
    > 9. *pushes* denote the left with the left of the right, with the right of the right,
    >    
    > 10. *hems* denote the left with, the right of the left with the right,
    > 
    > of waow their compound does.
    >
    > Predicates are
    >
    > 11. *consistent* waow their negation isn't valid (some of their SLSs denote somewhere) ,
    >
    > 12. *implied* by others waow the conjunction of their self (the conclusion) with the negation of the other (the premise) isn't consistent (each of their SLSs denotes where the same of the other does), and
    >
    > 13. *equivalent* to others waow they are mutually implicative (each of their SLSs donotes waow the same of the other does).
    >
    > Examples:
    > 
    > 14. Alternations of compounds with their negations are valid (they denote waow the compound does or its negation does, i.e. waow it does or does not, so, each SLS denotes everywhere).
    >
    > 15. Conjunctions of compounds with their negations are not consistent (they denote waow their compound does and its negation does, and, hence, waow it does and does not, so, each SLS of the conjunction does not denote everywhere i.e. the negation of the conjunction is valid).
    > 
    > 16. Compounds are implied by and equivalent to their self.

2. Little steps.



### 2025 0705 1515

1. Write up an even simpler construction of Alan Macdonald's "An elementary construction of the geometric algebra" which follows notational extensions of R. L. Goodstein's primitive recursive equation calculi by giving the permutation parity function and other combinatory functions (and APL like functions).
    - <https://www.faculty.luther.edu/~macdonal/>
    - <https://www.faculty.luther.edu/~macdonal/GAConstruct.pdf>

2. Write a memon on Goodstein's primitive recursive equation calculus and its notational extensions through to primitive recursive analysis e.g. the primitive recursive elementary functions and primitive recursive calculus (maybe even primitive recursive geometry through the Tarski/Hilbert form of the predicate 'x between y and z' or 'x is colinear to y with z' or 'xyz colinear' or, just, 'colinear'.

3. The aim is primitive recursive quantum field theory i.e. primitive recursive functional analysis and operator theory.

4. Collect primary source I already have in my library, and collate with references from e.g. [A Short Introduction to the Quantum Formalisms](https://arxiv.org/abs/1211.5627) by Francois David.

5. Also, what's up with the behaviorist Jack Michael? <https://en.wikipedia.org/wiki/Jack_Michael> He appears to have caused a lot of trouble for the science of behavior by building what he called "Applied Behavior Analysis".
The problem appears to be that the applied behavior analysts are not so much worried about the science of behavior as they are with its consequences in capitally controlled cultures i.e. they appear only to take advantage of those fragments of the science of behavior which they find most convenient without being subject to the scientific countercontrol which the science of behavior would otherwise enforce.

6. But first, more drafts of the memo on logic.
    > *Once upon a time, a man grew up all alone. One day, a woman taught him to speak: she held up an apple, said 'red', he held it up, said 'red', she smiled, he held up another, said 'red', she frowned, held it up, said 'green', and so on.*
    >
    > *Predicates* do or don't denote (are true or false of) where they occur.
    > *Constructions* connect predicates, *lexicons* list atomic predicates, and *grammars* generate *categories* from recurrent constructions on lexicons.
    > *Logic* reveals *validities* whose supplemented lexical substitutions (SLS) denote everywhere (Quine).
    >
    > Constructions inheret where they denote from their components (Tarski).
    > They sort into three bins:  *recombic*, e.g. components of
    >
    > 1. *drops* denote the left of the left with the right,
    >    
    > 2. *pushes* denote the left with the left of the right, with the right of the right,
    >    
    > 3. *hems* denote the left with, the right of the left with the right,
    > 
    > of waow their compound does; functional e.g.
    >
    > 4. joint denials denote waow each of their components don't,
    >
    > 5. negations (self joint denials) denote waow their component does not,
    >
    > 6. alternations (negations of joint denials) denote waow some of their components do,
    >
    > 7. conjunctions (joint denials of negations) denote waow each of their components do,
    >
    > 8. alternative denials (alternations of negations) denote waow some of their components don't; and 
    > 
    > determinative, e.g.
    >
    > 9. closures denote waow there is somewhere denoted by their component, and 
    >
    > 10. components of croppings denote when some item is the right of the left with the right of waow the compound denotes.
    >
    >
    > Predicates are
    >
    > 1. *consistent* waow their negation isn't valid (some of their SLSs denote somewhere) ,
    >
    > 2. *implied* by others waow the conjunction of their self (the conclusion) with the negation of the other (the premise) isn't consistent (each of their SLSs denotes where the same of the other does), and
    >
    > 3. *equivalent* to others waow they are mutually implicative (each of their SLSs donotes waow the same of the other does).

7. Work on the definition of croppings that ultimate ends up in 6
    - components of croppings denote the left with something, with the right of waow their compound does
    - croppings denote the left of the left with the right of waow some item is such that the left with it, with the right  of waow is denoted by the compound, is denoted by the component
    - components of croppings denote when some item is the right of the left with the right of waow the compound denotes


8. Sometimes I get really mad at the world for letting so many bright stars burn out rather than helping them to shine on.

9. Read Kleene's 1981 THE THEORY OF RECURSIVE FUNCTIONS, APPROACHING ITS CENTENNIAL


### 2025 0705 1353

1. Components of *lefts* and *rights*, and the left and right components of *Cartesian pairs*, denote the corresponding parts of waow their compound does.

2. Wait, are these recombically complete?
If there is no duplication then there is no way to construct the relfection of a predicate.

3. It just occurred to me that I've been down this road before when looking for a recombically complete set of predicate functors.

4. After you define the left, the right, and the Cartisian product functor, you see that you can't construct reflections, so you add reflections constructions in e.g. reflections denote the ordered pair whose left and right part match where and only where (waow) their component does.

5. Then you're left without a construction for the symmetric/swap of a prediate i.e. symmetrics denote the right with the left part of waow their component denotes.

6. Next, it is uncovered that there is no way to move items around the trees of items constructed from lefts, rights, cartesian pairs, reflections, and symmetrics, so the left and right shifts come up as a solution i.e. the component of the left shift denotes the left with the elft of the right, with the right of the right of waow the comopund denotes, and the comopnet of the right shift denotes the left with the left of the right, with the right of the right of waow the compound denotes.

7. Here, with the left and right shifts, we have the stack operations of push and pop, or in traditional forth notation '>R' and 'R>' which are read aloud as 'to ar' and 'ar from'.
You can stick with these accumulated operations that are entirely ignorant of the duality of lists and stacks as the right and left parts of an ordered binary tree, but there are diminishing returns as it is uncovered that the left and right shifts which are called for inevitably, permit the construction of reflections and symmetrics that are dealt with in calculations with commensorate methods. 

8. For example, components of reflections denote the left with, the right of the left with the right of waow their compound denotes, and components of symmetrics denote the left of the left with the left of the right, with the right of the left with the right of the right of waow their compound denotes.

9. The definitions of 4 and 5 are then of *major* reflections and symmetrics, and the definitions of 8 are their *minor* extensions.

10. You're not bad for feeling things even though the things that you feel may feel bad.

11. Draft of definitions of recombically complete predicate functors for memo on logic:
    > The component of a
    >
    > - *drop* denotes the left of the left with the right,
    >    
    > - *push* denotes the left with the left of the right, with the right of the right,
    >    
    > - *hem* denotes the left with, the right of the left with the right,
    > 
    > of waow the compound does.

## 2025 0703

### 2025 0703 1245

1. Finish collecting Quine quotations on singular terms in [2025 0629 1959](#2025-0629-1959).

2. Memo work:
    1. The left part of the occasion is called the pile, the right part is called the list. The left part of the pile is called the rest, the right part is called the top. The left part of the list is called the head, and the right part is called the body.
    2. The component of the drop denotes the rest with the list of where and only where (waow) the compound does.
    3. The component of the push denotes the pile with the head, with the body of waow the compound does.
    4. Drop components denote the rest with the list of waow their compound does.
    5. Push components denote the pile with the head, with the body of waow the compound does.
    7. Hem components denote the pile with, the the right part of the rest with the list of waow the compound does.
    8. Components of
        1. drops denote the rest with the list of waow their comopund does
        2. pushes denote the pile with the head, with the body of waow the comopund does
        3. hems denote the pile with, the right part of the rest with teh list of waow the compound does
    9. Drop components denote the left part of the left part with the right part of waow their compound does.

3. Cumulative draft of memo on predicate logic
    > *Once upon a time, a man grew up all alone. One day, a woman taught him to speak: she held up an apple, said 'red', he held it up, said 'red', she smiled, he held up another, said 'red', she frowned, held it up, said 'green', and so on.*
    >
    > *Predicates* do or don't denote (are true or false of) where they occur.
    > *Constructions* connect predicates, *lexicons* list atomic predicates, and *grammars* generate *categories* from recurrent constructions on lexicons.
    > *Logic* reveals *validities* whose supplemented lexical substitutions (SLS) denote everywhere (Quine).
    >
    > Predicates are
    > 1. consistent waow their negation isn't valid (some of their SLSs denote somewhere) ,
    > 2. implied by others waow the conjunction of their self (the conclusion) with the negation of the other (the premise) isn't consistent (each of their SLSs denotes where the same of the other does), and
    > 3. equivalent to others waow they are mutually implicative (each of their SLSs donotes waow the same of the other does).
    >
    > Constructions inheret where they denote from their components (Tarski) e.g.
    > 1. joint denials denote waow each of their components don't,
    > 2. negations (self joint denials) denote waow their component does not,
    > 3. alternations (negations of joint denials) denote waow some of their components do,
    > 4. conjunctions (joint denials of negations) denote waow each of their components do,
    > 5. alternative denials (alternations of negations) denote waow some of their components don't, and 
    > 6. closures denote waow there is somewhere denoted by their component
    > 7. 


## 2025 0701

### 2025 0701 1317

1. Each substitution in *valid* predicates for (supplemented) lexicon denotes everywhere.

2. Logic Programming Notes by Frank Pfenning <https://people.mpi-sws.org/~dg/teaching/lis2014/modules/lp-fp-07.pdf>

## 2025 0629

### 2025 0629 1959

1. It has been a while since I've worked on my calculus notes.
I now have a clearer aim to put it into a memo.

2. Calculus, for me, is indistinguishable from primitive recursive analysis.
In particular, the synthesis of algebra and geometry which hits the big tools of math in the rest of the sciences is precisely the synthesis of projective geometry and primitive recursive arithmetic that fits closely with Tarski and Givant's metamathematical methods in geometry.

3. Sadly, there is no glory in diving into these methods of calculus without first finishing the memo on (predicate) logic.
The exact primitive recursive methods, either in the geometric or algebraic constructions, are those which follow from the proper premises as in logic programming.

4. Logic programming is to be shown to follow immediately from Quine's main method.
Specifically, it follows immediately from the functional normal form of Quine's main method where the purely notational tool of compound singular terms (which plays the logical part of Skolem's functions).

5. Quine's work on singular terms changed my outlook on logic: it turned it active where it had once been transcient.

6. The relevant definitions are these.
    > In terms of logical structure, what it means to say that the singular term "purports to name one adn only one object" is just this: *The singular term belongs in positions of the kind in which it would also be coherent to use variables 'x', 'y, etc.* (or, in ordinary language, pronouns). pg.260

    > General terms, in contrast to singular ones, do not occur in positions appropriate to variables. pg. 261

    > The 'x' of an open sentence may refer to objects of any kind, but it is supposed to refer to them one at a time; and then application of 'each {x:' or 'some {x:' means that what the open sentence says of *x* is true of all or some objects taken thus one at a time. pg. 261

    > Just as the sentence letters in a schema stand as dummy sentences and the term letters ad dummy general terms, so the free variables may be seen as standing as dummy singular terms. pg. 262

    > It is usual in logic to write 'the {x:', with the inverted iota, to mean "the object x such that'. pg 274

    > Singular terms are called *descriptions* when written in this form. The singular terms of ordinary language which may be represented thus as descriptions begin typically with the singular 'the', but by no means necessarily so. pg. 274

    > In general a singular term purports to name one and only one object, and in particular a singular term of the form 'the {x: Fx}' purports to name the one and only object of which the general term represented by 'F' is true. Thus, if y is the object the {x: Fx}, then y must be such that y Fs and nothing-but-y Fs. this conjunction amounts to saying that, for each thing x, 'F' is true of x if x=y, and false of x otherwise. In short: each {x: Fx if and only if x=y}. pg. 274

    > If 'F' is true of nothing or of many things, then there is no such thing as the {x: Fx}. Actually the term appearing in the role of the 'F' of 'the {x: Fx}' in verbal examples from ordinary discourse very frequently needs supplementary clauses to narrow it down to the point of being true of only one object, but this situation can commonly be viewed merely as a case of the familiar practice of depending on context or situation to resolve ambiguities of ordinary language. Moreover, it would be unnatural to construe all use of the singular 'the' in this way; often a better account is simply pronominal. Commonly 'the boy', 'the car', serves merely as a pronoun whose grammatical antecedent is some name or description or perhaps some general term and quantifier. In logical notation such a pronoun might come through simply as a bound variable. pg. 275

    > arguments involving a singular term can be carried through by straight quantification theory with a free variable, say 'y', for the singular term, but that the application of the results depend on construing y as the object named by the singular term, and hence is contingent on existence of such an object. This construing of y, and the existence assumption on which it rests, figured nowhere in the schematism of the proof, but only in the informal step of application. pg. 275

    > the beauty of descriptions is that here the construing of y as the named object can itself be schematized quite explicitly as an additional premise of the form 'each {x: Fx if and only if x=y}'. So our technique for arguments involving descriptions is as follows: we use free variables for the descriptions as for any singular term, but we also add a *descriptional premise* of the form 'each {x: Fx if and only if x=y}' for each description. an axiom of identity is also usually called for, because of the '=' in the descriptional premise.

    >


### 2025 0629 1706

1. The "Foxfire" series on Appalachian culture is priceless.
Such records can be made by any culture.
Cultures can be deliberately designed to make and maintain such records.
    1. <https://en.wikipedia.org/wiki/Foxfire_(magazine)>
    2. <https://www.foxfire.org/>

2. Now more than ever I am in a position to evolve a new experimental culture which, like the older tinier one, is governed by the principles of an experimental analysis of behavior.

3. The hints, memos, and notes made by members of such a new culture are only secondary to the concrete contingencies of the world from which they are selected.

4. Copleston's History of Philosophy
    1. 1946 Greece and Rome
    2. 1950 Augustine to Scotus
    3. 1953 Ockham to Suarez
    4. 1958 Descartes to Leibniz
    5. 1959 Hobbes to Hume
    6. 1960 Wolff to Kant
    7. 1963 Fichte to Nietzsche
    8. 1966 Bentham to Russell
    9. 1975 Maine de Biran to Sartre
    10. 1986 Russian Philosophy
    11. 1956 Logical Positivism and Existentialism

5. Copleston's history of philosophy might otherwise have been written as something like Foxfire.

6. In 2 of [2025 0629 1518](#2025-0629-1518) Quine's initial definitions of 'lexicon' and 'particle' are given.
In his book "Philosophy of Logic Second Edition" (POL2) they are defined as "words".
This is refined in "From Stimulus to Science", indirectly, through his definition of 'predicate': "a predicate in my sense is always an integral word, phrase, or clause, grammatically a noun, adjective, or verb. Some are generated from others by grammatical constructions, notably the relative clause or, formally, predicate abstraction or predicate functors." (pg. 61 From Stimulus to Science).

7. In POL2, Quine only briefly mentions the construction, and endless enlargement of the category, of predicates by way of predicate functors.

### 2025 0629 1518

1. I want fast answers -- that are sound, salient, and smooth -- to apt questions.
Rarely having them energizes me.

2. In "Philosophy of Logic Second Edition" Quine first defines lexical and partical items as "the words classed in the categories comprise the lexicon, whereas the words or signs that are not thus classified but are handled only as parts of specific constructions are the particles" (pg. 26-27).

3. The books I take notes from are hard to keep open.
When I type I use both of my hands and have none free to hold open books.
A bookstand may help me.

4. An experiment with a bookstand is like my experiment with earmuffs.

5. On the earmuff experiment:
    1. When I sit down to my computer or sit down to think I have frequently put on my earmuffs.
    2. They dampen all sounds, except perhaps for the lowest of frequencies.
    3. When the earmuffs are on I am less likely to respond to sounds that otherwise control my attention e.g. dogs barking and bird chirping (though I much prefer birds chirping).
    4. Wearing earmuffs has uncovered a few unexpected things.
    5. The low frequencies are not as often noticed when they are combined with the rest of the sounds that I hear e.g. the low yet loud hum of some far off semi engine breaking, or, to my surprise, the low and pulsating hum of some overhead fans.
    6. One of the most surprising things I discovered is that I have volentary control over my tensor tympani muscle.
    7. I just learned that my father has this volentary control as well.
    8. It was uncovered because I heard the roaring sound in my right ear even though I had the earmuffs on.
    9. Even after flipping the muffs around, so that the one over the left ear was now over the right, the sound remained.
    10. I then discovered that by focusing on relaxing my neck and upper back, it went away.
    11. It came back when I breathed through my nose if I did not deliberately try to relax my neck and back.
    12. It occurred to me that I make the rushing sound whenever I swallow.

6. Most of the bookstands I saw from a quick search where more than I would pay for what they offer.

## 2025 0627

### 2025 0627 2141

1. I've wasted so much time trying to figure out how spaces work with lists and sublists.

2. In working out how to alphabetize a schema the problem of freedom and bondage occurs.
This is dealt with conspicuously by predicate abstracts and [HOW PREDICATE ABSTRACTS WORK](#how-predicate-abstracts-work).

3. Sadly, quantifiers are still so popular as to intrude even upon Quine's methods of logic 4th edition as in this fundamental restriction on substitution:
    > Quantifiers of the substituted abstract must not capture variables of the schema in which the substitution takes place, and variables of the substituted abstract must not be captured by quantifiers of the schema in which the substitution takes place." (pg. 162 MOL4)

4. The notation '(such (..x) F..x..y)' is a way to regiment the predicate abstracts in a LISP like language or the language of Fefermann's Finitary Inductively presented logics.

### 2025 0627 1904

1. Among the rules of passage mentioned by Quine in "Free Logic, Descriptions, and Virtual Classes" 1994, there is not one for negation:
    1. some {x: not Fx} if and only if (iff) not each {x: Fx}
    2. each {x: not Fx} iff not some {x: Fx}

2. Such rules together with De Morgan's laws
    > not (p and q), iff (not p) or not q
    >
    > not (p or q), iff (not p) and not q

    and with the law of double negation
    > not not p, iff p

    yield the negational normal form where only predications are negated: push all negations inward and they pile up on predications until they are eliminated by hitting another already attached to a predication.

3. "an instance of a quantification exactly matches the old open schema that followed the quantifier, except that it may show a different variable in place of the recurrences of the variable of the quantifier. If it does show a different variable, it must show it in all the places (at least) where the old variable had been free in the old open schema that followed the quantifier. Moreover, it must show it *free* in those places. The reader will recognize in these requirements the effects of the restrictions on substitution for term letters."[pg. 180 MOL4]

4. "A universal quantification implies each of its instances, and an existential quantification is implied by each of its instances." [pg. 180 MOL4]

5. The main method proves inconsistency of a sentence by
    1. putting it into prenex form
    2. if the outer most quantifier is universal, then, for each free variable of the sentence write the instance of the universal quantification with 

6. evolving some sentences
    - write instances of the universal quantification for each free variable as an instantial variable
    - write each instance of the universal quantification where the instantial variable is one of the free variables of the sentence

7. The definition in 3 of 'an instance of a quantification' misses the chance to define 'instantial variable' in the relevant context.
Quine defines it here: "The *instantial variable*, which is substituted for the variable that was bound by the dropped existential quantifier, must be *new*. More accurately: it must be free in no line prior to this instantiation." [pg. 190-191 MOL4]

8. There is a lot of relettering stuff that seems like it could be avoided by another normal form e.g. alphabetized normal form.
Presumably this is a form where all free and quantified variables are replaced from left to right with variable letters in some alphabetical order.

9. Alphabatized form is reached by first establishing an alphabetical order to all variables.
When subscripted variables, aka complex variables, aka open singular terms, aka compound or complex singular terms, aka Skolem names, aka Skolem functions are in play, dictionary order takes care of any hiccups.

### 2025 0627 1410

1. More memo work

2. The Rules of Passage from "Free Logic, Descriptions, and Virtual Classes" by Quine 1994 and included in "Selected Logic Papers" pg. 279.
The text is paraphrased to conform to the methods of predicate abstracts (see [HOW PREDICATE ABSTRACTS WORK](#how-predicate-abstracts-work)).
    > A basic technique in quantification theory is transformation of a formula in such a way as to bring all its quantifiers out to the beginning (*prenexing*) or, alternatively, to drive every quantifier in so that it governs only clauses in which its variable recurs (*purifying). The transformations depend on eight familiar equivalences called the rules of passage:
    > 1. some {x: p and Fx} if and only if (iff) p and some {x : Fx}
    > 2. each {x: p and Fx} iff p and each {x: Fx}
    > 3. some {x: p or Fx} iff p or some {x: Fx}
    > 4. each {x: p or Fx} iff p or each {x: Fx}
    > 5. some {x: p only if Fx} iff p only if some {x: Fx}
    > 6. each {x: p only if Fx} iff p only if each {x: Fx}
    > 7. some {x: Fx only if p} iff each {x: Fx} only if p
    > 8. each {x: Fx only if p} iff some {x: Fx} only if p
    >
    > Four of these eight fail for the empty universe. Only 1, 4, 6, and 8 carry over.

3. Prenexing and purifying are key to building up uniform methods e.g. pure existentials, sentences/predicate brought into prenex form such that the prefixed quantifiers are a list of universal quantifiers followed by a list of existential quantifiers, are decidedly valid or not.
    > A pure existential is valid if and only if we get a truth functionally valid schema by taking the alternation of the results of substituting the free variables for the existential ones in the matrix.

    Where the matrix is the truth functional part of the prenex form (chop off the quantifiers and the matrix is what you're left with).
(I think the phrase 'matrix' is from Russell.)

4. The decision procedure for validity of pure existentials suggests that when prenexing always bring out universal quantifiars first in the hope that a pure existential shall be got.

5. It would seem that there is no general shortcut to checking if a schema is a pure existential or not other than to go through the steps of prenexing in every way possible.
Something like this seems to follow
    > > each {x: some {y: each {z: Fxy, not Fxx, and Fyz, only if Fxz}}}
    >
    > From a 1933 paper by Godel it is known that this prefix 'each {x: some {y: each {z:' is the simplest that a prenex infinity schema can have.
    > 
    > There is no complete proof procedure for showing schemata to be infinity schemata. For, if there were, we could add it to our complete proof procedure for finite consistency, and get a complete proof procedure for consistency. (pg. 215 of Methods of Logic 4th ed.)

    where 
    > The schemata that are consistent by not finitely so are the stubborn kind. They may be called *infinity schemata*. [pg. 215 of Methods of Logic 4th ed.]

6. The practical course of action is to identify grammatical categories which are just narrow enough to admit a decision procedure.
That there is no complete proof procedure for consistency tells us that this practical course must always leave out some schemata that we may later be interested in.
This is not a sadness because practical action also suggests we work on problems that are just narrow enough to be solved during an average human life span, or on the scale of the human species.

7. Quine's main method mechanizes the following method of disproof (which is easily mistaken as a proof by contradiction, i.e. "*reductio ad absurdum*: the disproof by derivation of a clear contradiction"[pg. 190 MOL4], but which can not be taken as such since, as shall be seen, implication is not forthcoming at each step):
    > We see thus that our main method is a sound one. It is a rather natural one, moreover, despite the austerity of the argument above. The patter (1)-(12) could be verbalized, as a disproof of some conjunction of actual statements in lieu of (1)-(3), along the following line. According to (1), there is something that is *F* to everything. Very well, call it *z*. So we have (4). But (2) said that everything *x* is such that ... . Well then in particular *z* will be that way. So we have (5): that there is something such that ... . Call it *t*. Continuing thus, we get to the contradiction (7), (10)-(12). The premises (1)-(3) are thus disproved. One or two of them may be true, but not all three. [pg. 193 MOL4]

8. It is important to mention that this "verbalization, as a disproof" is natural in that it is the one which can be found in many of Socrate's arguments as told by Plato.
Furthermore, it is that method of argument which often has the strongest rhetorical punch: "so you say that such and such is the case, well let us assume it is, then there is such a thing as what you say, call it 'x', and so it is true of the other thing you've said, but x can not be both so and not so, we must have started wrong."

9. The giant problem in firmly grasping the soundness of the main method is in the apparent absense of implication with respect to steps from existential premises to the lexically restricted instances.
This is beautiful because it explains why the strictures of implication are not apparent throughout Plato.
The role of implication is overblown by modern methods.
Validity is too, but less so, and though proof theory has done the most to curb our enthusiasm, logical methods are, metaphorically, everything and nothing: they deal with everything because they deal with nothing in particular.

10. The soundness of the main method is established beautifully in that it combines, without reference to the complexity of, e.g., 'syntactic models' and problems of 'witnesses', steps from the most familiar results, to a modern student of mathematical logic, without trapping us in terminology or without stepping far from the cumulative results of previous chapters, each of which slides smothly into the next.
One day I may find a better way of explaining this, but it remains astounding to me that so few are unfamiliar with Quine's main method: it is easy to explain, and easy to prove sound, and its completeness proof is delightfully simple (relatively speaking).

## 2025 0626

### 2025 0626 1806

1. What is "Tiny Chef"?

## 2025 0625

### 2025 0625 2333

1. Predicate memo work: bringing together pieces.
> *Once upon a time a man grew up in isolation. He only knew how to grunt. One day, a woman taught him to speak. She held up an apple, said 'red', he held it up, said 'red', and then she smiled. He held up another apple, said 'red', she frowned, held it up, and said 'green'. On and on it went.*
>
> *Predicates* denote or don't, i.e. are true or false of, the occasions on which they occur.
> *Constructions* pair *component* predicates with *connectives*.
> *Lexicons* list *atoms*, predicates without components.
> *Grammars* generate *categories* from repeated constructions on lexicons.
> *Negations*, *caps*, *crops*, *dups*, *drops*, *pushes*, *pops*, and *swaps* pair their component with 'not', 'cap', 'crop', 'dup', 'drop', 'push', 'pop', and 'swap' respectively.
> *Conjunctions*, *alternations*, and *alternative denials* pair their left and right components with 'and', 'or', and 'nor' respectively.
>
> Constructed compounds inheret where they denote from their components.
> Negations denote where and only where (waow) their component doesn't.
> Conjunctions/alternations denote waow each/some of their components do.
> Alternative denials denote waow each of their components don't.
> Caps denote waow there is somewhere denoted by their component.
> 
> Occasions, like conjunctions, alternations, and alternative denials, split into left and right parts: the *pile* and the *list*.
> The list and pile do too: the right part of the pile is called the *top*, the left part of the list is called the *head*, the left part of the pile is called the *rest of the pile* and the right part of the list is called the *rest of the list*.
> 
> One occasion is the *pop* of an other waow 
> 1. the pile of the one is the rest of the pile of the other,
> 2. the head of the one is the top of the other, and
> 3. the rest of the list of the one is the list of the other.
> 
> One occasion is the *push* of an other waow 
> 1. the rest of the pile of the one is the pile of the other,
> 2. the top of the one is the head of the other, and 
> 3. the list of the one is the rest of the list of the other.
> 
> One occasion is the *drop* of an other waow
> 1. the pile of the one is the rest of the pile of the other and 
> 2. the list of the one is the list of the other.
> 
> One occasion is the *dup* of an other waow 
> 1. the top of the one is the top of the other,
> 2. the rest of the pile of the one is the pile of the other, and 
> 3. the list of the one is the list of the other.
> 
> One occasion is the *swap* of an other waow
> 1. the list of the one is the list of the other, 
> 2. the top of the one is the right part of the rest of the pile of the other, and
> 3. the right part of the rest of the pile of other is the top of the other.
> 
> One occasion is the *hem* of an other waow
> 1. the head of the one is the right part of the rest of the pile of the other, 
> 2. the rest of the list of the one is the list of the other, and 
> 3. the pile of the one is the pile of the other.
> 
> The pop, dup, and swap of an occasion can be defined from the push, drop, and *hem* of an occasion.
>
> Two occasions with the same items can be described from the other with push, drop, and hem: **recombic completeness**.
>
> Crops denote the drop of waow its component denotes.
> A component denotes 
> 1. the dup of waow its dup does,
> 2. the drop of waow its drop does,
> 3. the push of waow its push does,
> 4. the pop of waow its pop does,
> 5. the swap of waow its swap does, and
> 6. the hem of waow its hem does.

2. This all seems a bit much:
    1. Can focus be brought to the predicates away from the occasions?
    2. Can Russell's argument for n place predicates be used to simplify and unify here?
    3. Can the presentation be achieved without mentioning ordered pairs explicitly or mentioning their principle of extensionality?
    4. Can the definitions of dup through hem be flipped? Should they be? What do they look like?
    5. What expedients can be introduced that shorten phrases like "the rest of the pile" and "the rest of the list"?

3. The order of introduction, what Russell calls the epipstomological order, is
    1. Grammar
    2. Denotation (of compounds)
    3. Validity

4. Validity can be introduced without mentioning denotation by direct reference to a purported complete proof procedure: how does that change the way I would write this memo on predicate logic?
    1. e.g. via Quine's main method, conjunctions are constructed one after another and checked for truth functional inconsistency, in the grand way prescribed by the completeness proof of the main method in "Methods of Logic 4th edition".
    2. It is unknown whether such a member of this elaborate category is eventually generated by the grammar.
    3. It is known that restrictions to this category, e.g. pure existentials and universals, are eventually generated.

5. What is the recombic version of full alternational normal form?
    1. There must be a standard way of arranging dups, drops, etc.
    2. or perhaps there is something about the undecidability of the word problem lurking here
    3. standard recombic compounds

### 2025 0625 2259

1. Wore earmuffs while thinking in a chair today.

2. Added hint "how to deal with anhedonia and uncertainty"

3. The earmuffs appear to have helped.

4. While wearing them and thinking while sitting I was able to see that I had already written on the operations required to deal with the division of occasions into left and right parts.

5. I was able to then see that it connected exactly as Russell describes in "An Inquiry into Meaning and Truth" and "Human Knowledge".

6. Then the sentences of the previous entry occurred to me, though only the first few since the rest were written at the computer with the earmuffs on.

7. I introduced the 'tie up' section at the beginning of notes.
So far it lists under each loose end some of the entries that need to be tied together by, what I can only suppose, is a memo.
It must be emphasized that this is all still very much a verbal discovery expedition.

### 2025 0625 2043

1. Predicate memo work on the dichotomy of occasions

2. The left part of an occasion is called its pile, and the right part its list.

3. Occasions split into left and right parts: the pile and the list.

4. The list and pile do too: the right part of the pile is called the top, the left part of the list is called the head, the left part of the pile is called the rest of the pile and the right part of the list is called the rest of the list.

5. One occasion is the *pop* of an other where and only where (waow) the pile of the one is the rest of the pile of the other and the head of the one is the top of the other and the rest of the list of the one is the list of the other.

6. One occasion is the *push* of an other waow the rest of the pile of the one is the pile of the other, the top of the one is the head of the other, and the list of the one is the rest of the list of the other.

7. One occasion is the *drop* of an other waow the pile of the one is the rest of the pile of the other and the list of the one is the list of the other.

8. One occasion is the *dup* of an other waow the top of the one is the top of the other and the rest of the pile of the one is the pile of the other, and the list of the one is the list of the other.

9. One occasion is the *swap* of an other waow the list of the one is the list of the other, the top of the one is the right part of the rest of the pile of the other, and the right part of the rest of the pile of other is the top of the other.

10. The pop, dup, and swap of an occasion can be defined from the push, drop, and *hem* of an occasion.

11. One occasion is the hem of an other if the head of the one is the right part of the rest of the pile of the other, the rest of the list of the one is the list of the other, and the pile of the one is the pile of the other.

12. Two occasions with the same items can be described from the other with push, drop, and hem: **recombic completeness**.

13. Read the final two chapters of Russell's "An Inquiry into Meaning and Truth" titled "Analysis" and "Language and Metaphysics", and read "The Principle of Individuation" in "Human Knowledge: its scope and limits" also by Russell.

14. These both deal with the problem at hand: the division of occasions into parts and the influence, if any, this has on logical method.

15. Russell's predicate is 'x is part of y' and mine is 'x pairs y with z'.

16. For Russell, the question is if the predicate 'x is part of y' is or is not always definable in other terms of a given logical theory.

17. An example of something like this is Quine's schematic methods in a schematic theory of indiscernibility/identity.

18. Somehow, in ways that are still beyond me, Russell concludes that predicates as opposed to just proper names/nouns are necessary in some sense which is presumably metaphysical.

19. Russell clings to qualities and other such stuff: it is inaccessible to me.
It is only through carrying over his arguments into something like Quine's "ordered sequence of active receptors" or "global stimulus" that I am likely to grasp what Russell "had in mind".
 
### 2025 0625 1537

1. I began an experiment on controlling my auditory environment.
    1. I bought 3M PELTOR Hearing Protection Optime 98 Earmuffs.
    2. They arrived today.
    3. I'm wearing them now.
    4. I'm sensitive to sounds, though I am grateful that they are not so often overwhelming as similar sounds can be for others.
    5. I recently lost my hearing for a few days becuase of severe allergies: my ears were clogged and only Pseudoephedrine cleared them.
    6. I noticed that it was easier to sit and think when my ears were clogged than it seemed it was when they were not.
    7. It is stil not clear how accurate the principle which generalizes from that instance is.
    8. I listen to music when I work sometimes, and sometimes it feels as if it helps, other times it feels as if it hurts.
    9. These earmuffs are comfortable enough, though I have only been wearing them for about twenty or thirty minutes.
    10. To help me better grasp the effects of wearing or not wearing earmuffs, I may indicate in a note when I am or am not.

### 2025 0625 1426

1. **HOW TO DEAL WITH ANHEDONIA AND UNCERTAINTY**
    1. Read Dalio's "Life and Work Principles" when you feel anhedonia.
    2. Read Quine's "Methods of Logic 4th Ed." when you feel uncertain.

2. More memo work.
    > *Predicates* denote or don't, i.e. are true or false of, the occasions on which they occur.
    > *Constructions* pair *component* predicates with *connectives*.
    > *Lexicons* list *atoms*, predicates without components.
    > *Grammars* generate *categories* from repeated constructions on lexicons.
    > *Negations*, *caps*, *crops*, *drops*, *pushes*, *pops*, and *swaps* pair their component with 'not', 'cap', 'crop', 'drop', 'push', 'pop', and 'swap' respectively.
    > *Conjunctions*, *alternations*, and *alternative denials* pair their left and right components with 'and', 'or', and 'nor' respectively.
    >
    > Constructed compounds inheret where they denote from their components.
    > Negations denote where and only where (waow) their component doesn't.
    > Conjunctions/alternations denote waow each/some of their components do.
    > Alternative denials denote waow each of their components don't.
    > Caps denote waow there is somewhere denoted by their component.
    > 
    > Occasions, like conjunctions, alternations, and alternative denials, are said to have left and right parts.
    > These parts can be divided further into left and right parts e.g. the left part of the left part of the occasion, the right part of the left part of the occasion, and so on.
    > Whether occasions break into an endless dichotomy or whether they terminate into atomic parts is a problem I have not yet solved.
    > 

3. Just as the brave logician checks the empty universe as a degenerate special case, so too must the brave philosopher check the solipsistic universe as a degenerate special case.

4. Reading Bertrand Russells "Inquiry into Meaning and Truth" and "Human Knowledge" to deal with the problem of individuation, entification, reification, etc.

5. Access points to solutions to the individuation problem:
    1. Does 'this' denote or designate?

    2. Ordered pairs are identical if and only if their left parts are identical and their right parts are identical.

6. Few walk the talk.


## 2025 0624

### 2025 0624 1607

1. More work on memo
    > *Predicates* are true or false of the occasions on which they occur.
    > *Constructions* pair *component* predicates with *connectives*.
    > *Lexicons* list *atoms*, predicates without components.
    > *Grammars* generate *categories* from repeated constructions on lexicons e.g. *negations*, *crops*, *drops*, *pushes*, *pops*, and *swaps* pair their component with 'not', 'crop', 'drop', 'push', 'pop', and 'swap' respectively; and *conjunctions*, *alternations*, and *alternative denials* pair their left and right components with 'and', 'or', and 'nor' respectively.

## 2025 0623

### 2025 0623 2130

1. More work on the first paragraph of the memo on predicate logic.
    > Predicates are true or false of the occasions on which they occur e.g. 'red' and 'green'.
    > Constructions make compound predicates from component predicates and connective particles e.g.
    > 1. the conjunction 'red and green' is made from 'red', 'green', and the particle 'and',
    > 2. the alternation 'red or green', and 
    > 3. the negation 'not red'.
    >
    > Lexicons list particles and atoms -- predicates without components.
    > Grammars give lexicons and constructions.

2. Make a list of "loose ends" at the top of these notes: pull the threads.
Better, call it "TIE UP" and just list the loose ends that must be tied up.

### 2025 0623 1521

1. More work on **HOW TO FIND A GOOD PSYCHOLOGIST**
    1. **Never** settle for a bad one.
    2. Good ones are exceptional people who happen to be psychologists.
    3. They are cultural liasons.
    4. You're more likely to find a good psychologist through a friend than you are through a phycisian.

2. The only way to control email is not to have it.

3. The previous drafts of the memo on predicate logic gave an inaccurate definition of predicate logic: they said that predicate logic sorts out which compounds are true or false of which occasions.

4. Predicate logic sorts out valid ones from the others.

5. Validity is defined from grammar and denotation (where 'denotes' is short for 'true of' so that each predicate denotes each seperate item it is true of; by contrast, a singular term purports to designate one and only one item).

6. How is validity defined from grammar and denotation?

7. Grammar not only sorts out what counts as a predicate and what doesn't, it also explains how to construct and deconstruct a predicate into its most basic parts, and it is through these basic parts that denotation of compound predicates of a theory are defined (this being Tarski's analysis of truth applied to a consistent theory of denotation).

8. What are the methods of grammar?

9. [Sidorov S.A. (MSU), Shumakov M.N. (NIISI RAN) DSSP AND FORTH. COMPARE ANALYSIS](https://web.archive.org/web/20230405010717/http://brokestream.com/daf.txt)

10. The methods of grammar relevant to logic are

    1. Accept or reject the form of a response as that from a given language.

    2. Recall, a language is a collection of the reinforcing practices of a verbal community: it is a social (verbal) environment composed of verbal contingencies that are built, maintained, and mediated by speakers and listeners.

    3. Like the methods of any other effective science, grammar is primitive recursive: each of its steps conspicuously initiates and terminates with the same kind of control we have come to expect from swinging a hammer at a nail.

        1. There is an unhealthy outlook in science and logic that follows primarly from misinterpretations of the absense of a decision procedure for predicate logic that I must correct.

        2. Given a complete proof procedure for predicate logic it is known that each step prescribed by the procedure is mechanically given and one step is taken after another just as mechanically, but, on any given walk down the path(s) prescribed by the procedure, there is no guarentee of a terminus.

        3. This is then interpreted as saying that, as with the halting problem of Turing's machines, there is always a next step that must be taken after any given step and that once a walk has begun it is unending.

        4. I've tried to sketch the situation using language similar to that of Zeno's paradoxes in order to bring it up indirectly.

        5. We do not, as logicians, get stuck, as in Zeno's Dichotomy paradox, whenever go on the path set out by a proof procedure (note, I am, almost always, speaking with Quine's main method in mind because it is a straight path and mechanically prescribed so there is no whiff of a problem like that of Frost's road not taken).

        6. At each step we may stop, and often do: note how frustrating it is to finally find a proof and to then try to explain how it occurred to you (this is another important feature of Quine's main method: the mechanical method can be subverted immediately by a happy accident which can be run in a zig zag fashion with the grand and comprehensive procedure prescribed in the completeness proof of the main method).

        7. That we stop at some step rather than get stuck on an endless walk makes all the difference between primitive recursive methods and those which purport to be general recursive.

        8. The moral of the story, which I have not done a good enough job to teach, is that the metaphor of general recursion does not carry over to the other sciences in the same way that the metaphor of a convergent sequences does not carry over to a walk down Zeon's Dichotomous lane.  

    4. The lexicon of a grammar lists the connective particles and atomic predicates which together with rules for connecting component predicates with connective particles into compound predicates, are said to generate the forms of response of a language grammatically.

    5. Each construction rule is expected to have a unique deconstruction rule when dealing with a logical grammar: this eliminates ambiguities which florish in nonlogical langauges and speeds up the grammatical analysis of a compound into its particles and atoms.

### 2025 0623 1412

1. How to find a good psychologist
    1. A good psychologist is an exceptional person who happens to be a psychologist.
    2. A good psychologist is a cultural liason.

2. How to sit and think
    1. Sit in a fine wooden rocking chair e.g. from the Amish.
    2. Close your eyes and rock away the world around you until you begin to settle into each breath.
    3. As you settle into each breath you rock less.
    4. Think to unify.
    5. Breath to relax every part of your head and neck: the chair must be such that it is hard yet comfortable, this is acheived by the design of the chair in accordance with human anatomy so that when the back of the head is rested upon the chair and the arms are rested upon the arms of the chair, the neck and back are not tight and are stretched ever so slightly into a comfortable resting position.
    6. The chair must be made of strips of wood so that the chair does not retain the heat of the body as it would if it i was a solid piece of wood, slots for air to pass through

## 2025 0622

### 2025 0622 1516

1. Where many say "That's how it is.", scientists say "I don't know."

2. [The Origins of Cognitive Thought](https://www.appstate.edu/~steelekm/classes/psy5150/Documents/Skinner1989.pdf) by B. F. Skinner 1989

3. More work on the first memo. Must include definition of validitiy which requires definition of lexicon and grammatical compounds.
    > *Once upon a time a man grew up in isolation. He only knew how to grunt. One day, a woman taught him to speak. She held up an apple, said 'red', he held it up, said 'red', and then she smiled. He held up another apple, said 'red', she frowned, held it up, and said 'green'. On and on it went.*
    >
    > Phrases like 'red' and 'green' are **predicates**: they are true or false *of* the occasions on which they occur.
    > **Compound** predicates are made from **component** predicates and grammatical **particles**.

4. Microcassette recorders are so expensive because no one manufactures them anymore.

### 2025 0622 1237

1. Why are microcassette recorders so expensive? Some are hundreds of dollars.

2. More work on the first memo.
    > *Once upon a time, a man grew up alone in the jungle. He only knew how to grunt. One day, a woman taught him to speak. She held up an apple, said 'red', he held it up, said 'red', and then she smiled. He held up another apple, said 'red', she frowned, held it up, and said 'green'. On and on it went.*
    >
    > Phrases like 'red' and 'green' are called **predicates** because they are true or false *of* the occasion on which they are said.
    > **Compound** predicates are built from **component** predicates and grammatical **particles** e.g. 'red and round' from 'red' and 'round' and the particle 'and'.
    > **Atomic** predicates have no components e.g. 'red', 'parent of', 'takes'.
    > **Predicate logic** sorts out which compounds are true or false of which occasions.
    > The rest of the sciences sort out which atoms are true or false of which occasions.
    >
    > Technically, one place predicates -- e.g. 'dog', 'round', 'runs' -- are true or false of items; two place predicates -- e.g. 'parent of', 'ontop of', 'less than' -- are true or false of ordered pairs of items; three place predicates are ture or false of ordered triples; and so on.


## 2025 0620

### 2025 0620 1506
1. Notes on
    1. implicit quantifier notation (aka. Quine's or Dreben's functional normal form)
    2. Substitution and Unification
    3. Modified Modus Ponens
    4. Forward and backward chaining

    from [Artificial Intelligence: Knowledge Representation and Reasoning](https://youtube.com/playlist?list=PLyqSpQzTE6M-t-Qr2Z0Gf_KP5RC97uOKC&si=08qpVdUiKkdTtJu6)

2. The problem to solve here is that the presentation by Deepak Khermani includes gems that are obscured by the clutter of murky logical methods.
First I have to introduce the list notation that goes with the methods of logic when they developed within a LISP like programming language.

3. The aim is to pull these methods inside out: from logic programming to stack (or what I call pile) programming languages and then list (or what is properly called 'sequence') programming langauges.
None of these sentences are satisfactory. *frown*

4. Space seperated parentheticals are used to write out the grammatical structure of a logical compound composed of variables and dummy predicate letters from the particles of grammar as follows.
    1. '(and F G)' for 'F and G' (or, properly, '(F) and (G)')
    2. '(and F ..G H)' for 'F, ..G, and H'
    3. '(or F G)' for 'F or G'
    4. '(or F ..G H)' for 'F, ..G, or H'
    5. '(not F)' for 'not F'
    6. '(some F)' for 'some F'
    7. '(each F)' for 'each F'
    8. '(such (..x) F)' for '{..x: F}'
    9. '(is (..x) F)' for 'F..x'

5. It occurs to me that 4 gives something like predicate functors until 'such' and 'is' are introduced.

6. Under this notation, concretion is:
    > (iff (is (..y) (such (..x) (is (..x) F))) (is (..y) F))

    or
    > (iff (is (..y) (such (..x) (is (..x ..z) F))) (is (..y ..z) F))

    where the length of ..y and ..x must be equal, or some convention like Quine's in 'Philosophy of logic' for satisfaction by sequences, or my convention of remainder of division by length, or remainder of division by number of children and cycling back to root when indexing beyond an atom.

7. I must give an example of that last one to make it concrete.
A binary tree like '((1, 2), 3)' can be indexed into with a binary sequence '01011' so that the following are equal (in this context)
    1. ((1, 2), 3) 01011
    2. (1,2) 1011
    3. 2 011
    4. ((1, 2), 3) 11
    5. 3 1
    6. ((1, 2), 3)

    Another example with sequences -- e.g. '(1 2 3 4)' is short for '(1, (2, (3, (4, ()))))' where '()' is the empty binary tree -- that demonstrates the remainder of the quotient by the length of the sequence.
    The following are equal (in this context):
    1. (1 2 (3 4) ((5 6) 7 8 9)) (10 9 8 7)
    2. (3 4) (9 8 7)
    4. 4 (8 7)
    5. (1 2 (3 4) ((5 6) 7 8 9)) (7)
    6. ((5 6) 7 8 9)

8. Quine's way is to repeat the last item as you do when you write unending decimal numbers that repeat the same digit over and over again e.g. zero point nine repeating (usually indicated by putting a line over the last digit that is to be repeated).

9. Note, the phrase 'in this context' is needed because indexing into a number like '4' as an atom (which it is in this case) could turn up an entirely different sequence in a different context.
That is, numbers can occur within any sequence just as sequences can occur within sequences and the sequences within which they occur are their respective contexts.

### 2025 0620 1337
1. Short story for predicate logic memo
> Once upon a time, a man grew up alone in the jungle and only learned to grunt.
> One day, a woman taught him to speak.
> She held up an apple, said 'red', he held it up, said 'red', and she smiled.
> He held up another apple, said 'red', she frowned, held it up, and said 'green'.
> On and on it went.

2. Updated some hints (old versions can be found at [2025 0610 1606](#2025-0610-1606)).
    > **HOW TO MAKE IT LOGICAL**
    >
    > 1. List the nouns, adjectives, and verbs you use to talk about it.
    > 2. Define as many as you can from as few as you can.
    > 3. List true sentences made from those few.
    > 4. Conclude as many as you can from as few as you can.
    > 5. Go to 1.
    >
    > **HOW TO CONTROL YOUR BEHAVIOR**
    >
    > 1. Consequences select behavior.
    > 2. Control consequences to control behavior.
    > 3. Build new behavior from old behavior.
    > 4. Build new controls from old controls.

3. I didn't take a hint: instead of editing a fresh copy of the story I wrote in 1, I made edits to 1 directly.
This even after having written 2.
I've deprived myself and others of the opportunity to better grasp the origins of my verbal behavior by erasing past records.
Don't do that.

4. The memo on predicate logic may talk about the shapes of predicates: one, two, three, and many place predicates are a run of familiarly shaped predicates.
Since we have names for numbers in the vernacular and we do not have names for trees in the vernacular, the many place predicates are more familiar than the tree shaped predicates.
Instead of saying 'many place predicates' it is technically correct to say 'sequence shaped predicates', but such technical terms are not to be introduced lightly.

5. Technical terms weigh heavy on the reader: they slow things down rather than speed things up.

6. Drink more water than you think you need (largely because you can).

7. Primitive recursive analysis is significant if not just because it is that which follows almost immediately from the methods of logic programming.

8. More work on the first paragraph of the memo on predicate logic.
    1. Instead of saying 'true of' or 'false of' we say 'denotes' or 'does not denote'
    2. We say 'denotes' for 'true of' and 'does not denote' for 'false of'.
    3. Designation plays no part at all in predicate logic.
    4. Denotation is distinguish from designation where a singular term purports to designate on and only one item 
    5. A predicate denotes each item of which it is true and a singular term purports to designate one and only one item.
    6. Without loss of generality, designation plays no part in predicate logic.
    7. The conjunction of similarly shaped predicates denotes where and only where (waow) each does; and their alternation denotes waow some do.
    8. Negations denote waow their component does not.
    9. A compound is compounded functionally from its components with respect to some one place predicate waow whether the predicate does or doesn't denote the compound is unchanged by substitution of components for components which the predicate correspondingly denotes or not.
    10. whether the predicate denotes the compound, or not, is unchanged by substitution of components for others which the predicate correspondingly denotes or not
    11. Conjunctions of similarly shaped predicates denote where and only where (waow) each component does -- e.g. 'red and round' denotes waow 'red' denotes and 'round' denotes-- and alternations denote waow some component does -- e.g. 'blue or square' denotes waow 'blue' denotes or 'square' denotes'.
    12. Link up the similarly shaped predicates 'red' and 'round' with the grammatical particle 'and' to build the compound predicate 'red and round' called the conjunction of the components.
    13. There are at least 4 grammatical particles in predicate logic. The logical one: cropped alternative denial. And, the recombic ones: drop, hem, push.

9. Present the grammatical part first, what is otherwise mentioned unhappily as 'well formed formula', then present the denotative part, what is otherwise mentioned very unhappily as models and proofs.

10. It is only because logical behavior is being conditioned from a native language that validity is first defined as 'denotes from every substitution and supplementation of lexicon' and then the main method is given from that.
Otherwise, the main method is all that is needed even though it seems very arbitrary indeed.
In such a case the main method includes the full alternational normal form got by flattening the tree of alternational developments.
Like the alternational developments of compound predicates, the main method goes from one conjunction to another with the special form of trailing conjunctions each of which is an instant of the previous one with appropriately supplemented predicates for instances of (existential) croppings. 


## 2025 0619

### 2025 0619 2004

1. Vernacular has number names but not tree names.

2. Just as we can speak of the shape of an array in the J programming language, so too can I speak of the shape of a predicate.

3. Predicates can be tree shaped, sequence shaped, and any other shape that can be set up from the introduction of ordered pairs as those items which are identical where and only where their left components are identical and the right components are identical.

4. As Quine mentions on page 37 of "Philosophy of Logic 2nd edition"
    > Longer sequences can be defined by iteration: (x,y,z) as ((x,y),z), then (x,y,z,w) as ((x,y),z,w), and so on. But David Miller has shown me that this method leads to ambiguities of length if we admit sequences of different lengths in the same contexts, as I shall be doing.

    and as Feferman mentions on page 4 of "Finitary Inductively Presented Logic" (with my paraphrasing)
    > *Abbreviations.* (x,y) := the pair whose left part is x and whose right part is y; (x) := x; (...x, y) := ((...x), y).
    > Note that this representation makes each n-tuple (...x,...y) an m-tuple ((...x),...y) since (...x,...y) = ((...x),...y).
    > Finite sequences will be represented below in a modified form in such a way that each finite sequence has definite length.

5. Another attempt to write the short story starting the memo on predicate logic
    > Once upon a time a man grew up grunting in the jungle until a woman taught him to speak.
    > She held up an apple, said âredâ, he held it up, slowly said âredâ, and then she smiled.
    > He held up another apple, said âredâ, she frowned, held it up, and then said âgreenâ.

### 2025 0619 1718
1. The first memo must be on predicate (functor) logic.

2. It must combine Quine's "Methods of Logic 4th edition", "Philosophy of Logic 2nd edition", and the 1954 entry in the Encyclopedia Americana on symbolic logic.

3. The encyclopedia entry is included in "selected logic papers Enlarged edition".

4. It covers the entirety of an earlier edition of methods of logic in a few pages.

5. Many arguments are left out and since it was written in 1954 it does not contain Quine's main method.

6. It also doesn't contain the distinctions that Quine would later point up as distinctions between logic and mathematics.

7. I submit that the methods of logic can be completely, clearly, and effectively given in a memo of only a few pages.

8.  The key is to entirely avoid the complexities of variables, schema, truth values, and other supplementary methods which obscure the role that grammar and denotation play in logic.

9. Here is a short story I wrote as a candidate introduction to the memo on predicate logic:
> Once upon a time a man grew up in the jungle learning only how to grunt until one day a woman taught him to speak.
> She held up an apple, said 'red', the man help it up, grunted 'red', and she showered him with praises.
> He held up another apple, grunted 'red', she said 'no', held it up, and said 'green'.
> So the story goes.

10. It occurred to me that a single paragraph of italic text could be put at the beginning of a memo to tell a story that makes it eaiser to find the memo and to set up its conclusions.

11. In this case the story sets up the first paragraph
> Sayings like 'red' and 'green' are called predicates because they are true or false *of* the occasion on which they are said.
> Technically, one place predicates -- e.g. 'dog', 'round', 'runs' -- are true or false of items; two place predicates -- e.g. 'parent of', 'ontop of', 'less than' -- are true or false of ordered pairs of items; three place predicates are ture or false of ordered triples; and so on.

12. It just occurred to me that when taking notes it is probably best that I number each phrase I compose rather than simply bullet point them; I just looked at a preview of this entry where bullet points where used instead of numbers and it was clear that numbers would help more than bullet points when finding and perhaps mentioning the specifics of this note in any note (including this one).

13. Something about the specificity of numbers makes them concrete markers when read as part of a page of text.

14. Bullet points just run together like a laundry list: this is an accurate report not a laundry list, and, ultimately, it aims to link conclusions with templates; do numbers help this? Yes. They enforce an order that is part of how the writing occurred.

15. It is a common mistake for people to say that their thoughts are multidimensional or tree like, as if there is a branch tree of choices that they are able to contemplate as a kind of superposition of comparisons; but that is inaccurate; we behave linearly and locally; it is only the mentalistic metaphors that point to some other world where the topology of concepts takes on supernatural shapes.

16. When writing numbered lists like these in markdown, put a space between each numbered item.
If you don't then what starts out as an easy to navigate number list turns into a big blob and your eye has to hunt for where the numbers are and there's always the question if they are just the end of a sentence that coincidentally runs onto the beginning of the next line.

17. I should be able to look back on past notes and see the change in style from before and after I took the advice in the book "Writing that Works" by Roman and Raphaelson.

18. Another thing that writing with numbered lists allows me is to avoid complete sentences i.e. to avoid adding needless words; I can just go on to the next thing and leave the last thing behind; this is not the case when it comes to memos, but, there, numbered lists serve an even more powerful purpose: they can churn up effective action again and again, and if there is a particularly powerful item, its number can be learned and make finding it even that much easier.

19. Numbering items also makes it easier to break things into managable chunks.

20. A second attempt at writing the single paragraph story from 9.
> Once upon a time a man grew up in the jungle.
> The only noises he made were grunts until one day a woman taught him to speak.
> She held up an apple, said 'red', he held it up, slowly said 'red', and she mirrored for him to take a bite out of it.
> He held up another apple, said 'red', she held it up, said 'no', then said 'green'.

21. I'm apt to change 'sayings' to 'phrases' because 'phrase' has an etymology closer to that of verbal response.

22. In the past, 'verbal response' was what I would write when talking about logical behavior as a kind of verbal behavior, but it does not work when I'm talking to those who are not already involved in the science of behavior or who are unfamiliar with the details of its experimental analyses.
Technical terms like 'experimental analysis' is unlikely to be read with the trappings of a concrete noun like 'gadget', although an experimental analysis can be described concretely as what goes on in a lab.
Sadly, this then leaves the phrase further undefined: what happens in labs?
Does the person reading this know what really goings on in labs?
It is as part of an answer to this question that I think Skinner was primed to frequently refer to "Wundt's brass instruments".

23. Another practical reason to number these notes is that they are more likely than ever to be referenced in conversations with others.
I am learning more about how best to connect with others without burdening them with time wasted or the promise there of.
Most of my writing is of this sort, and whatever I can do to cut through it as early as possible, i.e. to make accurate and salient records of thinking and feeling, the faster I can get out effective memos, make effective conclusions, and build up a treasure trove of strong templates.

24. Second attempt at writing the first paragraph from 11.
> Phrases like 'red' and 'green' are predicates.
> They are true or false *of* the occasion on which they are said.
> - When they are said, they are true or false *of* the occasion.
> - They are true or false *of* where they are said.
> - They are true or false where they are said.

25. The phrases 'true of' and 'false of' are for those who are unfamiliar with 'denotes' and for those who do not yet distinguish between 'denotes' and 'designates'.
Following Quine -- who followed Mill -- I say that predicates denote each thing they are true of and singular terms purport to designate one and only one thing.

26. It just occurred to me that my single sentence explanation of how predicate abstracs work is worthy of joining the other hints at the top of this document.
    I edited the single paragarph 
    >The English relative clause 'who loves Dick' and the pidgin 'x such that x loves Dick' are uniformly paraphrased by the *predicate abstract* '{x: x loves Dick}' which *abstracts* 'Tom' from 'Tom loves Dick' by *binding* the *free* occurrence of 'x' in the *open* sentence 'x loves Dick' with the prefix 'x:' so that the *predication* '{x:x loves Dick}Tom' *concretes* to 'Tom loves Dick': whatever can be said of a thing can be said by predicating a predicate of it i.e. *predicational completeness*.

    into 
    > **HOW PREDICATE ABSTRACTS WORK**
    >
    > 1. The English relative clause 'who loves Dick' and the pidgin 'x such that x loves Dick' are uniformly paraphrased by the *predicate abstract* '{x: x loves Dick}'.
    > 2. It *abstracts* 'Tom' from 'Tom loves Dick' by *binding* the *free* occurrence of 'x' in the *open* sentence 'x loves Dick' with the prefix 'x:'.
    > 3. The *predication* '{x:x loves Dick}Tom' *concretes* to 'Tom loves Dick'.
    > 4. **predicational completeness** *whatever can be said of a thing can be said by predicating a predicate of it*

27. Third attempt at editing 11.
> Phrases like 'red' and 'green' are predicates.
> They are true or false *of* where they are said.
> Specifically, one place predicates -- e.g. 'dog', 'round', 'runs' -- are true or false of items; two place predicates -- e.g. 'parent of', 'ontop of', 'less than' -- are true or false of ordered pairs of items; three place predicates are ture or false of ordered triples; and so on.
> Technically, there are types of predicates for every shape of nesting of ordered pairs e.g. nested ordered pairs are, in general, called binary trees, and n-tuples are taken as sequences which are themselves special binary trees which terminate down their left or right branch with the empty tree (a unique binary tree which has no left or right components).

28. I was delighted when I read Quine's encyclopedia article on 'symbolic logic' to see that he immediately emphasized alternational normal forms before introducing conditionals or biconditionals.
In his presentation he has access to the truth values and interpretations of dummy sentence letters as truth values (and then the dummy predicate letters are having assigned a truth value for each tuple of items in the prescribed "universe of discourse").
These methods fail to emphasize the key to modern logical methods: Tarski's analysis of truth.

29. Tarski's analysis of truth points up exactly those ways in which logical methods are derivative of the methods of grammar and the methods of denotation.
Most people are familiar with the methods of grammar: they are taught that complete sentences have a subject and a predicate and that compound sentences are composed from component sentences that are compounded with grammatical particles like 'and', 'not', or 'or'.
Most people are unfamiliar with the methods of denotation: they are not taught how verbal behavior is shaped by the reinforcing practices of a verbal community as in the story about the man who grunted in the jungle until he met a woman who spoke and listened in English.

30. Just lost like 10 minutes to twitter: picking it up to send off a tweet is followed by scrolling.
The origins of picking up the phone to send the tweet in the first place are in that elusive schedule of reinforcement called "posting".
Just as babies appear to be born babblers, adults appear to be born tweeters [groucho face].

31. I recently became interested in how subtitles and captions are composed.
I watch almost everything with subtitles on, and happened to notice that those of BritBox were exemplary.
Here are some hints on making captions work: [Captioning Tip Sheet](https://dcmp.org/learn/225-captioning-tip-sheet) and [Captioning Key](https://dcmp.org/learn/captioningkey)

32. Went away from the keyboard to make some book recommendations for a friend; got lost reading on evolutionary biology; fear I'm letting my free time slip away unproductively (making a book recommendation is productive, getting lost reading something that is not scheduled is unproductive).

33. Should I start a new timestamped note after a certain number of items have been listed? after I've gotten up from the keyboard?

34. I wrote some notes on 'how to sit and think' and 'how to walk and think', but I can't find them now.
I'm mad.

35. Because of 32, I'm going to do dishes.

## 2025 0618

### 2025 0618 2104
Some rejected designs for the hint on what I must do before I die:

> **WHAT I MUST DO BEFORE I DIE**
> 1. Discover, predict, and control changes--*in counts, rates, and accelerations*
> 2. as selections from variations--*on physical, chemical, biological, behavioral, and cultural scales*
> 3. by making and maintaining strong practices--*mediated by strong people marked by strong principles*
> 4. from the sciences: 
> 5. logic--*denotative, Boolean, functor*
> 6. mathematics--*calculi, collections, categories*
> 7. physics--*quantum field theory, statistical thermodynamics, gravity*
> 8. chemistry--*phyiscal, biophysical, and biological*
> 9. biology--*oranelles, organisms, environments*
> 10. behavior--*biological, biosocial, social*
> 11. culture--*history, science, technology*

> **WHAT I MUST DO BEFORE I DIE**
> 1. Discover, predict, and control changes
>   - *in counts, rates, and accelerations*
> 2. as selections from variations 
>    - *on physical, chemical, biological, behavioral, and cultural scales*
> 3. by making and maintaining strong practices 
>    - *mediated by strong people marked by strong principles*
> 4. from the sciences 
>    - *logic--denotative, Boolean, functor*
>   - *mathematics--calculi, collections, categories*
>   - *physics--quantum field theory, statistical thermodynamics, gravity*
>   - *chemistry--phyiscal, biophysical, and biological*
>   - *biology--oranelles, organisms, environments*
>   - *behavior--biological, biosocial, social*
>   - *culture--history, science, technology*



### 2025 0618 2009
These dated entries are now officially notes in the following sense.

**HOW TO DO IT**

Conclude from contemplation on templates:
1. Notes link templates to conclusions.
2. Memos link contemplations to templates.
3. Hints link conclusions to contemplations.
4. Good notes accurately report.
5. Good memos accelerate change.
6. Good hints adaptively govern.


Memos and hints evolve from notes.

I'm now writing under the control of 

**HOW TO WRITE WELL 99% OF THE TIME**

1. Read "Writing that Works" by Joel Raphaelson and Kenneth Roman.
2. Read "The Elements of Style" by E. B. White and William Strunk Jr.
3. Go to 1

**HOW TO WRITE WELL THE OTHER 1% OF THE TIME**

Journal your thoughts and feelings.


### 2025 0618 1953

**HOW TO WRITE WELL 99% OF THE TIME**
1. Read "Writing that Works" by Joel Raphaelson and Kenneth Roman.
2. Read "The Elements of Style" by E. B. White and William Strunk Jr.
3. Go to 1

**HOW TO WRITE WELL THE OTHER 1% OF THE TIME**

Journal your thoughts and feelings.

**HOW TO SOLVE MATH PROBLEMS WITH MY NEPHEWS OVER THE PHONE**
1. Show me the problems.
2. Show me the work.
3. Call me.
4. Talk it out.
5. Share hints.

**FROM TWITTER**
- "The Golden Road" by William Dalrymple
- Hard Chip: MOSFETs and Spaceships
- Better methods tend to deal with more pains--not fewer.
- Sir Gary Oldman
- Cascadian Chorale performs "Reqium" by Jeremy Kings
- Replace 'rules' with 'hints'.
- "Augmented Vertex Block Descent" by Giles, Diaz, and Yuksel--compare to XPBD


## 2025 0616

### 2025 0616 1545

Evolving sentences.

> Quotations record verbal responses without recording their occasion or their consequence.

Became:

> Quotations show you what was said without showing you who said it, when they said it, where they said it, or what hapened after they said it e.g. 'This is red.' does not show that a child said it of a blue ball to an adult who then said 'No, that is blue.'

Now for some live work.

1. Linguists find rules for making all the quotations they meet.
2. Linguists list quotations and make rules for generating as many quotations on the list as they can.
3. Linguists look at quotations. 
4. Linguists collect quotations.
5. Linguist collect quotations and organize them into languages and grammars.
6. Linguist collect quotations and behaviorists collect the quotation, the occasion on which it occurred, and the consequence.
7. Logic, borrowing from the linguists as grammarians, 

## 2025 0610

### 2025 0610 1648
Notes on "the brain is not a computer".


The brain is not a computer: it is not a consequence of centuries of science and logic.
It evolved, like any other organ, by natural selection by consequences from variations.

It evolved without anything like the deliberate designs which brought about modern digital computers and their analogue predecessors.
We can say that the brain evolved as part of a behaving organism and that the practices which brought about computers evolved as social environments brought more and more of their deferred consequences under scientific and logical control.

A common mistake, especially among brain scientists, is to say that the mind is what the brain does.
That different parts of the mind are the computations which run on different parts of the brain as biochemical computers.

What I finally wrote (and I regret sending it):

> The brain is not a computer in the same way that the physical world is not an equation. Computers are a consequence of centuries of science and logic, but the brain is a consequence of some 4 billion years of natural selection. Zeno's paradoxes teach how such metaphors miscarry.


### 2025 0610 1606

**HOW TO CONTROL YOUR BEHAVIOR**

Your behavior is under the control of its consequences.
The more control you have over the consequences of your behavior, the more (self) control you have over your behavior.
1. Find the controls you have.
2. Build better controls from them.
3. Go to 1.


**HOW TO MAKE A THEORY LOGICAL**

1. List the nouns and verbs you use when you talk about it.
2. Define as many of them as you can from as few as you can.
3. List sentences made from those few that are true.
4. Conclude as many of them as you can from as few as you can.
5. Go to 1.


**HOW TO ACT**

Conclude from contemplation on templates:
1. Notes link templates to conclusions.
2. Memos link contemplations to templates.
3. Rules link conclusions to contemplations.
4. Good notes accurately report.
5. Good memos accelerate change.
6. Good rules adaptively govern.

If memos, notes, and rules were lights of a traffic signal they would be colored red, yellow, and green.

Logic is the axis on which the lever arm of language turns.
Without logic, all the work in the world is without fixed purpose.


## 2025 0608

### 2025 0608 1515
This continues work on my paper on logic from [2025 0521 1232](#2025-0521-1232).

In order to better show how old notation gives way to better methods by way of the substitutional definition of validity, and to take a big step towards finishing my paper on logic, I shall write out my thinking as I prove the theorems which, at first, appear to decidedly take truth functional (denotative functional) logic into truth functional and existential closure logic of one place predicates (Boolean closure logic).

This corresponds to the arguments from chapter 18 of Quine's "Methods of Logic 4th edition".
First a review of terms:

> Compounding is (denotative) functional when, exclusively, each like compound denotes or each like compound does not denote where and only where (waow), exclusively, each like component denotes or each like component does not denote. Chains are compounds compounded functionally e.g. joint denials denote waow each of their components do not, negations are self joint denials (they denote waow their component does not), alternations are negations of joint denials (they denote waow some of their components do), and conjunctions are joint denials of negations (they denote waow each of their components do).
>
> Subcompounds of compounds are their self or those of their components. Subsitutions of like compounds for like nonchain subcompounds are (denotative) functional. Compounds are (functionally) valid waow each of their functional substitutions denotes everywhere, consistent waow their negation is nonvalid (some of their substitutions denote somewhere), implied by others waow the conjunction of their self (the conclusion) with the negation of the other (the premise) is nonconsistent (each of their functional substitutions denotes where the same of the other does), and equivalent to others waow they are mutually implicative (each of their functional substitutions denotes waow the same of the other does).

The last addition made to the evolving paper was the definition of (existential) closures and what I call Boolean chains and what Quine called "Boolean statement schemata".
Quine could call them statements, or, as he tended to in his later work, sentences because they were built directly from the truth functional combinations of sentences as instances of truth functional schema of sentence letters e.g. 'It is raining or it is cold' is an instance of the truth functional schema 'p or q' where 'p' and 'q' are sentence letters and 'p' is substituded by the sentence 'it is raining' and 'q' is substituted by the sentence 'it is cold'.

Here, there are only predicates, and the shift from 'is true' to 'is true of' is coupled with a disavowal of schematic methods entirely for purely grammatical ones e.g. 'rain or cold' is the alternation of the predicates 'rain' and 'cold'.
Thus, the alternation of 'rain' and 'cold' is denotes where and only where 'rain' denotes and 'cold' denotes i.e. 'rain or cold' is true where and only where 'rain' or 'cold' is.
To be absolutely clear in traditional terms: there is somewhere, call it x, such that 'rain or cold' denotes x if and only if 'rain' denotes x or 'cold' denotes x.
I am hesitant to put things in these terms because they do not properly carry the methods at hand.

They posit some item (here called 'x') which poses as an individuated or reified object, but what I have in mind is much more like an undifferentiated occasion or, now speaking with overt metaphors, a global stimulation or a kind of passing show or, unhappily, a unitary essence (though I would never agree to anything like this last blatently metaphysical description).

The plan is to attack the problem of individuation and reification through the abstract items known as ordered pairs and subject to the single rule that ordered pairs are identical if and only if their corresponding components are.
This simple plan is actually much more difficult to execute than it would seem because the problem of individuation and reification are difficult.
They are difficult in that individuation and reification are remote from talk of them: they are not easy to examine by speaking and listening alone.
The wedge of simplicity that I plan on pushing through this problem is that talk of the left and right part of an ordered pair is sufficiently similar to what is called for by individuation that it can sturdy any drift into metaphysical speculation.

This long prelude to what is ultimately a report on my discovery expedition into a proof about Boolean chains is warrented by the problem posed by introducing the principle to prove.
Quine was able to give the law as follows

> If a Boolean term schema is consistent, then in any universe containing a given object there is an interpretation fo term letters that makes the schema come out true of that object. [pg. 115]

A Boolean term schema is just a truth functional composition of one place predicates which Quine then paraphrases into truth functional sentence schema by distributing the name of an object, e.g. 'x', down the truth functional connectives until it is predicated of each one place predicate.
The steps are given by the first paragraph of chapter 18:

> To say of an object x that it is an F, we write 'Fx'. Here then is a new sort of sentence schema: 'Fx', 'Gx', etc. These may be compounded by truth functions; e.g. 'Fx and not Gx, or Gx only if Hx'. We may conveniently abbreviate such compounds by extracting the 'x' everywhere and putting it at the end, thus: '[F and not G, or G only if H]x'. We arrive in this way at schematic representations of certain complex terms: schemat such as 'not G', 'F and G', 'G only if H', 'F and not G, or G only if H', etc. They will be called *Boolean term schemata*. [pg. 114]

Since I begin with predicates rather than sentences, I have no recourse to the logic of sentences and their truth functional structure.
I also do not have access to methods of truth value analysis that Quine uses throughout his text.
The consequence of these restrictions is a prestine simplicity: what comes out as truth functional analysis in Quine's text comes out as equivalence to succesive alternational developments of subcomponents.
I've not gone through the details of this development in the paper yet because it would break the law of premature optimization.

Most importantly, I do not have names for things: variables play no part at all nor do singular terms nor do singular descriptions.
All of these carry over into singular predicates which denote one and only one item: all methods of designation reduce to this use.
Predication is achieved by, e.g., an existential cropping of a conjunction of a singular predicate with a one place predicate.
Existential croppings are straight out of Quine's presentations of predicate functor logic.
Croppings do not crop up anywhere in the Boolean parts of my paper on logic.

Returning now to the law at hand in Quine's words.
The consistency of the Boolean term schema of the law guarentees that there is some object of some universe under some interpretation that it is true of.
What the law says is something more than what consistency directly implies: pick an object in any universe whatsoever and there is an intrepretation that makes that consistent Boolean term schema true of that object in that universe.
Quine's argument is short and simple in ways that are not accessible directly by my methods:

> For, being consistent, the schema resolves to the true under some substitution of the true and the false for the term letters. We have merely to interpret each term letter as true or false of the given object according as its substitute was the true or the false. [pg. 115]

One note on my quotation of Quine's words: I've replaced quotations of a special bold letter tee and its upside down reflection with the phrases 'the true' and 'the false' even though Quine otherwise mentioned these notations as short for 'truth' and 'falsity' as truth values.
This is primarly to draw a closer connection between Frege's outlook and Quine's (it is mostly to remind my future self that there is more to say about this later).

> Anywhere is denoted by some functional substitution of a consistency.

That is the problem to solve.
It is not solved here because I stopped here writing before I solved it.

## 2025 0605

### 2025 0605 1541

Henkin's theorem is presented in Ebbinghaus, Flum, and Thomas as 

> Let capital phi be a consistent set of formulas which is negation complete and contains witnesses. Then, for all phi, 
> > the term interpretation of capital phi satisfies phi if and only if capital phi proves phi.

This reveals one of the weaknesses of Ebbinghaus, Flum, and Thomas' book (and a lot of books on mathematical logic): a needlessly complex sequence of symbols and definitions are lofted atop each other in haste to establish a narrow statement of an otherwise general result.

There are a number of things that I have to get out of the way in order to write on this:

1. Almost all books on mathematical logic (and some on logic) make a grand dichotomy between syntax and semantics as if these are two complementary and comprehensive classifications of the methods of logic i.e. as if a method of logic is syntactic or semantic (or perhaps both, although usually only by some equivalence).
This yields the myopic view that there are two classifications of logical methods: model theoretic and proof theoretic.
In EFT model theoretic methods are a kind of mathematical semantics and proof theoretic methods are a kind of matheamtical syntax.
But this is not really just a pecularity of EFT's book on mathematical logic, it can be found throughout books on mathematical logic e.g. the classic by Joseph R. Shoenfield.

    This is a mistake of history and says more about the insensitivity of mathematical logicians to logicians than it does anything about logic or mathematical logic.

2.


Well, there's a fragment of writing that I'll never finish.




## 2025 0602

### 2025 0602 1615 The Ethics of Logic
There is a grand misconception that since logic is not about anything in particular then logic is not about the world and hence logic, as with science, is without ethical or moral values.
Logic is part of the world.
It "deals with" or "is about" the world through each of its primary origins: grammar and denotation (as 'is true of' and not as 'designates').

Ethical and moral contingencies (as consequences from responses on occasions e.g. a door opens from a push on a lever, an electron emits from a photon on an atom, etc.) are as much a consequence of logic as they are grammar and denotation.
For most people, it is easier to note the part played by denotation in ethics than it is the part played by grammar.
Grammar is traditionally dealt with as something secondary to denotation as if denotation covered meaning rather than meaning following as a trivial kind of reference accomodated entirely by denotation of a special sort (just as designation is a special case of denotation e.g. denotation by a one place singular predicate).

The origins of grammatical and denotative verbal behavior dispell such traditional fictions and fictious explanations leaving a concrete link between logic and the rest of the world: the autoclitics of Skinner's "Verbal Behavior".

...

The ethics of logic works much like the grammar of logic in that the logic of grammar strengthes the grammar of logic and so on like so many other human practices.
Just as the logic of grammar is a consequence of the gramamr of logic, so to is the logic of ethics a consequence of the ethics of logic.

...

Logic is the fulcrum of languages.

the ethics of extensionality

the ethics of validity

the predicates of ethical theories

## 2025 0529

### 2025 0529 1511
This continues my work on my little lisp from way back in [2025 0514 2226](#2025-0514-2226).

It has been a while since I last worked on my little lisp, and, as is so often the case when I return to what I left incomplete, I must begin by collecting the entirety of what was done.
As a reminder: I do my programming in a browser by first going to the url "about:blank" and then opening the developer tools on it.
In Google's Chromium, script snippets are my programming environment.
This is not because it is "the best programming environment", but because this happens to be the easiest way I can program on whatever computer happens to be available to me.


```
// pairs
let theEmptyPair={}
, isEmpty = it => it == theEmptyPair
, pairOf = (it, that) => [it, that]
, leftOf = it => isEmpty(it) ? it : it[0]
, rightOf = it => isEmpty(it) ? it : it[1];

// sequences as pairs
let theEmptySequence = theEmptyPair
, isEmptySequence = isEmpty
, singletonSequenceOf = it => pairOf(it, theEmptySequence)
, headOf = leftOf
, restOf = rightOf
, concatOf = (it, that) => isEmptySequence(it) ? that 
  : pairOf(headOf(it), concatOf(restOf(it), that))
, prependSingletonOf = (it, that) =>
   concatOf(singletonSequenceOf(it), that)
, appendSingletonOf = (it, that) =>
   concatOf(that,singletonSequenceOf(it));

// stacks as pairs
let theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, singletonStackOf = it => pairOf(theEmptyStack, it)
, pushOf = pairOf
, dropOf = leftOf
, topOf = rightOf
, secondOf = stack => topOf(dropOf(stack))
, drop2Of = stack => dropOf(dropOf(stack))

, prependOf = stack => pushOf(drop2Of(stack)
  , prependSingletonOf(topOf(stack), secondOf(stack)))
, appendOf = stack => pushOf(drop2Of(stack)
  , appendSingletonOf(topOf(stack), secondOf(stack)));

// letters, strings, concatenations and runes
let isConcatenationOf = (x,y,z) => x == y.concat(z)
, isEmptyConcatenation = x => isConcatenationOf(x,x,x)
, theEmptyConcatenation = ''
, isIdenticalConcatenation = (x,y) => 
   isConcatenationOf(x,y,theEmptyConcatenation)
, theConcatenationOf = (x,y) => x.concat(y)

, stringOf = (...letters) => theEmptyConcatenation.concat(...letters)
, firstLetterOf = letters => isEmptyConcatenation(letters) ? theEmptyConcatenation : letters[0]
, restLettersOf = letters => isEmptyConcatenation(letters) ? theEmptyConcatenation : letters.slice(1)

, theAlphabet = '() 0123456789abcdefghijklmnopqrstuvwxyz'

, runeHelpOf = (it, abc) =>
 isEmptyConcatenation(abc) || isIdenticalConcatenation(it, firstLetterOf(abc)) ? theEmptyPair
 : pairOf(theEmptyPair, runeHelpOf(it, restLettersOf(abc)))
, runeOf = it => runeHelpOf(it, theAlphabet)

, letterHelpOf = (it, abc) => isEmptyConcatenation(abc) ? theAlphabet
 : isEmpty(it) ? firstLetterOf(abc)
 : letterHelpOf(rightOf(it), restLettersOf(abc))
, letterOf = it => letterHelpOf(it,theAlphabet)

, runesOf = letters => isEmptyConcatenation(letters) ? theEmptySequence
  : prependSingletonOf(runeOf(firstLetterOf(letters))
    , runesOf(restLettersOf(letters)))

, lettersOf = runes => isEmptySequence(runes) ? theEmptyConcatenation
  : stringOf(letterOf(headOf(runes)), lettersOf(restOf(runes)))

// read and print sequences
let theOpenRune = theEmptyPair
, isOpenRune = isEmpty
, readOpenRuneOf = (stack, runes) => 
   readSequenceOf(pushOf(stack,theEmptySequence), restOf(runes))

, theCloseRune = pairOf(theEmptyPair, theOpenRune)
, isCloseRune = it => !isEmpty(it) && isEmpty(leftOf(it)) && isOpenRune(rightOf(it))
, readCloseRuneOf = (stack, runes) =>
   readSequenceOf(appendOf(stack), restOf(runes))

, readDefaultRune = (stack, runes) =>
   readSequenceOf(appendOf(pushOf(stack,headOf(runes))), restOf(runes))

, readSequenceOf = (stack, runes) => 
  isEmptySequence(runes) ? headOf(topOf(stack))
  : isOpenRune(headOf(runes)) ? readOpenRuneOf(stack,runes)
  : isCloseRune(headOf(runes)) ? readCloseRuneOf(stack, runes)
  : readDefaultRune(stack, runes)
, readOf = runes => readSequenceOf(theEmptyStack, runes);

let parenOf = runes => prependSingletonOf(theOpenRune
    , appendSingletonOf(theCloseRune, runes))

, printSequenceOf = sequence =>
  isEmptySequence(sequence) ? theEmptySequence
  : concatOf(printOf(headOf(sequence))
    , printSequenceOf(restOf(sequence)))

, printOf = sequence =>
  isEmptySequence(sequence) ? parenOf(theEmptySequence)
  : parenOf(printSequenceOf(sequence))   

, read = letters => readOf(runesOf(letters))
, print = sequence => lettersOf(printOf(sequence));
```

The definition of 'isConcatenationOf' aspires to more than it can reach in Javascript: a three place predicate in the lexicon of a theory of concatenation.
The definition of 'theConcatenationOf' most clearly reveals the weakness (to anyone who wasn't already confused by the invocation of '.concat' in the definition of 'isConcatenationOf'): it is not short for a singular description.
The three place functional predicate 'is the concatenation of' is constructed with the help of singular descriptions as

> x, y, and z are such that x is identical to the (w such that w concatenates y with z)

where 'is identical to' is coextensive with 'is indistinguishable from' as in a schematic thoery of identity.
This is the basic context within which Russell's elimination of singular descriptions occurs:

> x, y, and z are such that some item is (w such that x is identical to w and each item is (u such that u is identical to w if and only if u concatenates y with z)).

I shall leave that behind and go on to what occurs to me in response to copying the latest iteration here.

I've been looking for a better way to weave together the work so that it is cumulative without imposing additional constraints which are extraneous (as was done in an older iteration which built stacks from sequences rather than from pairs).

The problem remains printing and reading, as much as these are likely to stick around in later iterations.
A new solution occurred to me which is no more foreign to me than any other solutions that have occurred to me (that is they were already present in some work or another that I was exposed to and can not even remotely be attributed to me except perhaps for passing through me).

By including a default function which catches what the defined reader doesn't, i.e. as in the default case of a case statement in a standard imperitive programming language, readers and printers can be combined like LEGOs to accomidate evolving occassions that must be dealt with e.g. as when the items of a sequence are dealt with as something other than sequences.

The names of functions defined are already far longer than they would ever be if I was programming privately, perhaps this is for the better or perhaps this is for the worst, I can not yet tell.
I'm inclined to introduce the following obtuse names "readStringAsRune", "readStringAsRunes", etc. so that then all is readers e.g. "readPairAsString" is the new name of the old function "printAsString".
It may even be more convenient then to just name the functions "letterAsPair", "lettersAsPair", "lettersAsSequence", etc. where, e.g., "lettersAsSequence" happens to be identical to the composition of the functions designated by "lettersAsSequence" and "sequenceAsPair".
But this doesn't make sense.

The distinction between reading and writing is an io problem.
So, "inputAsSequence" or "readAsSequence" makes sense because everthing after "inputAs" is, by explicit design, a pair.

Aha, ignore all that.
I uncovered a potential solution that simplifes and unites.
Instead of creating a seperate set of functions for letters, strings, and the like, I can just go straight to a sequence of runes.

Darn, I'll have to come back and fill this out later.
It has been very hard to find time to write out what I have to say as it occurs to me, even with respect to these little steps along this programming project.

The following incomplete and uncommented code is left here as a record of what was done but not as what was explained.

```
, where = (is, seq) => isEmptySequence(seq) 
  || is==headOf(seq) ? theEmptySequence 
  : prependSingletonOf(theEmptySequence, where(is, restOf(seq)))
, here = (at, seq) => isEmptySequence(at) ? headOf(seq)
  : itemAt(restOf(seq), restOf(at))
, each = (f, seq) => isEmptySequence(seq) ? theEmptySequence
  : prependSingletonOf(f(headOf(seq)), each(f, restOf(seq)))
, over = (f, f0, seq) => isEmptySequence(seq) ? f0
  : f(headOf(seq), over(f, f0, restOf(seq)))

, til = (stop, base, step, part, rest, it) => stop(it) ? base
  : step(part(it), til(stop, base, step, part, rest, rest(it)))

, sequenceOf = items => til(x=>x.length==0, theEmptySequence, prependSingletonOf, x=>x[0], x=>x.slice(1), items)
, here = (at, seq) => til(isEmptySequence, headOf, x=>x )

, theAlphabet = sequenceOf('() 0123456789abcdefghijklmnopqrstuvwxyz')
, readRune = rune => where(rune, theAlphabet)
, printRune = pair => here(pair, theAlphabet)

, readRunes = runes => each(readRune, sequenceOf(runes))
, printRunes = pair => 
```

It is unclear if taking up these methods of functions on functions is sufficiently smooth to warrent admission into this discovery expedition.
It occurs to me that since I have only recently begun sharing public notes that those reading these may not know how familiar I am with the scope and limits of different methods of programming and their mathematical theories.
Ultimately this is of no concern to me because if I can avoid burdening others with all that I have learned and subsequently passed over, then they will be left with that much more energy to learn all that I never shall in my life.

### 2025 0529 1419 The Philosophy of Composition by Edgar Allan Poe
R.P. shared a delightful paper with me upon reading my last entry on Paul Graham's "Good Writing":
[The Philosophy of Composition by Edgar Allan Poe](https://www.poetryfoundation.org/articles/69390/the-philosophy-of-composition).
It is to be compared to B. F. Skinner's talk "On Having a Poem" a recording of which can be heard here: <https://www.bfskinner.org/bf-skinners-lecture-on-having-a-poem/>

Standards of records required to provide explanations for the origins of poetic behavior, or any verbal behavior whatsoever, are to be found in Skinner's ["Cumulative Record Definitive Edition"](https://www.bfskinner.org/wp-content/uploads/2015/02/CUMULATIVE_RECORD.pdf) on page 115 titled "A Case History of Scientific Method" which appeared originally in around 1955.
The details mentioned by Skinner in that paper are not sufficient, but his larger work "[Schedules of Reinforcement](https://www.bfskinner.org/wp-content/uploads/2015/05/Schedules_of_Reinforcement_PDF.pdf)" with Ferster provides sufficient conditions for explaning the behavior of pigeons under controlled laboratory conditions.
Similar methods are at least required to explain human behavior (however inconvenient they might be).

I do not have time to write on all these things now, and have left this note here in the off chance that I may return to it later.

But, here is what I have to say of what I have already read of Poe's paper.
The meat of the matter is found in the eigth paragraph.
The earlier paragraphs simply set the reader next to Poe as if all that was read had been just penned before you.
It is a delightful trick, and much of the paper itself follows the same methods as those who purport to be only comedians and yet have surprisingly strong political effects.

The eigth paragraph begins after all the essential features of a behavior analysis would otherwise have ended:

> Let us dismiss, as irrelevant to the poem, per se, the circumstanceâor say the necessityâwhich, in the first place, gave rise to the intention of composing a poem that should suit at once the popular and the critical taste.

Such a dismissal is perhaps the most profound deception in the entirety of Poe's paper.
The 'per se' pricks the vail of deception and provides Poe with permission, from the reader who continues to read, to be as sly as is his want.
This is a perfect example of what I was talking about when I said in my paper on Paul Graham's "Good Writing" that

> the initial ambiguity and the consequent incompatible methods could have been charitably explained away as nonrhetorical literary language e.g. as a linguistic ignis fatuus.

Where Graham failed, Poe has instructively succeeded.

I very much appreciate R.P. bringing this beautiful example of good writing to my attention.

## 2025 0526

### 2025 0526 2141 The First Two Words of Paul Graham's Paper "Good Writing" 
Here I sincerely analyze Paul Graham's latest paper on ["Good Writing"](https://www.paulgraham.com/goodwriting.html) (I transcribe the snarky analysis I made on twitter at the end for completeness).

The first sentence

> There are two senses in which writing can be good: it can sound good, and the ideas can be right.

begins with the ambiguous phrase "There are".
Vernacular expands it to either "there are at least" or "there are exactly" (there are exactly three ways "there are" can be taken and the third is as "there are at most" as in "there are at most three Stooges" but this is not carried by the vernacular).
Logicians, having noted this ambiguity, regiment "there are" as "there are at least" e.g. "There are at least two senses in which writing can be good".
This regimentation is not merely a convenience.
It is the difference between existence and uniqueness e.g. "there is exactly one even prime number" and "there is at least one even prime number".
Lawyers also noted this ambiguity and regiment "there are" as "there are exactly" under the rule "Expressio unius est exclusio alterius".
Between logicians and lawyers the language of the home leaves the first sentence of Paul Graham's essay woefully underdetermined.

The strong reader rallies with contextual clues.
But, the first clue is very far from the proximate context.
In the eleventh paragraph the lawyers finally win: "So yes, the two senses of good writing are connected".
But then, before the sentence can end, everyone looses.
The full sentence is "So yes, the two senses of good writing are connected in at least two ways." and it spits on the languages of lawyers, logicians, and locals.
For, "the two senses" implies the lawyers rule but "in at least two ways" breaks it since two and only two connections are given.

Had Graham's "Good Writing" been on something other than good writing, then the initial ambiguity and the consequent incompatible methods could have been charitably explained away as nonrhetorical literary language e.g. as a linguistic ignis fatuus.
But, charity extends to the paper as an example of good writing in exactly those ways explained: "it can sound good, and the ideas can be right."
In other words, the ambiguous and incompatible methods are covert rather than accidental.
The deferred cosequences of covert methods are rarely helpful i.e. the ninth item of Russell's decalogue, "Be scrupulously truthful, even if the truth is inconvenient, for it is more inconvenient when you try to conceal it." 

The etymology of 'sense' likely goes to the PIE root \*sent- for "to go" and is the "source also of Old High German sinnan "to go, travel, strive after, have in mind, perceive," German Sinn "sense, mind," Old English siÃ° "way, journey," Old Irish set, Welsh hynt 'way'" (<https://www.etymonline.com/word/sense>).
Replacing 'sense' with 'way' as in "There are two ways in which writing can be good" uncovers the metaphorical role of 'sense' in any sentence in which it occurs.
But, a reason to avoid "way" in Graham's sentence is because "two ways" is near "diverging paths" and the point is made later in the essay that these ways are not independent.
Alternatively, different ways can just as well diverge and yet later intersect and in general, the nearer a pair of people are to a destination the fewer ways there are to finish their trip i.e. the more likely they are to take the same way.

This negative outlook can be countered with the German's "sinn" which is most famous as a single word from Frege's "Ãber Sinn und Bedeutung".
It is clear from the remainder of this essay that Graham is committed to a mentalistic outlook, one that follows the German 'sinn' through 'sense' to 'mind'.
Thus, the occurrence of 'sense' primes talk of mental things.
Ideas, as in the vernacular (or that special part of the vernacular called folk psychology), are objects of the mind (charitably, the mind is what the brain does), expressed by words.
Then, writing is the a process by which ideas are expressed through words.
This is the same story as in hoodoo rootwork where a spell is written on a slate and washed into a glass of water which somehow expresses the cure and is absorbed by the drinker.

More charitably, the mind is what the body does, and the body as much has ideas as it has behaviors.
In fact, the intermediate talk of ideas which are later expressed by behaviors then dissolves and writing as a repertoire is said to be good or bad in as far as it is reinforced by prevailing cultural practices.
Then, to identify the key features of good writing is to identify the reinforcing practices which build and maintain one repertoire of writing rather than some other.
This is not the same as concluding with a kind of cultural relativism nor with a kind of cultural darwinism ("survival of the fittest" is so often interpreted as "'fit' is equivalent to 'survive'" that any such short story on the process of selection by consequences from variation on cultural scales is bankrupt).

Good verbal behavior presumably contributes to the survival of the culture within which it occurs.
It presumably takes into account the deferred consequences of its appearence in a culture sensitive to contributions of the deferred consequences of its practices to its survival.
It no more helps to supplant these descriptions by "sounds good and gets ideas right" than it does to supplant the principles of aerospace engineering with Kelly Johnson's "If it looks good, it will fly well" (which I was unable to find a record of from a search with DuckDuckGo or Google).

The result of my snarky analysis is copied here from Twitter (one of the worse places to write on the internet):

> The whole paper is paraphrased as follows:
>
> Good writing is sound, salient, and, essentially, slick. Why essentially? Lo, Paul Graham knows it is, and thou shalt not show Him otherwise. Soundness and salience are essential too. Why? Hark! Thou shalt know it as He does for thou art made as Him. He shakes the heavens and the earth with his grace and all becomes beautiful in its perfection: "In the beginning was the Word, and the Word was with God, and the Word was God."


### 2025 0526 1619 Books on my Desk and Reading Shelf
Books on my reading shelf:

* 1935 "The Story of Civilization: Volume I. Our Oriental Heritage" by Will and Ariel Durant
* 2017 "Understanding Behaviorism: Behavior, Culture, and Evolution 3rd Edition" by William M. Baum
* 1985 "Behaviorism: A conceptual Reconstruction" by G. E. Zuriff
* 1999 "Cumulative Record Definitive Edition" by B. F. Skinner
* 1973 "I.Q. in the Meritocracy" by R. J. Hernnstein
* 1996 "The Bell Curve: Intelligence and Class Structure in Americal Life" by Hernnstein and Murray
* 2020 "Human Diversity: They Biology of Gender, Race, and Class" by Murray
* 1993 "Cognitive Foundations of Natural History: Towards an Anthropology of Science" by Scott Atran
* 2022 "The Structure of Scientific Revolutions: 50th Anniversary Edition" by Kuhn and Hacking
* 2016 "Classical Philosophy: A history of philosophy without any gaps, Volume 1" by Peter Adamson
* 1984 "The Presocratic Philosophers: A Critical History with a Selection of Texts 2nd Edition" by G. S. Kirk, J. E. Raven, M. Schofield
* 1755 "Discourse on Inequality" by Jean-Jacques Rousseau
* 2011 "Introduction to Reliable and Secure Distributed Programming 2nd ed." by Christian Cachin, Rachid Guerraoui, LuÃ­s Rodrigues
* 1985 "Communicating Sequential Processes" by C. A. R. Hoare
* 2005 "Finite Model Theory 2nd Edition" by Ebbinghaus and Flum
* 2019 "Geometric Algebra for Electrical Engineers: Multivector electromagnetism" by Joot
* 2011 "Linear and Geometric Algebra" by Alan Macdonald 
* 2012 "Vectora and Geometric Calculus" by Alan Macdonald
* 2024 "Projective and Geometric Algebra Illuminated" by Eric Lengyel
* 1943 "The Glass Bead Game" by Hermann Hesse
* 1920 "The Mysterious Affair at Styles" by Agatha Christie
* 2013 "Hercule Poirot: The Complete Short Stories: A Hercule Poirot Mystery: The Official Authorized Edition" by Agatha Christie
* 2018 "Hey Mom: Stories for My Mother, But You Can Read Them Too" by Louie Anderson 

Books on my desk:
* 1950 "The Story of Art" by EH Gombrich and Leonie Gombrich
* 1952 "Anatomy Lessons From the Great Masters" by Robert Beverly Hale and Terence Coyle
* 1979 "Tage Frid Teaches Woodworking: Three Step-by-Step Guidebooks to Essential Woodworking Techniques" by Tage Frid
* 1967 "From Frege to GÃ¶del: A Source Book in Mathematical Logic, 1879-1931" edited by Jean van Heijenoort
* 1997 "The Frege Reader" edited by Michael Beaney
* 1960 "Word and Object" by Quine
* 1995 "Selected Logic Papers" by Quine
* 1971 "Set Theory and its Logic Revised Edition" by Quine
* 1994 "Mathematical Logic 2nd ed." by Ebbinghaus, Flum, Thomas
* 2023 "The Earth and Its Peoples 8th ed." by Richard Bulliet, Pamela Crossley, Daniel Headrick, Steven Hirsch, and Lyman Johnson
* 1973 "Essentials of Earth History: An Introduction to Historical Geology 3rd ed" by William Lee Stokes
* 2017 "The Little Book of Big History" by Ian Crofton and Jeremy Black
* 1973 "A History of American Law 1st ed." by Friedman

> the 4th edition was published in 2019 (maybe get that)

* 2009 "History of the Ojibway People" by William W. Warren
* 2014 "History of the World 6th ed." by J. M. Roberts and Odd Arne Westad
* 1992 "History of the World 3rd ed." by J. M. Roberts (this one has pictures)
* 1993 "The Harper Encyclopedia of Military History" by R. Ernest Dupuy and Trevor N. Dupuy
* 2013 "The Encyclopedia of Warfare" by Dennis E. Showalter
* 2011 "The Rise and Fall of the Third Reich 50th anniversary edition" by William L. Shirer

> get Richard J. Evanâs âThird Reich Trilogyâ:
> * âThe Coming of the Third Reichâ  2003
> * âThe Third Reich in Powerâ 2005
> * âThe Third Reich at Warâ 2008

* 2023 "The Economic Government of the World: 1933-2023" by Martin Daunton
* 1969 "The Rise of Anthropological Theory: A History of Theories of Culture" by Marvin Harris

> the updated edition was published in 2001

* 1977 "Cannibals and Kings" by Marvin Harris 
* 1968 "The Sacred and the Profane: The Nature of Religion" by Mircea Eliade
* 1902 "The Varieties of Religious Experience" by William James
* 1999 "JPS Hebrew-English Tanakh, 2nd Edition" by the Jewish Publication Society
* 2007 "The Tibetan Book of the Dead: First Complete Translation (Penguin Classics Deluxe Edition)" by Coleman, Padmasambhava, et al.
* 2002 "The Holy Quran with English Translation and Commentary" by Maulana Muhammad Ali
* 2007 "The Bhagavad Gita" translated by Eknath Easwaran
* 1611 "The Holy Bible" commissioned by King James I of England
* 1999 "Selected Writings (Penguin Classics)" by Thomas Aquinas and edited by Ralph McInerny
* 426 "The City of God" by Augustine of Hippo
* 397-400 "The Confessions" by Augustine of Hippo
* 1321 "The Divine Comedy" by Dante Alighieri
* 1990 "The Victorian Fairy Tale Book" edited by Michael Patrick Hearn
* 1998 "Legends and Tales of the American West" by Richard Erdoes.
* 1997 "Yiddish Folktales" by Beatrice Weinreich translated by Wolf
* 1982 "Norwegian Folk Tales" by Peter Christen AsbjÃ¸rnsen and JÃ¸rgen Moe
* 1970 "Asimov's Guide to Shakespeare" by Isaac Asimov
* 1961 "Call for the Dead" by John le CarrÃ©
* 1999 "The Well-Trained Mind: A Guide to Classical Education at Home" by Susan Wise Bauer and Jessie Wise
* 2015 "The Reading Teacher's Book of Lists, 6th Edition" by Jacqueline E. Kress and Edward B. Fry
* 2011 "The New York Times Guide to Essential Knowledge" by The New York Times staff
* 1989 "Madrigal's Magic Key to Spanish" by Margarita Madrigal
* 1786 "The Diversions of Purley" by John Horne Tooke
* 1905 "The Elements of Psychology" by Edward L. Thorndike
* 1896 "An Outline of Psychology" by Edward Bradford Titchener
* 1957 "Verbal Behavior" by B.F. Skinner
* 1971 "Beyond Freedom and Dignity" by B.F. Skinner
* 1974 "About Behaviorism" by B.F. Skinner
* 1996 "Modern Political Thought: Readings from Machiavelli to Nietzsche" edited by David Wootton
* 1963 "History of Political Philosophy" by Leo Strauss and Joseph Cropsey
* 2006 "Introduction to Philosophy: Classical and Contemporary Readings, Fourth Edition" edited by John Perry, Michael Bratman, and John Martin Fischer
* 2019 "The History of Philosophy" by A. C. Grayling
* 1926 "The Story of Philosophy" by Will Durant
* 1946 "A History of Western Philosophy" by Bertrand Russell
* 1912 "The Problems of Philosophy" by Bertrand Russell
* 1898 "Pragmatism: a new name for some old ways of thinking" by William James
* 1516 "Utopia" by Thomas More
* 1626 "New Atlantus" by Sir Francis Bacon
* 1623 "The City of the Sun" by Tommaso Campanella
* 1996 "Structure and Interpretation of Computer Programs 2nd Edition" by Harold Abelson, Gerald Jay Sussman, and Julie Sussman
* 1995 "The Little Schemer, 4th Edition" by Daniel P. Friedman and Matthias Felleisen
* 1995 "The Seasoned Schemer 2nd edition" by Daniel P. Friedman and Matthias Felleisen
* 2018 "The Reasoned Schemer 2nd Edition" by Daniel P. Friedman, William E. Byrd, Oleg Kiselyov, and Jason Hemann.

## 2025 0523

### 2025 0523 1401 What would I tell my younger self?
I ask myself "What would I tell my younger self?" as a way of checking on how I am doing and how I might do better sooner rather than later.
For example, would I tell my younger self to work on the practices of logic as much as I have been over these past months?
How would I even explain logic to my younger self?
Also, what does my younger self really have to do with my present self or my future self?
These are just some questions which flow from me like an endless stream of unstoppable curiosity.
Do I have answers for most of my questions?
No.
Do I have answers for a few of them?
Only very few.

So, what would I tell my younger self about logic?
Would telling my younger self to do something different with respect to logic really change his behavior in any way?
Are counterfactual conditionals like these even helpful?
They are relative to a theory of the world and it is relative to such a theory that logic would have the greatest impact, even for my younger self.

Do we say that young people have theories of the world, ones that evolve over time as they are tested and as trials and errors pile up for or against this or that variation on some working theory of the world?
That's a long question.
Long questions rarely have short answers that are also self contained i.e. answers that encapsulate the question without posing it.

Rather than start with how logic would help my younger self I'll start with how logic has helped me, the older self to my younger self.
Logic helps me deal with the world and all the problems it presents to me.
Putting a problem into words is the first step and paraphrasing those words into a logical theory helps me factor some large theory of the world into tiny parts that I can see at a glance.
It focuses me on the predicates of a theory and the potential premises whose implied conclusions are so often fated to be overthrown by experiment.
None of this is what a young me would care about or notice.

When I help my nephews with their math homework I almost always do it over the phone so that we have to talk out a solution without simply sending each other pictures of things we've written down.
The first thing I tend to teach them is to read out the problem aloud.
To most people this may seem like a tiny thing: don't people know how to read aloud?
No, a lot of people don't.
They were barely taught to read selections from books aloud to each other, and are rarely asked to read math problems aloud.
How do you read most of the notation anyway?

That there are effective ways of reading mathematical notation aloud makes all the difference between noticing a parenthesis and skipping it (and messing up the rest of the problem).
What does this have to do with logic?
Logic takes our exposure to our native tongue and shapes it up into a verbal machine that introduces mechanical advantages to a language.
But, what does that mean to a youngling?

There are different ways you can talk about the world.
Some of them are helpful and some of them are hurtful.
If you want the way that you talk about the world to help you and not hurt you then speak logically.
Logic is made for dealing with what is true.
As much as dealing with what is true helps you to deal with the world, logic helps.

How do you speak logically?
First you have to notice that the way you talk about the world affects how you deal with it.
Lots of teachers struggle to teach this.
We know they struggle because so many kids say "Will this be on the test?" or "What does this have to do with the real world?".
If teachers were honest they would tell kids "It has nothing to do with the real world. This is a game that we made. We make you play it because we don't know what else to do. We don't know how else to help you deal with the world."
Instead, they threaten students with bad grades.
Bad grades are not what education avoids: they are just side effects of teachers not knowing what to do.
It is the bad consequences of dealing with the world poorly that actually threaten the young and old alike.

Education is not overtly presented as "the way to avoid dealing poorly with the world and the pains that follow from doing so".
It is presented as some sort of obligation or insurance or self contained arena of competition.
As if the educational environment is of independent interest relative to the rest of the world.
The practices of education have survived because they just barely make the educated person more effective than the uneducated person in almost everything that they do.
"Just barely" is all it takes: those who talk up 10x or 100x multipliers are credit/debit shills.
The snowball of fundamental productivity plows through the ups and downs of short sighted accelerationists (who now, more than ever, invoke the "long term" as nothing more than a verbal hedge or buzzword).

So what have I actually said of logic itself?
There is little to say, especially now that we have Quine's main method of proof for quantificational logic: a sentence is inconsistent if when put into prenex form a truth functional inconsistency is accumulated from instantiation so that existentials always receive new variables.
Logic links truth with grammar through validity either as that which can be derived from Quine's main method (as inconsistency of negation) or that which is true under all interpretations (where interpretation can be taken semantically or syntactically if at least the premises of placevalue notation are present).

From these technical conclusions there are vernacular conventions e.g. very few sentences are of any consequence what so ever since validity is no kind of honorific but rather an expedient of verbal linkage.
Where logic links up the essential sentences of a theory, it binds their fate in any practical experiment.
Deferred consequences of logical links come out in surprising ways that are fatal to a fledgling theory.
Those insensitive to logic are more easily spotted as its practices become more and more familiar.
Such logical deviants break up logical links either out of careless insensitivity or out of coercive intervention.


## 2025 0522

### 2025 0522 2202
Yesterday I finally got out about five sentences that open up the world of existential closures to my methods of logic.
I also carried through the subtle shift from "denotes" in the definition of validity to "denotes everywhere".

Much self reflection occurred yesterday and today, but it is not written here.
Most of it was silent.


## 2025 0521

### 2025 0521 1232

Today is the day that I make a big step with the continued work on my logic paper from [2025 0514 1345](#2025-0514-1345).

Sometime last week I compressed eight cards of outline on my logic paper into three cards.
Those new notes do not include the serious edit of adding "denotes everywhere" wherever I had written "denotes" and carrying through the rest of the paper with the corresponding predicate "denotes somewhere".
Here I shall write out the new notes with new edits and follow through the consequences of editing the definition of denotative functional validity as "each functional substitution denotes everywhere" rather than the old version "each functional substitution denotes".
I shall add comments on what is written or what is to be written (as a record of my thinking at the time of making the revisions) using the standard markdown notation for quotations (on most sites this is displayed as text in a box of a different color).

#### Science, Logic, Functional Compounding, and Chains
Logic is the science of validity and validity is a consequence of grammar and denotation.

Compounding is (denotative) functional when, exclusively, each like compound denotes or each like compound does not denote where and only where (waow), exclusively, each like component denotes or each like component does not denote.
Chains are compounds compounded functionally e.g. joint denials denote waow each of their components do not, negations are self joint denials (they denote waow their component does not), alternations are negations of joint denials (they denote waow some of their components do), and conjunctions are joint denials of negations (they denote waow each of their components do).

> The phrase "where and only where" is critical here: it links up the practices of logic with talk of the rest of the world.
> This turns on a geometric interpretation of spacetime which I do not mention except through the occurrence of 'where' and 'when'.
> A deferred consequence of this spacetime trick is that when I change "this" from a pronoun to a predicate it is easier to navigate how individuation is bound up with logic and the rest of the world.
> In general, I take 'it' as designating and 'this' as denoting chunks of spacetime.
> That does not imply that I take spacetime as prior to or primitive relative to what I write on logic.
> It is an outrider to my logical methods: it is on the periphery and only gently touches the outer rim of logical methods.

#### Subcompounds, Functional Substitutions, Validity, Consistency, Implication, Equivalence, and Laws
Subcompounds of compounds are their self or those of thier components.
Subsitutions of like compounds for like nonchain subcompounds are (denotative) functional.
Compounds are (functionally) valid waow each of their functional substitutions denotes everywhere, consistent waow their negation is nonvalid (some of their substitutions denotes somewhere), implied by others waow the conjunction of their self (the conclusion) with the negation of the other (the premise) is nonconsistent (each of their functional substitutions denotes where the same of the other does), and equivalent to others waow they are mutually implicative (each of their functional substitutions denotes waow the same of the other does).

> This is where the big change occurs.
> The addition of the single word "everywhere" to the definition of validity, which now reads "Compounds are (functionally) valid waow each of their functional substitutions denote everywhere", carries on into the definition of consistency as "Compounds are (functionally) consistent waow their negation is nonvalid" the fundamental consequence of which is that "Compounds are consistent waow some of their functional substitutions denote somewhere."
> This is key to making arguments about the validity or consistency of an existential (or univeral) closure of a chain as defined.
> I've stuck to the spatiotemporal "where"s and occasional "when"s so that there is a constant reminder that in explaining the practices of logic, there is nothing like the logic explained which is yet available.
> This is hard to get out in a way that doesn't churn up more confusion than the verbal constructions can bear.
>
> I had planned to avoid this early mention of "somewhere" and "everywhere" by getting away with the connectives "where" and "only where" as well as the conspicuous "each" and "some" which carry over firmly from this language to logic.
> The phrases "somewhere" and "everywhere" are sometimes adverbs and sometimes nouns in their native language of English.
> Since my aim is on logic and not English and its grammar, I've not done as Quine did in "word and object" and gone out of my way to link up the elementary school grammar of English with the technical grammar of Logic.
> But, here I am confronting the problem head on in this comment.
>
> The "somewhere"s and "everywhere"s as they occur in these definitions are picked to play a grammatical part in English which is between adverb and noun.
> It is from Logic and not from English that I got this usage: the phrase "everywhere" is an implicit universal quantification and the phrase "somewhere" is an implicit existential compound.
> The method of implicit quantification, in this form, is Dreben's: the construction of implicit existential compounds from their implicit universal components depends, with Dreben's method, upon the scope of purported quantifiers.
> Quine's method of functional normal forms makes the simplifying assumption that all problems of scope are established by assuming prefix normal form.
> In logic programming, the historical origin of both Quine and Dreben's method is emphasized by referring directly to either implicitly universally quantified variables or to Skolem constants, or to Skolem functions of implicitly universally quantified variables and Skolem constants, and so on.
> 
> In the definition of validity then "valid where and only where each of their functional substitutions denote everywhere" there are only universals, and carry over to predicate abstract notation as
>
> > each {xyz : x is valid if and only if y is a functional substitution of x and y denotes z}
>
> under Quine's interpretation, and as 
>
> > each {x: x is valid if and only if each {y : y is a functional substitution of x and each {z : y denotes z}}}
>
> under Dreben's (more or less).
> But, in rendering this definition as a sentence in predicate abstract logic it is important to note that the letter to the right of 'denotes' does not effectively carry the complex whereabouts which ultimately come through the later development.
> In an implicit quantifier notation of some logical programming languages, this definition carries over to the sentence
>
> > ?x is valid if and only if ?y is a functional substitution of ?x and ?y denotes ?z
>
> but none of these examples shows the notation for implicit existential compounds.
>
> The carry over of the fundamental consequence of the definition of consistency "consistent waow some functional substitutions denote somewhere" does.
> Quine's and Dreben's interpretations are both then
> > each {x : x is consistent waow y_(x) is a functional substitution of x and y_(x) denotes z_(y_(x))}
> which emphasizes with the underscore that Quine gives the notation with subscripts rather than with additional notation like underscores and parentheses.
>
> Since neither variables nor quantifiers are part of predicate logic (they are part of quantificational logic), none of this actually concerns us here.

Alternations of compounds with their negations are valid (they denote waow the compound does or its negation does, i.e. waow it does or does not, so, each functional substitution denotes everywhere).
Conjunctions of compounds with their negations are nonconsistent (they denote waow their compound does and its negation does, and, hence, waow it does and does not, so, each functional substitution of the conjunction does not denote everywhere i.e. the negation of the conjunction is valid).
Compounds are implied by and equivalent to their self.

> I'm uncertain if I should include these supporting arguments in something more than the parenthetical.
> That compounds are implied by and equivalent to their self is shown by the same step by step reduction to previous and accumulating results that have been built up thus far.
> A compound implies itself when the conjunction of it with its negation is nonconsistent and that is precisely what was just shown and the same conjunction occurs twice in the case of self equivalence.

#### Functional Substitutions Keep Validity, Nonconsistency, Implication and Equivalence
Functional substitutions in
* validities are validities (each functional substitution of the functional substitution of the validity is a functional substitution of the validity and hence denotes everywhere),
* nonconsistencies are nonconsistencies (each functional substitution of the negation of the functional substitution of the nonconsistency is a functional substitution of the negation of the nonconsistency i.e. is a functional substitution of a validity and hence the negation of the functional substitution of the nonconsistency is valid so that the function substitution of the nonconsistency is nonconsistent),
* implications are implications (the conjunction of the conclusion with the negation of the premise is nonconsistant and hence its functional substitution is nonconsistent and identical to the conjunction of the functional substitution of the conclusion with the negation of the functional substutituion of the premise), and
* equivalences are equivalences (functional substitutions of mutual implications are mutual implications).

#### Interchanges of Equivalents are Equivalent
Interchanges of equivalents in a compound are equivalent to that compound (each functional substitution of a compound matches the same of its interchange, except perhaps for the same of the equivalents which otherwise denote in tandem, so each denotes waow the other does i.e. they are equivalent).

#### Interchnage of Equivalents Keeps Validity, Nonconsistency, Implication, Equivalence, Nonvalidity, Consistency, Nonimplication, and Nonequivalence

Interchanges of equivalents in
* validities are validities (each functional substitution of the interchange denotes waow the same of the validity does),
* nonconsistencies are nonconsistent (their negation is valid and so the interchange in the negation is a validity),
* implications are implications (interchange into the nonconsistency is a nonconsistency),
* equivalents are equivalents (interchange of mutual implications are mutual implications),
* nonvalidities are nonvalidities (a compound is nonvalid waow some functional substitution does not denote somewhere, i.e. some functional substitution of its negation denotes somewhere; since the negation of the interchange is identical to the interchange of the negation then the interchange of the negation is equivalent to the negation, and hence the negation of the interchange denotes waow the negation does which denotes somewhere from some functional substituion i.e. the interchange is nonvalid)
* consistencies are consistencies (the negation of the interchange is identical to the interchange of the negation which is equivalent to the negation and hence nonvalid),
* nonimplications are nonimplications (nonimplication is consistency of the conjunction ...)
* nonequivalences are nonequivalences (one is a nonimplication ...).

> I still don't feel quite right about the argument for nonvalidities.
> I just fixed it by distinguishing between the step of equivalence and the step of the negation denoting somewhere from some functional substitution (so that the interchange of the negation, being equivalent to the negation, denotes somewhere from the same functional substitution).
> Why avoid notation though?
> Notation undermines the sensitivities conditioned by carefully designed sentences.
> There are very likely better sentences than the ones I've made thus far, but these are the ones I have and they work well enough.

#### Equivalents of Identity
Compounds are equivalent to
* their double negation (which denotes waow the negation of the compound does not, i.e. waow it does, so, each functional substituion of it denotes waow the same of its double negation does),
* their self alternation/conjunction (which denotes waow some/each of its components does i.e. waow the compound does), and 
* their alternation/conjunction with nonconsistencies/validities.

#### Equivalents of Distributivity of Conjunctions and Alternations
* Alternations of a component with an alternation are equivalent to the alternation of the alternations of the component with each of the others.
* Alternations of a component with a conjunction are equivalent to the conjunction of the alternations of the component with each of the others.
* Conjunctions of a component with an alternation are equivalent to the alternation of the conjunctions of the component with each of the others.
* Conjunctions of a component with a conjunction are equivalent to the conjunction of the conjunctions of the component with each of teh others.

#### Equivalents of Development: Alternational and Conjunctional
* Compounds are equivalent to their alternations with nonconsistencies (equivalents of identity), and, in particular, with conjunctions of other compounds with their negation (by the law of contradiction) which are themselves equivalent to the conjunction of their alternations with the other compound and its negation (by distributivity of alternation over conjunction) i.e. conjunctive development of the one compound with respect to the other.
* The dual for conjunction.

#### Equivalents of Associativity
* The conjunction of the first component with the conjunction of the second and third is equivalent to the conjunction of the conjunction of the first and second with the third.
* The alternation of the first component with the alternation of the second and third is equivalent to the alternation of the alternation of the first and second with the third.

#### Iterated Alternations and Conjunctions
The equivalents of associativity yield the many component alternations and conjunctions which are equivalent to iterated nestings of alternations or conjunctions down their left or right components.

#### Equivalents of Commutativity
* The alternation of the left component with the right component is equivalent to the alternation of the right component with the left.
* The conjunction of the left component with the right component is equivalent to the conjunction of the right component with the left.

#### Equivalents of Distributivity of Negations
* Negations of alternations are equivalent to the conjunctions of the negations of their components.
* Negations of conjunctions are equivalent to the alternations of the negations fo their components.



#### Laws of Validity
Validity is
* inconsistency of negation (which is not consistency of negation, and, hence, not nonvalidity of negation of negation i.e. validity of negation of negation which, by equivalents of identity, is validity)
* nonimplication of negation,
* negational nonimplication
* nonequivalence of negation
* negational nonequivalence, and

#### Laws of Consistency
* Nonconsistency is validity of negation.
* Nonconsistency implies nonvalidity.

#### Laws of Implication
* One chain implies an other and the other a third only where the one implies the third.
* Chains imply their self.
* Chains imply validities.
* Validities do not imply nonvalidities.
* Validities do not imply nonconsistencies.
* Validities only imply validities (each functional substitution of the former that denotes everywhere is one where the latter denotes everywhere)
* Nonconsistencies imply chains.
* Consistencies do not imply nonconsistencies.
* Nonconsistencies only imply nonconsistencies.

#### Laws of Equivalence
* One chain is equivalent to an other and the other a third only where the one is equivalent to the third.
* Chains are equivalent to their self.
* One chain is equivalent to an other waow the other is equivalent to the one.
* Validities are equivalent to and only equivalent to validities.
* Nonconsistencies are equivalent to and only equivalent to nonconsistencies.

#### Conditionals, Biconditionals, Exclusive Alternations, and Sequents
* Conditionals are alternations of the negation of their (antecedent) left component with their (consequent) right component.
* Biconditionals are conjunctions of the distinct conditionals of their components.
* Sequents are conditionals whose antecedent is the conjunction of their antecdent components and whose consequent is the alternation of their consequent components.
* Exclusive alternations are negations of the biconditionals of their components.
* Implication is validity of conditionals.
* Equivalence is validity of biconditionals.
* Nonequivalence is validity of exclusive alternations.

#### Relays, Literals, and Clauses
* Relays are their component or its negation.
* Literals are relays of nonchain compounds.
* Each component of a clausal chain is a literal.

#### Disjoint, Full, and Empty Chains
* No nonchain subcompounds of different components of *disjoint* chains match.
* The nonchain subcomponents of each component of *full* chains are the same.
* Empty chains have no components: often they are replaced by a relevant validity or inconsistency so as to carry an empty method into a nonempty one.

#### Laws of Equivalent Grammatical Categories
Each clause is equivalent to a disjoint or empty clause.
Each chain is equivalent to
* one with only joint denials
* one with only negations and conjunctions
* one with only negations and alternations
* one with only negations and conditionals
* one without conditionals and biconditionals
* one where only nonchain components are negated
* a conjunction of negations of clausal conjunctions
* the alternational dual of the above
* an alternation of disjoint clausal conjunctions (alternational normal form)
* a conjunction fo disjoint clausal alternations (conjunctive normal form)
* a full alternation of unique disjoint clausal conjunctions
* a full conjunction of unique disjoint clausal alternations

#### Existential Closures and Closed Chains
(Existential) closures denote waow there is somewhere denoted by their component.
Closed chains are closures of (open) chains.

#### Boolean Chains and Triviality
Boolean chains are closed or are chains of Boolean chains.
Without whereabouts closures denote nowhere since there is nowhere denoted by their component i.e. they are nonconsistent.
Only nontrivial results are contemplated here.
Check the trivial case by interchanging each closure with a nonconsistency.

The alternation of the closure of a compound with the closure of the negation of that compound is (nontrivially) valid and trivially nonconsistent.

#### Closure Conditionals
Consequences of closure conditionals are closed chains and their antecedents are conjunctions of them.
Consequences or antecedents of degenerate closure conditionals can be empty.


## 2025 0518

### 2025 0518 1857
This is a little experiment where I try to get out the distinction between the two specializations of 'denotes' which are 'denotes somewhere' and 'denotes everywhere'.
Usually when I write on something like this, the context is already available to me in that I write after having looked exactly where the problem I am solving actually occurred.
In this case, I have not looked at my notes on my paper on logic.
Thus, I am without context other than that which I bring to this keyboard unsupplemented by recent exposure to the problem.

The way the paper is designed now, it introduces denotative functional methods of compounding components in the same way that truth functional methods of compounding components is introduced along classical lines except it avoides the most familiar appeal to truth values.
The definition of truth functional compounding is usually given in the following way: a method of compounding components is truth functional if the truth value of the compound is a function of the truth values of the components.
So, in a sentence like "It is red and it is round." the compound is the whole sentence and it is compounded, from a logical grammar of which 'and' is an atom, of the components 'it is red' and 'it is round' and the truth value of 'it is red and it is round' is a function of the truth values of 'it is red' and 'it is round'.
Assigning truth values to the components is one way of interpreting them, and calculating the truth value of the compound with reference to a truth table is one way of specifying the truth function which carries the truth values of the components to the truth value of the compound.

Whether 'it is red' is assigned the value of truth or falsity is often confronted as a problem of interpretation, though when the sentence is given directly and not through the indirect methods of schematisms, e.g. 'x is F' or 'it is F' or just 'Fx', then the problem of interpretation is seen in its more familiar form e.g. selecting a specific item which 'x' or 'it' designates and specifying a particular predicate which is substituted for 'F', in this case substituting 'red' for 'F' in 'it is F' gives 'it is red' and taking 'it' as designating a red thing results in the assignment of truth to 'it is red'.
If it so happens that under this same interpretation 'it' designates a round thing then the compound 'it is red and it is round', which is an instance of the schemata 'it is F and it is G' or 'Fx and Gx' where 'F' is substituted by 'red' and 'G' is substituted by 'round', then since both the components 'it is red' and 'it is round' are assigned truth and since the connective 'and' marks the compound as receiving the value of truth where and only where each of its components are assigned the value truth, then the compound 'it is red and it is round' is interpreted as true.

Under these methods, the truth functional methods that go out of their way to introduce abstract objects called truth values and functions there of, each assignement of truth values to the truth functional components of a compound fixes the truth value of the compound.
Now, having gone out of my way to describe this method, I am left saying that this is not the path taken here.
Nor, and this is the crux of the matter, is it the strongest path that can be taken through the methods of logic.
In fact, taking this path has come to weaken logical and mathematical practices in ways which are largely unseen but which are of great consequence: they've all but broken the most powerful links between language and logic.
This is unexpected for many reasons, but the most relevant reason is this: those who are most familiar with the links between language and logic are precisely the ones who are most likely to break up the links between them and least likely to point up the most powerful links.

How this has come to pass is a pressing problem and one whose solution is well within my sights, but it reaches beyond the scope of language and logic and is best put aside as I have just as well put aside how it has come that different philosophers go in and out of style.

So what is the alternative that I submit is so much more powerful and important?
It is well known but is not carefully dealt with in the same systematic way that any mathematical logician would quickly demand of an introduction to model theory or proof theory (or, beyond an introduction, to the concerns of mathematical logic as a whole).
It is the method explicated by Quine in his "Philosophy of Logic" which can be briefly and artfully stated as "Logic chases truth up the tree of grammar." and "Logic is the sum of two components: grammar and truth." 

The predicates of grammar are coupled with the predicates of truth (that I must always explain myself when I say "predicates of truth" as if it was not shown almost a century, now, ago that there is no consistenty theory of a predicate of truth which satisfies the minimal requirement of disquotation, or that I have to go through the way in which disquotation is an instance of the maxim of minimal mutilation, should be alarming enough to those who know better).
When presenting logic, i.e. when teaching it to a fresh learner, there is nothing but the verbal communities with which that learner has been in contact from which to teach them.
Logical behavior is taught to those without any logical behavior: this is far more controversial than it looks.

That logical behavior can be taught to an organism without any logical behavior often comes as a surprise to most people, especially logicians and mathematicians.
This is largely because of the historical principles which guide both philosophers and logicians away from anything like the experimental analysis of behavior.
Logic, and its methods, are to transcend our material world through some metaphysical mist, or so it is said.
That we, as mere mortals, are in touch with some otherworldly essence of logic is merely an aftereffect or perhaps even an uninteresting collateral effect of some more fundamental metaphysical principles.

This is something which must be explained, but which would take me too far from this path.
To be clear, those who hold to metaphysical practices must be accounted for in an effective theory of the world, though, like most effective theories, it may not account for them in the way they might hope.

I shall also not go into the methods by which a pigeon can be conditioned to behave logically without, in the case of a pigeon, clearly having any logical behavior like that so often attributed to even the most isolated human (that a pigeon can be made to "do logic" is often astounding to people who are mostly unfamiliar with what little there ultimately is to logic in itself since they are unable to seperate logic from its application, which is another one of the key problems caused by traditional methods).

The grammatical part of logic can be introduced with some basic predicates, though they are not introduced as the application of logic so they can not be relied upon to bring appropriate behavior of the learner under control without being a tiny variation on some nonlogical behavior.
The nonlogical behavior often relied upon when introducing logic is that of the vernacular.
An example of a minimally viable repertoire is that what is written here can be read: much more work must be done to make this as precise as it is, e.g., in a text like Skinner's "verbal behavior" (for all that many have said to extinguish its control over scientific practice).

Sentences are taken, tentatively, as nothing more than quotations i.e. as concatenations of letters as in the familiar strings of computer science.
This is displeasing for many people who hold the form of a given response as sacred and who are unaware of the contributions of the consequences and the occasions upon which a response of a given form is emitted.
I can not go out of my way to explain that here now.
As far as the following description goes, I make no commitment to quotations and you can pick your favorite stand in items.

A single grammatical predicate does the work of the truth functional part of a language: x jointly denies y with z.
Joint denial is not the only such predicate e.g. 'x alternatively denies y with z' is another such.
I prefer joint denial because 'nor' is a sentence connective in my native tongue i.e. those who speak in a language with 'nor' are already that much closer to behaving logically with respect to it.

With this grammatical predicate a predicate of truth can be combined and a tiny example of Quine's outlook can be demonstrated in a familiar way.
In general, it is supposed that the predicate 'is true' is an abbreviation for the highest of the truth predicates among those in the inductive hierarchy of Tarski's analysis which belongs to the lexicon of our present language.

The following is what I call the synthetic categorical of joint denial (this is not quite correct, as the categoricals are to be distinguished from the universal closures of conditionals, but I have assumed that logic is being taught to a learner without a history of exposure to a community of logicians and hence if they can read this sentence at all then it will bring them under the control characteristic of categoricals):

> each item is (x, y, and z such that x jointly denies y with z, y is not true, and z is not true, only if x is true)

The analytic categorical is 

> each item is (x, y, and z such that x jointly denies y with z and x is true, only if y is not true and z is not true)

The bicategorical is

> each item is (x, y, and z such that x jointly denies y with z, y is not true, and z is not true, if and only if x is true)

The synthetic categorical chases truth up the tree of grammar as Quine said, and the analytic categorical chases truth down the tree, and the bicategorical chases it both ways.

Looking back on the traditional way of introducing joint denial, it is seen that no method of truth values or truth functions or models or proofs have been used to explain joint denial.
Using the method of predicate abstracts, which I've mentioned many times before and which can be quickly found with a simple text search among previous entries, it can easily be explained how it comes that Frege began talk of "truth functions".

The predicate (abstract)

> (x, y, and z such that x jointly denies y with z, y is not true, and z is not true, only if x is true)

is *almost* functional in its first place.
Note, schematically

> 'Functional F' is short for "each item is (w,x,y, and z such that Fxyz and Fwyz, only if x is indiscernible to w)" where indiscernibility is given schematically (I'm not sure if I've explained that elsewhere or not, but I'm pretty sure I have).

The predicate 'x jointly denies y with z' is where functionality goes faulty since there may be many different ways that a compound may be formed from its components into a joint denial of its components e.g. "neither Tom loves Dick nor Dick loves Tom" or perhaps "Tom loves Dick nor Dick loves Tom" or perhaps "not Tom loves Dick or Dick loves Tom".

This is where the all important methods of paraphrasing are revealed as of foundational importance.
Logical practices restrict the predicate '(x,y, and z such that x jointly denies y with z)' so that it is functional and then the predicate

> (x, y, and z such that x jointly denies y with z, y is not true, and z is not true, only if x is true)

is functional.
Note, without the early constraint that the learner of logic have some language (as the reinforcing practices of a verbal community) in which they speak, there is little that can be done from simply presenting the synthetic and analytic conditionals.

Also, note, that paraphrasing is linked with indiscernibility by the logicians insistence of paraphrasing so that "Functional (xyz : x jointly denies y with z)" is true.

Sadly I must end here without going from 'is true' to 'is true of'.

### 2025 0518 1403 Scott Atran's Cognitive foundations of natural history
Another great recommendation from R.P. arrived yesterday: Scott Atran's 1990 "Cognitive foundations of natural history: towards an anthropology of science".

The introduction unveils the inspiration for the rest of the text: the theories of Chomsky and Piaget are nonconsistent.
A nonconsistency is constructed of science from Chomsky's premise that 'each fundamental type of human-knowledge arises from a specialized cognitive aptitude" (pg. x Artran 'cognitive foundations') and Piaget's premise that "the innate and universal foundations of human thinking reduce to an undifferentiated intelligence, which is responsible in the same way for all cognitive operations" (pg. ix) from which talk of an intelligent scientist goes sour.

It is already clear from teh introduction that those items of Chomsky's theory which Atran keeps upon rejecting the contrasting principle from Paiget's theory are those which root themselves in teh behaviorist tradition i.e. the contingencies of the world, and Earth in particular, are such that global uniformities are selected without any centralized coordination so that the practices of science are as much a consequence of the evolved tendancy of organisms to bring the world (of which other members of their species are a part) under their control.
This permits a diversity among practices of presciences, and admits of evolution therein, while explaining those features across cultures that prompt those like Piaget to speak of a unifying intellect.

It is the contingencies of the world which select uniformities (for reasons that may be initially capricious) and which maintain them: not some ephemeral intellect.
The problem for Chomsky, and hence for Atran in support of him, shall be looking beyond the individual to the contingencies of natural, operant, and social selection and variation that explain how bits of behaving biology build and maintaining scientific communities.

The first chapter points to the mistake which is easily corrected by the radical behaviorist: it is not some essence of common sense that haunts the brains of the masses, but rather the vernacular as the language of the household where language is nothing more, nor less, than teh reinforcing practices of a verbal community.
When a person is said to be "using common sense" we can look at their speach and trace its origins through the listeners that selected it from its past variations.
Correspondance between common sense as speaking in the vernacular is easily established by examining the world upon which we speak.
It is one that is filled with other members of our species and which has been sufficiently stable so as to not only permit the survival of the species but also the survival of those verbal practices which compose cohesive cultures.

The practices between cultures do not always carry over perfectly because the world is not so uniform, e.g., as to select exactly the same form of response across great distances.
Geography is not sufficiently uniform so as to strengthen vernacular which is diverse in some places, e.g. as when it was established in 2010 in [Chapter 16 entitled "Franz Boas and Inuktitut Terminology for Ice and Snow: From the Emergence of the Field to the 'Great Eskimo Vocabulary Hoax' by Igor Krupnik and Ludger MÃ¼ller-Wille](https://gwern.net/doc/psychology/linguistics/2010-krupnik.pdf) from the 2010 book ["SIKU: Knowing Our Ice Documenting Inuit Sea Ice Knowledge and Use" ](https://link.springer.com/book/10.1007/978-90-481-8587-0) that there are a larger number of root words for their world of ice and snow than in other languages of the world.

Vocal musculature is a critical uniformity among organisms of a given species that permits verbal communities to acquire new members whether they are old or young.

As I mentioned in my last entry, there are only tiny chunks of writing that I can do right now and though that frustrates me I am at least glad to have gotten this bit out.


### 2025 0518 1344 Copleston's History of Philosophy
Last night I watched more of "The Great Philospohers" from 1987 which I mentioned in [Bryan Mageeâs âMen of Ideasâ and âThe Great Philosophersâ](#2025-0505-1709-bryan-magees-men-of-ideas-and-the-great-philosophers).
Frederick Copleston spoke on Schopenhauer and Magee mentioned Copleston's series "A History of Philosophy" which is now eleven volumes but which was published originally as nine between 1946 and 1975.
Here is the link to the wikipedia page: <https://en.wikipedia.org/wiki/A_History_of_Philosophy_(Copleston)>.

It will take some time for me to get through it all, as it takes me so very long to read even one book in the way that I do (like a loom weaving threads from dozens of books at a time).
In anticipation I shall do as I do and write out the volumes titles and dates of publication:

1. 1946 Greece and Rome
2. 1950 Augustine to Scotus
3. 1953 Ockham to Suarez
4. 1958 Descartes to Leibniz
5. 1959 Hobbes to Hume
6. 1960 Wolff to Kant
7. 1963 Fichte to Nietzsche
8. 1966 Bentham to Russell
9. 1975 Maine de Biran to Sartre
10. 1986 Russian Philosophy
11. 1956 Logical Positivism and Existentialism

They are sold as a series of eleven books though they were originally published as a series of nine.
The last two were added into the series in 2003 for reasosn that I do not yet know.


### 2025 0518 1335
Another scrapped entry from yesterday that I was unable to complete and which went unpublished.
It continues work on my logic paper, but sadly rests as a mere record of my past about which I know so very little even this single day later.
Recently, I have struggled to find the large chunks of time which tend to release the greatest potential from any writing that I might do.
As much as there is a temporary moment of frustration it is so common that it almost goes without saying except in these peculiar circumstances where detailed records of responses are demanded.

>2025 0517 1625
> A lot of work on my paper on logic is occuring at once and I have not been able to complete it all.
> I am doing my best to make a record of all the responses which appear with respect to it.
> This continues the last entry on the paper [2025 0516 1352](#2025-0516-1352).
>
> Quantificational logic, as opposed to predicate (functor) logic, defines validity with respect to the truth of its sentences (relative to provisions for grammatical constructions with free variables e.g. treating them as dummy singular terms or as abbreviations of their universal closures).
> Predicate logic defines validity with respect to the denotation of its predicates: there are no variables and hence no provisions for free variables (and, happily, no complex rules of substitution for bound variables).
>
> Quantificational logic seems to predicate the quotation of a sentence of the predicate 'is true' as in "'it is raining or it is not raining' is true".
> By Tarski's analysis, this sentence is equivalent to "'it is raining' is true or 'it is not raining' is true" which is itself equivalent to, by the same analysis, "'it is raining' is true or 'it is raining' is not true" whihc, as an instance of a truth functionally valid schema, is true (we also know from Tarski's analysis that the predicate 'is true', which occurs repeatedly throughout the quoted equivalents, is not coextensive with the predicate occuring right before this parenthetical).
> Note, the use of 'predicate' and 'sentence' picks out nothing but what is spelled out by a quotation e.g. 'is true' is a predicate and 'it is raining' is a sentence.
>
> The sentence "it is raining or it is not raining" is itself an instance of a truth functionally valid schema and hence it is true without recourse to the intermediate steps of the argument by equivalents in the previous paragraph.
> On the other hand, the sentence "it is raining" is not an instance of a valid schema (either when 'it' is taken as a dummy singular term or when the whole is taken as an abbreviation for "each item is it such that it is raining") i.e. whether, exclusively, 'it is raining' is true or 'it is not raining' is true, or, directly, whether, exclusively, it is raining or it is not raining (exclusivity has nothing in particular to do with this example, whether 'or' is read exclusively or not in the previous paragraph).
> These two examples demonstrate the difference between a triviality and the potential urgency of an umbrella.
>
> Predicate logic, at first, seems to predicate the quotation of a predicate and some chunk of the world of the two place predicate 'is true of', aka 'denotes', as in "'rain or not rain' denotes this" which is equivalent to "'rain' dentoes this or 'not rain' denotes this".
> Equivalently, "'rain' denotes this or 'rain' does not denote this"
> Now, instead of being an instance of a truth-functionally valid schema, the compound predicate "'rain' denotes this or 'rain' does not denote this"


## 2025 0516

### 2025 0516 1352
This is an incomplete and, it would appear, incoherent note that continues work on my paper on logic from [2025 0514 1345](#2025-0514-1345).

> Logic is the leverarm of language, and those who speak in it can lift the weight of the world with their words.
> There are multiple ways of specifying valid verbal behavior, but only two are mentioned to most modern students: the model theoretic and the proof theoretic.
> Under appropriate assumptions, these methods are shown to be equivalent.
> In most cases, both methods are presented schematically so that upon establishing one or the other validity there is a secondary step to projecting one or the other definition of validity to the verbal behavior that occurs in logical practice.
>
> It is with great sadness that the model theoretic methods are called out as semantic and the proof theoretic methods are called out as syntactic.
> Neither the word 'semantic' nor the word 'syntactic' are worthy of a place in logic except from its application e.g. in formalisms, perhaps in those special formalisms called "axiomatic systems", of model theories (set theories within which the semantic methods are derived by interpretation) or of syntactic theories (concatenation theories within which the syntactic methods are derived by calculation).
>
> Logic is linguistic like grammar.
> It tells us something about the reinforcing practices of verbal communities.
> Principles aimed at verbal communities generally are said to be transcendent, and those aimed at a single community are said to be immanent.
>
> On a given occasion the response "Red" and the response "Round" are reinforced by a listener, or punished, 
> 
> To say of a denotative functional compound that it denotes somewhere or that it denotes everywhere is to do something like the implicit quantifier notation used in logic programming 
>
> * <https://www.youtube.com/watch?v=MsuaWozZowg>
>
> The definition of validity of a functional compound must be ammended to explicitly mention "denotes everywhere" where "denotes" was always short for in previous editions.
> The reason for this is that "consistency" as "some functional substitutions denote" must be altered to "some functional substitutions denote somewhere" so that problems of existential closure can be dealt with uniformly through what Quine calls "Boolean Term Schema".
>
> Compound Whereabouts and Quine's Functional Normal Form
> x_(y_(z)uv_(ws)) becomes ((z),u,(w,s))
> this is important to Hilbert and Bernay's completeness, especially as presented in Kleene's "Introduction to Metamathematics".
write on quine's explanation of hilbert and bernay's completeness in "selected papers on logic"

### 2025 0516 1348

This fragment of a note was written yesterday as I began to read the 1994 book "The Bell Curve" by Richard J. Herrnstein and Charles Murray.
I stopped writing on it because it no longer presented itself as scientific literature.
It became obvious that it is a part of nationlistic literature more so than anything scientific.
Thus, a deep reading has been delayed for when I address the collection of nationalistic writers e.g. Thomas Sowell, Milton Friedman, and Francis Fukuyama.

Though their writings contain copious references to what clearly purports to be scientific literature, there is nothing in the design of their documents which brings up science as the primary source of control over what they have to say.
Much of what is written is of this sort, and even much of what is written here by me is of this sort.

We, as humans, speak under the control of a wide variety of social environments, and, through them, have contact with a wide variety of contingencies of the world.
Not all of them are scientific.
Not all of them purport to be under scietific control.
Some do, and among those, there are some which do so to share, by association, in the strengths of scientific practice.
Whether they are the result of scientific practices, as in a theoretical analysis of the social consequences of a science of behavior, is important to me because of the control that scientific practices have over my own behavior as a reader and writer.

A copy of Richard Herrnstein's 1973 "I.Q. in the Meritocracy" is arriving soon and it purports to be, like "The Bell Curve", under the control of scientific practices.
Until it arrives I have stopped writing on "The Bell Curve" and have simply been reading it as time has allowed.

Here is the fragment:

> 2025 0515 1911 The Bell Curve
> For much of the past year I have avoided digging into Herrnstein's work because there is so much of it which is incompatible with the science of behavior as a science of behavior rather than a science of cognition (which is nothing more than a reincarnation of a science of mind but with that ever present seasoning of "computational" or "informational" sprinkled about).
> But, the genetic control of behavior continues to be poorly dealt with, as in Charles Murray's 2020 book "Human Diversity: The biology of gender, race, and class", and the less I read the less I'm likely to know.
> 
> The general problem is simple: there is no room in scientific practices for events happening in other dimensions other than those under investigation.
> People are no more haunted by more or less intelligent minds than houses are haunted by more or less malevolent ghosts.
To say so is to already 


## 2025 0514

### 2025 0514 2226
This continues my work on my little lisp from [2025 0512 1400](#2025-0512-1400).

Much as you do when you are solving a problem in physics, I shall write down the basic programs that have evolved thus far.
This note adds a layer to the onion of grammars growing out the primitive parenthetical language.

```
// pairs
let theEmptyPair={}
, isEmpty = it => it == theEmptyPair
, pairOf = (it, that) => [it, that]
, leftOf = it => isEmpty(it) ? it : it[0]
, rightOf = it => isEmpty(it) ? it : it[1];

// sequences as pairs
let theEmptySequence = theEmptyPair
, isEmptySequence = isEmpty
, singletonSequenceOf = it => pairOf(it, theEmptySequence)
, headOf = leftOf
, restOf = rightOf
, concatOf = (it, that) => isEmptySequence(it) ? that 
  : pairOf(headOf(it), concatOf(restOf(it), that))
, prependSingletonOf = (it, that) =>
   concatOf(singletonSequenceOf(it), that)
, appendSingletonOf = (it, that) =>
   concatOf(that,singletonSequenceOf(it));

// stacks as pairs
let theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, singletonStackOf = it => pairOf(theEmptyStack, it)
, pushOf = pairOf
, dropOf = leftOf
, topOf = rightOf
, secondOf = stack => topOf(dropOf(stack))
, drop2Of = stack => dropOf(dropOf(stack))

, prependOf = stack => pushOf(drop2Of(stack)
  , prependSingletonOf(topOf(stack), secondOf(stack)))
, appendOf = stack => pushOf(drop2Of(stack)
  , appendSingletonOf(topOf(stack), secondOf(stack)));
```
Eventually these basic functions will settle down and an instructive sequence of tiny steps will lead the novice through these methods of programming.
What follows are the new basic functions on concatenations as javascript strings.
A small change in initial defintions is given to better follow the theory of concatenations like those of Tarski.
```
// letters, strings, concatenations and runes
let isConcatenationOf = (x,y,z) => x == y.concat(z)
, isEmptyConcatenation = x => isConcatenationOf(x,x,x)
, theEmptyConcatenation = ''
, isIdenticalConcatenation = (x,y) => 
   isConcatenationOf(x,y,theEmptyConcatenation)
, theConcatenationOf = (x,y) => x.concat(y)

, stringOf = (...letters) => theEmptyConcatenation.concat(...letters)
, firstLetterOf = letters => isEmptyConcatenation(letters) ? theEmptyConcatenation : letters[0]
, restLettersOf = letters => isEmptyConcatenation(letters) ? theEmptyConcatenation : letters.slice(1)
```
Now, the alphabet is introduced as a special concatenation.
```
let theAlphabet = '() 0123456789abcdefghijklmnopqrstuvwxyz';
```
The following new definitions for making runes and letters skip over the intermediate steps of working with tallies.
Even though such methods otherwise would be instructive, the definitions given are basic methods of "finding where a thing is at" and are more than good enough.
```
let runeHelpOf = (it, abc) =>
 isEmptyConcatenation(abc) || isIdenticalConcatenation(it, firstLetterOf(abc)) ? theEmptyPair
 : pairOf(theEmptyPair, runeHelpOf(it, restLettersOf(abc)))
, runeOf = it => runeHelpOf(it, theAlphabet)

, letterHelpOf = (it, abc) => isEmptyConcatenation(abc) ? theAlphabet
 : isEmpty(it) ? firstLetterOf(abc)
 : letterHelpOf(rightOf(it), restLettersOf(abc))
, letterOf = it => letterHelpOf(it,theAlphabet)

, runesOf = letters => isEmptyConcatenation(letters) ? theEmptySequence
  : prependSingletonOf(runeOf(firstLetterOf(letters))
    , runesOf(restLettersOf(letters)))

, lettersOf = runes => isEmptySequence(runes) ? theEmptyConcatenation
  : stringOf(letterOf(headOf(runes)), lettersOf(restOf(runes)))
```
Then an example, which deviates from the prior example by including a 'not':
```
lettersOf(runesOf("(()())() this should not be ignored")) 
 (()())() this should not be ignored
```
Which is a good sign (sadly, not a proof).

The change in alphabet has led to a slight change in the placement and form of the prior definitions of "theOpenRune" and "theCloseRune": it is now with the definitions of reading and printing sequences.

```
// read and print sequences
let theOpenRune = theEmptyPair
, isOpenRune = isEmpty
, readOpenRuneOf = (stack, runes) => 
   readSequenceOf(pushOf(stack,theEmptySequence), restOf(runes))

, theCloseRune = pairOf(theEmptyPair, theOpenRune)
, isCloseRune = it => !isEmpty(it) && isEmpty(leftOf(it)) && isOpenRune(rightOf(it))
, readCloseRuneOf = (stack, runes) =>
   readSequenceOf(appendOf(stack), restOf(runes))
```
The default method of reading runes which were not open or close runes was to skip over them.
Now any nonparenthetical runes are added to the end of the sequence under construction.
This makes for minor alterations of the method of reading sequences.
```
let readDefaultRune = (stack, runes) =>
   readSequenceOf(appendOf(pushOf(stack,headOf(runes))), restOf(runes))

, readSequenceOf = (stack, runes) => 
  isEmptySequence(runes) ? headOf(topOf(stack))
  : isOpenRune(headOf(runes)) ? readOpenRuneOf(stack,runes)
  : isCloseRune(headOf(runes)) ? readCloseRuneOf(stack, runes)
  : readDefaultRune(stack, runes)
, readOf = runes => readSequenceOf(theEmptyStack, runes);
```
The rest is as it was.
```
let parenOf = runes => prependSingletonOf(theOpenRune
    , appendSingletonOf(theCloseRune, runes))

, printSequenceOf = sequence =>
  isEmptySequence(sequence) ? theEmptySequence
  : concatOf(printOf(headOf(sequence))
    , printSequenceOf(restOf(sequence)))

, printOf = sequence =>
  isEmptySequence(sequence) ? parenOf(theEmptySequence)
  : parenOf(printSequenceOf(sequence))

, read = letters => readOf(runesOf(letters))
, print = sequence => lettersOf(printOf(sequence));
```
The examples let it be shown that nothing has changed from when they were last run.
```
lettersOf(parenOf(runesOf('(()()())'))) 
 ((()()()))
print(read('()')) 
 ()
print(read('(()(())())')) 
 (()(())())
print(read('(())(()(())())')) 
 (())
print(read('(()(())())((()))')) 
 (()(())())
print(read('))))((')) 
 ()
```
None of these include a space rune so that has yet to be treated as any special case.
But, the next example with other than close or open runes paves the way for the next layer of the onion mentioned at the beginning of this note.
```
print(read('(this is a test)')) 
 ((()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()())(()()()()()()()()()()()()()()()()()()()())(()()()()()()()()()()()()()()()()()()()()())(()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()())(()())(()()()()()()()()()()()()()()()()()()()()())(()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()())(()())(()()()()()()()()()()()()())(()())(()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()())(()()()()()()()()()()()()()()()()())(()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()())(()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()))
```
The method of making runes is now revealed as a way of reinforcing the parenthetical reader and printer: nonparenthetical runes are read as sequences of of empty sequences.
This opens up the following more sophistocated methods without entailing further alterations to the parenthetical reader and printer.
```
letterOf(headOf(read('(this is a test)'))) 
 t
letterOf(headOf(restOf(read('(this is a test)')))) 
 h
lettersOf(read('(this is a test)')) 
 this is a test
```
This took far longer to finish than I had aimed at, but I'm happy to be this far.
Though it is easy to go back and see that I've been near a similar target in past iterations, they relied more on peculiarities of javascript than on the basic logic of a design of a lisp like language which does not depend on its implementation.

### 2025 0514 1420

Quine's main method corresponds, in a significant way that I have not fully grasped, to Aristotle's principle of noncontradiction: this also happens to be the crux of the methods of logical programming.
The demonstration of the relevant inconsistency constructs, by conjunctive description that comes out to a generalization of 'the primitive recursive value of a the functional predicates of a given theory', the item whose existence is presumably implied by noncontradiction. 

### 2025 0514 1345
This entry ended up contributing to the work on my paper on logic from [2025 0510 1238](#2025-0510-1238).

The past two days have been filled with private conversations and wayward contemplations that were sadly ungoverned by the work done here.
It happens, and yet, the more often it happens, the less often I stumble into a tiny success in any of my little endevours.

For example, I've failed to note my responses to watching more of the series I mentioned in [Bryan Mageeâs âMen of Ideasâ and âThe Great Philosophersâ](#2025-0505-1709-bryan-magees-men-of-ideas-and-the-great-philosophers).
I've also failed to note the arrival of Kir, Raven, and Schofield's "The Presocratic Philosophers", or the multiple rereadings of key parts of Quine's fourth edition of "Methods of Logic" (for all the times I've read the book from cover to cover there is still more than I can carry with me without referencing it).

I see each of these unnoted events as failures to hit where I have aimed.
Thankfully, a few bits of minor productivity do carry me onward if not wayward.

As mentioned in my last note on the evolution of my paper on logic, I can not wait to finish the sections covering denotative functional duality prior to starting on the new grammatical category of existential closures.
Yet, such duality is more a labor saving device (it multiplies the consequences of any result on functional compounding by two) than a milestone.
The milestone, under the grammatical definition of valdity as substitution on supplementation of lexicon (by some finite list of previously unintroduced predicates), is the equivalence of each funtional compound to one in the grammatical category of alternational normal forms and conjunctive normal forms.
It yields a decision procedure for validity: a n component chain is valid waow its full alternational normal form is compounded of n to the power of two clauses.
This retreives, without oblique references to items purported by the phrase "denotative values" or "truth values", the breadth and depth of methods of truth tables and, much more importantly, truth trees, though, to be clear, the law of validity just stated is the carry over of truth tables, and the grammatical import of truth tree methods has yet to be described conspicuously as such (by anyone).

The grammatical import of truth trees are a consequence of alternational and conjunctive development which are already presented in the parts of the paper on logic that I've produced thus far.
Such methods are generalizations of Gentzen's sequents along the lines of Quine's method of existential conditionals which is a decision procedure for Boolean statement schemata.
I introduce sequents as special conditionals long before reaching out from Quine's main method to those of natural deduction, which I expect to show are nothing more than the application of Quine's main method to sequents so intorduced (for me, a sequent is to be a denotative functional compound whose antecedents components are conjoined as the antecedent of a conditional whose consequent is the alternation of the consequent components of the sequent).
The parallels between implications and conditionals are the smooth passage from Quine's main method applied to sequents and the methods of natural deduction.


## 2025 0512

### 2025 0512 1400
This continues work on my little lisp from [2025 0509 1722](#2025-0509-1722).

It occurred to me in the shower that I can use the defined operations for manipulating concatenations of letters as javascript strings to give some interesting and important definitions of basic predicates and functions on concatenations.
It also occurred to me that this fits perfectly into both Quine's "Concatenation as a Basis for Arithmetic" as well as my outlook on bit strings and binary trees.

The first function which hit me was one that checks whether a letter is in a concatenation of letters.
This led me to a definition that either constructs directly the appropriate rune, constructed step by step, and then to Quine's definition of tallies in a theory of concatenation with at least two distinct atoms.

Since there can be, and are, many distinct atoms in the concatenations of letters as javascript strings, a basic predicate of identity of letters is unavoidable, or at least unadvisable at this moment.
The following sketch of code appeard.

```
let isIdenticalLetter = (it, that) => it == that
, isLetterOf = (it, that) => !isEmptyLetter(that) 
  && (isIdenticalLetter(it,firstLetterOf(that))
     || isLetterOf(it,restLettersOf(that)))
, theAlphabet = '() 0123456789abcdefghijklmnopqrstuvwxyz'
, isLetter = it => isLetterOf(it, theAlphabet)
, isIdenticalLetters = (it, that) =>
  (isEmptyLetter(it) && isEmptyLetter(that))
  || (isIdenticalLetter(firstLetterOf(it), firstLetterOf(that))
     && isIdenticalString(restLettersOf(it), restLettersOf(that)))
```

The alphabet is no longer a javascript array each item of which is a javascript string: it is now just a javascript string.

There are two next steps that appeared: 1) start with the basic predicate 'x concatenates y with z' in javascript and build the rest from there so that it is closer to traditional theories of concatenation (the newer theories focus on a family of functions that prepend or appened a single atom/letter to its argument, and while something like that is easier with modern programming langauges to set up, it does not leverage the power of pure predicate logic like the older techniques which have their origins through Tarski's theories of concatenation), and 2) define the extended versions 'letterOf' and 'lettersOf' and 'tallyOf' etc.

The predicate 'x concatenates y with z' is defined as
```
let isConcatenationOf = (x,y,z) => x == y.concat(z);
```
then, the empty string, or empty letter as I've been calling it, is the item denoted by the following predicate.
```
let isEmptyString = x => isConcatenationOf(x,x,x);
```
Somewhere while I was coding these things up it finally occurred to me that 'Of' is actually just a mark of a relative predicate, and not a good indicator of what is classically taken as a function value: that is done by 'the' as in singular descriptions, but these are not to be confused with functions and I am reluctant to conflate matters further.
But I shall, just to see how it goes.

```
let theConcatenationOf = (x,y) => x.concat(y)
, theEmptyConcatenation = '';
```

Social commitments ended this entry prior to any conclusions.


## 2025 0511

### 2025 0511 1844
The work that must be done on my paper on logic can not be done here.
It has done best when worked on with B. F. Skinner's Thinking Aid.
The thinking aid is nothing more nor less than a way of arranging index cards onto a sturdy flap of a folder with masking tape so that each part of a paper can be flipped to so as to cover up all others and leave labels for each section visible.
Items can be easily moved around or replaced.

Right now I am doing a grand compression of those elements that have settled down and not undergone many edits e.g. definitions and examples of denotative functional compounds.
Another change is in the method of arranging the cards.
In my first experiment with Skinner's thinking aid, I only put the cards on the outside of a folder whose contents were loose notes which primed and prompted the cards I made.
Now I am arranging the cards on the inside of the folder so that I can get two full pages when laid flat.

Upon getting eight cards down to three I couldn't stand writing anymore.

## 2025 0510

### 2025 0510 1238
This continues work on my paper on logic from [2025 0507 1421](#2025-0507-1421).

Upon reading my latest work on my logic paper I made the following notes:
1. relays, literals, clauses as well as disjoint, full, and empty chains can go right before or as some part of "Laws of Equivalent Grammatical Categories" as they are even more specific classifications of grammatical categories.
2. Functional completeness is also a special problem of equivalent grammatical categories: it deals with the problem of grammatical supplementation.
For example, the current presentation takes advantage of the purported equivalence between conjunction as a denotative functional method of compounding components and a compound of joint denials of the same components to introduce conjunction as a compound of joint denials.
Each denotative functional method of compounding components is introduced as some compound of joint denials.
This is not the same as showing that joint denials are functionally complete.
Functional completeness contemplates a grammar with a method of denotative compounding which is newly called 'conjunction' and the compounds of which must be shown to be functionally equivalent to some compounding of joint denials.

    The problem of functional completeness is the one of indefinite supplementation and subsequent equivalence of some one grammatical category (such as that of pure negations and conjunctions) with an other (such as that of joint denial).
A general method can be introduced which puports to demonstrate how, under any supplementation of the grammar of a language by a functional method of compounding, a corresponding compound in a canonical category (such as that of pure joint denials) can be constructed.
Wittgenstein mistook the indefiniteness for limitlessness when he failed to establish quantificational sentences as unending truth functional compounds in the Tractatus.

    There is an other way of approaching the problem of functional completeness: translation or paraphrasing.
This solves the problem in that it disolves it: the foreigner is now the responsible party in that the fruits of the grammar of the native is open to them only in so far as they pick out their method of functional compounding among those of the native.
The native is concerned by such problems of paraphrasing in the same way as the foreigner: if the native is to enjoy the advantages of some foreign grammar then the foreign must be superimposed upon the native.
This is the line that I've taken when, e.g., negations are given as self joint denials.

    That there are only finitely many n component methods of functional composition and that the defining characteristic of a functional composition is that its compounds exclusively denote or do not denote and that whether the compound does or does not denote is implied by whether each of its components exclusively do or do not gives the basis for any satisfactory method of paraphrasing from a foreign grammar or proving equivalence of a supplemented grammatical category to some initial and cononical category.

3. A few of the sentences at the beginning of some sections, e.g. laws of validity, consistency, implication, and equivalence, are now redundant since they repeat what was paraphrased immediately following the introduction of the relevant term.

4. The problem of individuation is likely to be case as the transition from existential closures 'some*'as "there is somewhere" to existential croppings 'some' as "there is something".
The ultimate resolution of 'some*' as "some items are" or perhaps 'some*' as a specific iteration of 'some's depends significantly upon the specific recombic functors that are introduced.
It occurred to me yesterday that my [Stack Notation for Predicate (Functor) Logic ](#a-stack-notation-for-predicate-functor-logic-2025-0414-1626) solves a great problem with smoothing the transition from those unfamiliar with predicate functor logic and yet familiar with quantificational logic e.g. 'xFy' a instance of which is 'x is a member of y' or, in short, 'x in y' is familiar for having a lowercase (variable) letter on the left and on the right, and it is this basic division between left and right which not only paves the way (following Russell's "Human Knowledge: scope and limits") for some method of individuation while also smoothly supporting the analysis of "somewhere" as a binary tree of "somethings" which extends the ordered tuple methods which are already familiar in the carrying over of n place predicates to n place relations and which shall come to be the carry over of tree shaped predicates to tree shaped relations: this is already a standard method in many programming langauges, e.g. Paul Graham's [bel](https://www.paulgraham.com/bel.html), and is called 'destructuring assignment'.

5. If I wait to get to existential closures until I am done with denotative functional duality, then I may never get on with the more pressing matter of presenting Quine's main method in its autonomous predicate functor form.

6. The fact that existential closures of denotative functional compounds imposes homogeneity upon the nonchain components of the functional compound may not be as obvious or inevitable as it seems: there may be other alien methods of grammatical analysis that do not fit this rubric.

---
#### Logic, Science, and Validity

Logic is the science of validity and validity is a consequence of grammar and denotation.

#### Functional Compounding and Chains

Compounding is (denotative) functional when, exclusively, each like compound denotes or each like compound does not denote, where and only where (waow), exclusively, each like component denotes or each like component does not denote.
Chains are compounds compounded functionally.

#### Example Chains: Joint Denials, Negations, Alternations, and Conjunctions

Joint denials denote waow each of their components do not.
Negations are self joint denials: they denote waow their component does not.
Alternations are negations of joint denials: they denote waow some of their components do.
Conjunctions are joint denials of negations: they denote waow each of their components do.

#### Subcompounds and Functional Substitutions

Subcompounds of compounds are their self or those of their components.
Substitutions of like compounds for like nonchain subcompounds are (denotative) functional.
Functional substitutions of functional substitutions of compounds are functional substitutions of their self.

#### Functional Validity, Consistency, Implication, and Equivalence

Compounds are (functionally)
* valid waow each of their functional substitutions denote,
* consistent waow their negation is nonvalid (i.e. soem of their functional substitutions denote),
* implied by others waow the conjunction of their self (the conclusion) with the negation of the other (the premise) is nonconsistent (i.e. each of their functional substitutions denotes where the same of the other does), and
* equivalent to others waow they are mutually implicative (i.e. each of their functional substitutions denotes waow the same of the other does).
[See pg. 36 of POL]

#### Example Validities and (Non)consistencies: Laws of Excluded Middle, Contradiction, Self Implication, and Self Equivalence

Alternations of compounds with their negations are valid (they denote waow the compound does or its negation does, i.e. waow it does or does not, so, each functional substitution denotes).
Conjunctions of compounds with their negations are nonconsistent (they denote waow their compound does and its negation does i.e. waow it does and does not, so, each functional substitution does not denote).
Compounds are implied by and equivalent to their self.

#### Functional Substitutions Keep Validity, Nonconsistency, Implication and Equivalence

Functional substitutions in
* validities are validities (each functional substitution of the functional substitution of the validity is a functional substitution of the validity and hence denotes),
* nonconsistencies are nonconsistencies (each function substitution of the negation of the functional substitution of the nonconsistency is a functional substitution of the negation of the nonconsistency i .e. is a functional substitution of a validity and hence the negation of the functional substitution of the nonconsistency is valid so that the function substitution of the nonconsistency is nonconsistent),
* implications are implications (the conjunction of the conclusion with the negation of the premise is nonconsistant and hence its functional substitution is nonconsistent and identical to the conjunction of the function substitution of the conclusion with the negation of the functional substutituion of the premise), and
* equivalences are equivalences (functional substitutions of mutual implications are mutual implications).

#### Interchanges of Equivalents are Equivalent
Interchanges of equivalents in a compound are equivalent to that compound (each functional substitution of a compound matches the same of its interchange, except perhaps for the same of the equivalents which otherwise denote in tandem, so each denotes waow the other does i.e. they are equivalent).

#### Interchnage of Equivalents Keeps Validity, Nonconsistency, Implication, Equivalence, Nonvalidity, Consistency, Nonimplication, and Nonequivalence

Interchanges of equivalents in
* validities are validities (each functional substitution of the interchange denotes waow the same of the validity does),
* nonconsistencies are nonconsistent (their negation is a validity and so the interchange in the negation is a validity),
* implications are implications (interchange into the nonconsistency is a nonconsistency),
* equivalents are equivalents (interchange of mutual implications are mutual implications),
* nonvalidities are nonvalidities (a compound is nonvalid waow some functional substitution does not denote, i.e. some functional substitution of its negation denotes, i.e. its negation is consistent, and since the negation of the interchange is identical tot he interchange of the negation which is consistent and consistency is kept by interchange then the negation is consistent i.e. it is nonvalid)
* consistencies are consistencies (the negation of the interchange is identical to the interchange of the negation which is nonvalid hence it is nonvalid),
* nonimplications are nonimplications (nonimplication is consistency of the conjunction ...)
* nonequivalences are nonequivalences (one is a nonimplication ...).

#### Equivalents of Identity
Compounds are equivalent to
* their double negation (which denotes waow the negation of the compound does not, i.e. waow it does, so, each functional substituion of it denotes waow the same of its double negation does),
* their self alternation/conjunction (which denotes waow some/each of its components does i.e. waow the compound does), and 
* their alternation/conjunction with nonconsistencies/validities.

#### Equivalents of Distributivity of Conjunctions and Alternations
* Alternations of a component with an alternation are equivalent to the alternation of the alternations of the component with each of the others.
* Alternations of a component with a conjunction are equivalent to the conjunction of the alternations of the component with each of the others.
* Conjunctions of a component with an alternation are equivalent to the alternation of the conjunctions of the component with each of the others.
* Conjunctions of a component with a conjunction are equivalent to the conjunction of the conjunctions of the component with each of teh others.

#### Equivalents of Development: Alternational and Conjunctional
* Compounds are equivalent to their alternations with nonconsistencies (equivalents of identity), and, in particular, with conjunctions of other compounds with their negation (by the law of contradiction) which are themselves equivalent to the conjunction of their alternations with the other compound and its negation (by distributivity of alternation over conjunction) i.e. conjunctive development of the one compound with respect to the other.
* The dual for conjunction.

#### Equivalents of Associativity
* The conjunction of the first component with the conjunction of the second and third is equivalent to the conjunction of the conjunction of the first and second with the third.
* The alternation of the first component with the alternation of the second and third is equivalent to the alternation of the alternation of the first and second with the third.

#### Iterated Alternations and Conjunctions
The equivalents of associativity yield the many component alternations and conjunctions which are equivalent to iterated nestings of alternations or conjunctions down their left or right components.

#### Equivalents of Commutativity
* The alternation of the left component with the right component is equivalent to the alternation of the right component with the left.
* The conjunction of the left component with the right component is equivalent to the conjunction of the right component with the left.

#### Equivalents of Distributivity of Negations
* Negations of alternations are equivalent to the conjunctions of the negations of their components.
* Negations of conjunctions are equivalent to the alternations of the negations fo their components.



#### Laws of Validity
Validity is
* inconsistency of negation (which is not consistency of negation, and, hence, not nonvalidity of negation of negation i.e. validity of negation of negation which, by equivalents of identity, is validity)
* nonimplication of negation,
* negational nonimplication
* nonequivalence of negation
* negational nonequivalence, and
* kept by functional substitutions.

#### Laws of Consistency
* Chains are nonconsistent when each of their functional substitutions does not denote.
* Nonconsistency is validity of negation.
* Nonconsistency implies nonvalidity.

#### Laws of Implication
* One chain implies an other and the other a third only where the one implies the third.
* Chains imply their self.
* Chains imply validities.
* Validities do not imply nonvalidities.
* Validities do not imply nonconsistencies.
* Validities only imply validities (each functional substitution of the former that denotes is one where the latter denotes)
* Nonconsistencies imply chains.
* Consistencies do not imply nonconsistencies.
* Nonconsistencies only imply nonconsistencies.

#### Laws of Equivalence
* One chain is equivalent to an other and the other a third only where the one is equivalent to the third.
* Chains are equivalent to their self.
* One chain is equivalent to an other waow the other is equivalent to the one.
* Validities are equivalent to and only equivalent to validities.
* Nonconsistencies are equivalent to and only equivalent to nonconsistencies.

#### Conditionals, Biconditionals, Exclusive Alternations, and Sequents
* Conditionals are alternations of the negation of their (antecedent) left component with their (consequent) right component.
* Biconditionals are conjunctions of the distinct conditionals of their components.
* Sequents are conditionals whose antecedent is the conjunction of their antecdent components and whose consequent is the alternation of their consequent components.
* Exclusive alternations are negations of the biconditionals of their components.

* Implication is validity of conditionals.
* Equivalence is validity of biconditionals.
* Nonequivalence is validity of exclusive alternations.

#### Relays, Literals, and Clauses
* Relays are their component or its negation.
* Literals are relays of nonchain compounds.
* Each component of a clausal chain is a literal.

#### Disjoint, Full, and Empty Chains
* No nonchain subcompounds of different components of *disjoint* chains match.
* The nonchain subcomponents of each component of *full* chains are the same.
* Empty chains have no components: often they are replaced by a relevant validity or inconsistency so as to carry an empty method into a nonempty one.

#### Laws of Equivalent Grammatical Categories
Each clause is equivalent to a disjoint or empty clause.
Each chain is equivalent to
* one with only joint denials
* one with only negations and conjunctions
* one with only negations and alternations
* one with only negations and conditionals
* one without conditionals and biconditionals
* one where only nonchain components are negated
* a conjunction of negations of clausal conjunctions
* the alternational dual of the above
* an alternation of disjoint clausal conjunctions (alternational normal form)
* a conjunction fo disjoint clausal alternations (conjunctive normal form)
* a full alternation of unique disjoint clausal conjunctions
* a full conjunction of unique disjoint clausal alternations

---

I may need to work more on this with Skinner's Thinking Aid.


## 2025 0509

### 2025 0509 1722
This continues work on the little lisp from the last entry [2025 0509 1429](#2025-509-1429).

I was tempted to introduce functions that took a letter and turned it into the corresponding rune as a slower introduction to the method of encoding letters by their numeric index in a given alphabetization and then encoding that number as a tally (where a tally is the proper name for a sequence of empty pairs which is equivalent to the iterated pairing of the empty pair on the left of the empty pair).
Now that I write of that temptation I am prepared to indulge it rather than leap to the 'index of' methods.
These grammatical methods, though they may seem trivial to someone already familiar with the breadth and depth of algorithms from a classical education in computer science, are of great consequence in the traditional methods of arithmetizing syntax.

```
let run=code=>console.log(code,'\n',eval(code)) // for examples

// pairs
let theEmptyPair={}
, isEmpty = it => it == theEmptyPair
, pairOf = (it, that) => [it, that]
, leftOf = it => isEmpty(it) ? it : it[0]
, rightOf = it => isEmpty(it) ? it : it[1];

// sequences as pairs
let theEmptySequence = theEmptyPair
, isEmptySequence = isEmpty
, singletonSequenceOf = it => pairOf(it, theEmptySequence)
, headOf = leftOf
, restOf = rightOf
, concatOf = (it, that) => isEmptySequence(it) ? that 
  : pairOf(headOf(it), concatOf(restOf(it), that))
, prependSingletonOf = (it, that) =>
   concatOf(singletonSequenceOf(it), that)
, appendSingletonOf = (it, that) =>
   concatOf(that,singletonSequenceOf(it));

// stacks as pairs
let theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, singletonStackOf = it => pairOf(theEmptyStack, it)
, pushOf = pairOf
, dropOf = leftOf
, topOf = rightOf
, secondOf = stack => topOf(dropOf(stack))
, drop2Of = stack => dropOf(dropOf(stack))

, prependOf = stack => pushOf(drop2Of(stack)
  , prependSingletonOf(topOf(stack), secondOf(stack)))
, appendOf = stack => pushOf(drop2Of(stack)
  , appendSingletonOf(topOf(stack), secondOf(stack)));

// letters and Runes
let theEmptyRune = theEmptyPair
, isEmptyRune = isEmpty
, theOpenRune = pairOf(theEmptyPair, theEmptyRune)
, isOpenRune = it => !isEmpty(it) && isEmpty(leftOf(it)) && isEmptyRune(rightOf(it))
, theCloseRune = pairOf(theEmptyPair, theOpenRune)
, isCloseRune = it => !isEmpty(it)&&isEmpty(leftOf(it))&&isOpenRune(rightOf(it))

, theEmptyLetter=''
, isEmptyLetter = it => it == theEmptyLetter
, theOpenParen = '('
, isOpenParen = it => it == theOpenParen
, theCloseParen = ')'
, isCloseParen = it => it == theCloseParen

, stringOf = (...letters) => 
   letters.length ? letters.shift() + stringOf(...letters) : theEmptyLetter
, firstLetterOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters[0]
, restLettersOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters.slice(1)

, runeOf = letter => isOpenParen(letter) ? theOpenRune
  : isCloseParen(letter) ? theCloseRune
  : theEmptyRune

, letterOf = rune => isOpenRune(rune) ? theOpenParen
  : isCloseRune(rune) ? theCloseParen
  : theEmptyLetter

, runesOf = letters => isEmptyLetter(letters) ? theEmptySequence
  : prependSingletonOf(runeOf(firstLetterOf(letters)), runesOf(restLettersOf(letters)))

, lettersOf = runes => isEmptySequence(runes) ? theEmptyLetter
  : stringOf(letterOf(headOf(runes)), lettersOf(restOf(runes)))

run('lettersOf(runesOf("(()())() this should be ignored"))');

// read and print sequences
let readOpenRuneOf = (stack, runes) => 
   readSequenceOf(pushOf(stack,theEmptySequence), restOf(runes))

, readCloseRuneOf = (stack, runes) =>
   readSequenceOf(appendOf(stack), restOf(runes))

, readSequenceOf = (stack, runes) => 
  isEmptySequence(runes) ? headOf(topOf(stack))
  : isOpenRune(headOf(runes)) ? readOpenRuneOf(stack,runes)
  : isCloseRune(headOf(runes)) ? readCloseRuneOf(stack, runes)
  : readSequenceOf(stack, restOf(runes))
, readOf = runes => readSequenceOf(theEmptyStack, runes);

let parenOf = runes => prependSingletonOf(theOpenRune
    , appendSingletonOf(theCloseRune, runes))

, printSequenceOf = sequence =>
  isEmptySequence(sequence) ? theEmptySequence
  : concatOf(printOf(headOf(sequence))
    , printSequenceOf(restOf(sequence)))

, printOf = sequence =>
  isEmptySequence(sequence) ? parenOf(theEmptySequence)
  : parenOf(printSequenceOf(sequence))   

, read = letters => readOf(runesOf(letters))
, print = sequence => lettersOf(printOf(sequence));

run("lettersOf(parenOf(runesOf('(()()())')))")
run("print(read('()'))")
run("print(read('(()(())())'))")
run("print(read('(())(()(())())'))")
run("print(read('(()(())())((()))'))")
run("print(read('))))(('))")
```

This appears to have simplified definitions while also pointing to general methods e.g. the definitions of empty, open, and closed runes are ripe for inductive generalization as was done in past versions that gave the alphabet as a javascript array on which functions of search were invoked.


### 2025 0509 1429

Here is the completed code from yesterday's work  [2025 0508 2207](#2025-0508-2207) on my little lisp.
I also put the appropriate printer code that was missing from the last note into that note.

```
let run=code=>console.log(code,'\n',eval(code)) // for examples

// pairs
let theEmptyPair={}
, isEmpty = it => it == theEmptyPair
, pairOf = (it, that) => [it, that]
, leftOf = it => isEmpty(it) ? it : it[0]
, rightOf = it => isEmpty(it) ? it : it[1];

// sequences as pairs
let theEmptySequence = theEmptyPair
, isEmptySequence = isEmpty
, singletonSequenceOf = it => pairOf(it, theEmptySequence)
, headOf = leftOf
, restOf = rightOf
, concatOf = (it, that) => isEmptySequence(it) ? that 
  : pairOf(headOf(it), concatOf(restOf(it), that))
, prependSingletonOf = (it, that) =>
   concatOf(singletonSequenceOf(it), that)
, appendSingletonOf = (it, that) =>
   concatOf(it,singletonSequenceOf(that));

// stacks as pairs
let theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, singletonStackOf = it => pairOf(theEmptyStack, it)
, pushOf = pairOf
, dropOf = leftOf
, topOf = rightOf
, secondOf = stack => topOf(dropOf(stack))
, encatOf = stack => pushOf(dropOf(dropOf(stack))
  , appendSingletonOf(secondOf(stack), topOf(stack)));

// letters
let theEmptyLetter=''
, isEmptyLetter = it => it == theEmptyLetter
, stringOf = (...letters) => 
   letters.length ? letters.shift() + stringOf(...letters) : theEmptyLetter
, firstLetterOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters[0]
, restLettersOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters.slice(1)

, theOpenParen = '('
, isOpenParen = it => it == theOpenParen
, theCloseParen = ')'
, isCloseParen = it => it == theCloseParen

, runesOf = letters => isEmptyLetter(letters) ? theEmptySequence
  : isOpenParen(firstLetterOf(letters)) ? prependSingletonOf(theOpenRune, runesOf(restLettersOf(letters)))
  : isCloseParen(firstLetterOf(letters)) ? prependSingletonOf(theCloseRune, runesOf(restLettersOf(letters)))
  : runesOf(restLettersOf(letters))

, lettersOf = runes => isEmptySequence(runes) ? theEmptyLetter
  : isOpenRune(headOf(runes)) ? stringOf(theOpenParen, lettersOf(restOf(runes)))
  : isCloseRune(headOf(runes)) ? stringOf(theCloseParen, lettersOf(restOf(runes)))
  : lettersOf(restOf(runes));
run('lettersOf(runesOf("(()())() this should be ignored"))');

// read and print sequences
let theOpenRune = theEmptyPair
, isOpenRune = isEmpty
, readOpenRuneOf = (stack, runes) => 
   readSequenceOf(pushOf(stack,theEmptySequence), restOf(runes))

, theCloseRune = pairOf(theEmptyPair,theEmptyPair)
, isCloseRune = it => !isEmpty(it)&&isEmpty(leftOf(it))&&isEmpty(rightOf(it))
, readCloseRuneOf = (stack, runes) =>
   readSequenceOf(encatOf(stack), restOf(runes))

, readSequenceOf = (stack, runes) => isEmptySequence(runes) ? headOf(topOf(stack))
  : isOpenRune(headOf(runes)) ? readOpenRuneOf(stack,runes)
  : isCloseRune(headOf(runes)) ? readCloseRuneOf(stack, runes)
  : readSequenceOf(stack, restOf(runes))
, readOf = runes => readSequenceOf(theEmptyStack, runes);
run("isEmpty(read('()'))")

let parenOf = runes => 
  prependSingletonOf(theOpenRune, appendSingletonOf(runes, theCloseRune))
, printSequenceOf = sequence =>
  isEmptySequence(sequence) ? theEmptySequence
  : concatOf(printOf(headOf(sequence))
    , printSequenceOf(restOf(sequence)))
, printOf = sequence =>
 isEmptySequence(sequence) ? parenOf(theEmptySequence)
 : parenOf(printSequenceOf(sequence))   

, read = letters => readOf(runesOf(letters))
, print = sequence => lettersOf(printOf(sequence));

run("lettersOf(parenOf(runesOf('(()()())')))")
run("print(read('()'))")
run("print(read('(()(())())'))")
run("print(read('(())(()(())())'))")
run("print(read('(()(())())((()))'))")
run("print(read('))))(('))")
```

## 2025 0508

### 2025 0508 2207
This entry shall be different than prior ones.
Some time in the near future I shall look back on what I have done and select what has worked from what has not.
The only way I know how to carry out that task effectively is to print out what I have written and go over it with a pen in my hand and some note paper near by, but this will be the first time that I attempt to do this in public.

There are a lot of firsts among what is written here.
This is another first not entirely divorced from the more detailed review of what I have done that is yet forthcoming.
Originally I had aimed at scheduling a public review at the end of last month.
As the days go by I shall miss the mark more and more, but I do not expect to be that far off.
Perhaps I am simply being optimistic.

This entry is a response to what I have to say about what I have done that I recall without looking back over a print out of what is recorded here.
There are three threads that are woven between my writings:

1. logic
2. programming
3. philosophy.

For all the philosophy that is here, I have never been a big fan of it.
The moment I was exposed to the science of behavior, as outlined in any one of B. F. Skinner's books, I left philosophy as anything more than a narrow science of a narrow kind of verbal behavior (what I have since come to call "the science of smooth dialogue").

To my surprise, the science of behavior has yet to take up much space here.
As much as I've tried to keep all my episodes of reading on the record here, I have failed to do so on many occasions.
In particular, I have read selections from Skinner's "Cumulative Record" without noting what my responses to them were here.
Even when I carry around a notepad and a pen in my pocket, I still read without making any record of what occurs to me while reading.
There is an excuse looming; something about thinking without writing being faster and easier than writing without thinking.

I've spent far more time working on my little lisp than I ever planned.
This is partially becuase I am frequently reminded of the missing link between Feferman's Finitary Inductively Presented Logics and something like LISP.
It is also because I must make sure that there is nothing I have missed about each step in the construction and conventions of LISP that might later leave me blind when I finally come to compare and contrast it with FORTH or some other concatenative language.

Another reason I've spent so much time on it is because I see each obstacle at some later stage of any longer work as an opportunity to introduce a foundation whose deferred consequences entirely avoid such obstacles.
The more obstructions I confront the more likely I am to examine the link between whatever premises I've overtly identified and how presently obstructing conclusions follow from them.
This is rarely a matter of formal deduction, but almost always a result of methods with a strong familial similarity to logic.
Almost always, major problems are the result of a poorly selected lexicon.
The basic predicates with which a problem is stated and in which its solutions are proposed almost always decide my fate.

In the past LISP has been saddled with lambda calculi.
No such theories are likely to turn up here: lambda calculi are nonconsistent without some crippling and complex system of typing.
Furthermore, the methods of lambda calculi are merely the theoretical echos of better logical methods e.g. predicate abstraction and concretion are the logical operations that permit talk of lambda abstraction and application.
This has been known since Quine and Church traded reviews of each others works in the early half of the 20th century.
The moment Quine revealed that application could be reinterpreted as the collective relate (an operation on relations uncovered by Russell among his short summaries of work he and Whitehead had done), it should have been clear that no result obtained from the lambda calculus should be taken as special to it.
Little did either Quine or Church know that Quine's later methods of predicate abstraction and concretion revealed the logically universal way of dealing with the multitude of theories mathematicians may propose (be they lambda calculi, relational calculi, set theories etc.)

Thus, I've spent much time fussing over tiny distinctions between various attempts at getting out something like LISP: the methods are to be closer to those of Goodstein's equation calculi than to any lambda calculus.
But, it is more than that.
Ordered pairs are the most modest abstract objects that may be admitted to any theory and which release most of the grammatical methods that have found greatest use in the analysis of carefully designed languages.
This is the premise not only revealed at the end of Quine's "Word and Object" but also a turning point in Feferman's F.I.P.L.s.

Up until very recently this was the path I planned on following in order to reach out from logic to the practices of programming.
It is only upon uncovering "Logic Programming" that the entirety of programming practice can be reduced to the predicate "is valid", or, as the case often is, the kindred predicate "is inconsistent".
Had this been shown to me earlier I may have avoided much of my work on primitive recursive functions of ordered pairs.
But, I'm still stuck in a world of Forths and Lisps rather than a world of logic.

What little I have learned of Prolog has no promise to me.
It works on a limited slice of logical methods and Datalog doesn't appear to do much better.
The rush to monetize logic programming and its fundamental relation to database systems generally ruined the logic.
The modern world, one where large masses of capital are committed to the matrix methods of machine learning, is technically one where logic programming is primed to reign supreme for the following simple reason: any path which leads to inconsistency works but some paths are faster than others for no reason in particular.
Pro- and data- log aimed to only permit constructions which admit short chains of relevant implications.
The historic obsession with implication as in formal deduction has blinded modern computer scientists and logicians to the role of logical methods prior to any science e.g. mathematical or computational.
It is only a matter of time before the window for logical programming closes once again for the same reason it did in the 80s.

Through my little lisp I still plan on exploring the methods of logic programming.
This shall be done in parallel with my other programming language which is much like uhdForth.

The problem I'm posed with upon writing these paragraphs is this: can I juggle more while also building a new experimental culture?

### 2025 0508 1650
This continues my work on my little lisp from [2025 0507 2029](#2025-0507-2029).

I'm hopeful that whatever logical errors in the code I produced yesterday will be solved today: I am more awake now than I was when I was working on this last night.
It seems that I forgot about the output at the end of the reader: instead of just taking the top of the stack as the appropriately constructed sequence, I must actually take the head of the sequence atop the stack.
This is due to how a close parenthesis (or close rune) works in the degenerate case where there are less than two items on the stack.
In this degenerate case the last parenthesis (of a sequence of runes with balanced parenthesis, as I have not checked the unbalanced cases yet) actually encats the singleton sequence of the top of the list to what it takes as an empty sequence, but which is really the result of a degenerate case of taking the top of the empty stack: it returns the empty sequence.

As I went to copy the code over here I again fell prey to coding rather than copying.
It occurred to me that there are errors that have slipped in because I have once again not properly factored out the boundaries between languages that are under construction.

Thanks to my prior work I have happily seperated the defining features of sequences from those of lists.
Though it is not a traditional distinction, it is consistent with Feferman's Finitary Inductively Presented Logics use of sequences as special ordered pairs and lists as a sequence grammatically tagged as a sequence.
The analysis of a LISP list as a sequence tagged grammatically as a sequence is key to general grammatical methods at work in modern programming practices and which are sadly conflated with problems of types as an abstract concern.

My aim now is to simplify the reader and printer to work entirely with sequences in order to sus out any logical errors at the level of a purely parenthetical language rather than one that also induces the complexity of runes and strings.

The new starting point is this code:

```
// pairs
let theEmptyPair={}
, isEmpty = it => it == theEmptyPair
, pairOf = (it, that) => [it, that]
, leftOf = it => isEmpty(it) ? it : it[0]
, rightOf = it => isEmpty(it) ? it : it[1]

// sequences as pairs
, theEmptySequence = theEmptyPair
, isEmptySequence = isEmpty
, singletonSequenceOf = it => pairOf(it, theEmptySequence)
, headOf = leftOf
, restOf = rightOf
, concatOf = (it, that) => isEmptySequence(it) ? that 
  : pairOf(headOf(it), concatOf(restOf(it), that)) 

// stacks as pairs
, theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, singletonStackOf = it => pairOf(theEmptyStack, it)
, pushOf = pairOf
, dropOf = leftOf
, topOf = rightOf
, secondOf = stack => topOf(dropOf(stack))
, encatOf = stack => pushOf(dropOf(dropOf(stack))
  , concatOf(secondOf(stack), singletonSequenceOf(topOf(stack))));
```

The only part that still needs a bit of explanation is what the function designated by 'encatOf' actually does and why it keeps occurring in the code (or why some function like it continues to reoccur).
My explanation for now is that it is the least intrusive way to take advantage of the power of stack based methods without bringing every defined operation on pairs and sequences to stacks.
To do so would be to create a stack based language like FORTH, also called a concatenative language.
That is not the aim here.

What the function designated by 'encatOf' does is to help with how to interpret a closed parenthesis.
This gets us to the new function being defined.
For now it is designated by 'readSequenceOf'.
It aims to take a sequence of runes, each of which is an open or close rune, and construct the corresponding iterated sequences of empty sequences.
The open rune begins a new sequence by pushing the empty sequence onto the stack and the close rune adds the item at the top of the stack to the end of the sequence that is second from the top of the stack.

Here is my first attempt based on all that I've learned so far.

```
// read sequences from a sequence of open and closed runes
let theOpenRune = theEmptyPair
, isOpenRune = isEmpty
, readOpenRuneOf = (stack, runes) => 
   readSequenceOf(pushOf(stack,theEmptySequence), restOf(runes))

, theCloseRune = pairOf(theEmptyPair,theEmptyPair)
, isCloseRune = it => !isEmpty(it)&&isEmpty(leftOf(it))&&isEmpty(rightOf(it))
, readCloseRuneOf = (stack, runes) =>
   readSequenceOf(encatOf(stack), restOf(runes))

, readSequenceOf = (stack, runes) => isEmptySequence(runes) ? headOf(topOf(stack))
  : isOpenRune(headOf(runes)) ? readOpenRuneOf(stack,runes)
  : isCloseRune(headOf(runes)) ? readCloseRuneOf(stack, runes)
  : readSequenceOf(stack, restOf(runes))

, readOf = runes => readSequenceOf(theEmptyStack, runes);
```

Now only a small number of specialty operations on javascript strings appear to be needed to interface with these functions as defined.

```
// letters
let theEmptyLetter=''
, isEmptyLetter = it => it == theEmptyLetter
, stringOf = (...letters) => 
   letters.length ? letters.shift() + stringOf(...letters) : theEmptyLetter
, firstLetterOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters[0]
, restLettersOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters.slice(1)

, theOpenParen = '('
, isOpenParen = it => it == theOpenParen
, theCloseParen = ')'
, isCloseParen = it => it == theCloseParen;
```

They are the same old same old, except for the last two that single out two particular letters.
It finally occurred to me to add the following two definitions to the end of the definitions of functions for treating ordered pairs as sequences:

```
let prependSingletonOf = (it, that) =>
   concatOf(singletonSequenceOf(it), that)
, appendSingletonOf = (it, that) =>
   concatOf(it,singletonSequenceOf(that));
```

Then, the definition of the function designated by 'encatOf' becomes just as follows.

```
let encatOf = stack => pushOf(dropOf(dropOf(stack))
 , appendSingletonOf(secondOf(stack), topOf(stack)));
```

Construction of a sequence of runes from a string of letters of which only the parenthesis are of concern proceeds as follows.

```
let runesOf = letters => isEmptyLetter(letters) ? theEmptySequence
  : isOpenParen(firstLetterOf(letters)) ? prependSingletonOf(theOpenRune, runesOf(restLettersOf(letters)))
  : isCloseParen(firstLetterOf(letters)) ? prependSingletonOf(theCloseRune, runesOf(restLettersOf(letters)))
  : runesOf(restLettersOf(letters))
```

Then the function that gets us back from a sequence of runes to letters is as follows.

```
let lettersOf = runes => isEmptySequence(runes) ? theEmptyLetter
  : isOpenRune(headOf(runes)) ? stringOf(theOpenParen, lettersOf(restOf(runes)))
  : isCloseRune(headOf(runes)) ? stringOf(theCloseParen, lettersOf(restOf(runes)))
  : lettersOf(restOf(runes))
```

A brief example demonstrates that the functions from letters to runes and vice versa work.

```
lettersOf(runesOf("(()())() this should be ignored")) 
 (()())()
```

This assures us that when we send a sequence of runes to the reader it corresponds to the parentheses in the given string.

Next the sequence printer.
It should print out a sequence of runes that, when put back into the reader and printed should output an identical sequence of runes.
The notation for sequences adopted in the reader has an open and close rune around its contents.

```
let parenOf = runes => prependSingletonOf(theOpenRune, appendSingletonOf(runes, theCloseRune))
```
An example:
```
lettersOf(parenOf(runesOf('(()()())'))) 
 ((()()()))
```

The sequence printer assumes that the pair it is passed is to be printed as a pure sequence.
If asked to print the empty sequence it prints an open rune followed by a closed rune.
Otherwise it encloses the print of each item in the sequence, one after the other, in open and closed runes.

```
let parenOf = runes => 
  prependSingletonOf(theOpenRune, appendSingletonOf(runes, theCloseRune))
, printSequenceOf = sequence =>
  isEmptySequence(sequence) ? theEmptySequence
  : concatOf(printOf(headOf(sequence))
    , printSequenceOf(restOf(sequence)))
, printOf = sequence =>
 isEmptySequence(sequence) ? parenOf(theEmptySequence)
 : parenOf(printSequenceOf(sequence))   

, read = letters => readOf(runesOf(letters))
, print = sequence => lettersOf(printOf(sequence));
```

The following examples test out the reader and the printer.
```
isEmpty(read('()')) 
 true
print(read('()')) 
 ()
print(read('(()(())())')) 
 (()(())())
print(read('(())(()(())())')) 
 (())
print(read('(()(())())((()))')) 
 (()(())())
```
The last two examples indicate conspicuously that the case in the function denoted by 'readSequenceOf' where the rune sequence is empty and returns the head of the top of the stack on which the corresponding sequence was being constructed.

This parenthesis notation is the infix version of the postfix notation from [Bit Strings and Binary Trees](#2025-0413-1513-bit-strings-and-binary-trees) but instead of interpreting everything as binary trees they are interpreted as pure sequences i.e. iterated sequences of the empty sequence.


This is the basic method upon which my earlier attempts at constructing a reader and printer for past versions of my list are based.


## 2025 0507

### 2025 0507 2029
This continues the work on my little lisp from [2025 0505 1725](#2025-0505-1725).

Last note left on a question: were all these distinctions between stacks as LISP lists as sequences and sequences as pairs helpful?
What I uncovered upon further reflection is that the answer to this question is yes and no.
These distinctions are still of critical importance, but not in the straight line from pairs up through stacks as LISP lists.
What occurred to me is that there are two languages that I've built upon native javascript and that I've failed to untangle them from each other.

The lowest language is the programmable part of the arithmetic of ordered pairs, and the language above that is my little LISP.
Thus there are two readers: one that constructs LISP lists and the one for my little LISP (which is more like a calculator or evaluator).

Sequences and stacks are then implemented as before the last entry: atop the arithmetic of ordered pairs.

```
// pairs
let theEmptyPair={}
, isEmpty = it => it == theEmptyPair
, pairOf = (it, that) => [it, that]
, leftOf = it => isEmpty(it) ? it : it[0]
, rightOf = it => isEmpty(it) ? it : it[1]

// sequences as pairs
, theEmptySequence = theEmptyPair
, isEmptySequence = isEmpty
, singletonSequenceOf = it => pairOf(it, theEmptySequence)
, headOf = leftOf
, restOf = rightOf
, concatOf = (it, that) => isEmptySequence(it) ? that 
  : pairOf(headOf(it), concatOf(restOf(it), that)) 

// stacks as pairs
, theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, singletonStackOf = it => pairOf(theEmptyStack, it)
, pushOf = pairOf
, dropOf = leftOf
, topOf = rightOf
, secondOf = stack => topOf(dropOf(stack))
, encatOf = stack => pushOf(dropOf(dropOf(stack))
  , concatOf(secondOf(stack), singletonSequenceOf(topOf(stack))));
```

All is back to how it was a few notes ago but with a few better names for the different operations.
These name changes also introduce some later distinctions that will come as the result of grammatical (type) analysis.
I've gone back to not focusing on a specific alphabetization of runes.
The reader is now assembled from simpler functions for reading the special open, close, and space runes.

There are a few other tricks of the trade that have been included e.g. javascript defaults to giving negative one when it does not find the index of an item you are looking for in an array, but it greatly simplifies a theory, in almost all cases, when it returns the length of the array.
Another very helpful trick is to return the entire alphabet when you give it an index that is outside the range of the alphabet.
Both of these are examples of degenerate cases that can be made to play nicely with the rest of a theory.

```
// letters
, theEmptyLetter=''
, isEmptyLetter = it => it == theEmptyLetter
, stringOf = (...letters) => 
   letters.length ? letters.shift() + stringOf(...letters) : theEmptyLetter
, firstLetterOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters[0]
, restLettersOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters.slice(1)

// letter as number
, abc =[...'() 0123456789abcdefghijklmnopqrstuvwxyz']
, alphabeticalIndexOf = letter => 
   abc.includes(letter) ? abc.indexOf(letter) : abc.length
, alphabeticalLetterOf = index => 
   0 <= index && index < abc.length ? abc[index] : abc

// number as tally (as pair)
, tallyOf = n => n>0 ? pairOf(theEmptyPair, tallyOf(n-1)) : theEmptyPair
, countOf = x => isEmpty(x) ? 0 : 1 + countOf(rightOf(x))

// rune as tally
, runeOf = letter => tallyOf(alphabeticalIndexOf(letter))
, letterOf = rune => alphabeticalLetterOf(countOf(rune))

// letters as sequences of runes
, runesOf = letters => 
   letters.length ? concatOf(
    singletonSequenceOf(runeOf(firstLetterOf(letters)))
    , runesOf(restLettersOf(letters)))
   : theEmptySequence
, lettersOf = runes =>
   isEmptySequence(runes) ? theEmptyLetter
   : stringOf(letterOf(headOf(runes)), lettersOf(restOf(runes)))

// recursive definition of identity of pairs
, id = (it, that) =>
  (isEmpty(it) && isEmpty(that))
  || (!(isEmpty(it) || isEmpty(that))
     && id(leftOf(it), leftOf(that))
     && id(rightOf(it), rightOf(that)))

// reader
, theOpenRune = runeOf('(')
, isOpenRune = it => id(it, theOpenRune)
, readOpenRuneOf = (stack, runes) => 
   readerOf(pushOf(stack, theEmptySequence), restOf(runes))

, theCloseRune = runeOf(')')
, isCloseRune = it => id(it, theCloseRune)
, readCloseRuneOf = (stack, runes) => 
   readerOf(encatOf(stack), restOf(runes))

, theSpaceRune = runeOf(' ')
, isSpaceRune = it => id(it, theSpaceRune)
, readSpaceRuneOf = (stack, runes) => !isEmptySequence(topOf(stack))   
  ? readerOf(pushOf(encatOf(stack), theEmptySequence), restOf(runes))
  : isEmptySequence(secondOf(stack)) ? readerOf(stack, restOf(runes))
  : readerOf(pushOf(stack, theEmptySequence), restOf(runes))

, isRune = it => countOf(it) < abc.length
, readRuneOf = (stack, runes) => 
   readerOf(encatOf(pushOf(stack, headOf(runes))), restOf(runes))

, readerOf = (stack, runes) =>
  isEmptySequence(runes) ? topOf(stack)
  : isOpenRune(headOf(runes)) ? readOpenRuneOf(stack, runes)
  : isCloseRune(headOf(runes)) ? readCloseRuneOf(stack, runes)
  : isSpaceRune(headOf(runes)) ? readSpaceRuneOf(stack, runes)
  : isRune(headOf(runes)) ? readRuneOf(stack, runes)
  : readerOf(stack, restOf(runes)) 

, readOf = runes => readerOf(theEmptyStack, runes)
```
This reader is increadibly powerful.
Here is a tiny printer that anticipates the method of lists as sequences which begin with the symbol "list".
```
// printer
, printSequenceOf = sequence => isEmptySequence(sequence) ? theEmptySequence
 : concatOf(printOf(headOf(sequence))
   , concatOf(singletonSequenceOf(theSpaceRune)
     , printSequenceOf(restOf(sequence))))
, parenOf = runes => concatOf(singletonSequenceOf(theOpenRune)
   , concatOf(runes, singletonSequenceOf(theCloseRune)))
, printParenSequenceOf = sequence => parenOf(concatOf(singletonSequenceOf(theSpaceRune), printSequenceOf(sequence)))

, isList = sequence => id(headOf(sequence), runesOf('list'))
, printListOf = list => printParenSequenceOf(list)

, printOf = item => isList(item)? printListOf(item) : item

// external read and print
, read = letters => readOf(runesOf(letters))
, print = item => lettersOf(printOf(item))
```

Neither the reader nor the printer seem to be working as expected e.g. reading a space rune goes wrong when there are lots of parentheses bunched up.


### 2025 0507 1421
This continues my work on my paper on logic from [2025 0412 1422](#2025-0412-1422).

The following presentation of logic is unique in a number of ways.
Primarily it is quantificational logic.
It does away with quantifiers as in "(each item is x such that)(x is human only if x is mortal)" by skipping over the intermediate methods of Quine's predicate abstracts as in "(each item is)(x such that x is human only if x is mortal)" which clearly factors out the predicate functor 'each item is' from the predicate 'x such that x is human only if x is mortal' by doing away entirely with the classical role of variables as in 'each human only if mortal'.

Predicate functor logic is a foreign language to those who speak natively with all the cross referential power of pronouns, whether they be free and taken as dummy singular terms or bound and taken as dummies of qunatification.
Though the grammars of (the unhappily named) natural languages are burdened with only indirect analyses of cross reference, the carefully designed language of predicate functor logic eliminates all referential oddities by explaining their cross referential operations as the result of the (logical) equivalence of a predicate with one in a special grammatical category (those constructed from the so called recombic predicate functors).

Although at most three predicate functors are sufficient to assemble all the compound predicates from the lexicon of a logical langauge, the predicate functor foreigner would find themselves lost in construction.
Thus, full predicate functor logic is introduced on a schedule which is sufficiently similar to classical treatments to admit little or no alarm among those already familiar with the path to quantificational logic from truth functional logic and through the Boolean closure logics of many place predicates (this is an urgent reminder that Boolean logic is the logic of existential closures of truth functional compounds of basic predicates, and that Boolean logic is not to be confused, as it so often is, with the algebra of truth functions as functions on the set whose sole members are the true and the false).

In addition to the unique methods of predicate functor logic, there is an additional and equally profound difference in this presentation of logic: it follows from the grammatical definition of validity as, broadly, substitution for lexicon.
Just as Quine failed to provide an autonomous proof procedure for predicate functor logic (one that did not depend on translation to quantificational logic and back from the same), so too did Quine fail to bring his grammatical definition of validity as truth of substitution under supplementation of lexicon to predicate logic.
The substitutional basis for the grammatical definition of validity is established by carefully combining the works of Hilbert and Bernay's in their proof of completeness of quantificational logic in the second book of their Grundlagen as a strengthening of Tarski's model theoretic definition of validity originally from his analysis of truth in his 1933 paper âThe concept of truth in the languages of the deductive sciencesâ but conspicuously from his 1956 paper "Arithmetical extensions of relational systems" with Robert Vaught, and the completeness theorems of Skolem, Herbrand, and Godel.
For more details on the delicate combination of these fundamental components of logical practice see Quine's "Philosophy of Logic second edition" section entitled "Adequacy of Substitution".

So the distinguishing features of this presentation are predicate functors and substitutional validity.
There are secondary features which are perhaps just as important, but which are not so easy to explain.
Traditional logical methods deal with schema and functions of truth values by building up schematic sentence letters with connectives parallel to functions from truth values to truth values as in truth tables, or, much better, as truth trees.
Following the guidence of grammatical methods, no such methods are found here.
They are secondary in that each branch of a truth tree is an otherworldly mirror on the grammatical structure of their alternational development and reduction by equivalents of identity.
This stricture has the benefit of eliminating confusion caused by schematic letters e.g. they are sometimes taken as more than dummies and made to refer or mean in one or another mysterious way.

I am not full convinced of the utility of these self contained grammatical methods, but shall push them as far as I can without losing track of how well the schedule upon which quantificational practices are built is carried over.

What follows is merely an outline.
It charts out the path that has not yet been taken based on the best available reports that have come back from those who have survived their trip into the jungle of logic without having been too harshly damanged by its wilds.
Russell cleared much of the way, but was left with unhappy impressions that, thankfully, do not seem to have harmed Quine into a frightful silence: Quine wrote on logic till his final days.

These notes are terse and a not inconsiderable amount of effort on the part of the reader is required to effectively navigate them: many of the later conclusions lack supporting arguments.
The key to taking each step, e.g. from your native tongue to the truth functional part of predicate functor logic or from the truth functional part to the Boolean closure part, is Quine's method of paraphrasing inwards.
No matter what the inner components of a sentence may be we can analyze their outermost grammatical structure while leaving their inner parts as yet unanalyzed.
But, now we must take our sentences as those of a predicate functor community i.e. as one where primitives and compounds are predicates as integral words or phrases such as "father of", "is man", or just "man".

I submit that the sentences of a predicate functor community are like those we attribute to early humans, e.g. "each human only if mortal" rather than the aristotelian universal "All humans are mortal" or the quantificational "Everything is such that it is human only if it is mortal", and that this is no mere accident of expedient analysis.

A final note on my methods of logic: I have tried to defer the introduction of as many distinguishing features as far as possible.
For example, conditionals and biconditionals are introduced far after functional equivalence.
This method of spreading out concerns is invaluable in any endevour to simplify and elucidate without adding comments of simplification adn elucidation.

 
#### Logic, Science, and Validity

Logic is the science of validity and validity is a consequence of grammar and denotation.

#### Functional Compounding and Chains

Compounding is (denotative) functional when, exclusively, each like compound denotes or each like compound does not denote, where and only where (waow), exclusively, each like component denotes or each like component does not denote.
Chains are compounds compounded functionally.

#### Example Chains: Joint Denials, Negations, Alternations, and Conjunctions

Joint denials denote waow each of their components do not.
Negations are self joint denials: they denote waow their component does not.
Alternations are negations of joint denials: they denote waow some of their components do.
Conjunctions are joint denials of negations: they denote waow each of their components do.

#### Subcompounds and Functional Substitutions

Subcompounds of compounds are their self or those of their components.
Substitutions of like compounds for like nonchain subcompounds are (denotative) functional.
Functional substitutions of functional substitutions of compounds are functional substitutions of their self.

#### Functional Validity, Consistency, Implication, and Equivalence

Compounds are (functionally)
* valid waow each of their functional substitutions denote,
* consistent waow their negation is nonvalid (i.e. soem of their functional substitutions denote),
* implied by others waow the conjunction of their self (the conclusion) with the negation of the other (the premise) is nonconsistent (i.e. each of their functional substitutions denotes where the same of the other does), and
* equivalent to others waow they are mutually implicative (i.e. each of their functional substitutions denotes waow the same of the other does).
[See pg. 36 of POL]

#### Example Validities and (Non)consistencies: Laws of Excluded Middle, Contradiction, Self Implication, and Self Equivalence

Alternations of compounds with their negations are valid (they denote waow the compound does or its negation does, i.e. waow it does or does not, so, each functional substitution denotes).
Conjunctions of compounds with their negations are nonconsistent (they denote waow their compound does and its negation does i.e. waow it does and does not, so, each functional substitution does not denote).
Compounds are implied by and equivalent to their self.

#### Functional Substitutions Keep Validity, Nonconsistency, Implication and Equivalence

Functional substitutions in
* validities are validities (each functional substitution of the functional substitution of the validity is a functional substitution of the validity and hence denotes),
* nonconsistencies are nonconsistencies (each function substitution of the negation of the functional substitution of the nonconsistency is a functional substitution of the negation of the nonconsistency i .e. is a functional substitution of a validity and hence the negation of the functional substitution of the nonconsistency is valid so that the function substitution of the nonconsistency is nonconsistent),
* implications are implications (the conjunction of the conclusion with the negation of the premise is nonconsistant and hence its functional substitution is nonconsistent and identical to the conjunction of the function substitution of the conclusion with the negation of the functional substutituion of the premise), and
* equivalences are equivalences (functional substitutions of mutual implications are mutual implications).

#### Interchanges of Equivalents are Equivalent
Interchanges of equivalents in a compound are equivalent to that compound (each functional substitution of a compound matches the same of its interchange, except perhaps for the same of the equivalents which otherwise denote in tandem, so each denotes waow the other does i.e. they are equivalent).

#### Interchnage of Equivalents Keeps Validity, Nonconsistency, Implication, Equivalence, Nonvalidity, Consistency, Nonimplication, and Nonequivalence

Interchanges of equivalents in
* validities are validities (each functional substitution of the interchange denotes waow the same of the validity does),
* nonconsistencies are nonconsistent (their negation is a validity and so the interchange in the negation is a validity),
* implications are implications (interchange into the nonconsistency is a nonconsistency),
* equivalents are equivalents (interchange of mutual implications are mutual implications),
* nonvalidities are nonvalidities (a compound is nonvalid waow some functional substitution does not denote, i.e. some functional substitution of its negation denotes, i.e. its negation is consistent, and since the negation of the interchange is identical tot he interchange of the negation which is consistent and consistency is kept by interchange then the negation is consistent i.e. it is nonvalid)
* consistencies are consistencies (the negation of the interchange is identical to the interchange of the negation which is nonvalid hence it is nonvalid),
* nonimplications are nonimplications (nonimplication is consistency of the conjunction ...)
* nonequivalences are nonequivalences (one is a nonimplication ...).

#### Equivalents of Identity
Compounds are equivalent to
* their double negation (which denotes waow the negation of the compound does not, i.e. waow it does, so, each functional substituion of it denotes waow the same of its double negation does),
* their self alternation/conjunction (which denotes waow some/each of its components does i.e. waow the compound does), and 
* their alternation/conjunction with nonconsistencies/validities.

#### Equivalents of Distributivity of Conjunctions and Alternations
* Alternations of a component with an alternation are equivalent to the alternation of the alternations of the component with each of the others.
* Alternations of a component with a conjunction are equivalent to the conjunction of the alternations of the component with each of the others.
* Conjunctions of a component with an alternation are equivalent to the alternation of the conjunctions of the component with each of the others.
* Conjunctions of a component with a conjunction are equivalent to the conjunction of the conjunctions of the component with each of teh others.

#### Equivalents of Development: Alternational and Conjunctional
* Compounds are equivalent to their alternations with nonconsistencies (equivalents of identity), and, in particular, with conjunctions of other compounds with their negation (by the law of contradiction) which are themselves equivalent to the conjunction of their alternations with the other compound and its negation (by distributivity of alternation over conjunction) i.e. conjunctive development of the one compound with respect to the other.
* The dual for conjunction.

#### Equivalents of Associativity
* The conjunction of the first component with the conjunction of the second and third is equivalent to the conjunction of the conjunction of the first and second with the third.
* The alternation of the first component with the alternation of the second and third is equivalent to the alternation of the alternation of the first and second with the third.

#### Iterated Alternations and Conjunctions
The equivalents of associativity yield the many component alternations and conjunctions which are equivalent to iterated nestings of alternations or conjunctions down their left or right components.

#### Equivalents of Commutativity
* The alternation of the left component with the right component is equivalent to the alternation of the right component with the left.
* The conjunction of the left component with the right component is equivalent to the conjunction of the right component with the left.

#### Equivalents of Distributivity of Negations
* Negations of alternations are equivalent to the conjunctions of the negations of their components.
* Negations of conjunctions are equivalent to the alternations of the negations fo their components.

#### Relays, Literals, and Clauses
* Relays are their component or its negation.
* Literals are relays of nonchain compounds.
* Each component of a clausal chain is a literal.

#### Disjoint, Full, and Empty Chains
* No nonchain subcompounds of different components of *disjoint* chains match.
* The nonchain subcomponents of each component of *full* chains are the same.
* Empty chains have no components: often they are replaced by a relevant validity or inconsistency so as to carry an empty method into a nonempty one.

#### Laws of Validity
Chains are nonvalid waow they are not valid and, hence, not where each of their functional substitutions denotes i.e. where some of its functional substitutions do not denote.

Validity is
* inconsistency of negation (which is not consistency of negation, and, hence, not nonvalidity of negation of negation i.e. validity of negation of negation which, by equivalents of identity, is validity)
* nonimplication of negation,
* negational nonimplication
* nonequivalence of negation
* negational nonequivalence, and
* kept by functional substitutions.

#### Laws of Consistency
* Chains are consistent when some of their functional substitutions denote.
* Chains are nonconsistent when each of their functional substitutions does not denote.
* Nonconsistency is validity of negation.
* Nonconsistency implies nonvalidity.

#### Laws of Implication
* Chains are implied by others waow each functional substitution of the one denotes where the same of the other denotes.
* One chain implies an other and the other a third only where the one implies the third.
* Chains imply their self.
* Chains imply validities.
* Validities do not imply nonvalidities.
* Validities do not imply nonconsistencies.
* Validities only imply validities (each functional substitution of the former that denotes is one where the latter denotes)
* Nonconsistencies imply chains.
* Consistencies do not imply nonconsistencies.
* Nonconsistencies only imply nonconsistencies.

#### Laws of Equivalence
* Chains are equivalent waow each of their functional substitutions denotes or not together.
* One chain is equivalent to an other and the other a third only where the one is equivalent to the third.
* Chains are equivalent to their self.
* One chain is equivalent to an other waow the other is equivalent to the one.
* Validities are equivalent to and only equivalent to validities.
* Nonconsistencies are equivalent to and only equivalent to nonconsistencies.

#### Conditionals, Biconditionals, Exclusive Alternations, and Sequents
* Conditionals are alternations of the negation of their (antecedent) left component with their (consequent) right component.
* Biconditionals are conjunctions of the distinct conditionals of their components.
* Sequents are conditionals whose antecedent is the conjunction of their antecdent components and whose consequent is the alternation of their consequent components.
* Exclusive alternations are negations of the biconditionals of their components.

* Implication is validity of conditionals.
* Equivalence is validity of biconditionals.
* Nonequivalence is validity of exclusive alternations.

#### Laws of Equivalent Grammatical Categories
Each clause is equivalent to a disjoint or empty clause.
Each chain is equivalent to
* one without conditionals and biconditionals
* one where only nonchain components are negated
* a conjunction of negations of clausal conjunctions
* the alternational dual of the above
* an alternation of disjoint clausal conjunctions (alternational normal form)
* a conjunction fo disjoint clausal alternations (conjunctive normal form)
* a full alternation of unique disjoint clausal conjunctions
* a full conjunction of unique disjoint clausal alternations

#### Laws of Functional Completeness
Each chain is equivalent to a compound compounded only by
* alternative denial
* joint denial
* negation and conjunction
* negation and alternation
* negation and conditionalization

-----

That's all I have the strength to do for now.


## 2025 0505

### 2025 0505 1725
This continues my work on my little lisp from [2025 0504 0140](#2025-0504-0140).

In the last entry it occurred to me that the only distinction between types of items that is implicit in LISP is that between lists and nonlists (where an example of a nonlist is a symbol).
Consequently, I introduced the following definitions:

```
// pairs as lists
, isList = item => !isEmpty(item) && isEmpty(carOf(item))
, singletonListOf = item => consOf(theEmptyPair, item)
, theEmptyList = singletonListOf(the)
, prependHelpOf = (x,y) => isEmpty(x) ? (isList(y) ? cdrOf(y) : y)
 : consOf(carOf(x), prependHelpOf(cdrOf(x),y))
, prependOf = (x,y) => isList(x) ? prependHelpOf(x,y)
 : prependHelpOf(singletonListOf(x),y)
```

The problem with this is that there is a big difference between a binary tree (as an iterated pairing of the empty pair) as a sequence and as a LISP list.
This difference broke the beauty of the symmetry of definitions that take binary trees as stacks on the one hand and sequences on the other.
While I was reading Feferman's "Finitary Inductively Presented Logics" it occurred to me that by distinguishing between sequences and lists I can further factor the underlying arithmetic of binary trees out from the implementation details of my little lisp.

The empty sequence is identical to the empty pair and a pair is the empty sequence when it is the empty pair.
The left part of a pair is called the head of the sequence.
It is also called the first item of the sequence.
The right part of a pair is called the rest of the sequence.
Sequences are constructed from other sequences by concatenation.

The code which corresponds to this specification is as follows.

```
// basic operations on pairs
let theEmptyPair={}
, isEmpty = it => it == theEmptyPair
, pairOf = (it, that) => [it, that]
, leftOf = it => isEmpty(it) ? it : it[0]
, rightOf = it => isEmpty(it) ? it : it[1]

// sequences as pairs
, theEmptySequence = theEmptyPair
, isEmptySequence = isEmpty
, singletonSequenceOf = it => pairOf(it, theEmptySequence)
, headOf = leftOf
, restOf = rightOf
, concatOf = (it, that) => isEmptySequence(it) ? that 
  : pairOf(headOf(it), concatOf(restOf(it), that)) 
```

A sequence whose head is the empty pair is now a LISP list.
Thus a pair is a LISP list if it is not the empty pair and if its left part is.
Promote a sequence to a list by concatenating the singleton sequence whose sole item is the empty pair with the sequence.
So, the empty list is the list of the empty sequence, and the singleton list of an item is the list of the singleton sequence of that item.
The first item of a list is called its 'car' and the list of the rest of the items in the sequence of the list is called the 'cdr'.
Lists shall be constructed by consing rather than concatenating so that nonlist items are automatically promoted to the appropriate singleton list.

```
// LISP lists as sequences
, isList = it => !isEmpty(it) && isEmpty(leftOf(it))
, listOf = sequence => pairOf(theEmptyPair, sequence)
, theEmptyList = listOf(theEmptySequence)
, isEmptyList = it => isList(it) && isEmptySequence(rightOf(it))
, singletonListOf = it => listOf(singletonSequenceOf(it))
, carOf = list => headOf(rightOf(list))
, cdrOf = list => listOf(restOf(rightOf(list)))
, consOf = (it, that) => 
  !isList(it) ? consOf(singletonListOf(it),that) 
  : isList(that) ? listOf(concatOf(rightOf(it),rightOf(that))
  : consOf(it, singletonListOf(that)))
```
In the past, I built stacks directly from pairs, but now I shall build them from LISP lists since they are needed as part of my little lisp and not as part of the arithmetic of pairs.

The empty stack is the empty list, an item is pushed onto a stack by consing it to the stack, the top of the stack is got with car, and the top is dropped from the stack with cdr.
The second from the top of the stack is the top of the drop of that stack.
Finally, the top two items of the stack are "enconsed" by replacing them with the cons of the second from top with the singleton list of the top: this is used mostly for implementing the reader.

```
// stacks as LISP lists
, theEmptyStack = theEmptyList
, isEmptyStack = isEmptyList
, pushOf = consOf
, dropOf = cdrOf
, topOf = carOf
, secondOf = stack => topOf(dropOf(stack))
, enconsOf = stack => pushOf(dropOf(dropOf(stack))
  , consOf(secondOf(stack), topOf(stack)))
```

Is this a good way of doing things though?
The old way clung to the arithmetic of binary trees as iterated ordered pairs of the empty pair.
This way goes back to the earlier method of distinguishing between runic lists and nonrunic lists, or runic symbols and nonrunic symbols.
The difference is that here, symbols are pairs rather than javascript strings.
This doesn't eliminate the problem of dealing with javascript strings, we still need to carry them over into some sequences of symbols.

Has this accomplished anything or is it like when you make a substitution into an algebra problem and end up with 'zero equals zero'.

### 2025 0505 1709 Bryan Magee's "Men of Ideas" and "The Great Philosophers"
My interests in philosophy are largely historic: the literature of philosophers tells us something about what was going on from its compatibility and incompatibility with cultural practices uncovered by archeology.
Bryan Magee (1930-2019) presented two series of television shows which gave the public a instructive look into the work of philosophers past and present:

1. "Men of Ideas" 1978
2. "The Great Philosophers" 1987

The first is composed of the following speakers and topics:
1. Isaiah Berlin: An Introduction to Philosophy
2. Charles Taylor: Marxist Philosophy
3. Herbert Marcuse: Marcuse and the Frankfurt School
4. William Barrett: Martin Heidegger and Modern Existentialism
5. Anthony Quinton: The Two Philosophies of Wittgenstein
6. A. J. Ayer: Logical Positivism and its Legacy
7. Bernard Williams: The Spell of Linguistic Philosophy
8. R. M. Hare: Moral Philosophy
9. Willard Van Orman Quine: The Ideas of Quine
10. John Searle: The Philosophy of Language
11. Noam Chomsky: The Ideas of Chomsky
12. Hilary Putnam: The Philosophy of Science
13. Ronald Dworkin: Philosophy and Politics
14. Iris Murdoch: Philosophy and Literature
15. Ernest Gellner: Philosophy, The Social Context

The second is composed of the following speakers and philosophers:
1. Myles Burnyeat: Plato
2. Martha Nussbaum: Aristotle
3. Anthony Kenny: Medieval Philosophy
4. Bernard Williams: Descartes
5. Anthony Quinton: Spinoza and Leibniz
6. Michael R. Ayers: Locke and Berkeley
7. John Passmore: Hume
8. Geoffrey Warnock: Kant
9. Peter Singer: Hegel and Marx
10. Frederick Copleston: Schopenhauer
11. J. P. Stern: Nietzsche
12. Hubert Dreyfus: Husserl, Heidegger and Modern Existentialism
13. Sidney Morgenbesser: The American Pragmatists
14. A. J. Ayer: Frege, Russell and Modern Logic
15. John Searle: Wittgenstein

Both series can be easily found on YouTube and are well worth a long watch, especially when the speaker is speaking on their own work e.g. Quine.

## 2025 0504 

### 2025 0504 1517
This begins my read of Peter Adamson's "Classical Philosophy: a history of philosophy without gaps" from 2014.

It was recommended to me by R.P. when he noticed that I was reading some [other histories of philosophy](#2025-0415-1915) in addition to Durant's "Story of Philosophy" which I last wrote about in [2025 0430 2324](#2025-0430-2324).

As the preface explains, this is one in a series of books that was written from a podcast on the history of philosophy.
Unlike the other histories of philosophy that I have on hand, this one purports to exhaust the nooks and crannies of philosophers throughout the world, hence the "without gaps".
More than anything, I look forward to the list of references promised: a good reading list can save a lifetime of woe.

Classical philosophy is very narrow in its scope, and the book is broken into the traditional three parts:

1. Early Greek Philosophy
2. Socrates and Plato
3. Aristotle.

The 'Early Greeks' are sometimes called the "presocratics" as if Socrates was the saviour of philosophy.
Each chapter of the part on early greek philosophy deals with a specific philosopher and, in a few places, with groups of them.
The last two parts deal exclusively with Socrates, Plato, and Aristotle.

Andamson says he links up the philosophers to their social environments and that he doesn't see a clean seperation between the concerns of the philosopher and those of the culture of which they were a part.
His claim that philosophy does not occur without affluence or some kind of social saftey net is common enough to be somewhat beyond question, but I do question it.
I submit that the philosophy which has survived to our day, e.g. as a corpus of literature if I am pressed to give a concrete account of philosophy as a part of our social environment, is a collateral effect of any verbal community and that what has survived is simply that.
Said humorously: the absense of evidence is not evidence of absense.

Philosophy is a consequences of the reinforcing effects of a verbal community which practices a kind of self control i.e. the deferred consequences of the reinforcing practices of the verbal community are taken into account by it.
Any culture which comes to teach its people to care for their own survival and the survival of their culture produces philosophers in that there are those sensitive to the contribution that verbal behavior can make to that survival.

When Socrates passes through Plato it is ethics that reigns supreme as " the study of ideal conduct; the highest knowledge, said Socrates, is the knowledge of good and evil, the knowledge of the wisdom of lifeâ [pg. 3 Durant "The Story of Philosophy"].
Here, ideal conduct is that which does more than avoid the threats and damages which doom the practices of a culture: it is that which contributes most to its survival for whatever reason no matter how consistent or inconsistent it may be with prevailing verbal practices.

The history of philosophy is, to me, the history of a conspicuous kind of cultural self control.
It is no surprise that the sciences descend from philosophy and that philosophy can be, in my measure, best taken as a kind of science.
In the past I have suggested that it is the science of smooth discourse in as much as it eeks out where and when verbal behavior works and where and when it does not.

---

The first chapter is "Everything is Full of Gods: Thales".
The Dorians invaded mainland Greece around 1100 BCE and those Athenians who escaped eastward over the Aegean Sea landed under the leadership of Ion.
Born c. 626/623, Thales grew up in the southern Ionian colony of Miletus.
He is seen by many as the first Greek philosopher.

As when I read Durant's description of Athens in the "Context of Plato" in [2025 0420 2247](#2025-0420-2247), Miletus was a nexus of cultural practices from its eastward and westward neighbors.
Thales may have visited Egypt, picked up some Babylonian astronomy, and used it to predict the solar eclipse of 585 BCE.
Reports on astronomical phenomena give us ways of calculating the ordering of various events in addition to the timing of the report itself (assuming such reports are not manufactured post hoc).

As with my conviction that the stories of Socrates are to be taken as I take most of history, that is as folklore, perhaps even that stronger folklore called just 'lore', so are the stories of Thales taken by me.
This outlook is also echoed in the next few pages of Adamson's sketch.

It is dawning on me that for many of these philosophers there are simply a list of basic stories that we can list and cling to without having to commit ourselves to their historic accuracy.
But, my interest in world history (and the role of archeology in setting up and taking down the histories we have inherited) prevents me from resting easy with such foggy commitments.
The urgency of writing on world history continues to press itself upon me.

The main stories of Thales are these
1. born c. 626/623 and died c. 548/545 BCE
2. he predicted a solar eclipse in 595 BCE
3. he may have visited Egypt
4. he was one of the Seven Sages of early Greece
5. his lobby for the unification of Ionian communities against eastern threats, failed
6. he foresaw a large olive harvest and cornered the olive press market
7. Plato says he fell into a hole because he was looking at the sky
8. he broke an unpassable river into two passable rivers
9. he wrote a book on navigation at sea
10. he thought water was of great consequence (in ways that are lost to history). Aristotle says Thales 
    * "beleived the world floats upon water, like a piece of wood"[pg.7 Adamson "Classical Philosophy"]
    * "thought that water was a cosmic *principle*"[pg.7]
    * claimed magnets and amber have a soul
    * "all things are full of gods"

Adamson contrasts Thales with 

1. Homer: *Iliad* and *Odysse* and
2. Hesoid: *Theogony*

as we do scientific behavior with literary behavior.

The chapter ends with a tiny argument assembled by Adamson as an example of a possible argument Thales may have made:

> "everything is full of gods, and I'll show you this using the example of the magnet. It seems to be lifeless, but it must have a soul, because it can initiate motion. So, by extension, you should at least be open to the idea that everything has a soul, which is divine."[pg. 7-8]

He then admits the evidence for this argument is weaker than the argument is.

### 2025 0504 0140
This continues my work on my little lisp from [2025 0502 2048](#2025-0502-2048).

An alternate design decision occurred to me: the only implicit distinction betwen teh part played by ordered pairs in a lisp is that between list and nonlist.
Thus, rather than checking if a pair is a symbol or not, it is more general to check if it is a list or not.
Then the rest of the distinctions can be made however the programmer desires.

This also has the benefit of, in the simple case where an item is either a list or a symbol, of identifying a symbol by its first rune which can never be empty because of the alphabetization adopted.
This also allows space to serve its traditional function without adding greatly to the complexity of the definition of the reader.
It also allows for a more uniform presentation of pairs as lists followed by pairs as stacks.

```
// basic operations on pairs
let theEmptyPair={}
, isEmpty = x => x == theEmptyPair
, consOf = (x,y) => [x,y]
, carOf = x => isEmpty(x) ? x : x[0]
, cdrOf = x => isEmpty(x) ? x : x[1]

// left and right singletons
, enlistOf = x => consOf(x,theEmptyPair)
, enstackOf = x => consOf(theEmptyPair,x)

// pairs as lists
, isList = item => !isEmpty(item) && isEmpty(carOf(item))
, singletonListOf = item => consOf(theEmptyPair, item)
, theEmptyList = singletonListOf(the)
, prependHelpOf = (x,y) => isEmpty(x) ? (isList(y) ? cdrOf(y) : y)
 : consOf(carOf(x), prependHelpOf(cdrOf(x),y))
, prependOf = (x,y) => isList(x) ? prependHelpOf(x,y)
 : prependHelpOf(singletonListOf(x),y)

// pairs as stacks
, theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, pushOf = consOf
, popOf = carOf
, topOf = cdrOf
, secondOf = stack => topOf(popOf(stack))
, drop2 = stack => popOf(popOf(stack)) 
, enpendOf = stack => 
   pushOf(drop2(stack)
   ,prependOf(secondOf(stack)
    ,enlistOf(topOf(stack))))

// reader
, isOpenParen = isEmpty
, isCloseParen = x => !isEmpty(x)&&isEmpty(carOf(x))&&isEmpty(cdrOf(x))
, isSpace = x => !isEmpty(x)&&isEmpty(carOf(x))&&isCloseParen(cdrOf(x))
, readHelperOf = (stack,runes) => isEmpty(runes) ? carOf(topOf(stack))
  : isSpace(carOf(runes)) ?
   ( isEmpty(topOf(stack)) ? readHelperOf(stack,cdrOf(runes)) 
     : readHelperOf(pushOf(enpendOf(stack),theEmptyPair), cdrOf(runes)))
  : isCloseParen(carOf(runes)) ? readHelperOf(enpendOf(stack), cdrOf(runes))
  : isOpenParen(carOf(runes)) ? readHelperOf(pushOf(stack,theEmptyList), cdrOf(runes))
  : readHelperOf(enpendOf(pushOf(stack,carOf(runes))), cdrOf(runes))
, readOf = runes => readHelperOf(theEmptyStack, runes)
```

I'll test this code and figure out the printer next time.

### 2025 0504 0139
Failure of cross reference does not require a referent.

## 2025 0503

### 2025 0503 1757
Today is a Bob Ross and Bosch day.
While I do not seem to enjoy reading Michael Connelly, I do enjoy watching his stories on screen.
They are that perfect drip of mid.
Law & Order is another.

### 2025 0503 1750
It is better to have gone out of the way to note the source of a quote than to go out of your way to find where you sourced it.

### 2025 0503 1531
This continues the work on [my paper on logic](#2025-0412-1422).
It also contains more on my philosophy of logic as well as a careful analysis of Quine's proof that no logical theory of the world is beyond supplementation (which itself contains an alternate presentation of Quine's analysis of denotation as a generalization of Tarski's analysis of truth).

My methods of logic are unique in that they present Quine's predicate functor methods independently of quantificational logic.
Quantificational logic is then constructed from predicate functor logic by way of Quine's predicate abstracts.
The difference between predicate functor logic and quantificational logic is the difference between 'is true' and 'is true of'.

A word on the words 'denote' and 'predicate' from Quine's "From Stimulus to Science":
>"For what follows we must come squarely to terms with 'denote'. Since it is often used interchangeably with 'designate', and a singular term normally designates one and only one object, readers are apt to think of denotation as relating a predicate likewise to a single object, namely the class of all thoe things it is true of, or a property shared by them. In my use of 'denote', as in John Stuart Mill's [Mill, ChapterII $5], a predicate denotes rather each separate thing of which it is true. The class or property is not involved.
>
>For years, to obviate confusion, I avoided 'denote' altogether in favor of 'true of'; but that evasion woudl be impracticable in these pages where denotation is becoming the center of action. Unlike Mill, I still withhold the word from singular terms; they are well served by 'designation'.
>
>A word of caution is in order regarding 'predicate' too. Some logicians take a predicate as a *way* of building a sentence aroudn a singular term or, more concretely, as what Peirce called a *rheme* [Volume 2, paragraph 95], a sentence with blanks in it, these being distinctively marked in the case of a many-place predicate. This version covers, implicitly, the potential output of predicate abstraction or predicate functors. But a predicate in my sense is always an integral word, phrase, or claus, grammatically a noun, adjective, or verb. Some are generated from others by grammatical constructions, notably the relative clause or, formally, predicate abstraction and predicate functors."[pg. 60-61 Quine "From Stimulus to Science"]

One of the virtues of this method is that less is spoken of than ever before.
This has the benefit of eliminating explanations that are ultimately offshoots of predicate logic itself.
In the past, and perhaps even in these passages, there are parts of the world which intruded upon logic and muddied its methods.
The most famous of which is mathematics.
There is much to be recommended from mathematical methods in matheamtical logic, but there is less to say about its ultimate contributions to logic itself.
For example, formalism continues to confuse those unable to seperate logic from mathematical logic as if the formalisms were the subject of logic itself.
One wonders by what methods such formalists make conclusions about their formalisms.

Logic no more has formalisms as its subject matter than it has anything in particular as its items of interest.
Some, like Frege, have mistaken the schematics which fascilitate the articulation of logical principles and practices as part of logic itself rather than as a technical tool for fascilitating an exacting analysis of the same.

> Specifically, Frege mistakes the mathematical metaphor of functions for functional predicates (and he never actually deals with the functional predicates relevant to his analysis of logic e.g. the predicate '{xy: some item is {uv: x is the truth value of u, y is the truth value of v, and u negates v}}' is confused with its purported extension).

Other times, like Frege, logic was so far removed from schematic methods that it bled into thoughts, judgements, senses, meanings, and other possessions of the minds which haunt human bodies and perhaps even that amazing organ we call the brain.

I submit that all these mistakes are not mistakes: they are each a piece in a larger puzzle which has yet to be pictured as a complete whole.
That practice which has done the most to put together all these pieces as if they fit into a larger whole is science.
It is only recently that the science of philosophy was so far divorced from the other sciences.
The philosophical revolt against psychology as a science of mind is justified in that it was easy to critique and its conclusions were in stark contrast with those of our stronger sciences.
Cognative scientists continued to pluck what they could from philosophy and any failures they were confronted with were attributed to weaknesses not in their own science but in some philosophical or logical method: the philosophers were punished for the past transgressions against psychology as the science of mind.

The science of behavior and the philosophy of radical behaviorism mends many such mistakes by revealing not only a sharper picture of what science has assembled but also an inclusive explanation of how such mistakes were so easy to make:

>"Mentalism kept attention away from the external antecedent events which might have explained behavior, by seeming to supply an alternative explanation. Methodological behaviorism did just the reverse: by dealing exclusively with external antecedent events it turned attention away from self-observation and self-knowledge. Radical behaviorism restores some kind of balance. It does not insist upon truth by agreement and can therefore consider events taking place in the private world within the skin. It does not call these events unobservable, and it does not dismiss them as subjective. It simply questions the nature of the object observed and the reliability of those observations.
>
>The position can be stated as follows: what is felt or introspectively observed is not some nonphysical world of consciousness, mind, or mental life but the observer's own body. This does not mean, as I shall show later, that introspection is a kind of physiological research, nor does it mean (and this is the heart of the argument) that what are felt or introspectively observed are the causes of behavior. An organism behaves as it does because of its current structure, but most of this is out of reach of introspection. At the moment we must content ourselves, as the methodological behaviorist insists, with a person's genetic and environmental histories. What are introspectively observed are certain collateral products of those histories."[pg. 18-19 Skinner "About Behaviorism"]

Under this rubric truth quickly distinguishes itself from validity:

>"*Truth*. The truth of a statement of fact is limited by the sources of the behavior of the speaker, the control exerted by the current setting, the effects of similar settings in the past, the effects upon the listener leading to precision or to exaggeration or falsification, adn so on. There is no way in which a verbal description of a setting can be absolutely true. A scientific law is derived from possibly many episodes of this sort, but it is similarly limitied by the repertoires of the scientists involved, The verbal community of the scientist maintains special sanctions in an effort to guarentee validity adn objectivity, but again, there can be no absolute. No deduction from a rule or law can therefore be absolutely true. Absolute truth can be found, if at all, only in rules derived from rules and here it is mere tautology."[pg.150 Skinner "About Behaviorism"]

The word "tautology" descends through Wittgenstein's failed attempt to reduced quantificational logic to a kind of never ending truth-functional logic (sometimes unhappily called "sentence logic").
The popularity of the word is unfortunate because it reinforces defective distinctions between clear cut definitions of validity which are very rarely mentioned and yet when mastered frequently simplify the methods of logic.
They are described by Quine as follows

>"It was remarked at the end of Chapter 7 that validity may be ascribed not only to truth-functional schemata but also, by extension, to the sentences whose forms those schemata depict; but that it is well then to add the qualifier 'truth-functional'. COrrespondingly a sentence obtainable by substitution in a valid quantificational schema is *quantificationally* valid. Such a sentence is true, or true for all values of its free variables. But it may or may not be truth-functionally valid; its truth may depend solely on its truth functional structure, or it may depend partially on how the quantifiers are arranged.
>
>We may also note an intermediate grade, monadic validity. A sentence is quantificationally valid if it can be got by substitution in a valid quantificational schema; it is monadically valid, more particularly, if it can be got by substitution in a valid quantificational schema which is monadic; and it is truth functionally valid if it can be got by substitution in a valid truth-functional schema." [pg. 176 Quine "Methods of Logic 4th Edition"].

A warning for those who are tempted, like Frege or Russell, to occasionally take schemata as the items of logic: all this talk of substition into valid schema to obtain valid sentences can be skipped by resorting directly to sentences and their grammatical structure.
The grammatical structure with which validity is concerned is specifically designed to avoid the miscarriage of conclusions.

> "Sentences have the same grammatical structure when they are interconvertible by lexical substitutions. Our definition of logical truth, then, can also be put thus: *a logical truth is a truth that cannot be turned false by substituting for lexicon. When for its lexical elements we substitute any other strings belonging to the same grammatical categories."[pg. 58 Quine "Philosophy of Logic Second Edition"]

There are two things of note here:

1. what Quine calls 'logicl truth', and what others have called the same, is what is called 'quantificationally valid' in Quine's "Methods of Logic 4th edition"; and 
2. substitution no longer leads from schema to sentences but from sentences to sentences by way of the lexicon of the langauge.

Whatever promise there is to this grammatical definition of quantificaitonal validity is lost when the grammatical part of variables is reflected upon as Quine does from having read Gilbert Harman's 1971 review in *Metaphilosophy*.
Quine's corrective is to contemplate only substitution for predicates and not for variables which are otherwise part of the lexicon of logical languages (Harman suggests taking variables as belonging to atomic grammatical elements which are more like logical connectives than items of the langauges lexicon).

Such restricted substitution no longer leads from one sentence to an other of the same grammatical structure: they may differ in the variables which they contain and no substitution for predicates alone can change one variable into an other.
There are further troubles that, when confronted, lead to a stronger definition of quantificational validity by grammatical method:

> "Our definition of logical truth in terms of substitution of lexicaon confronts also another and graver difficulty: it appeals only to substitution of predicates for predicates, as against substitution of sentences for simple sentences. We saw early in this chapter how the sentential approach coudl issue in a stronger, narrower concept of logical truth, screening out some cases that woudl have slipped through if only predicate substitution had been called for. IT can even be shown that the version in terms of predicate substitution will inevitably be too weak, as long as our stock of predicates is finite [This was pointed out to me by Mark L. Wilson]. The natural remedy, then is to exploit the indefiniteness of our category of predicates: to admit substitution not only of the predicates in some imagined list, but also of any predicates that might be added. Thus adjusted, then, the abstract version rusn as follows: *a logical truth is a sentence that cannot be turned false by substitution for lexicon, even under supplementation of lexical resources*."[pg. 59 Quine "Philosophy of Logic 2nd Edition"]

Here then is where predicate functor logic reigns supreme and reveals the intimate relationship between validity (or what Quine and so many others speak of as "logical truth"), grammar, and truth.
Since the only items of the lexicon of a predicate functor logic are predicates then there is no question of how substitution is to covey one sentence to an other with the same grammatical structure.
It is also clear that supplementation of lexicon is required from contemplation of a lexicon of only one place predicates as Quine did in "Word and Object":

> "But may we not still aspire to the discovery of some fundamental lexicon on the basis of which all traits and states of everything could in principle be formulated? No; we can prove that opennes is unavoidable, as long anyway as the predicates of a theory are included as items of the theory.
> 
>For, let S1, S2, ... be the sentences in the notation of a theory that have 'x' as their sole free variable. Each of them is, for each object of the universe as value of 'x', true or false of that object; hence each of them, being also an object of the universe, is true or false of itself. We easily show that no general term definiable in the theory is true of exactly those of S1, S2, ... that are false of themselves. (For, if 'F' were such a term, then 'Fx' would be true of itself if and only if false of itself. [This argument is in principle Cantor's. The form I have given it is reminiscent also fo Grelling's paradox, and the use made of it is reminiscent of Tarski.]) Such a term can be added, irreducibly supplementing the theory."[pg. 231-232 Quine "Word and Object"]

Note, I have changed a few words in the above quote to more closely bring what Quine said to bear on terminology used here.
It falls well within Quine's principles of paraphrasing.

This argument puzzled me at first, and as a result of the following analysis I was able to produce an even more surprising argument that directly reveals the immediate limitations on any consistent theory of denotation.
It also brings together the principles of prediate abstraction, predicate concretion, disquotation and (to my surprise) the deduction theorem while also just giving the exact predicate that is missing.

Under the assumption that among the items of the theory are the predicates themselves it is possible, following Quine's generalization of Tarski's analysis of truth to denotation (which I shall state), to define a predicate which, in anticipation of the definition to follow, shall be written out as 'degree 0 denotes'.
It is defined just as Quine specifies on pg.63 of "From Stimulus to Science" but with predicate functors as predicate abstracts in a quantificational logic as described in [An Incomplete Sketch of My Philosophy of Logic](#2025-0422-2322-an-incomplete-sketch-of-my-philosophy-of-logic)):

1. Where 'F' designates an n place primitive predicate define "{..x : 'F' degree 0 denotes ..x}" for 'F',
2. Where 'F' and 'G' designate n place predicates define
   * "{..x : 'not F' degree 0 denotes ..x}" for "{..x : not ('F' degree 0 denotes ..x)}"
   * "{..x : 'F and G' degree 0 denotes ..x}" for "{..x : F degree 0 denotes ..x} and {..x : G degree 0 denotes ..x}"
   * "{..y : 'some F' degree 0 denotes ..y}" for "{..y : some item is {z : 'F' degree 0 denotes ..yz}"
   * "{..xy : 'pad F' degree 0 denotes ..xy}" for "{..xy: 'F' degree 0 denotes ..x}"
   * "{..uv : 'refl F' degree 0 denotes ..uv}" for "{..uv: 'F' degree 0 denotes .uvv}"
   * "{a..cb: 'perm F' degree 0 denotes a..cb}" for "{a..cb: 'F' degree 0 denotes ab..c}".

> A detailed explanation of the construction of predicates which permit the manipulation of quotations as above can be found in the last chapter of Quine's "Mathematical Logic" on his theory of protosyntax.
> Feferman's Finitary Inductively Presented Logics are perhaps a simpler alternative but the presentation does not address this construction in particular.

Now, a predicate coextensive with 'degree 0 denotes' as defined above (rather than taking it as an abbreviation as has been done with the phrase 'for' these are changed to sentences of coextension) can not belong to such a theory i.e. such a predicate must be added as a primtive to "irreducibly supplement the theory" (the case where it is already among the primitives is covered after this).
For, assume it does, then so does the predicate '{x: not (x degree 0 denotes x)}'.

Suppose, '{x: not (x degree 0 denotes x)}' degree 0 denotes  '{x: not (x degree 0 denotes x)}'.
By disquotation (which is the defining principle given above), {x: not (x degree 0 denotes x)}'{x: not (x degree 0 denotes x)}'.
By concretion, it is not the case that '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}'.
Hence, by the deduction theorem, '{x: not (x degree 0 denotes x)}' degree 0 denotes  '{x: not (x degree 0 denotes x)}' only if it is not the case that '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}'.

Now, suppose it is not the case that '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}'.
By disquotation, it is not the case that {x: not (x degree 0 denotes x}'{x: not (x degree 0 denotes x)}'.
By concretion, it is not the case that it is not the case that (yes, the same phrase is repeated twice there) '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}' i.e. '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}'.
Hence, by the deduction theorem, it is not the case that '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}' only if '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}' i.e. '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}' if it is not the case that '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}'.

Therefore, '{x: not (x degree 0 denotes x)}' degree 0 denotes  '{x: not (x degree 0 denotes x)}' if and only if it is not the case that '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}', a contradiction.

Though that mays seem like a mouthfull of nit picking steps, it gives each principle that, step by step, permits the given contradiction.

The above contradiction followed by assuming that 'degree 0 denotes' was not a part of the lexicon of the theory but was coextensive with some compound constructed from the lexicon.
What's interesting is that there is nothing wrong with supposing 'degree 0 denotes' is a basic predicate, now added to the theory, which is defined as above (that is each instance of the corresponding coextensive schema) with respect to each OTHER basic predicate except for itself!
Read that very carefully because it is very slipery.

So, in that case, the case where you have 'degree 0 denotes' as stipulated, we are left again where Quine's argument starts, but now, we contemplate a new predicate 'degree 1 denotes' which is defined from the basic predicates, one of which is 'degree 0 denotes', and which, by the same argument, can not be coextensive with any compound of the basic predicates, one of which is 'degree 0 denotes'.
Thus, 'degree 1 denotes' can be added as a basic predicate defined from the other basic predicates by the inductive definition above.

Now, for any theory, there is a finite lexicon, and thus there is some smallest 'n' for which 'degree n denotes' does not belong to the lexicon i.e. there is no end to how a theory might be supplemented.

Quine's argument involving one place predicate glosses over the problem of denotation, as Quine frequently did, with the phrase "is true of", but it is precisely on the analysis of this predicate that Quine's argument (and Frege's work, for 'is true' is a degenerate case of 'is true of' i.e. 'denotes') hinges.

When I started this not I had an aim of typing up the rest of my sketch of my paper on logic i.e. the concrete consequences of arguments like those above.
That did not happen because now I'm going to switch to work on my little lisp before I forget what occurred to me with respect to it earlier today.


## 2025 0502

### 2025 0502 2048
This continues my work on my little lisp from [2025 0501 1754](#2025-0501-1754).

I made great progress in the last entry and between the end of the last note on this project and now I was able to solve the problem of going back and forth between lisp and forth.
I wish there was an easy way to write while sleepy or tired or whatever happened last night while I failed to put together the pieces as in the simple example that occurred to me while I was watching a show called "The Sticky".
It is common for me to work until exhaustion.
Sadly, the exhaustion which comes from reading, writing, and thinking does not look like it does when it is from lifting.
You are not a sweaty mess nor are your muscles fatigued in that way that makes it hard to walk up or down steps or get out of a chair (which, even writing about them brings that feeling too me).

My lingering on this problem of exhaustion is important: it is probably the biggest problem to solve when you must solve so many problems before you die.
There is no clear indication that I shall do all that I must before I die and the bottleneck to most of what I must do is largely a matter of dealing with the pains of weak thinking.
Perhaps the biggest part of this big problem is encapsulated in one of my most important principles: "Stop at the sign."

It can be hard to notice when thinking has gone weak.
This seems to be the result of prescientific practices around thinking, reading, and writing generally.
Rather than seeing a schedule of reinforcement that maintains the behaviors that have led to the most profound of scientific and logically discoveries, prescientific peoples see the individual as a willing agent or some other such mentalistic haunties of the brain by the mind.
But, if I waste more time now solving this problem then I will not get out the solutions to problems that I have already solved.
It is difficult to say both that "here we have the biggest problem and the biggest bottleneck to solving problems" and then "but now I will leave that to work on lesser problems."
It is precisely in being able to say both these things that schedules of reinforcement resolve what appears to be a contradiction.

Here is an exact replica of what occurred to me while watching "The Sticky" earlier today and which leads to the radical simplification of any reader for any lisp like language while also explaining exactly how it is that lisp is a forth:

```
((a)(bc))
0 | (a)(bc))
0,0 | a)(bc))
0,(a,0) | )(bc))
((a,0),0) | (bc))
((a,0),0),0 | bc))
((a,0),0),(b,0) | c))
((a,0),0),(b,(c,0)) | ))
((a,0),((b,(c,0)),0)) | )
(((a,0),((b,(c,0)),0)),0) |
```

Believe it or not, this shows that my sleepy claim that a simple alphabetization radically simplifies the reader of any language that purports to be some sort of parenthetical or tree like form, but which can be immediately interpreted as a string of stack operations.
This exactly generalizes the methods from [Bit Strings and Binary Trees](#2025-0413-1513-bit-strings-and-binary-trees).

Recall that the reader was previously composed of steps involving lexing and parsing: this new method goes straight from a list of runes to the list they purport to designate.
All of that complexity is eliminated by a change alphabatization (I've gone ahead and reproduced all the code for the current state of this project).

Ooops!
Once I opened the tab to bring the code over here I started coding and totally forgot about writing out what I was doing here.

Here's where I'm starting from:
```
let run=code=>console.log(code,'\n',eval(code)) // for examples

// basic operations on pairs
let run=code=>console.log(code,'\n',eval(code)) // for examples

// basic operations on pairs
let theEmptyPair={}
, isEmpty = x => x == theEmptyPair
, consOf = (x,y) => [x,y]
, carOf = x => isEmpty(x) ? x : x[0]
, cdrOf = x => isEmpty(x) ? x : x[1]

// left and right singletons
, singletonListOf = x => consOf(x,theEmptyPair)
, singletonStackOf = x => consOf(theEmptyPair,x)

// to and from tallies
, tallyOf = n => 
  n>0 ? singletonStackOf(tallyOf(n-1)) : theEmptyPair
, countOf = x => isEmpty(x) ? 0 : 1 + countOf(cdrOf(x))

// the alphabet
, abc =[...'()0123456789abcdefghijklmnopqrstuvwxyz ']
, alphabeticalIndexOf = letter => abc.indexOf(letter)
, alphabeticalLetterOf = index => abc[index]

// to and from rune
, runeOf = letter => tallyOf(alphabeticalIndexOf(letter))
, letterOf = x => alphabeticalLetterOf(countOf(x))

// basic string operations
, theEmptyLetter=''
, isEmptyLetter = x => x == theEmptyLetter
, concatenationOf = (...strings) => 
   strings.length ? strings.shift() + concatenationOf(...strings) : theEmptyLetter
, firstLetterOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters[0]
, restLettersOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters.slice(1)

// to and from runes
, runesOf = letters => 
   letters.length ? consOf(runeOf(firstLetterOf(letters))
    , runesOf(restLettersOf(letters)))
   : theEmptyPair
, lettersOf = x =>
   isEmpty(x) ? theEmptyLetter
   : concatenationOf(letterOf(carOf(x)),lettersOf(cdrOf(x)))

// prepending pairs as lists
, prependedListOf = (x,y) =>
  isEmpty(x) ? y : consOf(carOf(x), prependedListOf(cdrOf(x),y))

// pairs as stacks
, theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, pushOf = consOf
, popOf = carOf
, topOf = cdrOf
, secondOf = stack => topOf(popOf(stack))
, drop2 = stack => popOf(popOf(stack)) 
, enpendOf = stack => 
   pushOf(drop2(stack)
   ,prependedListOf(secondOf(stack)
    ,singletonListOf(topOf(stack))))

// recursive definition of identity
, id = (x,y) =>
  (isEmpty(x) && isEmpty(y))
  || (!(isEmpty(x) || isEmpty(y))
     && id(carOf(x),carOf(y))
     && id(cdrOf(x),cdrOf(y)))

// reader
, isOpenParen = x => id(runeOf('('),x)
, isCloseParen = x => id(runeOf(')'),x)
, isSpace = x => id(runeOf(' '),x)
, readHelperOf = (stack,runes) =>
  isEmpty(runes) ? carOf(topOf(stack))
  : isCloseParen(carOf(runes)) ? readHelperOf(enpendOf(stack), cdrOf(runes))
  : isOpenParen(carOf(runes)) ? readHelperOf(pushOf(stack,theEmptyPair), cdrOf(runes))
  : isSpace(carOf(runes)) ? readHelperOf(stack,cdrOf(runes))
  : readHelperOf(enpendOf(pushOf(stack,carOf(runes))), cdrOf(runes))
, readOf = letters => readHelperOf(theEmptyStack, runesOf(letters))

// printer
, parentheticalOf = x => isEmpty(x) ? runesOf('()') : prependedListOf(runesOf('('),prependedListOf(x,runesOf(')')))
, printListOf = list =>
 isEmpty(list) ? theEmptyPair
 : prependedListOf( printHelperOf(carOf(list)), printListOf(cdrOf(list)))
, printHelperOf = item =>
 isEmpty(item) ? runesOf('()')
 : isEmpty(carOf(item))? parentheticalOf(prependedListOf(runesOf('()'),cdrOf(item)))
 : parentheticalOf(printListOf(item))
, printOf = item => lettersOf(printHelperOf(item));
```

Everything is simpler than ever, but it is no longer clear that the specific alphabetization is as important as I thought, but there is still some opportunity.

I did so much that it is hard to go back and write through it.
Since there is still more that I must do I'm going to just start writing from here and hopefully get back to where I started.

It looks like the reader and the printer are still more complex than they need to be and that they can be simplified if I can figure out the appropriate stack operations.
As it stands the reader interprets each rune as a stack operation:

* if the first rune is an open parenthesis it pushes an empty pair onto the stack and reads the rest of the runes with that stack
* if the first rune is a close parenthesis it "enpends" the top two items on the stack and then goes on
* if the first rune is a space then it does nothing and goes on
* otherwise it pushes the first rune onto the stack and "enpends" it.

Enpending is something that happens often but which doesn't often get its own name: it's when you enlist an item, i.e. put it into a singleton list, and then append it to some other list of items.
There are distinctions for operations like this in the J programming language, but that is for another time.

Well this is one heck of a fragmentary note.

Surprisingly, I was able to entirely defer the introduction of the identity function and finally push off all the work that goes into external reading and printing so that everything comes together quickly.
Here it does end up being helpful to have a specific alphabetization: it is key to the simplicity of the internal reader and printer.
This is all quite exciting and there is so much more to come.
```
// basic operations on pairs
let theEmptyPair={}
, isEmpty = x => x == theEmptyPair
, consOf = (x,y) => [x,y]
, carOf = x => isEmpty(x) ? x : x[0]
, cdrOf = x => isEmpty(x) ? x : x[1]

// left and right singletons
, singletonListOf = x => consOf(x,theEmptyPair)
, singletonStackOf = x => consOf(theEmptyPair,x)

// prepending pairs as lists
, prependOf = (x,y) =>
  isEmpty(x) ? y : consOf(carOf(x), prependOf(cdrOf(x),y))

// pairs as stacks
, theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, pushOf = consOf
, popOf = carOf
, topOf = cdrOf
, secondOf = stack => topOf(popOf(stack))
, drop2 = stack => popOf(popOf(stack)) 
, enpendOf = stack => 
   pushOf(drop2(stack)
   ,prependOf(secondOf(stack)
    ,singletonListOf(topOf(stack))))

// reader
, isOpenParen = isEmpty
, isCloseParen = x => !isEmpty(x)&&isEmpty(carOf(x))&&isEmpty(cdrOf(x))
, isSpace = x => !isEmpty(x)&&isEmpty(carOf(x))&&isCloseParen(cdrOf(x))
, readHelperOf = (stack,runes) => isEmpty(runes) ? carOf(topOf(stack))
  : isSpace(carOf(runes)) ? readHelperOf(stack,cdrOf(runes))
  : isCloseParen(carOf(runes)) ? readHelperOf(enpendOf(stack), cdrOf(runes))
  : isOpenParen(carOf(runes)) ? readHelperOf(pushOf(stack,theEmptyPair), cdrOf(runes))
  : readHelperOf(enpendOf(pushOf(stack,carOf(runes))), cdrOf(runes))
, readOf = runes => readHelperOf(theEmptyStack, runes)

// printer
, theOpenParen = theEmptyPair
, theCloseParen = consOf(theEmptyPair,theEmptyPair)
, isSymbol = item => !isEmpty(item)&&isEmpty(carOf(item))
, theSymbolPair = theEmptyPair
, parenOf = runes => consOf(theOpenParen, prependOf(runes, singletonListOf(theCloseParen)))
, printListOf = list => isEmpty(list) ? theEmptyPair
 : prependOf(printOf(carOf(list)),printListOf(cdrOf(list)))
, printOf = item => isSymbol(item)? parenOf(prependOf(printOf(theSymbolPair),cdrOf(item)))
 : parenOf(printListOf(item))

// the alphabet
, abc =[...'() 0123456789abcdefghijklmnopqrstuvwxyz']
, alphabeticalIndexOf = letter => abc.indexOf(letter)
, alphabeticalLetterOf = index => abc[index]

// to and from tallies
, tallyOf = n => 
  n>0 ? singletonStackOf(tallyOf(n-1)) : theEmptyPair
, countOf = x => isEmpty(x) ? 0 : 1 + countOf(cdrOf(x))

// to and from rune
, runeOf = letter => tallyOf(alphabeticalIndexOf(letter))
, letterOf = x => alphabeticalLetterOf(countOf(x))

// basic javascript string operations
, theEmptyLetter=''
, isEmptyLetter = x => x == theEmptyLetter
, concatenationOf = (...strings) => 
   strings.length ? strings.shift() + concatenationOf(...strings) : theEmptyLetter
, firstLetterOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters[0]
, restLettersOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters.slice(1)

// to and from runes
, runesOf = letters => 
   letters.length ? consOf(runeOf(firstLetterOf(letters))
    , runesOf(restLettersOf(letters)))
   : theEmptyPair
, lettersOf = x =>
   isEmpty(x) ? theEmptyLetter
   : concatenationOf(letterOf(carOf(x)), lettersOf(cdrOf(x)))

// external read and print
, read = letters => readOf(runesOf(letters))
, print = item => lettersOf(printOf(item))

// recursive definition of identity
, id = (x,y) =>
  (isEmpty(x) && isEmpty(y))
  || (!(isEmpty(x) || isEmpty(y))
     && id(carOf(x),carOf(y))
     && id(cdrOf(x),cdrOf(y)));
```


## 2025 0501

### 2025 0501 2151
For reasons that escape me I've had a few different youtube playlists queued up in the background the past few days.
The first, and perhaps the most important is Groucho Marx's "You Bet Your Life".
There is nothing like Groucho.
He is certinaly one of my idols in that I mention him as often to those near me as I do Skinner and Quine to everyone else.

* [You Bet Your Life: Complete Episodes by Air Date](https://youtube.com/playlist?list=PLHaioNpr_GDbvsTj_taM-jO6C1658N1PC&si=gvhBN2wJP49ioY4W)

I don't know how accurate the title is.
Next, of all things, Marathon lore:

* [Marathon - Reviews, History & Lore (Bungie)](https://youtube.com/playlist?list=PLT1yGPRy2oPrXlQ730iOZ32eMI_-FhXS3&si=Tv62twCdgZVX8Mi4)

Last, and most certainly least, a playthrough of an old game I recall from my childhood:

* [Let's Play Lands of Lore Guardians of Destiny](https://youtube.com/playlist?list=PL2sIyFzFMWstwPV2XKG_uwoNolVaOSX0s&si=9pRtEwUv2As8rzuO)

Such things are part of modern folklore, though they very often lack the warmth that I associate with well worn folklore.
Perhaps such stories from games are nothing more than the folklore of *my* life.

Sometimes I need silence in order to solve problems.
Other times I need there to be something going on in the background.
I need to at least see that someone else at some other time was making progress on something they set out to do even if I might not be at this exact moment.

### 2025 0501 1754
This continues the work on my little lisp from [2025 0426 1851](#2025-0426-1851-purifying-my-little-lisp).

The basic definitions are now

```
let run=code=>console.log(code,'\n',eval(code)) // for examples
// basic operations on pairs
let theEmptyPair={}
, isEmpty = x => x == theEmptyPair
, consOf = (x,y) => [x,y]
, carOf = x => isEmpty(x) ? x : x[0]
, cdrOf = x => isEmpty(x) ? x : x[1]
// recursive definition of identity
, id = (x,y) =>
  (isEmpty(x) && isEmpty(y))
  || (!(isEmpty(x) || isEmpty(y))
     && id(carOf(x),carOf(y))
     && id(cdrOf(x),cdrOf(y)))
// left and right singletons
, singletonListOf = x => consOf(x,theEmptyPair)
, singletonStackOf = x => consOf(theEmptyPair,x)
// to and from tallies
, tallyOf = n => 
  n>0 ? singletonStackOf(tallyOf(n-1)) : theEmptyPair
, countOf = x => isEmpty(x) ? 0 : 1 + countOf(cdrOf(x))
// the alphabet
, abc =[...'0123456789abcdefghijklmnopqrstuvwxyz ()']
, alphabeticalIndexOf = letter => abc.indexOf(letter)
, alphabeticalLetterOf = index => abc[index]
// basic string operations
, theEmptyLetter=''
, isEmptyLetter = x => x == theEmptyLetter
, concatenationOf = (...strings) => 
   strings.length ? strings.shift() + concatenationOf(...strings) : theEmptyLetter
, firstLetterOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters[0]
, restLettersOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters.slice(1)
// to and from rune
, runeOf = letter => tallyOf(alphabeticalIndexOf(letter))
, letterOf = x => alphabeticalLetterOf(countOf(x))
// to and from runes
, runesOf = letters => 
   letters.length ? consOf(runeOf(firstLetterOf(letters))
    , runesOf(restLettersOf(letters)))
   : theEmptyPair
, lettersOf = x =>
   isEmpty(x) ? theEmptyLetter
   : concatenationOf(letterOf(carOf(x)),lettersOf(cdrOf(x)))
// prepending and appending pairs as lists
, prependedListOf = (x,y) =>
  isEmpty(x) ? y : consOf(carOf(x), prependedListOf(cdrOf(x),y))
, appendedListOf = (x,y) => prependedListOf(y,x);
```

Symbols, which I once took as native javascript strings, are now to be pairs whose left part is the empty pair:
```
let isSymbol = x => isEmpty(carOf(x));
```
Then, as a matter merely of convention, the right part of a symbol is taken as runes which spell out the name of the symbol.
Recall, way back in the first entry on this line of work, I mentioned that a candidate way of defining atoms was by making them identical to their left part.
After that I asked what of the right part?
To which I answered: this might help us to more easily, as humans, differentiate between symbols.
But, note, here, symbols are not atoms in this sense, they are just a special case of ordered pair.

This method of distinguishing symbols from nonsymbols is itself a convention imposed upon the particular designs of my pure little lisp: an other may come along and devise a method which behaves nothing like the symbols of LISPs past.
Such an eample is not as far off as it might seem to those wedded to LISP e.g. I've already mentioned in [my list of links on concatenative languages](#2025-0417-2020) that there is a calculus of trees which purports to do more with less from a foundation of ordered pairs as that contemplated here by me.
That, like so many other things that I mention among these notes, is for another time.

The introduction of symbols as special pairs interferes in one place with the broader convention of taking pairs as lists.
What of the difference between a pair playing the part of a list whose first item (its left part) is itself the empty list?
It is by answering this question that we find the first use of symbols: the symbol symbol.
```
let theSymbolSymbol = consOf(theEmptyPair, theEmptyPair);
```
But, "the symbol symbol" is a mouthfull and there is a name I am more fond of which is short and which came to me originally through my study of the J programming language:
```
let ace = theSymbolSymbol;
```
A more incremental presentation of the above and one that rids the design of its commitment to a particular pair as the symbol pair is as follows.

First, add a letter for the symbol pair to the alphabet.
Here I pick '$' as that letter because it resembles the letter 'S' in 'Symbol':
```
abc =[...'0123456789abcdefghijklmnopqrstuvwxyz ($)'];
```
Then the symbol rune *is* the symbol pair
```
let theSymbolRune = runeOf('$');
```
or just
```
let $ = theSymbolRune;
```
so that the symbol symbol, 'ace' for short, is
```
ace = consOf($,$); 
```
But, this too breaks the convention that the right part of a symbol is to be taken as spelling out the name of a particular occurrence of the symbol.
This can be remedied directly with
```
ace = consOf($,singletonListOf($));
```
This is unsatisfactory for many reasons, the least of which is that it makes it hard to see the convention being enforced (i.e. that 'ace' designates the symbol symbol and that the name of the symbol symbol is designated by "runesOf('$')".
So it is that there is one last alternation that can be made now and which not only satisfies this convention but eases reading and printing: it is taking the symbol pair as the runes of the item designated by "'$'" (and yes the double quotes enclosing the single quoted occurrence of '$' is needed).
```
let theSymbolPair = runesOf('$');
isSymbol = pair => id(pair, theSymbolPair);
theSymbolSymbol = consOf(theSymbolPair, theSymbolPair);
$ = theSymbolPair;
ace = theSymbolSymbol;
```
There is some temptation to designate the symbol symbol by '$$' but I'll avoid that for now.

Surprisingly, the way in which this convention aids in reading and printing imposes its own convention on the grammar of programs in this little lisp of mine.
It also happens to be the grammar that helps to bring LISP and FORTH closer together: they are already extremely close because LISP is a FORTH that is ashamed of its stacks.

The convention is this: programs are space seperated spellings of symbols.
Said another way: the names of the symbols of a program are spelled out by whatever is seperated by spaces.
There is more, because we have said that the symbol symbol is marked as a symbol by having the symbol pair as its left part, we have introduced the convention of marking the part of speech played by a list with its left part.
It shall be shown that the distinctino between symbols and lists is enough to mark out any grammatical distinctions that may later be used to the benefit of the programmer.

But, this is not quite right either because it is the job of the reader to build lists and symbols.
Thus, the program grammar is actually the result of further contemplation on what the reader must do and how it must do it.

Since this is a LISP and not a FORTH, parenthesis play a special part in the reader from the beginning.
The reader can be taken as a tiny interpreter/compiler (there is no distinction really between these things other than how we speak of them when we speak of them asperationally).
It reads a list of runes and builds a symbol or a list of symbols or a list each item of which is either a symbol or a list of symbols, and so on, from it.

The commands that go to the reader are the ones that build the pairs upon which the lisp works, whether it directly evaluates them or alters them by way of some grammatical rule of transformation (commonly known as a macro).
As said before, the commands are seperated by spaces, so there is actually another even tinier interpreter/compiler that sits between a list of runes and the reader (that is when you discard the native javascript transformation from a javascript string into a list of runes!)

This tinier interpreter/compiler is called a tokenizer or a lexer.
But, this is only appropriate when it is spoken of in relation to some other language.
Otherwise, it is just another programming language that takes a pair as a list of runes and builds a list of lists of runes from it!
The name "lexer" is the most suggestive of what is to be done: it is to break a complex compound into its lexical atoms.
The lexicon of a language is what you get if you grab that language by its grammatical trees and shake it until all its words fall out.

The alphabet of a language is what you get when you grab its lexemes, i.e. items of its lexicon, and shake them until its letters fall out.
It is from thes letters, or runes, that the lexer builds lexemes.
So, to recap, the lexer builds lexemes, and the rest of the reader builds symbols, lists of symbols, and so on from the list of lexemes made by the lexer.

So, on to the lexer!
Our convention of spelling lexemes by seperating their spellings with spaces divides the lexer into two operations: skipping spaces and making words from their spellings.
First, a method of detecting spaces:
```
let theSpaceRune = runeOf(' ')
, isSpace = x => id(x, theSpaceRune);
```
Next a way of taking a list of runes and trimming off any spaces from the front.
```
let trimSpacesOf = x => isEmpty(x) ? x : isSpace(carOf(x)) ? trimSpacesOf(cdrOf(x)) : x;
```
This is the easy step.
The hard step is making words from their spellings and putting them all together as a list of runes.
There are two paths that are before us: one where stacks are formally introduced or one where continuations are formally introduced.
The simpler path is stacks, but they are both the same thing (continuations are stacks in disguise, and this part of the "lisp is a forth ashamed of its stacks" thing I said earlier).
Also, stacks are the best, better even than lists (which are just the mirror of stacks, or stacks in reverse, which is itself foreshadowing).

The empty pair is taken as the empty stack, pushing an item onto a stack is making an ordered pair whose left part is the old stack and whose right part is the new top of stack, popping the empty stack gives the empty stack and popping a nonempty stack gives its left part, and, finally, peeking, or looking at the top of a stack returns its right part or the empty pair if you peek the empty pair.
```
let theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, pushOf = consOf
, popOf = carOf
, topOf = cdrOf;
```
Notice, these stack operations are all just different names for the same old operations on pairs that were introduced in the beginning.
Stacks are simply a different perspective on pairs, lists are another, and there are others still.
Ordered pairs are probably the most important abstract objets that humans have introduced into the world.

The plan for the lexer is to make a function that takes three arguments:

1. a stack of lexemes accumulated thus far
2. the lexeme currently being constructed
3. the list of runes being lexed.

Thus the lexer will just be a way of setting up the helper function with an empty stack, an empty list, and the list of runes to be lexed.
The first step is to trim off any spaces that happen to be at the front of the list.
So, the helper can assume that the list of runes to be lexed is already trimmed: this must now be enforced by the design of the helper itself.
One option is to make a helper for the helper, one that does nothing more than take the same three arguments as the helper and simply trim the third and then pass them onto the helper.
This seems excessive and can be accomplished by just making sure to trim any initial spaces from the third argument once a space is confronted.
At the same time, confronting a space is a command that the current lexeme under construction is no longer under construction and can be pushed onto the stack of lexemes lexed thus far.
All that remains is to take the first rune of the list to be lexed, check if it is a space or not, if it is not then append it to the lexeme under construction, and if it is then push the constructed lexeme onto the stack of lexemes lexed and trim the list of runes to be lexed before passing everything onto the helper again.

Oh!
It just occurred to me that I do not even need to worry about all that stack stuff!
I can just append lexemes to the list of lexemes because I already gave the definitions for append and prepending.
So, forget all that stack stuff for now.

```
let lexerHelperOf = (lexemes, lexeme, runes) =>
 isEmpty(runes) ? (
  isEmtpy(lexeme) ? lexemes 
  : prependedListOf(lexemes,singletonListOf(lexeme)))
 : isSpace(carOf(runes)) ? 
  lexerHelperOf(prependedListOf(lexemes,singletonListOf(lexeme))
  ,theEmptyPair,trimSpacesOf(runes))
 : lexerHelperOf(lexemes
  ,prependedListOf(lexeme,singletonListOf(carOf(runes))),cdrOf(runes))
, lexOf = runes => 
 lexerHelperOf(theEmptyPair,theEmptyPair,trimSpacesOf(runes));
```

The rest of the reader is usually called the parser for very good reason:

>"parse(v.): 1550s, in grammar, "to state the part of speech of a word or the words in a sentence," a verbal use of Middle English pars (n.) "part of speech" (c. 1300), from Old French pars, plural of part "a part," from Latin pars "a part, piece" (from PIE root *pere- (2) "to grant, allot") in the school question, Quae pars orationis? "What part of speech?" Transferred (non-grammatical) use is by 1788. Pars also was a common plural of part (n.) in early Middle English. Related: Parsed; parsing." <https://www.etymonline.com/word/parse>

From the list of lexemes the parser deduces the part of speach to which each lexeme belongs and, from doing so, constructs the grammatical tree from which they fell.
There are only two parts of speech for the items of my little LISP: symbol or list.
So the parser either builds lists or symbols based on the lexemes it is fed, one by one.
An open parenthesis starts the construction of a list, a closing parenthesis ends the construction of a list under construction, and everything else is a symbol.
The conventions established so far make it so that the any pair whose left part is not the symbol pair is a list.
This can be made overt rather than covert by introducing a general convention that the way of dealing with the right part of a pair is to be determined from its left part.

This is a general method e.g. FORTH's dictionaries are designed precisely in this way so that native code on the left tells the machine how to deal with the stuff on the right.
Since it appears as if symbols can be used to defer the introduction of this general method (so that the programmer of my little lisp can set such grammatical concerns up as they see fit), I shall keep with the implicit way of taking pairs as lists.

So, there are two things the parser must do: build symbols and build lists.
Parsing a symbol is easy: just make a pair whose left part is the symbol pair and whose right part is the lexeme which names the symbol being parsed.
```
let parseSymbolOf = lexeme => consOf($,lexeme);
```
Note that this automatically produces the symbol symbol when the lexeme is the symbol pair.
This is an example of one of the conveniences which follows from the conventions evolved here.

Parsing a list is only slightly harder.
Now, all that stuff about stacks can be unforgotten.
When the first lexeme of the list of lexemes to be parsed is an open parenthesis then whatever we were parsing up to that point goes on the stack of things we have to go back to parsing.

It's slightly easier to grasp this when you start parsing with an open parenthesis.
This tells you to start with an empty list, i.e. the empty pair, and build up the rest of the list based on the remaining lexemes.
Next suppose there is neither a closed parenthesis or an open parenthesis as the next lexeme.
Then it must be a symbol.
So you parse that symbol and then append it to the list under construction.
This is all fine and good until you run into another open parenthesis.
You need to start making a new list, but, when you're done making it, you have to append what you've made onto the end of the last list you were working on and continue parsing with whatever lexemes are left over.

If you're lucky, you have a helper that has an argument for the list you're going to get back to constructing, an argument for the list currently under construction, and the rest of the lexemes you have to go through to finish everything up.
But, since you can run into an unknown number of open parenthesis before reaching your first closed one, you can't just keep hopping you have helpers for helpers.
For me, stacks are the obvious answer.
They occur to me so readily because I deal with the world all the time with stacks.
There are stacks and stacks of things that I'm going to come back to after I finish working with the thing that is at the top of the stack.

Now, when you run into an open parenthesis you push whatever list is under construction onto the stack of lists under construction, and start with the empty list.
If the next lexeme is neither an open or closed parenthesis you append its parsed symbol to the list under construction.
If it's a closed parenthesis then you are done constructing the current list and append it to the list atop the stack of lists under construction.
```
let isOpenParen = x => id(runesOf('('),x)
, isCloseParen = x => id(runesOf(')'),x)
, parseHelperOf = (stack, list, lexemes) =>
 isEmpty(lexemes) ? list
 : isOpenParen(carOf(lexemes)) ? 
  parseHelperOf(pushOf(stack,list),theEmptyPair,cdrOf(lexemes))
 : isCloseParen(carOf(lexemes)) ?
  parseHelperOf(popOf(stack)
  , prependedListOf(topOf(stack),singletonListOf(list))
  , cdrOf(lexemes))
 : parseHelperOf(stack
   , prependedListOf(list
    , singletonListOf(parseSymbolOf(carOf(lexemes))))
   , cdrOf(lexemes))
, parseOf = lexemes => parseHelperOf(theEmptyPair, theEmptyPair, lexemes);
```
There are a few things that seem as if they are not going to work out well based on the above design:

* if the lexeme is just a list of symbols then what happens?
* does there really need to be a helper function for "parsing Symbols"?
* is there a simpler parser that just works with parenthesis, spaces, and everything else?

To answer that last question is to get to the heart of the matter I submit.
So, rather than parsing symbols and such I'll just take every lexeme that is not a parenthesis directly over to the list under construction:
```
parseHelperOf = (stack, list, lexemes) =>
 isEmpty(lexemes) ? list
 : isOpenParen(carOf(lexemes)) ? 
  parseHelperOf(pushOf(stack,list),theEmptyPair,cdrOf(lexemes))
 : isCloseParen(carOf(lexemes)) ?
  parseHelperOf(popOf(stack)
  , prependedListOf(topOf(stack),singletonListOf(list))
  , cdrOf(lexemes))
 : parseHelperOf(stack
   , prependedListOf(list
    , singletonListOf(carOf(lexemes)))
   , cdrOf(lexemes));
```
By simply dropping the step that parses symbols something important has been revealed: this is almost the same form of program as the primtiive encoder and decoder cooked up in [Bit Strings and Binary Trees](#2025-0413-1513-bit-strings-and-binary-trees).

The following supplementary functions make it clearer:
```
let secondOf = stack => topOf(popOf(stack))
, drop2 = stack => popOf(popOf(stack)) 
, appendOf = stack => pushOf(drop2(stack)
  , prependedListOf(secondOf(stack), topOf(stack)));
parseHelperOf = (stack, lexemes) =>
 isEmpty(lexemes) ? topOf(stack)
 : isOpenParen(carOf(lexemes)) ? parseHelperOf(pushOf(stack,theEmptyPair), cdrOf(lexemes))
 : isCloseParen(carOf(lexemes)) ? parseHelperOf(appendOf(stack), cdrOf(lexemes))
 : parseHelperOf(appendOf(pushOf(stack,carOf(lexemes))), cdrOf(lexemes));
parseOf = lexemes => parseHelperOf(theEmptyPair, lexemes);
```
Here, the open parenthesis plays the part of the zero of the encoder, the close parenthesis plays the part of the one of the encoder, and the default case appends the first of the lexemes onto the list at the top of the stack.

This suggests a certain ordering to the alphabet in order to make the similarities stronger along with a complete overhall of method.
It seems like it is worth it and that it also brings the similarities between lisp and forth even closer together while simplifying going between one and the other (and without eliminating lisp as a language).
It also suggests a pretty obvious alphabetization.

I was unable to see through the rest of what I had seen to my self.
Meditation and sleep are the most likely solutions to this problem.

## 2025 0430

### 2025 0430 2324
This continues my read of Durant's "Story of Philosophy" from [2025 0420 2247](#2025-0420-2247).

Last time I cracked open the spine of this old book I had just read that "Critias was a pupil of Socrates, and an uncle of Plato".
Critias is worthy of his own story from what little I read of his history outside of what less Durant mentioned.
He is also surprisingly relevant to the world we are presently faced with: the Sophists were selling aristocracy by bashing democracy and Critias ended up the very brief head of Athen's Oligarchical Party.
There is not enough written by Durant or that I have already read as part of my work on the history of the world to deduce to what extent "democracy" was anything that existed strongly enough to even be denounced e.g. what written records we have are not even enough to make too many commitments to the stories that happen to have survived to our day.

Archeology is a countercontrol to the division of the past into history and prehistory where prehistory is simply a weak stand in for "unrecorded" or "having no surviving records written in a language as the reinforcing practices of a verbal community".
It is quite clear that there is more to be got from simulating past environmental conditions and watching how humans navigate such contrived worlds in many cases than to take the word of a textual record that just so happens to have found its way to our modern world.
The validity of such methods depends on the difference between the primary reinforcers of the past and those now.
As far as I can tell they are still the same old same old: sex, food, water, violence.
There may be a few others, but these are the primary reinforcers from which most of the organisms are likely to evolve or condition the much larger collection of secondary reinforcres that come to be called social because they involve an other organism (perhaps of the same species) in some way key to the conditioning contingency.

Alas, archeology is not what I am reading, though I would love to know more about the archeological work that has been done on Athens during the times of Socrates and Plato.
That is for a different time.

The second section of the first part is titled "Socrates".
His bust bursts onto the scene as if a wart on his philosophic reputation: bald, round,

>"it was rather the head of a porter than that of the most famous of philosophers."[pg. 7]

There are various sculptures of Socrates purported from his time and their accuracy is probably more asperational than scientific e.g.

* <https://digital.library.cornell.edu/catalog/ss:172717>
* <https://www.ancientworldmagazine.com/articles/a-portrait-of-socrates/>
* <https://www.mfa.org/article/2021/portrait-head-of-socrates>
* <https://www.worldhistory.org/image/4425/socrates-bust-palazzo-massimo/>

and, last but not least, a collection of them as part of an article on the big question "Who was Socrates?"

* <https://guides.library.duq.edu/c.php?g=605283&p=4214778>

One bust or the other is a concrete connection with the ever ephemeral Socrates.

He spoke and listened in ways that differed remarkably from those of his time, and, presumably, of our time as well.
Just as the young and curious of our day are apt to ask "why? why? why?", whether as inquiries or objections, Socrates queried his fellow Athenians in search of something beyond question.

Perhaps this is all that was different about Socrates' verbal behavior and perhaps this was all that was required to bring a cross section of young and old back to his arena of discourse.
What is known of Socrates is second hand: no writing of his survives if ever it was made.
We shall never have the records we might need to recreate what made his verbal behavior so remarkable that descriptions of it have made their way to us as something beyond myth and history.

The stories of Socrates are, for me, folklore.
Most of what is called history is folklore in my measure.
There's little beyond the dutiful work of archeologists that allows me to speak with any certainty on the intersection of history and folklore.
When taken as folklore it becomes easier to see that there are certain stories that are told with a character called Socrates and, just as knowing the stories of Shakespeare can alert you to fitting surprises, knowing the stories of Socrates can strengthen otherwise remote cultural connections.

I for one cherish Shakespeare far more than the stories of Socrates, but can not doubt the effect that Plato's tales of Socrates have had on European philosophy, just as there is even less doubt of the effect Shakespeare has had on European literature.

### 2025 0430 2309
Two delightful links (from a friend) on a ternary stack machine:
 * <https://concatenative.org/wiki/view/DSSP>
 * <https://web.archive.org/web/20230405010717/http://brokestream.com/daf.txt>

### 2025 0430 1905 Frege Metaphorically Taken
My first reading of Begriffsschrift was a while ago when I first got the invaluable text "From Frege To Godel" Third Printing edited by van Heijenoor.
At that time I was mainly interested in von Neumann's outlook on set theory and logic.
But, it was already clear that Frege had made his Begriffsschrift and all else was commentary upon it.

R.P. and I have disagreed on this matter.
Last Friday I received my copy of "The Frege Reader" edited by Michael Beaney and began a close and careful analysis of Frege's writings without them being of secondary interest as they were when I was studying Quine's "Set Theory and its Logic" and was led to von Neumann's 1925 "An axiomatization of set theory".

Not only do I have a new respect for Frege.
I am more certain than ever that he shared my own outlook on his commentaries.
Today, I finally started reading Frege's 1891 "Function and Concept" and was delighted to see that he had this to say

> "There can be no question of setting forth my *Begriffsschrift* in its entirety, but only of elucidating some fundamental ideas."[pg. 131 Frege "The Frege Reader" edited by Michael Beaney]

So that the result of his comments upon Begriffsschrift can be taken merely as elucidation and nothing more.
The etymology of the root is here from <https://www.etymonline.com/word/*leuk->
> "*leuk-. Proto-Indo-European root meaning "light, brightness."
>
>It might form all or part of: allumette; elucidate; illumination; illustration; lea; leukemia; leuko-; light (n.) "brightness, radiant energy;" lightning; limn; link (n.2) "torch of pitch, tow, etc.;" lucent; lucid; Lucifer; luciferase; luciferous; lucifugous; lucubrate; lucubration; luculent; lumen; Luminal; luminary; luminate; luminescence; luminous; luna; lunacy; lunar; Lunarian; lunate; lunation; lunatic; lune; lunette; luni-; luster; lustrum; lux; pellucid; sublunary; translucent.
>
>It might also be the source of: Sanskrit rocate "shines;" Armenian lois "light," lusin "moon;" Greek leukos "bright, shining, white;" Latin lucere "to shine," lux "light," lucidus "clear;" Old Church Slavonic luci "light;" Lithuanian laukas "pale;" Welsh llug "gleam, glimmer;" Old Irish loche "lightning," luchair "brightness;" Hittite lukezi "is bright;" Old English leht, leoht "light, daylight; spiritual illumination," German Licht, Gothic liuhaÃ¾ "light."

We can see this is consistent with Frege's prior sentence:

>"Today I should like to throw light upon the subject from another side, and tell you about some supplementations and new conceptions, whose necessity has occurred to me since then."[pg. 130-131 Frege "The Frege Reader" edited by Michael Beaney]

This sentence appears, to some, to say that there are necessary changes that must be made to Begriffsschrift in light of his work on the forthcoming "Grundgesetze der Arithmetik, Volume I" published in 1893.
But, a careful reading of Begriffsschrift or a careful reading of the introduction or a careful reading of the rest of the paper reveals this can not be the case and that Frege is simply doing as he says e.g. supplementing and newly conceptualizing.

>"It is my intention, in the near future, as I have indicated elsewhere, to explain how I express the fundamental definitions of arithmetic in my Begriffsschrift, and how I construct proofs from these solely by means of my symbols. For this purpose it will be useful here to be able to refer to this lecture so as not to be drawn then into discussions which many might condemn as not directly relevant, but which others might welcome."[pg.130 Frege "The Frege Reader" edited by Michael Beaney]

His supplements and conceptualizations do not alter the Begriffsschrift.
It is besides the point if they are largely mistaken when they are taken literally.

Thus, where his elucidations are incompatible with his Begriffsschrift we can, by his own admission, take his elucidations in jest, error, or, what is perhaps most valuable, as metaphor.

But there is more!
Frege has already in the introduction of Begriffsschrift explained his metaphorical methods and their contribution to his designs!
He repeats the same in "Function and Concept".
We are, from the very introduction of Begriffsschrift, told of its arithematical origins and of the exact metaphorical extensions which smooth the steps from the arithmetic of numbers to that of thought or concept.

For those who have not yet undertaken an endevor like Frege's it may seem strange to treat his latter writings with such apparent disregard.
But, the disregard is only apparent, as it is otherwise familiar to those of us who have come to understand (or, as the case may be, to misunderstand) our younger self.

So it is that Frege can be taken as metaphor where he is incompatible with the strictures of Begriffsschrift, and all his elucidations remain as elucidating as ever, but now as metaphor.
Said another way, if later commentaries were to be given as foundational alternatives to Begriffsschrift then the entirety of the new foundation would be required as part of subsequent publications: this is known from the fact that Frege did not present Begriffsschrift as an alteration of his earlier attempts at such notation which are merely mentioned at the end of section three of Begriffsschrift. 



### 2025 0430 1830
I am once again drawn to entry [2025 0416 2358](#2025-0416-2358) in which I finally made a public record of being unable to keep up with my self.
There is more I have written than I have been able to write out here.
Thankfully, I am less likely to lose what it is I wrote than I ever was thanks to a social experiment I did some four or so years ago.

Together with two friends I designed a tiny culture around a collection of strict contingencies set up with the help of a somewhat elaborate sequence of index cards and index card holders.
The strict method was evolved until it was eventually dropped, but it had done its work on my friends and my self in ways which were mostly within the reach of my designs e.g. the often complex contingencies linking time, reading, and writing into a social environment where collective action was a major conclusion to any such work were revealed more clearly than any of us had ever seen before.

Whatever was dropped of the evolved practices of reading and writing continued to control all our subsequent interactions.
We had all come to keep a notebook in our pockets and to bring them out whenever we sat down to work with an other (especially when the other was one of the members of our tight knit group.

Somewhere along the way we landed on "The Pocket Notebook" by the Portage company as they appeared to, at the time, have the right price point.
Previously I had been working mostly with the beautiful "Field Notes" by the company of the same name: John Gruber had mentioned them more than once and they were sturdy and simple in those ways that I cherish.

There are still many of those pocket notebooks around and I occasionally buy replacements when I run out of them.
Spiral bound is important when you're really "on the go" but it is not effective when you're building cumulative records.
The choice between one or the other is largely a matter of "where am I?"
Pages can be torn from a spiral notebook and displayed as was once a major part of the tiny culture run via index cards.
They can also be kept in place.
When it comes to long form bound notes that are written as in a cumulative record in an experimental environment, I go with hardback books bound for artists.

Recently I've experimented with binding my own books as a further way of simplifying.
There is still promise in such methods if not just because I haven't tried them out in detail.


## 2025 0429

### 2025 0429 1424 A Preliminary Outline of The Method of Bringing Frege Through Quine
The first concrete hint of the transition from sense to schematism is in the first section (again I keep my etymological principles by presenting both translations of this selection from the books which I have in hand):
> "*I adopt this fundamental idea of distinguishing two kinds of symbols*, which unfortunately is not strictly carried through in the theory of magnitude, [consider 1, log, sin, Lim], *in order to make it generally applicable in the wider domain of pure thought*.
>I therefore divide all the symbols I use into *those by means of which one can represent different things* and *those that have a quite determinate sense*.
> The first are the *letters*, and these should serve primarily to express *generality*.
> For all their indeterminancy, it must be insisted that a letter *retain* in the same context, the meaning once given to it."[pg. 52 Frege 'The Frege Reader' edited by Michael Beaney]

> "*I adopt this basic idea of distinguishing two kinds of signs*, which unfortunately is not strictly observed in the theory of magnitudes [consider 1, log, sin, lim], *in order to apply it in the more comprehensive domain of pure thought in general*. I therefore divide all signs that I use into *those by which we may understand different objects* and *those that have a completely determinate meaning*. The former are *letters* and they will serve chiefly to express *generality*. But, no matter how indeterminate the meaning of a letter, we must insist that throughout a given context the letter *retain* the meaning once given to it."[pg. 10-11 Frege 'From Frege to Godel' Third Printing edited by van Heijenoor]

The former come immediately over to the variable letters as a grammatical category perhaps constructed from a single lowecase letter to which further members are added by iterated accentuation e.g. "x'" and "x''''".

Although it is somewhat out of place here, I shall mention overtly that there are many details to the problems of grammatical analysis which are better explaiend in Quine's "Philosophy of Logic 2nd Edition" than in any explanation I may give.
In general, the methods of the grammarian can be applied to the methods of the logician and have been so with great effect.
The result is that the methods of logic can then be applied to the methods of the grammarian and so on with ever greater effect (although there is presumably some kind of diminishing return).
Thus it is that the grammar of logic can be spoken of logically.
This is what is out of place here, but it is not so far out of place as to block me from further comment.

Talk of "the grammatical category of variables" can be dropped with all of its commitments of a theory of grammatical categories by giving a logical theory one predicate of which is 'is a variable'.
Here a warning is given about reading "predicate" which I repeat from Quine:

>"A word of caution is in order regarding 'predicate' too. Some logicians take a predicate as a *way* of building a sentence aroudn a singular term or, more concretely, as what Peirce called a *rheme* [Volume 2, paragraph 95], a sentence with blanks in it, these being distinctively marked in the case of a many-place predicate. This version covers, implicitly, the potential output of predicate abstraction or predicate functors. But a predicate in my sense is always an integral word, phrase, or claus, grammatically a noun, adjective, or verb. Some are generated from others by grammatical constructions, notably the relative clause or, formally, predicate abstraction and predicate functors."[pg. 61 Quine "From Stimulus to Science"]

This is given in the chapter "Denotation and Truth".
Note, for later reference, this restriction includes the results of predicate abstraction and predicate functors e.g. '{x : x loves Dick}' and 'Refl love' (which goes into 'love oneself') are predicates.
A reminder that an explanation of predicate abstraction from quantificational logic and predicate functors from predicate abstraction is to be found in my [2025 0422 2322 An Incomplete Sketch of My Philosophy of Logic](#2025-0422-2322-an-incomplete-sketch-of-my-philosophy-of-logic).

Under the control of this convention the predicate 'loves Dick' is coextensive with the predicate abstract '{x : x loves Dick}' which is to be distinguished from 'x loves Dick' which may be taken in one of three ways:

1. 'x loves Dick' is like the sentence 'He loves Dick' were 'He' is a pronoun which purports to designate one and only one item.
It is then for the rest of the sentences in a given theory to establish the existence (or non-existence) of the item purportedly designated by 'x' or 'he'.

    This is what has been called a Free Logic, but under the methods imposed by Quine in his paper "Free Logic and Virtual Classes" of 1994 and more carefully integrated into the methods of logic in his 1982 "Methods of Logic 4th edition".
Leblanc and Schock had different methods for dealing with what they called free logics which are clumsy compared to Quine's e.g. rather than simply alter the definition of 'traditional logic' to demand first contemplating a nonempty universe of discourse and only as a special case going through and marking existentials as false and universals as true in the degenerate case of an empty universe, they would demand traditional logic distinguish between those where an instance of a schematic premise such as "each item is {x : Fx only if Fx}" be introduced at every turn lest the theory fall to the degenerate case.

    The thrust of Quine's arguement for his method is two fold: 1) the rules of passage do not hold in the empty case but are part and parcel of traditional logic, 2) admiting singular terms which merely purport to designate one and only one item clears the way for a careful analysis of the descriptional premises which are so often invisible to those who invoke singular terms as if they could not even be so invoked without designating some unspecified item.

    This is my favorite method and it is also a delightful way of noticing how smoothly singular terms disolve into singular descriptions: a more detailed explanation of this dissolution and its combination with predicate abstracts as purported designations of their extensions shall not be found in this note.

2. 'x loves Dick' is short for its universal closure 'each item is {x such that x loves Dick}' with predicate functors and predicate abstracts or, in crusty quantificational languages, '(each item is x such that)(x loves Dick)'.
It is this way of taking 'x loves Dick' that is most familiar from the practices of arithmetic where an equation with variables such as 'x+x = 2x' is said to be "an identity" in that "each item is {x : x +x = 2x}" is true.

    It is also this method which is commonly used in logic programming where question marks are prepended to the variable letters that are under universal quantification e.g. '?x loves Dick' or '?x loves ?y'.
This is because of the way that some logic programming langauges take such sentences as querries upon which they generate substitutions that satisfy the sentence based on the contents of the database being querried.

    In general, this is the beginning of the method of Skolemization or its notational variation as Quine's functional normal form (which does away with Skolem functions while retaining their logical import).
Quine's functional normal form introduces compound variable-like letters which are either a classic variable letter or are a classical varible letter subscripted with a list of compound variable-like letters.
Then the variable letters which have no subscript are implicitely universal quantified and those with subscripts are implicitely existentially quantified and the order of these quantifications can be constructed from the structure of the subscripting.
At a probably much later time I shall give the algorithm for converting to and from functional normal form since it greatly simplifies logic programming with Quine's main method.

3. 'x loves Dick' is not a sentence nor a predicate nor an unmarked universal closure but rather a kind of sentence under construction. For example, the place marked by 'x' in 'x loves Dick' is waiting to be replaced by a proper singular term e.g. replace 'x' by 'Tom' in 'x loves Dick' to get 'Tom loves Dick' which is a complete sentence.
This is my least favorite method.

I've taken so much time to describe these different ways of taking 'x loves Dick' because it is the multitude of incompatible methods that were presented to Frege and which he had to navigate in order to arrive at his Begriffsschrift as something coherent, systematic, and, above all, extensional.
His inspiration was arithmetic, and in that way he went with the second method whenever he said that a sentence like 'x loves Dick' is used to express a generality like that expressed from arithmetic in 'x(y+z) = xy + xz', and it is in Quine's functional normal form that there are the most similarities between Frege's methods and those of arithmetic e.g. as when he says

> "The first are the *letters*, and these should serve primarily to express *generality*. For all their indeterminancy, it must be insisted that a letter *retain* in teh same context, the meaning once given to it."[pg. 52 Frege 'The Frege Reader' edited by Michael Beaney]

>"The former are *letters* and they will serve chiefly to express *generality*. But, no matter how indeterminate the meaning of a letter, we must insist that throughout a given context the letter *retain* the meaning once given to it."[pg. 10-11 Frege 'From Frege to Godel' Third Printing edited by van Heijenoor]

But, alas, none of these three methods actually coincide with Frege's usage or his own explanations of his own usage which is consistent throughout Begriffsschrift and wherever Begriffsschrift is directly invoked.

The methods of Frege are schematic and it is the way in which they are schematic which leads to sentences in Frege's texts that, if read carelessly or in isolation, can lead to a great deal of confusion at many different times e.g. at some times the schematism is part of a theory of schematisms and at other times there is no such theory under investigation and the schematic methods are purely logical (as in talk of 'valid schema' through which validities are got by substitution but with which no items or objects are concerned).




### 2025 0429 1407 An Analysis of "At Plataea the Persians were defeated by the Greeks"
The subject of "At Plataea the Persians were defeated by the Greeks" is "the Persians".
The predicate is "{x: At Plataea x were defeated by the Greeks}" via Quine's predicate abstract notation.
> Here is my single sentence summary of Quine's predicate abstract notation (for more on its definition in a quantificational logic see [2025 0422 2322 An Incomplete Sketch of My Philosophy of Logic](#2025-0422-2322-an-incomplete-sketch-of-my-philosophy-of-logic)):
>
>The English relative clause 'who loves Dick' and the pidgin 'x such that x loves Dick' are uniformly paraphrased by the *predicate abstract* '{x: x loves Dick}' which *abstracts* 'Tom' from 'Tom loves Dick' by *binding* the *free* occurrence of 'x' in the *open* sentence 'x loves Dick' with the prefix 'x:' so that the *predication* '{x:x loves Dick}Tom' *concretes* to 'Tom loves Dick': whatever can be said of a thing can be said by predicating a predicate of it i.e. *predicational completeness*.

The object of "At Plataea the Persians were defeated by the Greeks" is "the Greeks".
The most general predicate is "{x,y,z: At x y were defeated by z}" from which the sentence can be factored conspiucously into its predicate, subject, and objects as "{x,y,z: At x y were defeated by z} Plataea, the Persians, the Greeks".
This analysis is of major consequence to explaining Frege's main paragraph in Begriffsschrift which I first mentioned [here](#2025-0428-1517-frege-thru-quine).

### 2025 0429 1403
Rules of inference are the manufactured products of logical methods: they mark the end of logical heavy lifting and the beginning of logical economy for all.
Those obsessing over the by products of this or that collection of rules of inference are only indirectly engaged in logical methods.
Inference rules are the mechanical proxies of logical work.

## 2025 0428

### 2025 0428 1517 Frege Thru Quine
The translation of Frege (1848â1925) through Quine (1908-2000) begins with John Horne Tooke's (1736-1812) suggestion that

> "you substitute the composition, &c, of *terms* wherever he has supposed a composition of ideas"[pg.6 Quine "From Stimulus to Science"].

(Note, Tooke's "The Diversions of Purely" played a part of mention in Skinner's (1904 â1990) theoretical analysis of verbal behavior.)

This technique evolved through Bentham's (1748â1832) contextual fictions (I have not been able to yet identify if Bentham read Tooke), then Boole's (1815â1864) concrete differential notation, and directly on through Russell's (1872â1970) triumphant contextual definition of singular description where 'the {x such that Fx}' is elimianted (with major restrictions like those in Quine's "Free Logic and Virtual Classes" of 1994 and their projection back and forth through Quine's "Methods of Logic 4th edition" of 1982) as follows:

> 'G the {x such that Fx}' for 'some item is {y such that Gy and each item is {x such that x is identical to y if and only if Fx}}'.

Frege, Peirce (1839-1914), Peano (1858-1932), Dedekind (1831-1916), and Cantor (1845-1918) are in the firmament from which Russell's theory of descriptions appeared (I am simply using it here as a concrete checkpoint).

As an aside, Russell and Frege dreamed of deducing mathematical truth from logical truth where their logic contained set membership and is what we would today call a mathematical logic: it was GÃ¶del (1906-1978) in 1931 who finally severed the methods of mathematics from the methods of logic (Quine submits that Russell's paradox of 1902 is an earlier component of the full severence).
It is the analysis of the ancestral of a relation and its confusion with the transitive closure of a predicate that prompted Frege and Russell to easily mix mathematics and logic: they mixed copuli of predication with predicates of membership. 
The confusion between mathematical logic and (predicate) logic can be forgiven when carrying Frege through Quine because it is a confusion which almost all mathematicians and logicians continue to make either out of laziness or ignorance: the lazy maintain the ignorant.

Quine continues the Tookeian tradition through his method of semantic ascent, and it is with such methods that the bulk of Frege can be happily translated.
It is not effective to go straight to semantic ascent first though because there are a number of distinctions which Frege makes which are interdependent with each other in ways that are factored out by Quine in different ways that don't depend on semantic ascent.

First I shall collect the relevant selections from Quine and then I shall collect and collate the relevant sections from Frege.
But, it is impossible not to allude to Frege for there is so much which he already noticed and which is only slightly obscure relative to Quine's distinctions.
In fact, I shall already break my plan here at the beginning because there is a beautiful distinction which Frege makes and which is otherwise obscure to me but in light of the following key distinction by Quine:

>âA context is extensional if its truth value cannot be changed by supplanting a component sentence by another of the same truth value, nor by supplanting a component predicate by another with all the same denotata, nor by supplementing a singular term by another with the same designatum. Succinctly, the three requirements are substitutivity of covalence, of coextensiveness, and of identity, salva veritate. A context is intensional if it is not extensional.â[pg. 90 Quine âFrom Stimulus to Scienceâ].

The division between extensional and intentsional context is given by Frege in the third section of the first part of Begriffsschrift.
Since I know of no way of seperating Frege's overlapping concerns I must present the section in its whole each time I explain one dimension of distinction that Quine makes which is relevant to it.
I also have access to two different translations of Begriffsschrift and must put both of them right after each other because of my etymological principles.

>"A distinction between *subject* and *predicate* finds *no place* in my representation of a judgement. To justify this, I note that the contents of two judgements can differ in two ways: either the conclusions that can be drawn from one when combined with certain others also always follow from the second when combined with the same judgements, or else this is not the case. The two propositions 'At Plataea the Persians were defeated by the Greeks' and 'At Plataea the Persians were defaeted by the Greeks' differ in the first way. Even if a slight difference in sense can be discerned, the agreement predominates. Now I call that part of the content that is the *same* in both, the *conceptual content*. Since *only this* has significance for the Begriffsschrift, no distinction is needed between propositions that have the same conceptual content. If it is said, 'The subject is the concept with which the judgement is concerned', then this applies also to the object. It can therefore only be said: "The subject is the concept with which the judgement is primarily concerned'. The linguistic significance of the position of the subject in the word-order lies in its *marking* the place where what one particularly wants to draw the attention of the listener to is put. (See also $9.) This can have the purpose, for example, of indicating a relation between this judgement and others, thereby fascilitating the listener's grasp of all the interconnections. Now all those features of langauge that result only from the interaction of speaker and listener--- where the speaker, for example, takes the listener's expectations into account and seeks to put them on the right track even before a sentence is finished--- have no counterpart in my formula language, since here the only thing that is relevant in a judgement is that which influences its *possible consequences*. Everything that is necessary for a valid inference is fully expressed; but what is not necessary is mostly not even indicated; *nothing is left go guessing*. In this I closely follow the example of the formula language of matheamtics, in which subject and predicate can also be distinguished only by violating it. Imagine a language in which the proposition 'Archimedes was killed at the capture of Syracuse' is expressed in the following way: 'The violent death of Archimedes at the capture of Syracuse is a fact'. Even here, if one wants, subject and predicate can be distinguished, but the subject contains the whole content, and the predicate serves only to present it as a judgement. *Such a language would have only a single predicate for all judgements, namely 'is a fact'*. It can be seen that there is no question here of subject and predicate in teh usual sense. *Our Begriffsschrift is such a language and the symbol \|--- is its common predicate for all judgements.*
>
> In my first draft of a formula language I was misled by the example of ordinary language into constructing judgements out of subject and predicate. But I soon convinced myself that this was an obstacle to my particular goal and only led to useless proxlixity."[pg. 54 Frege 'The Frege Reader' edited by Michael Beaney]

I have read this section multiple times, and have written it out twice now: it is probably one of the most important paragraphs in human history.
It combines practically every major distinction which separates the practices of logic from all other human practices.
Here is the same selection from a different translation (note the difference in spelling from the last's 'judgement' to 'judgment'):

>"A distinction between *subject* and *predicate* does *not occur* in my way of representing a judgment. In order to justify this I remark that the contents of two judgments may differ in two ways: either the consequences derivable from the first, when it is combined with certain other judgments, always follow also from the second, when it is combined with these same judgements, [[and conversely,]] or this is not the case. The two propositions "The Greeks defeated the Persians at Plataea" and "The Persians were defeated by the Greeks at Plataea" differ in the first way. Even if one can detect a slight difference in meaning, the agreement outweighs it. Now I call that part of the content that is the *same* in both the *conceptual content*. Since *it alone* is of significance for our ideography, we need not introduce any distinction between propositions having the same conceptual content. If one says of the subject that it "is the concept with which the judgment is concerned", this is equally true of the object. We can therefore only say that the subject "is the concept with which the judgment is chiefly concerned". In orderinary language, the place of the subject in the sequence of words has the significance of a *distinguished* place, where we put that to which we wish especially to direct the attention of the listener (see als $9). This may, for example, have the purpose of pointing out a certain relation of the given judgment to others and thereby making it easier for the listener to grasp the entire context. Now, all those peculiarities of ordinary language that result only from the interaction of speaker and listener--- as when, for example, the speaker takes the expectations of the listener into account and seeks to put them on the right track even before the complete sentence is enunciated--- have nothing that answers to them in my formula language, since in a judgment I consider only that which influences its *possible consequences*. Everything necessary for a correct inference is expressed in full, but what is not necessary is generally not indicated: *nothing is left to guesswork*. In this I faithfully follow the example of the formula language of mathematics, a language to which one would do violence if he were to distinguish between subject and predicate in it. We can imagine a language in which the proposition "Archimedes perished at the capture of Syracuse" would be expressed thus: "The violent death of archimedes at the capture of Syracuse is a fact". To be sure, one can distinguish between subject and predicate here, too, if one wishes to do so, but the subject contains the whole content, and the predicate serves only to turn the content into a judgement. *Such a language would have only a single predicate for all judgments, namely "is a fact".* We see that there cannot be any question herer of subject and predicate in the ordinary sense. *Our ideography is a language of this sort, and in it the sign \|--- is the common predicate for all judgments*.
>
> In the first draft of my formula langauge I allowed myself to be misled by the example of ordinary language into constructing judgments out of a subject and predicate. But I soon became convinced that this was an obstacle to my specific goal and led only to useless prolixity.[pg.12-13 Frege 'From Frege to Godel' Third Printing edited by van Heijenoor]

Another reason I have presented this paragraph twice is becuase it can only really be taken as a whole the way that Frege presents it, and to factor it into its component parts, which is what can be done once each of Quine's methods is in hand, would be to dismantle the innerworkings of vintage CortÃ©bert pocket watch!

To make the concrete connection then between Quine's extensional and nonextension contexts and Frege's distinction between conceptual content and nonconceptual content of a judgement:

> "the contents of two judgements can differ in two ways: either the conclusions that can be drawn from one when combined with certain others also always follow from the second when combined with the same judgements, or else this is not the case." [pg. 54 Frege 'The Frege Reader' edited by Michael Beaney]

> "the contents of two judgments may differ in two ways: either the consequences derivable from the first, when it is combined with certain other judgments, always follow also from the second, when it is combined with these same judgements, [[and conversely,]] or this is not the case."[pg.12 Frege 'From Frege to Godel' Third Printing edited by van Heijenoor]

I did not grasp the distinction that Frege was making without interpreting what he wrote through Quine.
First, the distinction is between the complementary ways that the contents of a judgement can differ.
That is, he distinguishes between different (yet complementary) differences: this is why it is a bit confusing.
We take a pair of judgements, say there is a judgement to your left and a different judgement to your right, and look at each conclusion that is implied by the content of the left judgement under a potentially empty collection of supplementary judgements and look at each conclusion that is implied by the content of the right judgement under the same collection of supplementary judgements.
Then, if there is for each conclusion from the left judgement the same conclusion from the right judgement under the same collection of supplementary judgements, the different contents of the left and right judgements are different _only in_ their _non_-conceptual content.

The example Frege gives for the different contents of two judgements that differ only in their non-conceptual content is as follows:

> "The two propositions 'At Plataea the Persians were defeated by the Greeks' and 'At Plataea the Persians were defaeted by the Greeks' differ in the first way."[pg. 54 Frege 'The Frege Reader' edited by Michael Beaney]

> "The two propositions "The Greeks defeated the Persians at Plataea" and "The Persians were defeated by the Greeks at Plataea" differ in the first way."[pg.12 Frege 'From Frege to Godel' Third Printing edited by van Heijenoor]

This example, as given, does not emphasize that the contents are to be of a judgement: this is because Frege has said he shall only contemplate contents which are judgeable.
Following through the example in detail: 1) the two quotations are spelled differently i.e. they are not identical as concatenations of letters, 2) thus they differ in at least one way (as contents) and the question is how they differ with respect to the distinction Frege has made between conceptual and nonconceptual content, 3) Frege claims that there is no conclusion that can be got from the one that can not be got from the other with respect to the same supplementary judgements, 4) therefore they have the same conceptual content and differ only in their non-conceptual content, one part of which is, presumably, their spelling as quotations.

Now note that Frege, in this example, goes far beyond the difference in content that I have mentioned as a difference in the spelling of their quotations: he wishes to go beyond propositions as sentences that may differ in spelling or, that complex difference in spelling called a difference in phrasing.
This is seen in the sentence following the sentence with the examples:

> "Even if a slight difference in sense can be discerned, the agreement predominates."[pg. 54 Frege 'The Frege Reader' edited by Michael Beaney]

> "Even if one can detect a slight difference in meaning, the agreement outweighs it."[pg.12 Frege 'From Frege to Godel' Third Printing edited by van Heijenoor]

Here was also see our first divergence between translations with "sense" in about the same place as "meaning" is in the other.
I do not own the German editions (yet), but a bit of etymology obviates any problems of translation from there (I can not get into this in greater detail without become derailed entirely in Skinner's analysis of verbal behavior).

When I worked through Frege's example I neither mentioned the sense nor the meaning of a sentence: I stuck with the difference in phrasing and took that to be slight in that, following Frege, there is not a conclusion from the one phrasing that can not be got from the other phrasing under identical supplementation of auxiliary judgements.

This radical translation is the hallmark of my presentation here.
Though I may not land on any singular method of radical translation from Frege's words and sentences, I shall demonstrate how, through Quine and Skinner, nothing like what Frege or most philosophers expect, as in senses and meanings, is actually at work throughout Frege's writings: so much so that nothing need be edited within his texts besides the already well known slips and such.
I see this as being in the same vein as Tooke, Bentham, Boole, Russell, and Quine, but wish to make it clear that I feel no affinity to a sort of nominalism (it shall be hard for some to accept this last point but the only way I could make this clear is by going over Quine's early stint as a nominalist with Goodman in 1947 which comes a cropper in ways parallel to Carnap's Aufbau).

Thus I mention now Quine's outlook on propositions in 1995:

> "So it is in standing sentences that the notion of meaning goes shaky. But this is the very locus of the philosophic notion of *proposition*: the meaning of a sentence of fixed truth value. Many philosophers have seen propositions as abstract objects that statements served to express. They have seen them as the bearers of truth values; sentences were true and false only in the sense of expressing true or false propositions.
>
> There is indeed a usage of 'proposition' that is useful and unobjectionable. It can be construed as denoting the sentences themselves, rather than their meanings, but it is used instead of 'sentence' when we are concerned with the sentences as an object of belief (as we shall be in Chapter VIII) rather than with its morphology and syntax. I deny myself this convenient usage, for fear of beclouding issues; but it carries no commitment to sentence meanings."[pg.77 Quine "From Stimulus to Science"]

I make no such commitment as there outlined at this time, but give it as an example of a principle governing my contemplation.

It is in the next sentence, where Frege defines the conceptual content of a proposition, that the quote from Quine, where he defines extensional contexts, activates:

>"Now I call that part of the content that is the *same* in both, the *conceptual content*. Since *only this* has significance for the Begriffsschrift, no distinction is needed between propositions that having the same conceptual content."[pg. 54 Frege 'The Frege Reader' edited by Michael Beaney]

>"Now I call that part of the content that is the *same* in both the *conceptual content*. Since *it alone* is of significance for our ideography, we need not introduce any distinction between propositions have the same conceptual content."[pg.12 Frege 'From Frege to Godel' Third Printing edited by van Heijenoor]

This is another way of saying that the contexts of Frege's propositional conceptual contents are extensional: conceptual content is defined by its invarience over implication which is sufficient to guarentee extensionality in such contexts.
While Carnap is known for his principle of extensionality (later demoted to the mere conjecture that intensional contexts can be paraphrased by extensional contexts), it is Quine's "Word and Object" that I have in mind when I say that invarience over implication guarentees extensionality.

It is because the contexts of Frege's propositional conceptual contents are extensional that I can present the following elaboration of transition from sense/meaning to schematics.

The next fragmentary step of my view of Frege is here where the examples are further analyzed [An Analysis of âAt Plataea the Persians were defeated by the Greeksâ](#2025-0429-1407-an-analysis-of-at-plataea-the-persians-were-defeated-by-the-greeks)

Or you can jump over that and just go here: [2025 0429 1424 A Preliminary Outline of The Method of Bringing Frege Through Quine](#2025-0429-1424-a-preliminary-outline-of-the-method-of-bringing-frege-through-quine).


## 2025 0427

### 2025 0427 2333
First editorial comment added to an older note: the shortener at the end of [Bit Strings and Binary Trees](#2025-0413-1513-bit-strings-and-binary-trees) does not work in a degenerate case that is now described there!

### 2025 0427 2230 A Quick Response to Frege's "Thought"
I skimmed Frege's paper on thought.
Almost everything he says is covered by B. F. Skinner's theoretical analysis in "Verbal Behavior" 1957.

A theory of behavior may admit abstract items designated by a description of a contingency of reinforcement e.g. "a door opens from a push on a lever" can be made to designate an abstract ordered pair whose first component is a concrete chunk of doorish spacetime and whose second component is an ordered pair whose first component is a slice of spacetime which intersects the doorish chunk and the lever chunk through the chunk of behaving organism (perhaps narrowed by setting up a mechanical device called a detector or cumulative recorder) and whose second component is the leverish chunk.
This ordered pair is abstract in that its existence does not depend on a mental or physical theory: the components are singular descriptions and not singular terms.
That is, the ordered pair exists whether the components do or not.

You can do all of this and bring Skinner's analysis of verbal behavior into the abstract realm of orderd pairs to get something beyond 'x from y on z' with descriptive premises for 'x', 'y', and 'z' (as in Quine's "Free Logic and Virtual Classes" of 1994).
I set this up to shift from propositions to ordered pairs where the theory of ordered pairs is clear: they are identical when their components are.
What both Quine and Skinner do is to make all these excursions to simulate some abstract object templated on a mental or physical analysis moot.
You can skip them entirely and nothing sacred disappears.

It may continue to feel as if the sacred has been paraphrased away, but if you stick with Frege then that would be letting psycholgoy intrude upon philosophy.

### 2025 0427 1326
The word 'sadness' contains 'adn' which is also one of my common mispellings of 'and'.

### 2025 0427 1304 My Concrete Outlook on Frege
I am forgoing my traditional methods of reading Frege for the following reasons.

I can state my outlook on Frege concretely: it is the interpretation of Frege by Quine in "On Frege's Way Out" of 1955 in light of all that Quine later worked out in, e.g., "From Stimulus to Science" of 1996.
What I can do is paraphrase all that can be brought through Frege without breaking our best logical practices.
I have already noted such latter day methods on Twitter when I wrote of schematic theories of predicates of extensionality.
I will do my best to write that up and read from Frege.

The key to this transition was provided by B. F. Skinner's theoretical analysis in "Verbal Behavior" of 1957 which ends with chapters on logic, science, and thinking.
Whereas Quine indulges the attitudinists with their properties, propositions, concepts, information, attributes, and other nonextensional contexts long enough to dismiss them, he does not explain them away (as many have subsequently griped).
Skinner provides the details of such an explanation and there is some path from Quine's method of semantic ascent and Skinner's analysis of autoclitic verbal behavior which covers past efforts by Frege, Russell, Whitehead, and the moderns like Per Martin-LÃ¶f primed by their ill fated allegiance to their fellow attitudenists.

Quine's paper "On Frege's Way Out" points up Frege's position as one where attributes/concepts/propositions/information/essences/properties (and whatever new nonextensional contexts have been cooked up while I was writing this sentence) are secondary to extensional contexts: it is Russell and whitehead who based their theory of extensional contexts of classes on the nonextensional contexts of attributes.

The transition is one from sense to schematics.
It is the one Quine promoted and which, I submit, is also promoted by Frege, though I accept that it is perhaps only through Quine's later work that the paraphrase can be judged as satisfactory and read as perfectly parallel to Frege in Begriffsschrift.
Frege is to be judged solely on Begriffsschrift: the rest of his work is commentary and is like barnacles that must be cleaned from the hull of humanity's greatest achievement.

There is one last reason for the urgency of this all: as verbal machines become more articulate they shall make it hard for most people to cling to nonextensional contexts.
Furthermore, those who dispense with them sooner rather than later, as prescribed by Skinner, are more likely to survive: they can skip intermediate ruminations on attributes.
This saves time and extends the reach of extensioanl contexts which are among our greatest verbal technologies.

---
For those who may not be familiar with the distinction between extensional and nonextensional contexts here is Quine's definition:
>âA context is extensional if its truth value cannot be changed by supplanting a component sentence by another of the same truth value, nor by supplanting a component predicate by another with all the same denotata, nor by supplementing a singular term by another with the same designatum. Succinctly, the three requirements are substitutivity of covalence, of coextensiveness, and of identity, salva veritate. A context is intensional if it is not extensional.â[pg. 90 Quine âFrom Stimulus to Scienceâ].

## 2025 0426

### 2025 0426 1851 Purifying My Little Lisp
This continues my work on my little lisp from [202504211546](#2025-0421-1546).
If you haven't been following along then this is a good place to start, but, like the rest of the notes I've made thus far, it assumes that you know basic javascript and how to define functions by recursion.

Simplification does not defer design decisions: it pushes them out of the problem space entirely.
Upon approaching the design of my little lisp's reader, such simplifications occurred to me and, hence, solved major problems by disolving them rather than resolving them into simpler problems with simpler solutions.
This is a halmark of factoring as a problem elimination process rather than a problem solving process.
A problem can very often be factored out of existence faster than a solution can be found to it.

A pure lisp is one without any atoms.
The empty pair takes over the part played by the item previously designated by 'nil'.
The empty pair shall here be designated by 'theEmptyPair'.
Though I have a theory of ordered pairs in the works, I am committed here to javascript as the lingua franca of the internet.

```
let theEmptyPair={}
, isEmpty = x => x == theEmptyPair
, consOf = (x,y) => [x,y]
, carOf = x => isEmpty(x) ? x : x[0]
, cdrOf = x => isEmpty(x) ? x : x[1]; 
```
Identity is no longer a primitive:
```
let id = (x,y) =>
  (isEmpty(x) && isEmpty(y))
  || (!(isEmpty(x) || isEmpty(y))
     && id(carOf(x),carOf(y))
     && id(cdrOf(x),cdrOf(y)));
```
There is no longer the troublesome distinction between proper and dotted lists, between symbols or runes, and we are free to build such distinctions, if we so desire, from this simpler starting point.

Note, I was reluctant to start with a pure LISP because I am already familiar with the theory of ordered pairs upon which it may be based and had hoped to learn something new from the purported simplicity of more familiar implementations of LISP.
The design of the reader made it clear that there is nothing to be gained from mixing the construction of native and foreign items (the items of javascript and lisp respectively).

This switch to a simpler design also brings my little pure lisp into closer contact with my little concatenative language designed from simplifying Charles H. Moore's uhdForth (it is sad that so far there is no one who has written on uhdForth but me: the reason for this is that almost everything about uhdForth has to be deduced from short yet logically complete presentations in garbled youtube videos).

Under this simplification every item is a rune, a runic list, a proper list, and a number of dotted lists (no greater than the length of the proper list).
Thus, there is no longer a printer, but rather a collection of printers to be used depending on the context of the program as written.
This simplification also switches the emphasis from the printer to the reader: why?

There are no strings that cross the boarder from javascript to the pure lisp: strings are left behind!
Rather than wrestling with problems of peculiar implementations, we are left to establish whatever expedient conventions work for now.
They may be changed later, but such conventions do not change the lisp: they change how we set it up.
It shall later become clear that these conveniences are not about lisp at all, but rather the theory of ordered pairs.
An example of one such theory is [Finitary inductively presented logics](https://math.stanford.edu/~feferman/papers/presentedlogics.pdf) by Solomon Feferman.

There is one kind of item which crosses the boarder between javascript and my lisp which I have not mentioned all this time: the boolean values designated by 'true' and 'false' in javascript.
I've mentioned them in descriptions of past functions, but not as naturalized citizens of my lisp.
The traditional line is to take the empty pair as either the mark of truth or the mark of falsehood.
This can not be done with javascript without digging into late manglings of the language (though it is hard to tell when something that is already mangled is mangled more).
The reason is one which I have also not mentioned overtly, but which I shall mention now: I've been using the javascript ternary conditional notations 'p ? t : f' and hidden versions of the same in 'p && q' and 'p || q'.

In javascript, such expressions control the execution of their subparts e.g. if 'p' designates the same as 'true' then 't' is executed in 'p ? t : f' and 'f' is executed otherwise.
Mixing 'executed' and 'designated' and all such things here has gone against my better principles, but these are not the things I aimed at addressing here so I'm moving on without further comment.

The recursive definition of the function designated by 'id' above is not simple, but the following functions are and they establish a key convention:
```
let singletonListOf = x => consOf(x,theEmptyPair)
, singletonStackOf = x => consOf(theEmptyPair,x);
```
Almost all of the conventions that I shall adopt here are from my concatenative language that I have yet to present here.

The singleton functions provide a simple way of tallying, and hence transforming a native javascript number into a foreign tally:

```
let tallyOf = n => 
  n>0 ? singletonListOf(tallyOf(n-1)) : theEmptyPair;
```

Then, to read an external character is to find its index in some alphabet and take the tally of that:

```
let abc =[...'0123456789abcdefghijklmnopqrstuvwxyz ()']
, alphabeticalIndexOf = letter => abc.indexOf(letter)
, alphabeticalLetterOf = index => abc[index];
```

There is an even more devilish way of going back and forth acrosst he boarder between javascript and my little lisp.
Rather than use tallies, a bit based method can be introduced e.g. only a bit at a time crosses the boarder one way or the other.
I leave that for a few code fragements later.

The old names for functions can now be used anew (after introducing the familiar string functions under slightly different names):

```
let theEmptyLetter=''
, isEmptyLetter = x => x == theEmptyLetter
, concatenationOf = (...strings) => 
   strings.length ? strings.shift() + concatenationOf(...strings) : theEmptyLetter
, firstLetterOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters[0]
, restLettersOf = letters => isEmptyLetter(letters) ? theEmptyLetter : string.slice(1);
```
Oops! Forgot the inverse function to the one that makes tallies:
```
let countOf = x => isEmpty(x) ? 0 : 1 + countOf(cdrOf(x));
```
Now old names can be used anew!
```
let runeOf = letter => tallyOf(alphabeticalIndexOf(letter))
, runicListOf = letters => 
   letters.length ? consOf(runeOf(firstLetterOf(letters))
    , runicListOf(restLettersOf(letters)))
   : theEmptyPair
, letterOf = x => letterAtIndexOf(countOf(x))
, stringOf = x =>
   isEmpty(x) ? theEmptyString
   : concatenationOf(letterOf(carOf(x)),stringOf(cdrOf(x)));
```
Some much needed examples:
```
countOf(tallyOf(3)) 
 3
letterOf(runeOf('a')) 
 a
lettersOf(runesOf('this is a test')) 
 this is a test
```
Now for some strange examples that more closely reveal what I hinted at long ago about the joys of letting degenerate cases flourish:
```
letterOf(tallyOf(3)) 
 3
lettersOf(runeOf('a')) 
 0000000000
countOf(runesOf('this is a test'))
 14
```
For those who may have been put off by the definitions of the functions designated by 'tallyOf' and 'countOf' since they seemed to favor singleton stacks over singleton lists, I hope the last example calms your nerves: these conventions are selected by their native logic in javascript (and in any other language which purports to cover arithmetic and concatenation for that matter).
Said poetically: economize on externalities.

A further consequence of these simplifications is that I can bring together the work done from [2025 0413 1513 Bit Strings and Binary Trees](#2025-0413-1513-bit-strings-and-binary-trees) and explain how to cross the boarder between langauges bit by bit without leaving our native language:

```
let theEmptyStack = theEmptyPair 
, isEmptyStack = isEmpty
, pushOf= (stack,item) => consOf(stack,item)
, popOf = carOf
, peekOf = cdrOf
, topOf = stack => peekOf(stack)
, secondOf= stack => peekOf(popOf(stack))
, pop2Of = stack => popOf(popOf(stack))
, pairUpOf = stack => pushOf(pop2Of(stack),consOf(secondOf(stack),topOf(stack)))
, pairUpEachOf = stack =>
  isEmpty(popOf(stack)) ? topOf(stack)
  : pairUpEachOf(pairUpOf(stack))
, encodeHelper = (digits, stack) =>
  isEmptyLetter(digits) ? pairUpEachOf(stack)
  : isZeroDigit(firstLetterOf(digits)) ?
     encodeHelper(restLettersOf(digits), pushOf(stack,theEmptyPair))
  : isOneDigit(firstLetterOf(digits)) ?
     encodeHelper(restLettersOf(digits), pairUpOf(stack))
  : encodeHelper(restLettersOf(digits),stack)
, encode = digits => encodeHelper(digits,theEmptyStack);
```
with some examples (notice the last one)
```
decode(encode('001')) 
 001
decode(encode('0000')) 
 0000111
decode(encode('10100')) 
 001010011
decode(encode('00101011')) 
 000101011
decode(runeOf('a')) 
 000000000001111111111
lettersOf(encode(decode(runesOf('this is a test')))) 
 this is a test
```

That's enough for today.

### 2025 0426 1845 To and From Predicate Functor Logic
This uses notation from [A Stack Notation for Predicate Functor Logic](#a-stack-notation-for-predicate-functor-logic-2025-0414-1626).

Here I give a few quick examples of how to translate a sentence of quantificational logic to one of predicate functor logic.
As I finish up my little lisp I'll write out mechanical algorithms for accomplishing each of these steps and look for a few shortcuts.

As an aside, it is possible to reduce all of predicate logic to just three predicate functors:

1. '..xy(drop F)..z' for '..xF..z'
2. '..xyz(hem F)..a' for '..xyzFy..a'
3. '..x(F huh G)a..b' for 'some item is {y such that not (..xayF..b and ..xayG..b)}'

I'll save that reduction for another time maybe.

The basic predicate functors in this presentation of homogenization will be then

1. '...xy(drop F)...z' for '...xF...z'
2. '...wxy(hem F)...z' for '...wxyFx...z'
3. '...x(push F)y...z' for '...xyF...z'
4. '...x(not F)...y' for 'not ...xF...y'
5. '...x(F and G)...y' for '...xF...y and ...xG...y'
6. '...x(some F)...y' for 'some item is (z such that ...xzF...y)'.

In the linked note I show how to get 

7. '..xy(dup F)..a' for '..xyyF..z'
8. '..xy(pop F)..a' for '..xFy..a'
9. '..xyz(swap F)..a' for '..xzyF..a'

from those basic six starting functors.
With the appropriate combination of swaps, pops, and pushes, it is possible to move the variables of any predicate into any position:

10. '..xyz(swop F)..a' for '..xyz(swap pop F)..a' i.e. '..xzFy..a'
11. '..x..yz(ret^n F)..a' for '..x..yz(swop^n push^n F)..a' i.e. '..xz..yF..a' where '..y' is a list of n variables.

Note, for a predicate functor designated by 'f' the notation 'f^0' designates the same as 'f' and 'f^(n+1)' designates the same as 'f^n f' so that each predicate functor issues in its iterates in the expected way.

The pidgin 'ret' is short for 'retrojection' which I take from Quine's explanation of the permutational part of homoginization.

Now  predicates in quantificational logic are said to be of degree n when they have n variables attached to them (most often on the right hand side) e.g. 'Fxyxxzu' is of degree six and 'Gyxyxy' is of degree five.

The first step in translating a closed sentence of quantificational logic into a predicate functor is to attach the degree of each predicate in the sentence to its name as a numeral (this does not actually involve arithmetic in the notation any more than counting the number of variables attached to a predicate of quantificational logic does)e.g. 'Fxyxxzu' becomes 'F6xyxxzu' and 'Gyxyxy' becomes 'G5yxyxy'. This must be done for each predicate in the sentence being transformed. Sentence letters (e.g., often 'p', 'q', 'r') get carried over to degree zero predicates.

Next, to simplify matters, before or after attaching these numerals, the sentence is to be put into one of its equivalents which is compounded of only existential quantifiers, conjunctions, and negations.

Now, negations get taken over simply enough by bringing it into predicate functor form e.g. 'not Fxyz' becomes '(not F3)xyz' and 'some item is {x such that (not Fxyz) and (not Gux)}' becomes 'some item is {x such that (not F3)xyz and (not G2)ux}'.

Where ever there are duplicate occurrences of a variable they can be eliminated by retrojections, pops, pushes, and dups. A mechanical way of doing this to alphabatize the variables attached to a predicate and to put it in stack form (which favors the left hand side of a predicate) e.g. 'Fxyxzyz' becomes 'F6xyxzyz' then 'xxy(swap pop push^3 F6)zyz' to 'xxyyzz(pop swap pop^2 swap push^3 F6)' then to 

'(push dup push dup push dup pop swap pop^2 swap push3 F6)xyz'

which avoids retrojections and is adhoc really.

Now assuming this is done then it is only homogenization that is left and it only occurs when confronted by conjunctions e.g. given 

'Faxy and Gbz'

missing letters can be brought in by 'drop' e.g.

'bz(drop^2 F3)axy and ax(drop^2 G2)bz'

and alphabetized

'(..f drop^2 F3)abxyz and (..g drop^2 G2)abxyz'

by some appropriate sequence of functors designated by '..f' and '..g' respectively: the functors '(..f drop^2)' and '(..g drop^2)' are said to homogenize 'F' and 'G' respectively. This can obviously be generalized to connectives of more components than the two of primitive conjunction. Finally then, the 'and' is brought in so that

'((..f drop^2 F3) and (..g drop^2 G2))abxyz'.

Last, but not least, the variable of an existential quantifier is brought to the top of the left stack by appropriate pops, then 'some' is brought in e.g.

'some item is {z such that Fxyz}'

becomes 

'some item is {z such that xyz(pop^3 F3)}'

and then

'xy(some pop^3 F3)'.

Together these complete the translation to and from predicate functor logic.

## 2025 0425

### 2025 0425 2042 Reading the Frege Reader: The First Sentence
This is a record of my reading of "The Frege Reader" edited by Michael Beany.
It was recommended to me by R.P. (@ResonantPyre).
Since this is my first public reading of a primary text (I do not read secondary texts--- like I [have](#2025-0420-2247) Durant's "Story of Philosophy"--- as I might Plato's dialogues) I shall say a bit on what I do when I read.

First, this is a record of my verbal responses to my exposure to the text.
To read is to do more than listen with my eyes.
It is built upon gazing at the page and the marks upon it.
The narrower repertoire of looking, upon which the complex repertoire of seeing is built and upon which reading is built, bring the marks into focus and I can begin to see words, perhaps letters, sentences, and, the key feature of seeing, all that I see to myself which is not to be found on the page or in its marks.

To read is to go beyond seeing as listening with my eyes.
It is speaking in response to sights of, sounds of, feelings of, etc.
By uncovering what I have to say I am more likely to discover what I am thinking and feeling.
There is still much I have to learn about how best to grasp my thinking, my feeling, and what of past and present environments may be controlling it now.

Without further ado!

I opened to the page with all the publishing informatin on it.
It was first published in 1997.
Immediately, the first name of the dedication caught my eye "Peter Geach".
It occurred to me that he is mentioned by Quine, perhaps in "From stimulus to Science" where Quine discusses pronouns of laziness.
After checking the page (I can see to myself where in the book Quine would have written about that), it is found that yes, Geach is where Quine got the phrase "pronouns of laziness" (they are, for Quine, a focal point of reification).

Now I turn to the contents of "the Frege reader", and, as I very often do, I write them down:

1. Begriffsschrift (1879): Selections (preface and Part I)
2. 'Letter to Marty, 29.8.1882'
3. The Foundations of Arithmetic (1884): Selections (introduction and $$1-4, 45-69, 87-91, 104-9; with summaries of the remaining sections)
4. 'Function and Concept' (1891)
5. 'Letter to Husserl, 24.5.1891': Extract
6. 'On Sinn and Bedeutung' (1892)
7. '[Comments on Sinn and Bedeutung]' (1892)
8. 'On Concept and Object" (1892)
9. Grundgesetze der Arithmetik, Volumen I (1893): Selections (Preface, introduction, $$1-7, 26-29, 32-33)
10. 'Review of E. G. Husserl, Philosophie der Arithmetik I' (1894): Extract
11. 'Logic' (1897): Extract
12. 'On Euclidean Geometry' (c. 1900)
13. 'Letter to Russell, 22.6.1902': Extract
14. 'Letter to Russell, 28.12.1902': Extract
15. Grundgesetze Der Arithmetik, Volumen II (1903): Selections ($$55-67, 138-47, Appendix)
16. 'Letter to Russell 13.11.1904': Extract
17. 'Introduction to Logic' (1906): Extract
18. 'A brief Survey of my Logical Doctrines' (1906): Extract
19. 'Letters to Husserl, 1906'
20. 'Logic in Mathematics' (1914): Extract
21. 'Letter to Jourdain, Jan. 1914': Extract
22. 'My Basic Logical Insights' (c. 1915)
23. 'Thought' (1918)
24. 'Negation' (1918)
25. '[Notes for Ludwig Darmstaedter]' (1919)
26. 'Sources of Knowledge of Mathematics and the Mathematical Natural Sciences' (1924/5): Extract
27. 'Numbers and Arithmetic' (1924/5)

It contains selections from his larger works which I shall get at a later time (though I do wish there was a single complete volume with all his texts and relevant major correspondances).
They are presented in chronological order and are mostly extracts: I do not yet know what the difference is between an extract and a selection.

Next, the preface, though I turned to appendix 2 on Frege's logical notation to take a look at its layout.
I take Frege's notation as one of the greatest accomplishments in human history and was likely drawn to it from how often I ahve been reinforced by it in the past.

The preface says that Frege's works can not be allowed to "speak for themselves" to a student for there are subtleties that may easily go unnoticed but which are key to firmly grasping Frege's collective works.

I am impatient and shall skip any further introductory matters and go straight to Frege's first work "Begriffsschrift: a formal language of pure thought modeled on that of arithmetic".
Again, I am met with more commentary that I simply skip to get to Frege's "preface":

> "The recognition of a scientific truth generally passes through several stages of certainty."[pg. 48]

Already, the developmentalistic metaphors which plague psychology and the other nonbiological sciences appears: "passes through several stages" just as a child is said to pass through grades from kindergarten onwards and upwards.
For those interested in precise and accurate reports on changes in human behavior--- which 'recognition' purports to be--- it is not the passage of time between stages that matters (or, the metaphorical extension of the same to "certainty"), but rather what happens as time passes.

In the end, any developmentalist, when pressed, can give some description of purportedly characteristic features of any performance to be found within a deliniated stage.
Whether such characteristic descriptions are maintained or whether a given sequence of corresponding stages seperated by changes in characteristic performances are maintained is a problem of statistical aggregation.
For one aimed at something more foundational than flexable predictions relative to the overlap of an individual's behavior among differing performances in differing stages of development there is little of consequence to be kept besides the vague gesturings to unexamined environments.

It is the 'recognition' that "generally passes through stages of certainty", not the "scientific truth" itself.
Might "recognition" just as well be replaced by "cognitition"?
Are there nonscientific truths?
Is this law of recognition itself subject to a staged development?
If so, what stage is it in?
What stages are there?
Is it a spectrum from uncertainty to certainty as the word "certainty" suggests?
Is the generality here scientific or logical?

Following Skinner's principle "Etymology is the archeology of thought." from his paper "The Origin of Cognitive Thought" I shall show the etymology of "recognition" back to its proto-indo-european roots and say a bit more about why this is relevant: (each of the following selections are from <https://www.etymonline.com/> !)

1. recognition(n.)--- mid-15c., recognicion, "knowledge (of an event or incident); understanding," from Old French recognition (15c.) and directly from Latin recognitionem (nominative recognitio) "a reviewing, investigation, examination," noun of action from past-participle stem of recognoscere "to acknowledge, know again; examine" (see recognize).

   Sense of "acknowledgment of a service or kindness done" is from 1560s. Sense of "formal avowal of knowledge and approval" (as between governments or sovereigns) is from 1590s; especially acknowledgement of the independence of a country by a state formerly exercising sovereignty (1824). The meaning "a knowing again, consciousness that a given object is identical with an object previously recognized" is by 1798 (Wordsworth). The literary (especially stage) recognition scene "scene in which a principal character suddenly learns or realizes the true identity of another character" is by 1837 (in a translation from German).

2. recognize(v.)--- early 15c., recognisen, "resume possession of land," a back-formation from recognizance, or else from Old French reconoiss-, present-participle stem of reconoistre "to know again, identify, recognize," from Latin recognoscere "acknowledge, recall to mind, know again; examine; certify," from re- "again" (see re-) + cognoscere "to get to know, recognize" (see cognizance).

   With ending assimilated to verbs in -ise, -ize. The meaning "know (the object) again, recall or recover the knowledge of, perceive an identity with something formerly known or felt" is recorded from 1530s.

3. cognizance(n.)--- mid-14c., conisance, "device or mark by which something or someone is known," from Anglo-French conysance "recognition," later, "knowledge," from Old French conoissance "acquaintance, recognition; knowledge, wisdom" (Modern French connaissance), from past participle of conoistre "to know," from Latin cognoscere "to get to know, recognize," from assimilated form of com "together" (see co-) + gnoscere "to know" (from PIE root *gno- "to know").

   Meaning "knowledge by observation or notice, understanding, information" is from c. 1400. In law, "the exercise of jurisdiction, the right to try a case" (mid-15c.). Meaning "acknowledgment, admission" is from 1560s. The -g- was restored in English spelling 15c. and has gradually affected the pronunciation, which was always "con-." The old pronunciation lingered longest in legal use.

4. *gno-, Proto-Indo-European root meaning "to know." It might form all or part of: acknowledge; acquaint; agnostic; anagnorisis; astrognosy; can (v.1) "have power to, be able;" cognition; cognizance; con (n.2) "study;" connoisseur; could; couth; cunning; diagnosis; ennoble; gnome; (n.2) "short, pithy statement of general truth;" gnomic; gnomon; gnosis; gnostic; Gnostic; ignoble; ignorant; ignore; incognito; ken (n.1) "cognizance, intellectual view;" kenning; kith; know; knowledge; narrate; narration; nobility; noble; notice; notify; notion; notorious; physiognomy; prognosis; quaint; recognize; reconnaissance; reconnoiter; uncouth; Zend.

    It might also be the source of: Sanskrit jna- "know;" Avestan zainti- "knowledge," Old Persian xÅ¡nasatiy "he shall know;" Old Church Slavonic znati "recognizes," Russian znat "to know;" Latin gnoscere "get to know," nobilis "known, famous, noble;" Greek gignÅskein "to know," gnÅtos "known," gnÅsis "knowledge, inquiry;" Old Irish gnath "known;" German kennen "to know," Gothic kannjan "to make known."

Most people are stuck on linguistics and philosophy when trying to figure out what something means.
The science of behavior skips over them and goes straight to the contingencies (a contingency is a consequence from response on occasion e.g. a door opens from a push on a lever) of which we now only remotely speak.
It is through "The Diversions of Purely" by John Horne Tooke that Skinner and so many others have come to better appreciate the role of etymology in the analysis of verbal behavior.

The control that each part of a text has over the responses of a reader are almost entirely the result of etymology as the history of the characteristic consequences of verbal responses of given form on characteristic occasions.
We now know that the wheel, horses, and most language descend from the bronze-age peoples of the eurasian steppes (see "The Horse, the Wheel and Language" by David W. Anthony 2007).
Most cultures of the world remain under the control of circumstances which are not very different from those of the bronze age people, and each word of a modern language which purports to mention or refer to a mentalistic object such as an idea or thought can be traced through to its PIE root which reveals the concrete contingencies from which we speak though we are oh so remote from such ancient speakers.

Thus, going from "recognize" to "*gno-" and back again through "can" and "could" as "have power to", "be able to", or through "acquaint" as "to be near to" or "to bring near" or "intimate with", we reach out to the concrete contingencies which control the verbal behavior that is a consequence of them.
B. F. Skinner has the following to say as an introduction to "knowing":

> "We say that a newborn baby knows how to cry, suckle, and sneeze. We say that a child knows how to walk and how to ride a tricycle. The evidence is simply that the baby and child exhibit the behavior specified. Moving from the verb to the noun, we say that they possess knowledge, and the evidence is that they possess behavior. It is in this sense that we say that people thirst for, pursue, and acquire knowledge.
> 
> But this brings us at once to the question of what it means to possess behavior. ... to say that a response is emitted does not imply that it has been inside the organism. Behavior exists only when it is being executed. Its execution requires a physiological system, including effectors and receptors, nerves, and a brain. The system was changed when the behavior was acquired, and it is the changed system which is "possessed." The behavior it mediates may or may not be visible at any given moment. There are parallels in other parts of biology. An organism "possesses" a system of immune reactions in the sense that it responds to invading organisms in a special way, but its responses are not in existence until it is being invaded. It is often useful to speak of a repertoire of behavior which, like the repertoire of a musician or a company of players, is what a person or company is capable of doing, given the right circumstances. Knowledge is possesed as a repertoire in this sense."[pg. 152 Skinner "About Behaviorism"]

And then, jumping ahead to Skinner's section "Knowledge as Power and as Contemplation"

> Much of what is called contemplative knowledge is associated with verbal behavior and with the fact that it is the listener rather than the speaker who takes action. We may speak of the power of words in affecting a listener, but the behavior of a speaker in identifying or describing something suggests a kind of knowledge divorced from practical action.
Verbal behavior plays a principal role in contemplative knowledge, however, because it is well adapted for automatic reinforcement: the speaker may be his own listener.
There are nonverbal behaviors having the same effect.
Perceptual responses which clarify stimuli and resolve puzzlement may be automatically reinforcing.
"Getting the meaning" of a difficult passage is similar.
... Contemplation of this kind would be impossible, however, without a previous exposure to contingencies in which action is taken and differentially reinforced."[pg. 155-156 Skinner "About Behaviorism"]

So it is that I return to the first sentence from Frege

> "The recognition of a scientific truth generally passes through several stages of certainty."[pg. 48]

The word "recognition" is replaced with "*gno-" and the rest comes through a careful analysis of what people do when they are said "to know".
It is not passing through "several stages of certainty" that strengthens the possession of a scientific truth as a kind of knowledge or cognitition or even a conception (of which I expect there to be much talk later), but rather the concrete contingencies to which the knowing organism is exposed and which result in the contemplative knowing which purportedly reaffirms itself as in automatic reinforcement of a trained "knower of scientific truths".

I shall not take so long with each sentence as I submit that most of them cluster around a few threads of etymology that bring us back to the science of behavior and then follow from there.
There is much more to go yet I shall leave this here for now.

Tomorrow I may even get to the second sentence :)  

### 2025 0425 2005 Some Origins of My Interst in the History of the World
My interest in the history of the world is largely the result of my interest in the history of science and technology.
Technologies are concrete artifacts of cultural practices.
They mediate conspicuous behaviors and are an easy access point to the study of social/cultural selection and variation.

The sewing machine is my goto example.
The mechanization of the behaviors which once dominated the practices of sewing make it so that the consequences of sewing appear far more often without the direct participation of individaul sewers.
Repertoires of sewing are almost extinguished as the remote control of sewing machines eliminates the contingencies that once taught the people of the world to sew.

Washing machines are perhaps better known for the labors they are said to save.
While it may come easy to us to say that labors are saevd by washing machines, especially when the push of a button has the same consequences which were once the result of a long chain of responses, it is more accurate to say that a change in the environment changed prevailing behaviors and to seperate such changes from teh value judgements that are so quickly provided by a majority of our ethical practices.

The primary reason to stick with an accurate report of behavior rather than amending it with a value judgement is that the ethical practices which bring such judgements to the judge are not yet themselves the result of a conclusion from contemplation on accurate templates of the world.
More than that, they are very rarely so: the science of behavior is new and has had little to no effect on prevailing ethical practices in any of the cultures of the world.
It is to history that I look to see the changes in cultural practices that shaped modern ethical practices specifically and human practices generally.

Our world is now one with much more than just sewing and washing machines.
We now have verbal machines that do for verbal behavior what sewing and washing machines do for sewing and washing.
Fewer people are required than ever before to respond verbally in ways which once prevailed.

Computers have long since provided remote listeners (called electrical/computer engineers) with control over the behavior of computer programmers.
Circuits, once elaborate networks of relays, mediate the consequences of a listerner's response to the speech of programmers: the programmer sets up bits as a consequence of the verbal responses we call "computer programming" and the computer mediates the consequences of having exposed teh circuit designer to those bits without the circuit designer having any direct contact with the programmer as a contemporaneous listener.

This may all seem needlessly detailed or obtuse to anyone unfamiliar with the science of behavior and, specifically, the experimental analysis of behavior in laboratory environments.
Many are apt to say that what I wrote above is already a functional part of the vernacular and goes without saying.
But, it can not be so, especially in light of human history, where we so often find similarly authoritative proclamations about this or that kind of control over the world and our ethical responses to it.
The science of behavior has had little to no effect on the practices of programming, much less on our ethical practices in response to them.

So it is that my interest in history is the systematic reevaluation of reports on history in light of the science of behavior through the experimental analysis of behavior.


### 2025 0425 1943

R.P. (@ResonantPyre) has given me a few great book recommendations over the past few months:

* "The English and their History" by Robert Tombs
* "Wandering Significance" by Mark Wilson
* "A History of Philosophy without any gaps" by Peter Adamson (of which there are seven, for now)
* "The Frege Reader" edited by Michael Beaney

I have not yet bought "The English and their History".
Wilson's "Wandering Significance" was the fist that I bought up on R.P.'s recommendation.
It was recommended as a result of some conversations we had on Quine and physics.
Wilson is not the kind of writer that I'm used to: he takes a winding path around every single point rather than sticking it in your face.
One of the reasons I so love B. F. Skinner's writings is that he goes out of his way to stick your nose in everything he has to say.

"Wandering Significance" is presented as a very long essay on the origins and operations of meaningful language (though it is often much much more than that).
There was very little of it that I read and am likely to repeat: specific examples in physics didn't line up with what I had repeatedly witnessed physicists do (inside the lab and outside it).
May sections felt like the long delays you see in a movie where a person is rushing to finish a homecooked meal before the jig is up.
Though I may come back to it later, what little it has to say about Quine and Skinner combined with its strange outlook on the practices and behaviors of phycisists left me uninterested in reading more.
That has not stopped me in the past though, so the future of this book is open.

I just got the first volume of "A history of philosophy with gaps" on "classical philosophy" by Adamson.
I've flipped through some pages, read an introduction, and was excited to see that there were many references to primary texts that were prominently placed and mentioned.
This made me very happy: I am looking for a more comlpete and comprehensive yet singularly organized outline of major philosophers and their texts and this may just be it.

As much as I enjoy clicking through a few Wikipedia articles to jog my memory when I've fogotten some detail I once knew, I can not recommend it for anything much more than that.
This seems to especially be the case when it comes to philosophy.
There's just too much that people have said, and too little cohesion between entries.
As I get through Durant's "Story of Civilization" I'll have much more to say about monolithic efforts that have a kind of grand unification without presumming cosmic authority (this problem was already brought up from my reading of Bourbaki which is a series built by committee but governed by logical and mathematical practices which do much of the heavy lifting when it comes to cosmic unifications and simplifications).

My readings of Frege are limited to "The Foundations of Arithmetic" and what can be found in "From Frege to Godel" (a text that any student of logic must have: it is truly indispensible).
"The Frege Reader" says it covers his major writings: I still must buy his other books to read them in full.

Books are some of my favorite things.

### 2025 0425 1925
While I had hoped to write every day without breaking my streak, it has been two days since I last wrote something here.
My only excuse, if there is even a need for one (which I think there is not), is that I had some wonderful conversations with people about things I will soon be writing on (I aim to summarize two of the conversations I've had), but also I have been recovering from a workout injury.

For most of my life I did little more than math, physics, piano, and clarinet.
As I have grown older I have made more friends than I ever imagined.
Some of them are so kind and so generous with their time that they teach me new things.
A few months ago I was lucky enough that a family friend had found a new way to spend his retirement by being my weightlifting coach.
He spent most of his life weightlifting and competing in Highland games and he had a gym in his basement.

All of these things came together and because of what he has learned through his life I have been slowly strengthening my body in ways similar to how I have strengthend my mind.
Sadly, I swung a kettlebell the wrong way and was given an a cute pain in my back which I am still recovering from.
Thankfully, it has gotten better by the day and I have found it easier and easier to get into and out of chairs.

I let myself indulge in a sorta lavish relaxation all in the name of "letting my back heal".
Though I did not stop writing and reading, I did stop working here at the computer keyboard.
It will be interesting to see what other gaps occur in my writings as the blizzard of life envelops me anew each day.

## 2025 0422

### 2025 0422 2322 An Incomplete Sketch of My Philosophy of Logic
The signifcance of Quine's predicate functors can not be understated by me. He showed, by way of predicate functors, that variables are not a neccessary part of predicate logic, and hence, baring the embrace of some weaker practices purporting to undermine predicate logic, no logical theory involves variables neccessarily.

Thus, given a theory and its predicate logic, the predicate functors of Quine give a way, once and for all, to rid ourselves of the lgoical import of pronomial cross reference. This eliminates, then, all problems which turn up from such cross referencing e.g. all problems of scope and bondage.

It also turns our focus back to the probelms to solve: what is the logic of the theory under investigation? In predicate (functor) logic this means settling what predicates belong to the lexicon of the theory. The requirement that the lexicon be mentioned by *listing* the predicates is preformally stated as "the lexicon is finite", though the emphasis here is on the preformality of that statement otherwise the invocation of the predicate 'is finite' quickly demands its own logical theory.

Now, with the lexicon given, the paucity of premises is among the next steps. I say "among the next steps" because it is often through a sequence of steps that the predicates and premises of a given theory evolve from the conclusions of their implication.

It is here, at implication, where the supremacy of predicate logic is without question except for those who are wedded to deviant methods that always liven the outer realms of any inquiry.

But, let me linger here lest I be seen as some immoral logical supremacist! Thanks to Quine's main method, implication can be explained entierly by two short sentences "for those abreast of the jargon, it is as follows. To prove that a given set of premises implies a contemplated conclusion, prove that the premises are inconsistent with the negation of that colclusion. Do so by putting the premises and the negated conclusion into prenex form and then accumulating a truth functional inconsistency by persistent instantiation of the universal and existential quantifiers, taking care to use a new variable for each existential instantiation."[pg. 51 Quine "From Stimulus to Science"].

Whatever else anyone says they *actually mean* by implication can only be grasped natively by foreign import i.e. paraphrase into a logical theory as above.

This is staunch. It is also repulsive to many. I accept all such responses to such declarations as relevant to a full theory of logical and scientific behavior. What can not be accepted are methods which, e.g., break extensionality of a theory by recourse to some deviant logic: "A context is *extensional* if its truth value cannot be changed by supplanting a component sentence by another of the same truth value, nor by supplanting a component predicate by another with all the same denotata, nor by supplementing a singular term by another with the same designatum. Succinctly, the three requirements are substitutivity of covalence, of coextensiveness, and of identity, salva veritate. A context is *intensional* if it is not extensional."[pg. 90 Quine "From Stimulus to Science"].

Without extensionality, intersubjective agreement by qualified witnesses is verbally bankrupt. It is already hard enough to grasp a theory with its necessary (but not sufficient) predicate logic. But, to do so without predicate logic is to plunge the world into the caprice of uncontemplated cooperation. There can be no verbal coordination without extensionality.

Note, none of this says that a theory is its predicate logic. Whatever more a theory may be is hard to say, in the same way that it is hard to say when a given definition of verbal behavior is appropriate.

With all of that wawa by the way, let me give you a quick path from familiar and traditional quantificational logic through Quine's predicate abstracts (and their principle of concretion), to predicate functors so that you might more firmly grasp where I am coming from.

First, take '{x:Fx}y' as short for 'some item is {u such that u=y and some item is {x such that u=x and Fx}}'. Elsewhere I may come back and explain how it comes that the two place predicate of identity, written as '=' short for 'is identical to', is indiscernability in that they are coextensive and indiscernability is in hand from any lexicon like those here contemplated e.g. in a lexicon with one one place prediate 'F' and one two place predicate 'G' (these are not to be taken as schematic predicate letters as are used in the remainder) 'x is indiscernable from y' is short for 'Fx if and only if Fy, and each item is {u such that Gux if and only if Guy, and Gxu if and only if Gyu}'.

The English relative clause 'who loves Dick' and the pidgin 'x such that x loves Dick' are uniformly paraphrased by the *predicate abstract* '{x: x loves Dick}' which *abstracts* 'Tom' from 'Tom loves Dick' by *binding* the *free* occurrence of 'x' in the *open* sentence 'x loves Dick' with the prefix 'x:' so that the *predication* '{x:x loves Dick}Tom' *concretes* to 'Tom loves Dick': whatever can be said of a thing can be said by predicating a predicate of it i.e. *predicational completeness*.

Predicate abstracts as the logical import of the relative clause are the import of pronomial reference and respective problems of freedom and bondage.

The principle of concretion comes along for free as follows (assuming either that the lexicon of the language is finite or there is a predicate of identity provided subject to the following familiar constraints in either case).
Godel's premises of identity are 1) identity is reflexive, i.e. 'x=x', and 2) identity is substatutive, i.e. each instance of the schema of substitutivity 'x=y and Fx, only if Fy'. They are equivalent to Wang's premises, i.e. each instance of Wang's schema 'Fx iff some item is {y such that x=y and Fy}'. By two instances of Wang's schema, 'some item is {u such that u=y and Fu} iff some item is {u such that u=y and some item is {x such that u=x and Fx}}' and 'some item is {u such that u=y and Fu} iff Fy', '{x:Fx}y iff Fy' is true.

In a truth functional and quantificational logic of predicates without a predicate of identity '{x:Fx}y' is introduced by the schema of concretion '{x:Fx}y iff Fy'. Concretion ties relative clauses as predicate abstracts into a truth functional and quantificational logic of predicates.

Predicate abstracts of many places are defined with the help of 
'..x' for 'x.0 x.1 .. x.length(x)-1' where 'x' is a list of variables and 'x..y=u..v' for 'x=u and ..y=..v' (the case where 'y' and 'v' are of different lengths is handled by '..x=..y' for 'x.0..x.min(len(x),len(y)) = y.0..y.min(len(x),len(y))' so that
'{..x:F..x}..y' for 'some item is {..u such that ..u = ..y and some item is {..x such that ..u = ..x and F..x}}'
with the schema of generalized concretion following as
'{..x:F..x}..y iff F..y'.

Predicate abstracts like '{xy:Fxy}' are predicates just like 'father (of)' or 'older than'. They are not items like sets or relations. It so happens that extensionality along with a few meager assumptions on the existence of sets-- e.g. that they yield a serviceable theory of ordered pairs-- often permits predicate abstracts to moonlight as designators of abstract items like sets. Nothing like that is found here, but shall eventually be found elsewhere when I eventually get to that.

(Note that the present methods do not incorporate the stack based notation that I've recently uncovered. I have yet to carry that efficiency through this sequence of arguments.)

The basic recombic predicate functors can now be introduced as abbreviations for the appropriate predicate abstract:

* Major Padding) 'Pad F' or 'drop F' for '{x..y:F..y}'
* Minor Padding) 'pad F' for '{..xy:F..x}'
* Reflection) 'refl F' or 'dup F' for '{x..y:Fxx..y}'
special cases e.g. the relfection of a one or no place predicate is that predicate
* Permutation [needs a better name]) 'Perm F' for '{xy..z:Fx..zy}'
* Major Inversion) 'Inv F' for '{x..y:F..yx}'
* Minor Inversion) 'inv F' for '{xy..z:Fyx..z}'
* Retrojection) 'Ret.k F' for '{x..y..z:F..yx..z}' when length(y)=k

The basic logical predicate functors are then

* Alternative Denial) 'F nand G' for '{..x: not(F..x and G..x)}'
* Complement) 'comp F' or 'not F' or '-F' for 'F nand F'
* Alternation) 'F or G' for '(not F) nand (not G)'
* Joint Denial) 'F nor G' for 'not(F or G)'
* Complementary Conditional) 'F not only if G' for 'F nor not G'
* Conditional) 'F only if G' for 'not (F not only if G)'
* Converse Conditional) 'F if G' for 'G only if F'
* Complementary Converse Conditional) 'F not if G' for 'not(F if G)'
* Conjunction) 'F and G' for 'F not if (not G)'
* Biconditional) 'F iff G' for '(F only if G) and (F if G)'
* Exclusive Alternation) 'F xor G' for 'not(F iff G)'
* Minor Existential) 'some F' for '{..x:some item is y such that Fy..x}'
* Major Existential) 'Some F' for 'some^n F' where 'F' is an n place predicate and for a predicate funtor 'f' the iterates are 'f^1' for 'f' and 'f^(n+1)' for 'f^n f'
* Minor Universal) 'each F' for 'not some not F'
* Major Universal) 'Each F' for 'not Some not F'
* Inclusion) 'F => G' for 'Each(F only if G)'
* Converse Inclusion) 'F <= G' for 'G => F'
* Proper Inclusion) 'F > G' for '(F => G) and not (F <= G)'
* Converse Proper Inclusion) 'F < G' for 'G > F'
* Coextension) 'F <=> G' for '(F <= G) and (F => G)'.

Now for the predicate functors that are usually introduced as part of set theories:
* k Place Composition) 'F^m .k G^n' or 'F^m of^k G^n' for 'some^k ((Inv^(m-k) drop^(n-k) F) and (Inv^k drop^(m-k) Inv^(n-k) G))'
* Image) 'F^m " G^n' or 'F on G' for 'F of^n G'
* item) 'item' or 'U' (or sometimes '1' or 'the universe of discourse') for 'dup =' or 'refl ='
* void) 'nonitem' 'void' or sometimes '0' for 'not item' or '-U'

Some of the predicate functors from the theory of relations
* Symmetric) 'Sym F' for 'F <=> (swap F)'
* Asymmetric) 'Asym F' for 'F <= (not swap F)'
* Transitive) 'Trans F' for 'F => (F of F)'
* Intransitive) 'Intrans F' for '(not F) => (F of F)'
* reflexive) 'reflexive F' for 'each dup F'
* equivalence) 'equiv F' for '(reflexive F) and (trans F) and (symm F)'

therefore 'equiv =' is true. The next functors bridge something like the gap between "the relational part of a set" and their origins in predicate logic:

* two place part) 'F`' for 'some^(m-2) Inv^2 F'
* k place part) 'F`k' for 'some^(m-k) Inv^k F'
Further generalizations include slices of length k, and noncontiguous parts

Unlike relations, the complement '-F' of a two place predicate 'F' is a two place predicate, and '-F' is equivalent to '(-F)`', but where predicates come to do doulbe duty as names of their extension, the complement of a relation need not be the two place part of its complement. This is a tiny example of the way in which theories like the calculus of relations obscure their (stronger) logic. With the generalization of the k place part comes the generalization of composition. Is it possible to introduce the k place part from kplace composition cross products and confinements?

* cross product) 'F cross G' for (pad^n F) and (Pad^m G)'
* iterated cross) 'F^1' for 'F' and 'F^(k+1) for 'F cross F^k'
* right confinement) 'F]G' for 'F and (U^(m-n) cross G)'
* left confinement) 'F[G' for 'F and (G cross U^(m-n))'

component confinement follows from iterated left and right confinements. Together a left and right confinement is a sliced confinement

The following definitions introduce functional predicates and all that is often said to follow from theories of functions. Here, generalized composition is avoided, and, hence, the predicates operated on by the predicate functors are now two-place predicates.

* Functional) 'Func F' for '= => (F of (swap F))'
* Left Field) 'Lfied F' for 'F on U'
* Right Field) 'Rfield F' for '(swap F) on U'
* Field) 'field F' for '(Lfied F) or (Rfield F)'

The various theorems about functions then turn out to be applications of schema of functional predicates e.g. 'Func void', 'Func =', '(Func F) and (Func G), only if (Func (F of G))', and '(Func F) only if (Func (F and G))'
Talk of correlations comes up often and is an application fo the logic of correlative predicates:

* correlation) 'Corr F' for '(Func F) and (Func swap F)'

Then similarities are relative to a corraltive predicate

* similarity) 'F <=G=> H' for '(Corr G) and (F <=> Lfield G) and (H <=> Rfield G)'

Somewhere around this point in my exploration of predicate functor logic it became clear that what goes by the name 'naive set theory' is neither naive nor set theory: it is just logic, pure predicate logic. The notational abbreviations afforded by predicate functors reveal this This part of my exploration is where I saw the damage done by Tarski's success e.g. with set theoretic simjulations of logic and with comingling predicate logic and the calculus of relations. The entire affair is summed up by the difference between 'denotes' and 'designates'.

I've still avoided introducing the singleton of a predicate. It placys the part of singular terms in a pur predicate (functor) logic. It's just russell's theory of descriptions adapted to these variableless methods.

Singular predicates play the part once played by singular terms.

* Singular) 'Sing F' for 'not some^2 ((nip F) and (drop F) and (not =))' or 'each^2((nip F) or (drop F) or =)'

But, it is not a singular predicate which exactly plays the part once played by singular terms, it is another predicate which, for now, I call 'the singleton'

* singleton) '{F}' for 'each(= iff (nip F))'
* itemization) '{F,..,G}' for '{F} or {..,G}'

these carry over a lot of what went as finite set theory

Now the components of a predicate come through the image of the singleton (this is then an operation that generalizes 'application and the 'collective relate')

* components) 'F at G' or 'F{G}' for 'F"{G}'

The arguments of a two place predicate are those items of its second component which are true of that predicate with respect to only one first component. The definition is beautiful

* arguments) 'arg F' for 'some{F}'

Just as there is a serviceable theory of identity embedded in any logic with only finitely many predicates-- the universal closure of the conjunction of each biconditional of each parallel permutation of primitive predicates-- so to is there a serviceable theory of denotation.

Much earlier, i defined the left and right confinement from a predicate of equality. That was unnecessary. Those definitions become logically equivalent to the following when a predicate of equality is present.

* left confinement) 'F[G' for 'F and drop G'
* right confinement) 'F]G' for 'F and nip G'

Descending from the beautiful definition of argument of a predicate is the notorious method of function abstraction (aka lambda abstraction)

* lambda) 'lambda F' for 'F](arg F)'

note 'lambda lambda F' is equivalent to 'lambda F'. At first this seemed like an error: we can function (lambda) abstract again and again can't we? Then it dawned on me: this theorem covers degenerate lambda abstractions in the classical case. An example of a degenerate lambda abstraction in the classical case is 'lambda x lambda x f'. A carefully designed grammar eliminates any such degenerate 'expression'. The equivalence of 'lambda^2 F' to 'lambda F' suggests a collapse to 'lambda x f' i.e. a rule of evaluation.

Happily, 'func lambda F' is true i.e. the function abstraction of a predicate is a functional predicate.

From function (lambda) abstraction, the problem is posed as to what definition of 'function application' fits. I'm torn between two alternatives. The one assumes function application is always going to occur on a functional predicate:

* appliatoin) "F'G" for '(lambda F){G}'

The traditional definition of function application (with "F'x" as 'the y such that Fyx') dissolves into my prior definition of 'component' which generalizes function application and the collective relate.

All this being said, there is a lot of work to be done to bring these things together. But that is somewhat secondary to the larger problem to solve.

My methods of logic are to be pure predicate logic: predicates and predicate functors. The challenge is to make a story of logic which is autonomous from quantificational logic. Another part of the challenge is to free the world from the confines of Tarski's theory fo models which too strongly weds logic to mathematical logic and mathematical logic to theories of sets.

The prevailing solution has been to abandon predicate logic for deviant logics and deviant foundations which purport to be unbeholden to the constraints of, e.g., smooth discourse, e.g., fascilitated by extensionality. In such methods I simply see theories from theoraticians who have yet to present their predicates and premises.

Category theorists and type theorists are the most familiar of the gang as far as I know. Some computational theorists enter upon this territory, but, e.g., automata have yet to present themselves as foundational instruments in the same way as categories and types have. The closest is calculi, but there is not yet a general flag underwhich they fly (I'd propose Fefermann's Finitary Inductively Presented Logics if pressed).

## 2025 0421

### 2025 0421 2110
I had to stop my work on the latest little lisp entry in order to make note of a profoundly interesting interview that was just shared with me.
Australian identical twins Bridgette Powers and Paula Powers were recorded after they witnessed a car crash that turned into something more.
What is interesting is that they speak in a way that presumably makes it easy to hear when each of their verbal responses is strong or weak.

When they are speaking "in sync" they are emitting, what I can only guess is, the same operant, the same unit of response.
When they speak "out of sync" you get to listen to the verbal behavior that is presumably equally strong but which, if equally strong within the same skin, could not be emitted at the same time.

I do not have time now to do more research on them, but I submit that they have been interviewed prior to this most recent event.

### 2025 0421 1928

Today was a hammond b3 organ jazzy kinda day.

### 2025 0421 1546
This continues my work on my little lisp from [202504201615](#2025-0420-1615).

> and this is a reminder that my aim is to release a method of logic programming from Quine's main method by implementing each step to it he makes in "Methods of Logic 4th Edition".

In the last entry I did not give any examples of the functions defined running because I have not yet checked if they even work in such example cases.
First, a complete summary of the code that I'm working with (eventually each of the suprisingly disparate entries will come together under a unified collection of verbal practices which, ideally, approach something like Leibniz calculus ratiocinator and characteristica universalis, but I'll have more to say on that later).

```
let run=code=>console.log(code,'\n',eval(code)) // for examples

// some basic lisp functions
, consOf = (car,cdr) => [car,cdr]
, carOf = cons => cons[0]
, cdrOf = cons => cons[1]
, nil = 'nil'
, isIdentical = (x,y) => x==y
, isNil = x => isIdentical(x,nil)
, isPair = x => Array.isArray(x)
, isAtom = x => !isPair(x)

// proper and dotted lists
, isProperList = x => isNil(x) || (isPair(cdrOf(x)) && isProperList(cdrOf(x)))
, isDottedList = x => !isProperList(x)

// some basic javascript string functions
, emptyString=''
, isIdenticalString = (x,y) => x==y
, isEmptyString = string => isIdenticalString(string, emptyString)
, concatenationOf = (...strings) => 
   strings.length ? strings.shift() + concatenationOf(...strings) : emptyString
, firstCharOf = string => isEmptyString(string) ? emptyString : string[0]
, restCharsOf = string => isEmptyString(string) ? emptyString : string.slice(1)
, isString = x => 'string' == typeof x

// basic rune functions
, runeMark = '^'
, isRuneMark = x => isIdenticalString(x,runeMark)
, isRune = x => isString(x) && isRuneMark(firstCharOf(x))
, isRunic = list => isNil(list) || (isRune(carOf(list)) && isRunic(cdrOf(list)))

// from strings to runic lists and back again
, runeOf = char => concatenationOf(runeMark,char) 
, runicListOf = string => 
   isEmptyString(string) ? nil 
   : consOf(runeOf(firstCharOf(string)), runicListOf(restCharsOf(string)))
, charOf = rune => restCharsOf(rune)
, stringOf = runicList => 
   isNil(runicList) ? emptyString
   : concatenationOf(charOf(carOf(runicList)), stringOf(cdrOf(runicList)))

// prepending proper lists clears the way for buliding 
// runic lists from runic lists
, prependedProperListOf = (properList1, properList2) =>
   isNil(properList1) ? properList2
   : consOf(carOf(properList1)
     ,prependedProperListOf(cdrOf(properList1), properList2))

// how to make atoms and dotted lists proper lists
, singletonListOf = x => consOf(x,nil)
, properListOf = x => 
   isNil(x) ? nil
   : isAtom(x) ? singletonListOf(x)
   : consOf(carOf(x), properListOf(cdrOf(x)))

// generlization of prepepending proper lists to atoms and lists
, prependedListOf = (x,y) => 
   prependedProperListOf(properListOf(x),properListOf(y))
, appendedListOf = (x,y) => prependedListOf(y,x)

// printing atoms i.e. symbols
, isSymbol = x => isString(x)
, atomicPrintOf = atom =>
   isNil(atom) ? atomicPrintOf('()')
   : isSymbol(atom) ? runicListOf(atom)
   : atomicPrintOf('!?');
```
Now, examples for the latest functions defined in the latest entry
```
isRunic(prependedListOf(runicListOf('this is a test'),runeOf('!'))) 
 true
stringOf(prependedListOf(runicListOf('this is a test'),runeOf('!'))) 
 this is a test!
stringOf(atomicPrintOf(nil)) 
 ()
stringOf(atomicPrintOf('x')) 
 x
stringOf(atomicPrintOf(3)) 
 !?
```
That solves the problem of printing atoms where atoms in my little lisp are each symbols (notice that the last example gave '!?' as a default when the atom printer doesn't know how to print the item which purports to be an atom: in a different design I have no symbols, but that is closer to the little uhdForth-like language I'll show off after I get this little lisp done).

Next is the problem of printing proper lists and dotted lists.
First I'll work on proper lists because that seems simpler.
When a proper list gets printed it starts with '(' and ends with ')'.
Everything between those parenthesis has to be printed by the main printer that dispatches the appropriate kind of printer on the argument given i.e.
```
let printOf = x =>
   isAtom(x) ? atomicPrintOf(x)
   : isProperList(x) ? properListPrintOf(x)
   : isDottedList(x) ? dottedListPrintOf(x)
   : atomicPrintOf('!?')
, properListPrintOf = properList =>
   prependedListOf(runeOf('(')
   , prependedListOf(properListPrintEachOf(properList), runeOf(')')));
```
Dotted lists can wait.
The problem is reduced to printing each of the items in the proper list.
```
let properListPrintEachOf = properList =>
   isNil(properList) ? nil
   : prependedListOf(runeOf(' ')
     , prependedListOf(printOf(carOf(properList))
       , properListPrintEachOf(cdrOf(properList))));
```
As much as I do not like these long and camel case names, it is all I have for now.
With these definitions we should be able to test out printing everything but dotted lists.

Oh! But, it just occurred to me that there is a special case of proper list that must be handled: runic lists!
There is also another special case that just occurred to me: the runes of '(' and ')' must be distinguishable from the parenthesis printed around the printings from the list!

```
let openParenthesis = runeOf('(')
, closeParenthesis = runeOf(')')
, isParenthesis = x => isIdentical(x,openParenthesis) || isIdentical(closeParenthesis);
atomicPrintOf = atom =>
   isNil(atom) ? atomicPrintOf('()')
   : isParenthesis(atom) ? prependedListOf(runeOf('^'),runicListOf(atom))
   : isSymbol(atom) ? runicListOf(atom)
   : atomicPrintOf('!?');

let runicListPrintOf = runicList =>
   prependedListOf(runeOf("'")
   , prependedListOf(runicList,runeOf("'")));
printOf = x =>
   isAtom(x) ? atomicPrintOf(x)
   : isRunic(x) ? runicListPrintOf(x)
   : isProperList(x) ? properListPrintOf(x)
   : isDottedList(x) ? dottedListPrintOf(x)
   : atomicPrintOf('!?');
```
Now for some much needed examples:
```
stringOf(printOf(nil)) 
 ()
stringOf(printOf('thisIsABigAtom')) 
 thisIsABigAtom
stringOf(printOf(runicListOf('this is a runic list'))) 
 'this is a runic list'
```
So far so good.
Next to test printing proper lists.
A much needed function for making lists is added to the mix.
```
let listOf = (...x) => x.length ? consOf(x.shift(), listOf(...x)) : nil
```
The function designated by 'listOf' takes any number of arguments and returns a proper list each item of which is the appropriate argument.
Thankfully, I ran into an error when I tried to run
```
stringOf(printOf(listOf(nil,'thisIsABigAtom',runicListOf('this is a runic list'))))
```
It said that 'dottedList' was not defined.
My definition of proper list was missing a clause!
Here is the corrected version: 
```
isProperList = x => isNil(x) || isNil(cdrOf(x)) || (isPair(cdrOf(x)) && isProperList(cdrOf(x)));
```
Another crash and another correction!
The code for detecting parenthesis was missing an argument:
```
isParenthesis = x => isIdentical(x,openParenthesis) || isIdentical(x,closeParenthesis);
```
Now for a big example that almost works exactly as expected:
```
stringOf(printOf(listOf(nil,'thisIsABigAtom'
 ,listOf(closeParenthesis, 'x', openParenthesis)
 , runicListOf('this is a ) runic ( list'))))

 ( () thisIsABigAtom ( ^^) x ^^() 'this is a ) runic ( list')
```
Everything in there was expected except for the double '^^' in front of the parentheses.
Because I handled runic lists by wrapping them in single quotes I avoided the problem of having to prepend an extra escape character to atoms that begin with them: the function designated by 'runicListOf' escapes the escape character as expected!
So a quick correction:
```
atomicPrintOf = atom =>
   isNil(atom) ? atomicPrintOf('()')
   : isSymbol(atom) ? runicListOf(atom)
   : atomicPrintOf('!?');
```
and the big example becomes
```
stringOf(printOf(listOf(nil,'thisIsABigAtom'
 ,listOf(closeParenthesis, 'x', openParenthesis)
 , runicListOf('this is a ) runic ( list'))))

( () thisIsABigAtom ( ^) x ^() 'this is a ) runic ( list')
```
which is very satisfying, but there is a space missing before a closing parenthesis:
```
let spaceRune = runeOf(' ');
properListPrintOf = properList =>
   prependedListOf(openParenthesis
   , prependedListOf(properListPrintEachOf(properList)
     ,prependedListOf(spaceRune,closeParenthesis)));
```
The big example is then
```
stringOf(printOf(listOf(nil,'thisIsABigAtom'
 ,listOf(closeParenthesis, 'x', openParenthesis)
 , runicListOf('this is a ) runic ( list'))))

 ( () thisIsABigAtom ( ^) x ^( ) 'this is a ) runic ( list' )
```

All that is left is to deal with dotted lists.
They seem to be just like proper lists but instead of checking whether the relevant item is the item designated by 'nil' check if the relevant item is an atom:
```
let  dottedListPrintOf = dottedList =>
   prependedListOf(openParenthesis
   , prependedListOf(dottedListPrintEachOf(dottedList)
     ,prependedListOf(spaceRune,closeParenthesis)))
, dotRune = runeOf('.')
, dottedListPrintEachOf = dottedList =>
   isAtom(dottedList) ?
     listOf(spaceRune, dotRune, spaceRune, atomicPrintOf(dottedList))
   : prependedListOf(spaceRune
     , prependedListOf(printOf(carOf(dottedList))
       , dottedListPrintEachOf(cdrOf(dottedList))));
```
Now for the simplest example to check for (hopefully) easy to fix bugs:
```
stringOf(printOf(consOf(closeParenthesis, openParenthesis))) 
 ( ^) . ^(,nil )
```
Oh, wow, what an interesting mistake.
Everything works right up to the space after the dot, but then something goes wrong.
That thing is also something that often goes wrong when working with lists: prepending is not the same as listing out.
Prepending gets rid of trailing empty lists when putting each of its arguments together.
This should correct things.
```
dottedListPrintEachOf = dottedList =>
   isAtom(dottedList) ? prependedListOf(
     listOf(spaceRune, dotRune,spaceRune), atomicPrintOf(dottedList))
   : prependedListOf(spaceRune
     , prependedListOf(printOf(carOf(dottedList))
       , dottedListPrintEachOf(cdrOf(dottedList))));
```
The example:
```
stringOf(printOf(consOf(closeParenthesis,openParenthesis))) 
 ( ^) . ^( )
```
Great!
Now to put everything together into one really big example:
```
stringOf(printOf(consOf(listOf(nil,'thisIsABigAtom'
 ,listOf(closeParenthesis, 'x', openParenthesis)
 ,runicListOf('this is a ) runic ( list')))),'x')))

 ( ( () thisIsABigAtom ( ^) x ( ^) . ^( ) ^( ) 'this is a ) runic ( list' ) . x )
```
Wonderful.
That's a big problem solved.
The next problem to solve is the reader: it must take strings like the ones displayed from the printer and build the appropriate lists and atoms.

Since there was a lot of editing along the way, here are all the function definitions that were finally accepted.
I changed some names in order to avoid collisions e.g. 'listOf' by 'properListOf' and 'properListOf' by 'properOf' and now 'listOf' designates a function that can be used to produce both dotted and proper lists depending on whether the last argument is the empty proper list or some other atom.
Everything below was checked on the latest big example.
```
let run=code=>console.log(code,'\n',eval(code)) // for examples

// some basic lisp functions
, consOf = (car,cdr) => [car,cdr]
, carOf = cons => cons[0]
, cdrOf = cons => cons[1]
, nil = 'nil'
, isIdentical = (x,y) => x==y
, isNil = x => isIdentical(x,nil)
, isPair = x => Array.isArray(x)
, isAtom = x => !isPair(x)

// proper and dotted lists
, isProperList = x => isNil(x) || isNil(cdrOf(x)) || (isPair(cdrOf(x)) && isProperList(cdrOf(x)))
, isDottedList = x => !isProperList(x)

// some basic javascript string functions
, emptyString=''
, isIdenticalString = (x,y) => x==y
, isEmptyString = string => isIdenticalString(string,emptyString)
, concatenationOf = (...strings) => 
   strings.length ? strings.shift() + concatenationOf(...strings) : emptyString
, firstCharOf = string => isEmptyString(string) ? emptyString : string[0]
, restCharsOf = string => isEmptyString(string) ? emptyString : string.slice(1)
, isString = x => 'string' == typeof x

// basic rune functions
, runeMark = '^'
, isRuneMark = x => isIdenticalString(x,runeMark)
, isRune = x => isString(x) && isRuneMark(firstCharOf(x))
, isRunic = list => isNil(list) || (isRune(carOf(list)) && isRunic(cdrOf(list)))

// from strings to runic lists and back again
, runeOf = char => concatenationOf(runeMark,char) 
, runicListOf = string => 
   isEmptyString(string) ? nil 
   : consOf(runeOf(firstCharOf(string)), runicListOf(restCharsOf(string)))
, charOf = rune => restCharsOf(rune)
, stringOf = runicList => 
   isNil(runicList) ? emptyString
   : concatenationOf(charOf(carOf(runicList)), stringOf(cdrOf(runicList)))

// how to make lists of lists and atoms
, listOf = (...x) => x.length > 1 ? consOf(x.shift(),listOf(...x)) : x.shift()

// how to make proper lists of lists and atoms
, properListOf = (...x) => x.length ? consOf(x.shift(),properListOf(...x)) : nil

// prepending proper lists clears the way for buliding 
// runic lists from runic lists
, prependedProperListOf = (properList1, properList2) =>
   isNil(properList1) ? properList2
   : consOf(carOf(properList1)
     ,prependedProperListOf(cdrOf(properList1), properList2))

// how to make atoms and dotted lists proper lists
, singletonListOf = x => consOf(x,nil)
, properOf = x => 
   isNil(x) ? nil
   : isAtom(x) ? singletonListOf(x)
   : consOf(carOf(x), properOf(cdrOf(x)))

// generlization of prepepending proper lists to atoms and lists
, prependedListOf = (x,y) => 
   prependedProperListOf(properOf(x),properOf(y))
, appendedListOf = (x,y) => prependedListOf(y,x)

// printer dispatch
, printOf = x =>
   isAtom(x) ? atomicPrintOf(x)
   : isRunic(x) ? runicListPrintOf(x)
   : isProperList(x) ? properListPrintOf(x)
   : isDottedList(x) ? dottedListPrintOf(x)
   : atomicPrintOf('!?')

// how to print atoms i.e. symbols
, isSymbol = x => isString(x)
, atomicPrintOf = atom =>
   isNil(atom) ? atomicPrintOf('()')
   : isSymbol(atom) ? runicListOf(atom)
   : atomicPrintOf('!?')

// how to print runic lists
, quotationMark = runeOf("'")
, runicListPrintOf = runicList =>
   prependedListOf(quotationMark
   , prependedListOf(runicList,quotationMark))

// how to print proper lists
, openParenthesis = runeOf('(')
, spaceRune = runeOf(' ')
, closeParenthesis = runeOf(')')
, isParenthesis = x => isIdentical(x,openParenthesis) || isIdentical(x,closeParenthesis)
, properListPrintOf = properList =>
   prependedListOf(openParenthesis
   , prependedListOf(properListPrintEachOf(properList)
     ,prependedListOf(spaceRune,closeParenthesis)))
, properListPrintEachOf = properList =>
   isNil(properList) ? nil
   : prependedListOf(runeOf(' ')
     , prependedListOf(printOf(carOf(properList))
       , properListPrintEachOf(cdrOf(properList))))

// how to print dotted lists
, dottedListPrintOf = dottedList =>
   prependedListOf(openParenthesis
   , prependedListOf(dottedListPrintEachOf(dottedList)
     ,prependedListOf(spaceRune,closeParenthesis)))
, dotRune = runeOf('.')
, dottedListPrintEachOf = dottedList =>
   isAtom(dottedList) ? prependedListOf(
     listOf(spaceRune,dotRune,spaceRune,nil),atomicPrintOf(dottedList))
   : prependedListOf(spaceRune
     , prependedListOf(printOf(carOf(dottedList))
       , dottedListPrintEachOf(cdrOf(dottedList))));
```


## 2025 0420

### 2025 0420 2247
This continues my read of Will Durant's "The Story of Philosophy" from [202504142055](#2025-0414-2055).

The first chapter on Plato is divided into ten sections:
1. The Context of Plato
2. Socrates
3. The Preparation of Plato
4. The Ethical Problem
5. The Political Problem
6. The Psychological Problem
7. The Psychological Solution
8. The Political Solution
9. The Ethical Solution
10. Criticism

The first section "The Context of Plato" begins with a geological description:

> "If you look at a map of europe you will observe that Greece is a skeletonlike hand stretching its crooked fingers out into the Mediterranean Sea." [pg. 5 Durant "The Story of Philosophy"]

Geology selects geography which plays a large part in the survival of species, be they human or not.
Durant begins with a geological description that has not changed much since the time of ancient Greece from 1100 BCE to 146 BCE.
We are connected to the shape of greece on which Socrates and Plato lived out their entire lives.

He then describes the surrounding geographies and narrows his view quickly upon the "city-states":

> "Greece was broken into isolated fragments by these natural barriers of sea and soil; travel and communication were far more difficult and dangerous then than now; every valley therefore developed its own self-sufficient economic life, its own sovereign government, its own institutions and dialect and religion and culture.
In each case one or two cities, and around them, stretching up the mountainslopes, an agricultural hinterland: such were the "city-states" of Euboea, and Locris, and Aetolia, and Phocis, and Boeotia, and Achaea, and Argolis, and Elis, and Arcadia, and Messenia, and Laconia--- with its Sparta, and Attica--- with its Athens."[pg. 5 Durant "The Story of Philosophy"]

Having arrived at Sparta and Athens, the Greco-Persian Wars enter the scene: "Sparta provided the army and Athens the navy."[pg.6]
Athen's navy became a merchant fleet and new cultural practices flowed through the city's ports.
Finally, Durant has made his way to philosophy:

* "Democritus (460-360 BCE)--- 'in reality there is nothing but atoms and space.'"
* Epicurus (342-270 BCE)
* Lucretius (98-55 BCE)
* The Sophists
  * Gorgias
  * Hippias
  * Protagoras
  * Prodicus

The Sophists were
> "traveling teachers of wisdom, who looked within upon their own thought and nature, rather than out upon the world of things."
>
> "They asked questions about anything; they stood unafraid in teh presence of religious or political taboos"
>
> "In politics they divided into two schools. One, like Rousseau, agrued that nature is good, and civilization bad; that by nature all men are equal, becoming unequal only by class-made institutions: and the law is an invention of the strong to chain and rule the weak.
> Another school, like Nietzsche, claimed that nature is beyond good and evil; that by nature all men are unequal; that morality is an invention of the weak to limit and deter the strong; that power is the supreme virtue and the supreme desire of man; and that of all forms of government the wisest and most natural is aristocracy."[pg. 6-7]

Next, the Peloponnesian war (430-400 BCE) begot the oligarichal control of Athens by Critias and his "Thirty Tyrants".
So it is that Critias "was a pupil of Socrates, and an uncle of Plato"[pg. 7]


### 2025 0420 2201
Here are some notes that I told my self to make:

* Jonathan Gorard (@getjonwithit) made a use-mention mistake when invoking a theory of concepts to explain his ontological stance.
  > "Even the concepts of "places", "paths", "action", "distance" (and for that matter, "two") are ultimately just mathematical abstractions.
  >
  > The fact that we privilege some abstractions as more "real" than others is a byproduct of familiarity, not fundamentality."
  > 3:27 PM - 4/19/25 @getjonwithit
  *  This was in response to Sabine Hossenfelder (@skdh) explaining her ontological stance: "I think having a mathematical description is what it means to understand something"[11:38 - 4/18/25].
I get that most people may not see this clearly as a problem of ontology.
* Jonathan Gorard gave a definition of mathematics that does not address the key problem with respect to ontology (mentioned by Quine as "Every critically massive set of truths has some nonmathematical members."[pg. 52 Quine "From Stimulus to Science"]
  >"My personal definition of mathematics (i.e. "the set of all things about which one can reasonably prove theorems") is pretty broad. If an abstraction can be made precise, then it's mathematical."
  > 5:21 PM - 4/19/25 @getjonwithit
  * My response to this was to mention the limitations of a definition of mathematics with respect to, e.g., Quine's theory of protosyntax which is a descendent of Tarski's *formalized languages*.
If I was forced to give a clear cut definition of mathematics then it would be the theory of protosyntax, but if I was allowed to do as I want it would be to given no clear cut definition: "Mathematicity is perhaps a matter of degree." [pg.55 Quine "From Stimulus to Science"]
* Abi Daker (@abidaker) responded to my explanation of abstraction by way of predicate abstracts, concretion, and predicational compelteness.
She provided me with some brilliant examples from her work on early art:
  > "early use of abstraction was paeolithic people creating different sets of symbols for different animal/bird species which were based around the creatures footprints. It comes up a lot in cave paintings" [1:11 AM - 4/20/25]
 
  > "other nice bit of symbolism is that arches in churches exist because early churches were tree glades and when they started to build dedicated spaces for ceremony/worship, they wanted to mimic the tree arches.
  >
  > the domes are symbolic of the sun, as well"[1:49 AM - 4/20/25]

* @ResonantPyre mentioned an interesting sounding book "Culture and Society" by Raymond Williams.[2:15 PM - 4/13/25]

### 2025 0420 1615
This continues the work on my little lisp from [202504181456](#2025-0418-1456).

Runic lists are the foreign companion to javascript's native strings.
Just as strings can be concatenated together, so too can proper lists.
One proper list is prepended to an other by an appropriate sequence of pairings by the function designated by 'consOf'

```
let prependedProperListOf = (properList1, properList2) =>
  isNil(properList1) ? properList2
  : consOf(carOf(properList1)
    ,prependedProperListOf(cdrOf(properList1), properList2));
```
An example:
```
isIdenticalString(concatenationOf('this is ','a test')
 ,stringOf(prependedProperListOf(runicListOf('this is ')
   ,runicListOf('a test'))))
 true
```

A method of prepending proper lists, dotted lists, and atoms follows from transforming a dotted list into its associated proper list (which then also takes care of the atomic case):

```
let singletonListOf = x => consOf(x,nil)
, properListOf = x => 
  isNil(x) ? nil
  : isAtom(x) ? singletonListOf(x)
  : consOf(carOf(x), properListOf(cdrOf(x)))
, prependedListOf = (x,y) => 
  prependedProperListOf(properListOf(x),properListOf(y));
```

A note on speed and efficiency: they are the result of shortcuts which do not mutilate the logical consequences of the definitions given.
Before such definitions are given, there are no comparisons to be made, and hence no speed or efficiency of which to speak.

Appending mirrors prepending.
```
let appendedListOf = (x,y) => prependedListOf(y,x);
```

Finally, the printer prints a list or an atom as a runic list that can be sent to the native language of javascript as a string which is then displayed.
There is first the problem of transforming a symbol (which is just a native javascript string in my little lisp) into a runic list; but, surprise, we already implemented that as the function designated by 'runicListOf'.
There is only one catch, if the atom being printed is designated by 'nil' then print a runic list that gets displayed as '()' so as not to forget what part it now plays:
```
let isSymbol = x => isString(x)
, atomicPrintOf = atom =>
  isNil(atom) ? atomicPrintOf('()')
  : isSymbol(atom) ? runicListOf(atom)
  : atomicPrintOf('!?');
```

## 2025 0419

### 2025 0419 1349 The Parts of "Introduction to Philosophy" and My Outlook
The readings in the fourth edition of Perry, Bratman, and Fischer's "Introduction to Philosophy" are divided into six parts:

1. Philosophy and the Meaning of Life
2. God and Evil
3. Knowledge and Reality
4. Minds, Bodies, and Reasons
5. Ethics and Society, and
6. Puzzles and Paradoxes.

Each part is filled with selections from primary sources.
One of my aims is to cover the breadth of these sources and to supplement that breadth with the depth of the texts from which these selections were made.

There must be no doubt as to where I am coming from when looking over the data of philosophy: they are records of responses whose origins are through reinforcing practices of verbal communities whose cultures evolved by selection from variations on cultures past.
Such reinforcing practices were and are mediated by bits of behaving biology: organizations mediated by organisms mediated by organelles and so on.

There are doubts as to what specific chains of contingencies selected these organisms, these behaviors, and these cultures.
The boundary between what is known and what is unknown is often, and perhaps unavoidably, blurry.
Science and logic have done better than the rest of human's practices to uncover the shape of our strongest doubts.
There appears to be no better reason for them than that.

What others have to say about the world, or the worlds, enters into a mix.
The soup of social behaviors is seasoned by each of us.
Our favorite flavors are contingent upon the ingredients with which we are endowed.

Philosophy is a part of science.
The behavior of the philosopher is behavior and is, hence, part of the science of behavior.
Psychology as something other than the science of behavior is part of the speculation of philosophers.
Such psychologies are a part of science as much as any pseudoscience is part of what is to be explained by a complete account of human behavior.

Just as the molecular biologist can spend their life pouring over the peculiairites of abberant corners of the ocean, so too can the experimental analyst probe the behaviors and practices of prescientific practitioners.
They are, after all, the progenitors of modern science, which, if human practices are to be taken as a going concern, is the progenitor of future sciences largely unknown or, perhaps, even the progenitors of some more effective or, goodness forbid, less effective practices beyond science itself.

To read philosophy is to triangulate: where is what you have to say located with respect to what others have to say?
How far can speaking and listening take each of us when dealing with our shared world?
These are among the questions I ask, and which others have asked and answered well before me.
What can be learned from listening to my self as I do those others?
As an other to my self, what can be said of my speech and its relation to the speech of others better spoken than mine?

With that I turn to the texts, and I shall return here to tease out more questions, and, if I am so lucky, to find the threads of wisdom that often pose as our best tentative answers to them.

## 2025 0418

### 2025 0418 1601
A new mistake I make with markdown to add to the first I identified in [202504171525](#2025-0417-1525):
> I put the link inside parenthesis and then put the display text inside square brackets AFTER the parenthesis! 

### 2025 0418 1456
This continues the work on my little lisp from [202504171613](#2025-0417-1613).

I apologize for these fragments of progress on this particular project: I've decided to just get things done as they occur to me rather than go out of my way to first make them more easily explainable up front.
Everything will still be completely explained, but I am certain there are better ways of writing this all out than what I have done thus far.

Runes are special atoms that are designated by javascript strings that start with the "runeMark":

```
let runeMark = '^'
, isRune = x => ('string'== typeof x ) && runeMark == x[0];
```

Examples:
```
isRune('^test') 
 true
isRune('test') 
 false
```

A list is runic if it is proper and each of its left parts is a rune:
```
let isRunic = list => isNil(list) || (isRune(carOf(list)) && isRunic(cdrOf(list)));
```
Runic lists shall paly the part of strings in my little lisp (for now because I dont' know what consequences may come to select a new design).
This follows the convention of [Paul Graham's Bel](https://www.paulgraham.com/bel.html) where he spoke of chars I speak of runes: for this project chars are always native javascript.
The strange distinction must be made to preserve the distinction between symbols in lisp, runic lists, and javascript strings: this is something that has already tricked me and may very likely trick you as well!

I'll write some examples for the function designated by 'isRunic' after I introduce some slightly edited string functions from [Bit Strings and Binary Trees](#2025-0413-1513-bit-strings-and-binary-trees):

```
let emptyString=''
, isIdenticalString = (x,y) => x==y
, isEmptyString = string => isIdenticalString(string,emptyString)
, firstCharOf = string => isEmptyString(string) ? emptyString : string[0]
, restCharsOf = string => isEmptyString(string) ? emptyString : string.slice(1)
, concatenationOf = (...strings) => 
   strings.length ? strings.shift() + concatenationOf(...strings) : emptyString
, isString = x => 'string' == typeof x
, isRuneMark = x => isIdenticalString(x,runeMark);

isRune = x => isString(x) && isRuneMark(firstCharOf(x));
```

This is also the first example of redefining a variable in javascript: 'isRune' comes to denote a function specified in language more like the idioms I've adopted so far with their 'of's and 'is's.
The names are much longer than I'd write if I was writing just for my self, but I'm not doing that am I?
Examples (I do not yet have a comprehensive method of testing, but have tested out a few different methods of testing and so far examples are good enough):

```
isRune(concatenationOf(runeMark,'test')) 
 true
isRune('test') 
 false
```

Next, the two functions that go from strings to runic lists and back again:
```
let runeOf = char => concatenationOf(runeMark,char) 
, runicListOf = string => 
  isEmptyString(string) ? nil 
  : consOf(runeOf(firstCharOf(string)), runicListOf(restCharsOf(string)))
, charOf = rune => restCharsOf(rune)
, stringOf = runicList => 
  isNil(runicList) ? emptyString
  : concatenationOf(charOf(carOf(runicList)), stringOf(cdrOf(runicList)));
```

Examples:
```
isRunic(runicListOf('this is a test')) 
 true
stringOf(runicListOf('this is a test')) 
 this is a test
```

Why start with strings and runic lists?
Runic lists are the foreign companion to javascript's native strings and they shall be the porthole through which the foreign items of my little lisp are displayed in our native tongue.

The first main part of my little lisp that must be made is the printer.
It has to take a pair or an atom and transform it into a runic list that can then be read by our human eyes.
Perhaps if we could see into the computer like superman with x-ray vision, then we wouldn't need to worry about making a printer.
But, making a printer also helps when making a reader which transforms key presses into foreign code.

For my little lisp there is only going to be one kind of atom: symbols.
Runes are then special symbols: those that start with the rune mark as above.
What was spoken of as 'strings starting with the rune mark' shall now be spoken of as 'symbols starting with the rune mark'.
While I'd rather not use the word 'symbol' because it leads to questions like "What does this symbol symbolize?" there is a long history of its use in lisp and I am not yet prepared to break that chain of consequences.

Here is all the code from this entry in one place:

```
let run=code=>{console.log(code,'\n',eval(code));}

let consOf = (car,cdr) => [car,cdr]
, carOf = cons => cons[0]
, cdrOf = cons => cons[1]
, nil = 'nil'
, isIdentical = (x,y) => x==y
, isNil = x => isIdentical(x,nil)
, isPair = x => Array.isArray(x)
, isAtom = x => !isPair(x);
run('isPair(nil)');
run('isAtom(nil)');

let runeMark = '^'
, isRune = x => ('string'== typeof x ) && runeMark == x[0];
run("isRune('^test')")
run("isRune('test')")

let isRunic = list => isNil(list) || (isRune(carOf(list)) && isRunic(cdrOf(list)));

let emptyString=''
, isIdenticalString = (x,y) => x==y
, isEmptyString = string => isIdenticalString(string,emptyString)
, firstCharOf = string => isEmptyString(string) ? emptyString : string[0]
, restCharsOf = string => isEmptyString(string) ? emptyString : string.slice(1)
, concatenationOf = (...strings) => 
   strings.length ? strings.shift() + concatenationOf(...strings) : emptyString
, isString = x => 'string' == typeof x
, isRuneMark = x => isIdenticalString(x,runeMark);
isRune = x => isString(x) && isRuneMark(firstCharOf(x));
run("isRune(concatenationOf(runeMark,'test'))");
run("isRune('test')");

let runeOf = char => concatenationOf(runeMark,char) 
, runicListOf = string => 
  isEmptyString(string) ? nil 
  : consOf(runeOf(firstCharOf(string)), runicListOf(restCharsOf(string)))
, charOf = rune => restCharsOf(rune)
, stringOf = runicList => 
  isNil(runicList) ? emptyString
  : concatenationOf(charOf(carOf(runicList)), stringOf(cdrOf(runicList)));
run("isRunic(runicListOf('this is a test'))");
run("stringOf(runicListOf('this is a test'))");

```


### 2025 0418 1453
The notation used to [explain The Popr Programming Language](https://www.hackerfoo.com/posts/popr-tutorial-0-dot-machines.html) is very much like the notation I devised for my own language, and is a clear indication that there is a convergence of notational practices that lend themselves to visualization near these concatenative methods.

It is sad that they are spoken of as "concatenative" when, as far as I can tell, this is just one way of elaborating on any principles of programming stack machines.
Without the concrete contingencies of stack machines there is no concatenative programming and concatenative methods may not even end up being the most appropriate for stack machines.
Again, as far as I can tell, the key to a specific method of programming is how you would write an interpreter for that programming language in that programming language.

This is not just some honorary show to LISP: it is the most accessible way to bring up the logic of the theory upon which a given method of programming is built.
It forces the programmer to confront the difference between use and mention, logic and theory, and often reveals the language's place on among models of computability.


### 2025 0418 1439
A major conclusion from the substance of [accumulating the links](#2025-0417-2020) to papers on what is called "concatenative programming languages" is that modern writers do not know the difference between a theory and its logic.
The inability to notice that the predicate functors of Quine are not combinators as in the more familiar combinatory calculi is a great sadness: both to our sciences and to our teachers.

Finally done accumulating the links that I wrote down.
Now on to my little lisp.

## 2025 0417

### 2025 0417 2026

All the links across the internet are rotting.
Searches are no longer helpful: one system often says that there is nothing relevant while another quickly brings what you're looking for to the top only to bury it later.
My saddness knows no bounds.
There are a few people that could break out of the environments which control them (and the world) in unhelpful ways, but they have no good reasons for doing so.

If your shit isn't in plain text documents then it is more likely to rot than anything else.
There are no easy ways to accept or deal with this: it is a kind of culture war both foreign and yet somehow familiar.

### 2025 0417 2020

Some links to papers and presentations on concatenative programming languages and related things:

* ["Stanford Seminar - Concatenative Programming: From Ivory to Metal" by John Purdy, November 15, 2017](https://www.youtube.com/watch?v=_IgqJr8jG8M)
* ["The Theory of Concatenative Combinators" by Brent Kerby 2002](http://tunes.org/~iepos/joy.html)
    > This is unlikely to be around for long: it will be too hard to find it.
* ["Linear logic and permutation stacksâthe Forth shall be first" by Henry Baker 1994](https://dl.acm.org/doi/10.1145/181993.181999)
* ["Iota and Jot" by Chris Barker 2001](https://en.wikipedia.org/wiki/Iota_and_Jot)
   > There is a lot more to this one than can be got at right now.
   > I really do hope that someone is out there (besides just the way back machine) keeping track of things like this before all is lost.
* ['Chris Barker's Iota-Jot-Zot family of esolangs' 2020 by Ilia Chtcherbakov](http://cleare.st/code/iota-jot-zot)
   >Ilia shares my unhappiness with broken links directly to Chris Barker in this one!
* [Chris Barker's current homepage](https://cb125.github.io/) (who knows how long it will be up?)
* [The Joy Programming Language](https://hypercubed.github.io/joy/joy.html)
* ["Concatenative programming and stack-based languages" by Douglas Creager](https://www.youtube.com/watch?v=umSuLpjFUf8)
* [the online space of Douglas Creager.](https://dcreager.net/)
* [Douglas Creager links to concatenative programming stuff](https://dcreager.net/concatenative/)
* [âFactor: A Dynamic Stack-based Programming Languageâ 2010 by Slava Pestov, Daniel Ehrenberg, Joe Groff](https://dcreager.net/papers/Pestov2010/)
* ["A denotational semantics of a concatenative/compositional programming language" Jurij MiheliÄ, William Steingartner, Valerie NovitzkÃ¡. 2021](https://dcreager.net/papers/Mihelic2021/)
* [Robert Kleffner. âA foundation for typed concatenative languagesâ. Master's thesis, Northeastern University. April 2017](https://dcreager.net/papers/Kleffner2017/)
* ["Foundations of Dawn: The Untyped Concatenative Calculus" by Maddox](https://www.dawn-lang.org/posts/foundations-ucc/)
* ["Continuation-Passing Style, Defunctionalization, Accumulations, and Associativity" by Jeremy Gibbons 2021](https://arxiv.org/abs/2111.10413)
* [The Popr Programming Language by Dusty DeWeese](https://www.hackerfoo.com/posts/popr-tutorial-0-dot-machines.html)
    * [on github](https://github.com/HackerFoo/poprc/?tab=readme-ov-file)
* [Functional Bits: Lambda Calculus based Algorithmic Information Theory John Tromp April 23, 2023](http://tromp.github.io/cl/LC.pdf)
* [Tree Calculus by Barry Jay ](https://treecalcul.us/)
* [John Earnest](http://beyondloom.com/)
* [no stinking loops](http://www.nsl.com/)

It is in large part because of the difficulty in assembling this list of links (and finding the sources mentioned by the documents pointed to from these linked sites) that I have largely committed to monolithic methods of writing.
It is not what I would prefer, but it is also not so inconvenient when what I'm ultimately going for is an accurate record of the evolution of my verbal behavior.

### 2025 0417 1613
This continues work on my little lisp from [2025 0415 1548](#2025-0415-1548).

Nil is a proper list and any pair whose right part is a pair whose right part is itself a proper list is a proper list; and a nonproper list is called a dotted list.

```
let isProperList = x => isNil(x) || (isPair(cdrOf(x)) && isProperList(cdrOf(x)))
, isDottedList = x => !isProperList = x;
```

Historically, a pair whose left part is designated by 'x' and whose right part is designated 'y' was designated by '(x . y)' and '(x y)' was short for '(x . (y . nil))' so that, in general, '(x y ...z)' is schematically short for '(x . (y . (...z . nil)...))'.
Thus, the only abbreviations which contained a dot were those whose right parts were nonnil atoms e.g. '(^a ^b . ^c)' is short for '(^a . (^b . ^c))' and '((^a ^b) . ^c)' is short for '((^a . (^b . nil)) . ^c)' where all atoms beginning with '^' designate their self.



### 2025 0417 1525

Some writing projects that I may never get around to:

* [Metamath](https://us.metamath.org/) isn't like Quine's schematic methods (even though they go into a lot of detail about how they extend Quine's schematic methods)?
    * The theory upon which metamath is based: [Megill, âA Finitely Axiomatized Formalization of Predicate Calculus with Equality,â Notre Dame Journal of Formal Logic, 36:435-453, 1995](https://us.metamath.org/downloads/finiteaxiom.pdf)
    * Some explanation of how that theory works in practice: [Metamath is a metalanguage that describes first-order logic](https://us.metamath.org/mpeuni/mmset.html#mmname)
* FORTH and the final sentence of McCarthy's [history] of LISP.
   > Who am I kidding, I'm definitely going to write about that.
* [The Kitten Programming Language](https://kittenlang.org/) and its relation to FORTH.
* [Big History](https://en.wikipedia.org/wiki/Big_History) and ages old cosmological methods.
* more to come...

> Side note. the markdown convention of using square brakets around the display text for a link which is given in curved brackets does not work: I constantly put square brackets where curved brackets go and vice versa.
> This seems like something that would have been quickly uncovered with a little "beta testing".
> Dare I say that a solution may be found in some method of slashes? 

## 2025 0416

### 2025 0416 2358
Today is the first day that a problem I've had for some time was clearly revealed to me: I'm having trouble keeping up with my self.
Hopefully tomorrow I can get out some more work that I did yesterday and today:

* Paths in binary trees and bit strings (still haven't found a beautiful bijection between bits and trees)
* I never finished the [latest entry on my little lisp](#2025-0415-1548) (even though I worked a lot more on it that day)
* I never got around to writing out what I meant by "we live in a stack based world".
* Read more of Durant's "story of philosophy" and failed to write down any of what I had to say (just because I have something to say does not imply it is even remotely important, but it is easier to make that judgement once I've said it on a page than to my self).
* Haven't even started writing about working out through setbacks: I want to capture the pain of that dip and better understand why it always seems like a bigger deal than it ever really is.
Something about that old saying "You're stronger than you think." e.g. there is greater strength which your culture has yet to release.

Oh! And the Durant reading was on Greece, Socrates, and Plato.
To my great surprise, I just discovered that Whitman's 1865 "O Captain! My Captain!" is specifically described as a "Ship of State" metaphor which is most famously mentioned in Plato's "Republic" Book 6.
This is something like an example of what I mean when I say, metaphorically, reading always makes the world seem larger and smaller than it once was.

### 2025 0416 2313
I left a note for my self on twitter.
This is me making the note I'd made a note for my self to note.

The account @ResonantPyre, a friend, wrote a few paragraphs on Wittgenstein's 1935 'Lectures on Personal Experience'.
Nonsense is under the microscope.
'Sound and Sense' was the title of a book on poetry that I was forced to buy to finish some requirement of my undergraduate education (little did I know what a wonderful friend I would find in that professor even though I was a fish out of water).
I defaced my copy by prepending a 'non' at each place where 'sense' occurred.

Meaning, sense, semantic agreement, and all other spooks of sentences are cast out, reluctantly, with the full thrust of Ramsey's proxy functions encapsulated by Quine's "cosmic complement".
Indeterminancy of reference alone does not preclude propositions (the poltergeists of sentences).
On occasions we fix our meaning of a sentence to outright intersubjective agreement: *unus pro omnibus, omnes pro uno*.

The problem with propositions--- meanings made eternal--- is their purported transcendence: everywhere forever, forever everywhere.
A well paraphrased verbal response stands for now and does well enough without proposing propositions.
We lose nothing by leaving those who are lost without them: they come back around eventually.

We go to dictionaries "to get the meaning of the word" and when we open to the right page we get on where we left off.

There's obviously much more to this all than anything like the flob above.
It's okay to have some fun when you fill it out with fuller explanations sooner rather than later.

### 2025 0416 2234 O Russell! My Russell!

When I was young, and still very much under the control of Russell's *Principles* and *Principia*, I found my way to Egner and Denonn's "The Basic Writings of Bertrand Russell" and clung to it as an other might the primary text of their religion.
Everywhere I found the methods of Principia applied to Russell's world.
That world was one I hoped to glimpse within my own.

It is only now, some fifteen years later, that I see less of Russell and more of logic in his memorabilia.
It was always the logic of his works, and not their well written projections, that caught and kept me reading.
The error of Russell's ways were somewhat corrected by him.
Clarity and exactness were logic disguised and upon confronting this perversion (he excised logic overtly in "An Inquiry into Meaning and Truth") he left the fruits of philosophy for something more attractive to modern minds.

Logic looms throughout his works, but the mind left him open to mystical influence.
He rested in a way somewhat more obscure than Descartes "I think therefore I am."
It is, for me, the difference between singular terms and Quine's elimination of them by Russell's own singular descriptions.

Expedience was never sufficient for Russell, there was always more to it than just that.
Thus, even though he is so often worshipped as a controversial figure, it has gone unnoticed that public comprehension is a halmark of convention (whether such conventions are in vogue is another matter e.g. nothing sells quite like good versus good).

Russell was no real threat to the world.
For all that was hoisted upon him through his personal affairs, he made a cuckold of Frege and logic got lost in the shuffle.
Quine remediated what Russell ruffled.
Carnap did too, but never got far enough from Russell's outlook e.g. his philosophy of physics is limp.

What once of logic had exhausted Russell was energized by Quine: sprawling theories and unspooled speculations were cut off from their metaphysical wellsprings and reality pierced the veils of clarity and exactness.
Rather than complexify the world, Quine made due with the complications of a worldclass watchmaker.
But whither the warmth of love and the sensations of the soul amongst such machinations? 
Those who clutched for them betrayed their own insecurities.

Principia Mathematica was a report from the frontlines of the final war which we wage on to this day.
It tormented him for the rest of his life: had he done more to help the world or to hurt it by having so vividly revealed the brutality of the battlefront?
There seemed to be no peace nor no hope in the trenches.
This would not keep the homefires burning: what else was he to do?

## 2025 0415

### 2025 0415 1915
As is often the case, I have added some other books to read in parallel with Druant's "The Story of Philosophy". They are
* Russell's "The History of Western Philosophy", and
* Grayling's "The History of Philosophy".

I'm pretty sure those are the only books in my library that are directly aimed at giving a sorta story or a sorta history of philosophy as a whole.
There is also the classic

* "Introduction to Philosophy: Classical and Contemporary Readings, 4th edition" edited by Perry, Bratman, Fischer

that may come up in what may be a larger piece of the reading puzzle I have to solve in order to do all that I must before I die.
As much as I recommend people go directly to primary texts, there is no good way to get started on the whole of a thing than by reading how someone else did it: there are some writers who can explain what they did as if it was done by an other and they are to be cherished.
We're all human, in the end, and that alone binds us into the most startling of enterprises. 

### 2025 0415 1557

It seems that people who were a part of history wish they had written down more of the history of which they were a part.

There is less to learn from what was forgotten than from what was remembered.

Time tramples all.

> This occurred to me while reading [John McCarthy's 1979 "History of Lisp"](https://justine.lol/sectorlisp/lisp-history.pdf).

### 2025 0415 1548

First a summary of the work done on my little LISP from [202504112248](#2025-0411-2248)

```
let consOf = (car,cdr) => [car,cdr]
, carOf = cons => cons[0]
, cdrOf = cons => cons[1]
, nil = []
, isIdentical = (x,y) => x==y
, isNil = x => isIdentical(x,nil)
, isPair = x => Array.isArray(x)
, isAtom = x => !isPair(x);
```

In English (that carefully distinguishes between quotations and their purported designations for reasons that shall be explained later),
* the function designated by 'consOf' takes two arguments and gives back a javascript array with the first argument indexed by zero and the second argument indexed by one;
* the function designated by 'carOf' takes one argument, which it expects is the result of an application of 'consOf', and returns the item of the argument indexed by zero;
* the function designated by 'cdrOf' is like the one designated by 'carOf' but it returns the item of the argument indexed by one;
* the item designated by 'nil' is a javascript array of length zero;
* the function designated by 'isIdentical' takes two arguments and returns the javascript item designated by 'true' where the first argument is `==` to the second (in the language of javascript) and returns the javascript item designated by 'false' elsewhere;
* the function designated by 'isNil' takes one argument and returns the js item deisgnated by 'true' where it is `==` to the js item designated by 'nil', and returns the js item designated by 'false' elsewhere;
* the function designated by 'isPair' takes one argument and returns the js item designated by the application of 'Array.isArray' to it; and
* the function designated by 'isAtom' takes one argument and returns the js item designated by 'false' if the js item designated by the application of the function designated by 'isPair' is designated by 'true', and the js item designated by 'true' otherwise.  

From my work on [Bit Strings and Binary Trees](#2025-0413-1513-bit-strings-and-binary-trees), the practice of designating a function by ending it in 'Of' and designating the functional representation of a predicate by beginning it with 'is' helps a lot when working out the logic of the programs under construction.
This is also a partial explanation for all the 'designated by's and quotations that spell out the letters of what most people would call the *names* of the designated functions or objects.

I'm writing here now after having written the following section to say that it goes a little too far from talk about programming a LISP in javascript and if that's all that you care about then you can just skip past this break to the next one.
What you're missing is a more detailed explanation of the general plan for dealing with syntax and semantics.
It doesn't get too deep, but does go even further out of bounds than I can presumably tolerate.

---

As much as there is the temptation to write phrases like 'the function consOf' or, better, 'the function `consOf`' which resorts to a different typeface in order to weakly emphasize that there is something different about the purported designatum of the so rendered phrase, this has only led to decades and centuries of mistakes and wasted efforts promulgated by untaught writers and readers.

This is especially the case when writing and reading programs e.g. the problems that so many people have with so called 'pointer arithmetic' is no such simple thing as a 'bad design decision of a particular programming language'.
The problems of meaning and reference, the life blood of semantics, crop up wherever languages are planted (which includes some of our most powerful social technologies).
They are fundamental problems that can not be swept under the rug before the guests arrive.

When a writer leaves it to the reader to figure out what is supposed to be a quotation and what purports to be designated by such a quotation, there is only hell to pay.
Even if the writer commands the reader to take on the responsibility of "keeping conventions in mind" this only works if there are other reasons for them to do so (I think of Kleene's 1952 "Introduction to Metamathematics").

This problem is not as unfamiliar nor as fussy as it seems e.g. classic problems of scoping in logic and in programming are fundamentally about not having resolved a concrete method of reference or meaning which deals well with syntactic and semantic agreement.

Rather than ignore the fundamental problems I'll pick the strongest, perhaps tentative, methods that are already within my reach and take it from there: nothing is as final as finality claims to be.

I have already mentioned Quine multiple times throughout these notes and to anyone who is so unfortunate to have followed me on twitter.
As much as people detest Quine's obstinate rigor (a term that I only just discovered is already stated poetically as "Ostinato Rigore"), the articulation of his uncertainty is beyond anything else that I've seen.

The plan is simple: semantics breaks into reference and meaning, reference breaks into designation and denotation, and denotation 

> "is where the action is. It takes designation in stride, for a singular term can be recast as a predicate that happens to denote just one thing if any.
> The singular term 'Boston' *designating* Boston, can be reconstrued as a predicate 'is Boston', *denoting* only Boston.
> Anything said about Boston can be paraphrased using 'is Boston'."[pg. 59 of Quine's "From Stimulus to Science"]

Quine shows just how denotation comes to be defined in consistent theories by first generalizing Tarski's method of defining truth to denotation and then admitting a hierarchy of predicates of denotation, finite in number for a given theory of denotation.
This settles, once and for all, the work of denotation in a theory which is rich enough to admit a method of semantic ascent e.g. by way of quotations.

While such a hierarchy of predicates of denotation are less than what most expect, e.g., from a comprehensive theory of truth ("Truth, one might risk being quoted as saying, is just a degenerate case of denotation."[pg. 65 Quine's "From Stimulus to Science"]) it is all that can be got without plunging any such theory into the classic antimonies.

Now, among the practices of programming there are those called "denotational semantics".
This is but one way of explaining what so many people speak of as the meaning of a given program.
It is much easier to see how all such talk paraphrases into Quine's methods by beginning with a programming language which drops variables from the beginning and hence eases the complexity of any subsequent definition of programmatic denotation as a projection from a logically consistent theory of denotation.

Sadly, now is not the time or place for me to complete such a project.
As almost always seems to happen when working on interesting things, other interesting things come up, they are temporarily entertained for the sake of maintaining momentum, and then when they start to take over and drive at a new direction, there's nothing left to do but drop it and return to where the enthusiasm last let off on the path you had original planned.

I won't leave the other part of semantics hanging though: meaning.
The simplest explanation of how I deal with it is to point again to Quine.
This is not because I accept Quine without criticism or correction, but because there is no one who has yet combined the philosophies of Quine with the philosophy of Skinner's radical behaviorism in the way that seems to be characteristic of my outlook and actions in regards to these matters.

In the classical story, sentences have meaning.
It is what a sentence means that matters most when we speak with each other.
If you don't know what a sentence means, then the sentence is effectively worthless to you.
Sentences are merely the aftereffects of meaning.
In the metaphor of traditional signals processing, it is sentences that send the message but it is the meaning that is encoded or decoded from the sentence.

Skinner's analysis of verbal behavior ends all speculation as to what potential philosophic value there can be to disputing the existence or nonexistence of meanings (or information, in the modern parlence, or propositions in more sacred realms).
But, meanings die hard, and it is harder still for most to imagine a world without meanings much less to get along in one.
While Skinner drops meanings outright and simply begins without them until he comes around to the verbal behavior that we otherwise said would not occur without the existence of meanings, Quine gives traditionalists a good shake.

He starts with "sameness of meaning" rather than with meaning.
At least we can say when two sentences "have the same meaning" if we can not say what the meaning is, or if we might somehow get along without purporting anything more than a predicate "means the same as".
Just as in the case of denotation, I can not go through how "sameness of meaning" goes broke.
The concluding outlook is this:

* reification through joint reference in that weaker form of universal closures of conditionals that Quine calls "focal observation categoricals" and which burgeon into micro spatiotemporal theories of the world cover their own ground, i.e. we are justified in speaking of "sameness of reference" between the pronouns of focal observation categoricals,
* these very grounds give us the power of intersubjective sameness of reference with repsect to gross bodies (this being little more than the classic "by ostention"), and
* abstract items, such as orderd pairs, receive only the most conservative of treatments
   > "I submit that intersubjective sameness of reference makes no sense, as applied to abstract objects, beyond what is reflected in successful dialogue." [pg. 70 Quine's "From Stimulus to Science"].

For more detail on any of these things I can point to the text quoted, oh so many times in this entry, Quine's 1995 refinement of his 1990 talk "From Stimulus to Science".
It is a hard read, something which I have said many times before, but the value of each of its words and sentences is second only to the value of his "Methods of Logic 4th edition".

That all being said...

---

Thankfully I wrote up all that thinking because, for some reason which is not clear to me, it let me see that 'nil' is to designate a symbol/atom and that should help me with the design of this little LISP!

Here is the code from above revised 

```
let consOf = (car,cdr) => [car,cdr]
, carOf = cons => cons[0]
, cdrOf = cons => cons[1]
, nil = 'nil'
, isIdentical = (x,y) => x==y
, isNil = x => isIdentical(x,nil)
, isPair = x => Array.isArray(x)
, isAtom = x => !isPair(x);
```

This entirely avoids the problems that come with 'nil' designating a javascript array.
So the key examples that show the critical change are:

```
isPair(nil) 
 false
isAtom(nil) 
 true
```

The 'LISt' in 'LISt Processing' is defined inductively as

1. the item designated by 'nil' plays the part of the empty list
2. a list whose first item is designated by 'x' and the rest of whose items are listed by the item designated by 'y' is designated by 'consOf(x,y)'.

Recursively we define proper lists as 
```
let isProperList = x=> isNil(x) || (isPair(cdrOf(x)) && isProperList(cdr(x)))
, isDottedList = x => !isProperList(x);
```

As the second definition suggests, a list which is not proper is called dotted because of the historic practice of writing a pair whose left part is x and whose right part is y as '(x . y)'.
This matters because when printing atoms and pairs there will be times when a dotted list must be constructed so that the person looking at the printed result can tell the difference between '(^a ^b)' and '(^a . ^b)'.

## 2025 0414

### 2025 0414 2055
This entry continues my read of Will Durant's "The Story of Philosophy" from [202504132323](#2025-0413-2323).

The introduction hovers around a familiar analogy: science is to knowledge as philosophy is to wisdom.
He says that science analyzes and philosophy synthesizes; that science takes apart what philosophy must put back together; that science gives us the power to do only what philosophy can tell us is worth doing; that science without philosophy has no value in that science without philosophy is like a fact without a feeling; that we must not forget that science is a descendant of philosophy; and that philosophy brought science into this world and can just as easily take it out of this world.

Next, he breaks apart philosophy in the traditional way, only to follow his own advice and focus on "the great men of philosophy" rather than the great fields of philosophy:
* Logic "is the study of the ideal method in thought and research: observation and introspection, deduction and induction, hypothesis and experiment, analysis and synthesis" [pg. 3]
* Esthetics (or Aesthetics) "is the study of ideal form, or beauty; it is the philosophy of art" [pg. 3]
* Ethics "is the study of ideal conduct; the highest knowledge, said Socrates, is the knowledge of good and evil, the knowledge of the wisdom of life" [pg. 3]
* Politics "is the study of ideal social organization (it is not,a s one might suppose the art and science of capturing and keeping office); monarchy, aristocracy, democracy, socialism, anarchism, feminism--- these are the *dramatis personae* of political philosophy." [pg. 3]
* Metaphysics (which he disdains as you will see) "And lastly, *metaphysics* (which gets into so much trouble because it is not, like the other forms of philosophy, an attempt to coordinate the real in the light of the ideal) is the study of the "ultimate reality" of all things: of the real and final nature of "matter" (ontology), of "mind" (philosophic psychology), and of the interrelation fo "mind" and "amtter" in the process of perception and knowledge (epistomology)." [pg. 3]

The chapters outline the path Will takes through the story of philosophy:

1. Plato
2. Aristotle and Greek Science
3. Francis Bacon
4. Spinoza
5. Voltaire and the French Enlightenment
6. Immanuel Kant and German Idealism
7. Schopenhauer
8. Herbert Spencer
9. Friedrich Nietzsche
10. Contemporary European Philosophers
11. Contemporary American Philosophers

In the introduction to the second edition Will makes repeated apologies for what he has already apologized for in the main text: it is riddled with sentences that would have experts on the relevant topics fuming and it leaves out Eastern philosophers entirely (which he tried to correct in the first volume of "The Story of Civilization").

There is no book on philosophy as a whole which has not failed to present that whole without holes.


### 2025 0414 2041
Yesterday, I stumbled on <https://www.game-cities.com/> by Konstantinos Dimopoulos who shared a link to [Ultima and Worldbuilding in the Computer Role-Playing Game
Carly A. Kocurek and Matthew Thomas Payne](https://services.publishing.umich.edu/Books/U/Ultima-and-Worldbuilding-in-the-Computer-Role-Playing-Game) that can be read online for free.

It's a short and quick read.
I'm always left wanting more from the books I read on the history or philosohpy of various video games.
In the end, the books that I want on my favorite games are basically literate programs that give an elaborate history of each design decision.
That is not yet the kind of book that is easy to get your hands on.

I hope to come back to Dimopoulos' site and read some of his [articles](https://www.game-cities.com/articles-talks) on urban design in video games.


### A Stack Notation for Predicate (Functor) Logic 2025 0414 1626
A way of eliminating variables from predicate logic which combines the methods of Quine's predicate functors with the practices of programming in J and FORTH permits predicate letters which, initially, generalize the familiar two place predicate notation--- e.g. 'x is identical to y', 'x belongs to y', and 'x is the father of y'--- by schema such as 'xyzFstuvw' so that each predicate has a number of left places--- three in the example schema--- and a number of right places--- five in the example.
Thus, the predicate 'x pairs y with z' is rendered perspicuously as 'xPyz'.
The left places are referred to as the pile of the predicate and the right places are the list of the predicate.

The predicate functors of recombination (which generalize the stack notation of FORTH to predicate logic) are 

1. '...xy(DROP F)...z' for '...xF...z'
2. '...wxy(HEM F)...z' for '...wxyFx...z'
3. '...x(PUSH F)y...z' for '...xyF...z'

from which all others are defined e.g.

4. 'OVER F' for 'HEM PUSH F'
5. 'OVER2 F' for 'OVER OVER F'
6. 'OEM F' for 'OVER HEM F'
7. 'DSH F' for 'DROP PUSH F'
8. 'DUP F' for 'OEM DSH F'
9. 'DROP2 F' for 'DROP DROP F'
10. 'POP F' for 'OEM DROP2 F'
11. 'NIP F' for 'POP DSH F'
12. 'HIP F' for 'HEM NIP F'
13. 'HIP2 F' for 'HIP HIP F'
14. 'SWAP F' for 'HIP PUSH F'
15. 'PUSH2 F' for 'PUSH PUSH F'
16. 'TOR F' for 'HIP2 PUSH2 F'
17. 'ROT F' for 'TOR TOR F'

and so on.
The remaining predicate functors (which generalize the tacit notation of the J programming language to predicate logic) are defined from

18. '...x(NOT F)...y' for 'not ...xF...y'
19. '...x(F AND G)...y' for '...xF...y and ...xG...y'
20. '...x(SOME F)...y' for 'some item is (z such that ...xzF...y)'.

For more on predicate functors and the method of homogenization which permits translation back and forth between predicate functor logic and 'predication' logic see Quine's "Methods of Logic 4th edition".

> Predication logic is the most appropriate name for what is unhappily called 'first-order logic' or otherwise called 'predicate logic'.
> As I see it, with predicate functors elimination of variables, the only items in the lexicon of a logical theory are the predicates and hence 'predicate logic' is what might otherwise be called 'predicate functor logic'.

There are two things forthcoming:
1. an implimentation of the algorithms that transform a pure predicate functor term into a sentence (perhaps open or perhaps closed) of predicate logic, and vice versa; and
2. a further generalization of the above notation which incorporates all four of the following:
    * Quine's predicate functor notation,
    * FORTH's stack notation,
    * J's tacit notation, and
    * LISP's list notation.

There are additional changes which permit a full generalization of APL and J's functional operators that generalize e.g. matrix multiplication.
These correspond to predicate functor functors: you can continue to go up and up the grammatical hierarchy and even contemplate predicate functor functor functors if you desire.

Though, it appears as if there is a limit to how high you can go and still get something helpful out of it: it seems to me that predicate functor functors are as far as you can go since after that everything else looks exactly like the first three levels of operations on the grammatical hierarchy of a logical language.

### 2025 0414 1605
Today is a Nat King Cole day.
His voice, his piano, and his trio have brought me hours of delight.

Some things I'd like to get done today:
1. Finish the very short paper on "A Stack Notation for Predicate (Functor) Logic"
2. Finish my little parenthetical space sperated language
3. Read some more of "The Rise and Fall of the Third Reich" by Shirer and published October 17, 1960.

I've read that Shirer's account is 'outdated' or 'narrow' or 'not historically accurate'.
No book is historically accurate: we wouldn't know how to check for historic accuracy even if our life dependend on it.
That is a bit of an overexaggeration: it is largely the result of the metaphorical distance between our records of day to day life and works like "Schedules of Reinforcement" by Skinner and Ferster where the challenges of keeping an accurate record of behavior are uncovered in all their ugly details.

Whether Shirer's book is adequate for an understanding of the third reich is moot: no book is going to give "the whole story" as if there was some big story in the sky that would truly enlighten us if we could just reach it.
I see what I've read of Shirer so far as a starting point: it is so well written that you would be hard pressed NOT to finish it.
Not all history books have that same luxury.

So far, I've read that Richard J. Evan's "Third Reich Trilogy":

1. "The Coming of the Third Reich" published October 2003
2. "The Third Reich in Power" published October 2005
3. "The Third Reich at War" published October 2008

is a much more accurate report on the contingencies that selected and ultimately extinguished the third reich.

I have so many books to read that it is hard to see how I'll ever get to reading them all. 

## 2025 0413

### 2025 0413 2353
While skimming over [John McCarthy's 1979 "History of Lisp"](https://justine.lol/sectorlisp/lisp-history.pdf) two things caught my attention:
1. he mentioned that Quine had used prefix notation in some thing he or those around him had read and that this had some influence on LISP's prefix notation (pg. 7)
2. He mentioned a paper he wrote with Cartwright (1978) that "show how to represent pure LISP programs by sentences and schemata in first order logic and prove their properties"(pg. 8)

He also mentioned something about LISP having no effect on those working in recursion theory, but that now seems to be a historical hiccup.

My interest in (1) is much less than my interest in (2).
I am reading the mentioned paper now: ["First Order Programming Logic" by Cartwright and McCarthy in 1979](https://dl.acm.org/doi/10.1145/567752.567759).

I just finished reading it (202504140019) and am both happy and sad.
My interest was primarly the result of being consumed by the methods of logic programming.
But, alas, "programming logic" and "logic programming" though just one swap away from each other does not bring them together as one.
That is the sadness: this paper does not not reduce pure LISP to the methods of logic programming.

What it does do though is still very important, and I will have to read this paper again before I can explain it and the many things that it brings up as they relate to logic programming: it calms the anxieties of those who continue to seek something beyond predicate logic when making proofs about programs.

Cartwright shows how to set up a theory (kind of like the traces in what Hoare first introduced as [1978 "Communicating Sequential Processes"](https://dl.acm.org/doi/10.1145/359576.359585)) that transforms partial functions into total functions allowing for the introduction of an equivalence axiom that permits convenient proofs of properties of the original partial function e.g. an interpreter.

McCarthy's contribution is a minimization schema for each partial function (this is very likely akin to the schematic method I devised for setting up a schematic theory of transitive closures of an unspecified predicate of the lexicon of the theory) that ends up being equivalent to Cartwright's method.

Together these methods indicate clearly and exactly that no more than predicate logic is needed to reason about recursive programs, even those once as unfamiliar as interpreters/compilers.

These are happy things to know because it strengthens the general significance of my proposal that the practices of programmign are simply a subcollection of the practices of predicate logic.

The significance to this specific paper, and any others like it, is that such formalisms are no more a surprise than a formalism of predicate logic itself.


### 2025 0413 2323
I got a copy of Will Durant's "The Story of Philosophy" some months ago from a book fair.
Though I read some of it back then, I do not recall any of it now.
One of the joys of reading SO MANY *fundamentally interesting* things (sometimes over and over again) is that you can actually forget some of them!

Most of what I read is nonfiction, but I've started reading fiction recently e.g. Agatha Christe, le Carre, Chaucer, and Dante.
Most of the fiction I've read in my life is from when I was less than fifteen years old e.g. Asimov, Pullman, Tolkein, Salinger, Wells, Orwell, and Shelly.
Since I haven't reread those books I'm uncertain whether I have forgotten them or not.

Anyway, I finished reading the introduction to Will Durant's "The Story of Philosohpy" and had some things to say (apparently).

---

The success of Will's book "The Story of Philosophy" allowed him and his wife to write "The Story of Civilization" which I have yet to finish reading (in fact, I still have yet to finish the first book in the series!).
I am drawn to the Durants because they wished to see the world as a whole: to put Humpty Dumpty back together again.

Mine is the second edition and the eighth printing: copyrighted last in 1933 and published no later than 1953 by Simon and Schuster.

The introduction to this second edition echos my own outlook on our modern world: although we are technically more interconnected now than ever before, there is less wisdom among us than ever there has been.
Much of our world was designed and built by a few of us.
They have drained knowledge of all that makes it rich.

To be wise is to threaten the prevailing world order.
The wise see beyond the shades of political esotericism through to the fundamental conflict between controllers and countercontrollers.
Hope has not died, and it is still expected by a few of us that beyond the bounds of individualism and collectivism there is something like a peaceful culture: one that does the controlling and countercontrolling through us in only the most conspicuous of ways.


### 2025 0413 2249
I finally finished my entry on [Bit Strings and Binary Trees](#2025-0413-1513-bit-strings-and-binary-trees)!
It took longer than I expected, but I also uncovered some unexpected things along the way e.g. a quick and easy way to find shorter bit strings for a given binary tree.

Something I also learned: there is no faster way to catch bad writing than to publish it.

Something else I learned: if you don't end a markdown list with two returns it thinks the trailing line of text is part of the last entry in the list.

### 2025 0413 1754
A friend just introduced me to two great bits of music:
* [Alberto Ginastera](https://en.wikipedia.org/wiki/Alberto_Ginastera) - Harp Concerto (1956)
* [Seru Giran](https://en.wikipedia.org/wiki/Ser%C3%BA_Gir%C3%A1n_(album)).

I was able to guess that Ginastera had learned from Copland.
I described Ginastera as a "faster Copland".

Seru Giran was too short: I wanted more after listening to it.
It is a very comforting album that seemed to combine popular styles from both North America and England.

### 2025 0413 1644
If I could just finish one thing before finishing another thing then I would be able to finish more things that people care about now than things they'll care about later.
This is a roundabout explanation of why I work the way that I do.

### 2025 0413 1634 Buffett's Jet and Apple's Three Jets Full of iPhones

Warren Buffett wrote about selling and buying a jet in his [letter to Berkshire sharehodlers in 1989](https://www.berkshirehathaway.com/letters/1989.html).
On more than one occasion I have had to explain to people that it is more than a luxury, though that is mostly what the shareholder letters show off as the big problem.

There is almost no limit to the potential value of being able to go from point A to point B around the world with tangable objects in tow.
Apple's three jets full of iPhones should help people to more easily grasp why Warren went from naming his jet the 'indefensible' to the 'indispensable'.

### 2025 0413 1513 Bit Strings and Binary Trees
Bytes of memory stored in a computer can be treated as strings of binary digits or as little binary trees.
Let me explain what I mean while I give you a javascript program that decodes a binary tree into a bit string and encodes a bit string into a binary tree.
(Most people would swap 'encode' and 'decode' in that sentence.)

Computers tend to use bits, bytes, and sometimes other more exotic words to do either place value arithmetic (binary in almost all cases, ternary in some rare cases) or to address other words of memory.

> Technically they do place value arithemetic of remainders (sometimes with respect to the word size, and othertimes in less obvious ways that are otherwise more familiar as "logical operations").

Storing addresses is an easy way to avoid having to copy words to new locations: go to where the relevant data is rather than waste the time and space to replicate it somewhere else.

Words also have another vital role to play in computers: they store commands that end up telling the computer what to do with all those addresses and numbers.

There is an alternative way of looking at words as lists of ones and zeros: each string of bits corresponds to a binary tree and each binary tree corresponds to a string of bits.
It is easier to decode a binary tree into its binary string than it is to encode a binary string into its tree.

Since the binary trees contemplated here are all dirty--- they are all built from pairs whose subcomponents are pairs or the nil pair--- there is only one terminal condition which must be checked in any recursive definition: is this the nil tree?
Before that, here is the code for building pairs from their left and right parts and for getting the left and right parts from a pair.

```
let pair=(leftPart,rightPart)=>({leftPart,rightPart})
, leftPartOf=pair=>pair.leftPart
, rightPartOf=pair=>pair.rightPart;
```

The nil pair is not to be confused with the empty pair: the empty pair has no parts, but the nil pair is a special pair (or what some might call an atom) that is identical to its left and right parts (see [2025 0411 2248](#2025-0411-2248) for more on the logic of atomic and empty items).
The way we make an object like that in javascript is a bit esoteric:

```
let nil={get leftPart(){return this},get rightPart(){return this}};
```

Setting up getters (and setters) with javascript objects is just weird and I really only read the documentation for such things when I already really know what I want.
Otherwise, you will get lost trying to make sense of the design choices that brought us the programming language of the internet.
There is an alternate world where LISP was the language of the internet: that world would have been better than this one.
An even better world is one where it was FORTH, but I digress.

The above definition of 'nil' makes it so that 

```
nil == leftPartOf(nil) && nil == rightPartOf(nil)
  true
```

which happens to be the defining feature of atoms in a more general setting:

```
let atom=x=> x==leftPartOf(x) && x==rightPartOf(x);
```

Sadly, we won't be needing to talk any more about atoms.
We just need to know whether a given tree is nil or not:

```
let isNil=x=> x==nil;
```

Oh, I should explain the difference between 'dirty trees' and 'pure trees' because most people are familiar enough with 'pure trees' and few people use the phrase 'dirty trees'.
Dirty trees are ones that include something other than pairs or the empty pair among their subcomponents.
Pure trees are ones that don't include anything other than pairs or the empty pair among their components.
They are pure in more ways than one: they do not admit of talk of atoms, and if you go down one branch or another you eventually hit the empty tree (but it may take infintely long to get there in some cases, those also being cases that I would say are inappropriate).

So now we have all we need in order to talk about binary trees.
Onto what we need to talk about bit strings:

```
let zero='.'
, one=','
, concatenate=(...strings)=> strings.length ? strings[0]+concatenate(...strings.slice(1)) : '';
```

There are two building blocks to bit strings: zero and one.
They are to be distinct:

```
zero != one
  true
```

and they can be combined by concatenation.
The principles governing concatenation, like the principles governing binary trees, are actually a lot more elusive than it first seems: thankfully, if we just go with what javascript gives us there is no reason to get stuck in the theories of strings and trees, even though those can be fun places to be stuck.

The function designated by 'concatenation' takes any number of arguments that are strings and puts them together in the order they are taken (from left to right).

```
concatonate(zero,one,zero,one,one)
  .,.,,
```

Why use '.'s for zeros and ','s for ones?
Because then the binary decoding of a tree is also the program for the stack machine that actually constructs the binary tree it decodes!
But first, lets decode some trees:

```
let decode= tree => isNil(tree) ? zero 
: concatenate(decode(leftPartOf(tree)), decode(rightPartOf(tree)), one);
```

In English, decode takes a tree and first checks if it is identical to nil.
If it is then we already know how to decode that: it's just a zero.
If it isn't nil then we have already made the assumption (quite a strong one really) that it must be a pair with a left part and a right part.
So, all we need to know to write out the decoded tree is to
1. decode the left part of the tree and
2. decode the right part of the tree.

Once we have those two things we just write out the code for the left tree, followed by the code for the right tree, and cap it all off at the end with a one.

It will be shown that the decoded strings are almost always longer than they need to be: any initial zeros can be trimmed from the front of the decoded string without any loss to subsequent encoding.

Some examples

```
decode(nil) 
   .
decode(pair(nil,nil)) 
   ..,
decode(pair(nil,pair(pair(nil,nil),nil))) 
   ...,.,,
```

The decode function is one-to-one (injective i.e. each string that comes out of it is only the result of decoding one and only one tree).
There are some bit strings that will never be the result of such decoding e.g. ',' and '.,'.
Thus the method of decoding is not a one-to-one correspondence (bijection i.e. each tree matches to one and only one string and each string matches to one and only one tree).
In general, there is a philosophic simplicity to establishing one-to-one correspondences from one universe of items to another.
For more on the fruits of such labors see Quine's 1946 "Concatenation as a Basis for Arithmetic".

When treated as postfix notation (as in reverse [Åukasiewicz notation](https://en.wikipedia.org/wiki/Polish_notation)) the ',' comes to work as the function 'pairUp' of a [stack machine](https://en.wikipedia.org/wiki/Stack_machine).
So many of the practices of programming are described by tiny stack machines that there is great value to being familiar with them perhaps without ever programming a real stack machine.

From the method of decoding it can be seen that the last ',' pairs together the left and right parts of the tree.
This is how the encoder works!
It simulates a little stack machine and uses the bit string as the list of commands necessary to construct the decoded tree.

First, we can  use a tree to simulate a stack.
All we need is a way to push, pop, and maybe peek at the top of the stack.
When I implement stacks with trees, I always take the top item to be the right part of the tree simulating the stack.
Some people prefer to take the left part of the simulating tree as the top, and there's nothing mathematically wrong with that (perhaps not even something logically wrong with that), but the programming language LISP has long since forced us to interpret such a setup as a list and not a stack i.e. the left part of a tree is the first item in a tree simulating a list.

```
let push= (stack,item) =>pair(stack,item)
, pop= stack => leftPartOf(stack)
, peek = stack => rightPartOf(stack);
```

If instead of allowing nil into our universe of pairs we had stuck ourselves with a pure theory of pairs, then 'peek' and 'pop' would have to be written differently e.g.

```
let error = message => console.log(message)
, emptyPair=[]
, isEmpty = pair => pair.length == 0
, purePop = stack => isEmpty(stack) ? emptyPair : leftPartOf(stack)
, purePeek= stack => isEmpty(stack) ? error('empty stack') : rightPartOf(stack);
```

Most people teach and prefer the implementation that includes error messages.
They want the machine to tell them when something is wrong by anticipating how it could go wrong.
My preference is for a crash based method of design: rather than go looking for errors leave as much of the design as possible out on the rim of "don't cares".
Not only does this make your code a lot shorter (which makes it so there are fewer places where things could ever possibly go wrong), it also releases you from the anxiety of trying to catch your mistakes before you make them.

There are some standard techniques that we pick up over time for avoiding problems that have already happened again and again in the past.
Some people are taught by others to avoid them and some are taught by the immediate consequences of having made such mistakes.
The latter almost always teaches better than the former, but that is not the end of this general problem as to how to practice programming.

There is no universal way to detect logical errors in programs.
No one is there to look over your shoulder and tell you "Hey, this isn't actually what you wanted to make in the first place."
Though we have new machines that speak to us in such conversational tones--- after having been forcefed some scrap of code--- there is no part of such machines that prepares them for anything more than a world like the selecting past (but that goes a much longer way than most people once supposed, e.g., half a decade ago).

But, there is a way to entirely avoid logical errors: that is what logic itself is for!
Thus, rather than make trouble for ourselves by starting with the logic of pure binary trees (where, e.g., the definition of 'properPop' deals clumsily with degenerate cases), we begin with a theory that admits no such road bumps even if, in degenerate cases, it gives us unfamiliar results (e.g. the definition of 'pop' I have adopted here gives nil when you pop nil as the simulation of the empty stack).

Back to encoding bit strings to trees!
I already gave away the key to solving this problem: the bit string is a program that tells a tiny stack machine how to build the encoded tree.
Thus, we go along the bits, from left to right, and when we hit a zero we push a nil onto the stack, and when we hit a one we pop the top two trees on the stack and pair them into a new one that we push back on the stack.

Unlike pairs, there are no standard ways of getting at the anatomy of a string (there is much more to say about that and about Quine's protosyntax in general).
Thus, we go with some traditional methods:

```
let emptyString=''
, isEmptyString = string => string==emptyString
, isZero = string => string == zero
, isOne = string => string == one
, firstOf = string => isEmptyString(string) ? emptyString: string[0]
, restOf = string => isEmptyString(string) ? emptyString : string.slice(1);
```
Some examples:
```
isEmptyString(emptyString) 
  true

isEmpty(concatenate(zero,emptyString,one,one)) 
 false

isZero(firstOf(concatenate(zero,one,one,emptyString)))
  true

isOne(firstOf(restOf(concatenate(zero, one, one, emptyString))))
  true

concatenate(zero, one, zero) == restOf(concatenate(one, zero, one, zero))
  true
```

Not all of those string functions are really needed in the encoder, but there's nothing wrong with covering familiar ground when it doesn't take you far from your path.
There are two operations that our little simulation of a stack machine has to accommodate: push a nil and push a pair made from popping the top two items from the stack.
The first is simple enough:
```
let pushNil=stack=>push(stack,nil);
```
It is the second that poses a few problems (but not anything too troublesome).
Rather than explain it, I'll just show the code and go from there:
```
let emptyStack = nil
, isEmptyStack = stack => stack==emptyStack
, topOf = stack => peek(stack)
, secondOf= stack => peek(pop(stack))
, pop2 = stack => pop(pop(stack))
, pairUp = stack => push(pop2(stack), pair(secondOf(stack), topOf(stack)));
```
> For those familiar with stack operations in other circumstances, it may be strange to take a functional view of stacks: popping a stack returns a stack without its top, it doesn't change the state of some stack accessible outside of the scope of the current executing function.

Now all that is left is to break the encoder into a function that gets the ball rolling with an empty stack and one that takes a string and a stack and gets to work:

```
let encodeHelperBeta = (string, stack) =>
  isEmptyString(string) ? topOf(stack)
  : isZero(firstOf(string)) ? encodeHelperBeta(restOf(string), pushNil(stack))
  : isOne(firstOf(string)) ? encodeHelperBeta(restOf(string), pairUp(stack))
  : encodeHelperBeta(restOf(string),stack)
, encodeBeta = string => encodeHelperBeta(string,emptyStack)
```
Lo, this is not the last version of 'encode' and that is why it is called 'encodeBeta'.
There are three things of note in the beta definition given:

1. it asks if the string is empty and when it is it returns the top of the stack,
2. it checks if the first item of the string is zero or one and executes the corresponding operation on the stack (pushing a nil on top of the stack or pairing up the top two items on the stack), and
3. if it runs into an item of the string that we haven't talked about yet it does nothing and goes on its merry way.

Here are some examples, but because we have no way of seeing the encoded tree I have to put it through the decoder--- this ends up being helpful because it should decode into a string that we could put back into the encoder to make the tree all over again.
```
decode(encodeBeta(concatenate(zero, zero, one)))
  '..,'
decode(encodeBeta(decode(encodeBeta(concatenate(zero, zero, one)))))
  '..,'
decode(encodeBeta(concatenate(zero,zero,one,zero,one)))
  '..,.,'
decode(encodeBeta(concatenate(one,one)))
  '...,,'
```
> Alternatively, we could have built up the parenthetical notation familiar to most people for ordered pairs e.g. where '(x,y)' is short for "the pair whose left part is x and whose right part is y".
> It would also be helpful to know when two trees are equal so that we might be able to write out the construction of a tree and compare it to the encoding of its abbreviation as a bit string.
> While javascript comes with its own built in string functions, it doesn't give us built in pairs or functions for working with pairs.
> Given what is known from Solomon Fefferman's work on [Finitary Inductively Presented Logics](http://virtualmath1.stanford.edu/~feferman/papers/presentedlogics.pdf), it is probably a bad idea to design a language that doesn't work with ordered pairs (and for those less theoretically minded, there's always the conveniences of LISP to look at).

That last example is well worth looking at more closely: it took the concatenation of one with one (which is not a string that will ever come out of the decoder) and it encoded just fine and when we sent that tree through the decoder it returned the concatenation of zero, zero, zero, one, and one!
So we have a much shorter way of decoding this particular tree: we can write it as ',,' instead of '...,,' when we have said how 'encoderBeta' works!

The following example shows why it is still called 'encoderBeta':
```
decode(encodeBeta(concatenate(zero, zero, zero, zero)))
  '.'
```
Where did all those other zeros go that we concatenated together?
How is it that we only got a single zero when we decoded the encoded tree?

After the encoder checks that the string is empty it peeks at the top of the stack and returns it as the result.
Since there are no ones in that concatenation, there are no pairs made out of the nil trees on the stack (of which there are four before it shows us the top nil).

There's nothing wrong with the beta encoder except that we can make a new one that doesn't leave anything on the stack.
Presumably, doing so will give us some new ways of abbreviating binary trees as bit strings!
The simplest change is to pair up everything that's left on the stack and then return that.
```
let pairUpEverything = stack =>
  isEmptyStack(pop(stack)) ? topOf(stack)
  : pairUpEverything(pairUp(stack))

, encoderHelper = (string, stack) =>
  isEmptyString(string) ? pairUpEverything(stack)
  : isZero(firstOf(string)) ? encodeHelper(restOf(string), pushNil(stack))
  : isOne(firstOf(string)) ? encodeHelper(restOf(string), pairUp(stack))
  : encodeHelper(restOf(string),stack)
, encode = string => encodeHelper(string,emptyStack);
```

Everything on the stack gets paired up by repeatedly applying 'pairUp' to the stack until there is only one item left on it (this is checked by seeing if popping the stack leaves only the empty stack behind).
Some examples:
```
decode(encode(concatenate(zero, zero, one)))
  '..,'
decode(encode(concatenate(zero, zero, zero, zero)))
  '....,,,'
decode(encode(concatenate(one, zero, one, zero, zero, zero)))
  '..,.,...,,,'
decode(encode(concatenate(zero, zero, one, zero, one, zero, zero, zero, one, one, one)))
  '..,.,...,,,'
```
The last two examples suggest a simple way of getting the shorter decoded string from the longer decoded string of an encoded tree without having to guess and check encoding and decoding:

1. if the long code is a concatenation of all zeros or all ones then you already have the shorter bit string you're looking for, but
2. if at least one zero and one one both occur in the long code then trim off any zeros on the left and any ones on the right.

There are some more complicated javascript functions being used to define the operations of trimming and checking for occurrences of ones and zeros, but no more than what you can figure out for yourself from context clues or a quick search:

```
let occursIn = (string, item) => string.includes(item)
, trimLeftZeros = string => string.slice(string.indexOf(one))
, trimRightOnes = string => string.slice(0,1+string.lastIndexOf(zero))
, trim = string => trimLeftZeros(trimRightOnes(string))
, shorten = string =>
  isEmptyString(string) ? '.'
  : occursIn(string,zero) && occursIn(string,one) ? trim(string)
  : string;
```

And, here are some examples:

```
shorten( concatenate(zero, zero, one, zero, one, zero, zero, zero, one, one, one))
  ',.,...'
decode( encode( shorten( concatenate(zero, zero, one, zero, one, zero, zero, zero, one, one, one))))
  '..,.,...,,,'
```

There are a few ways that these shorter decoded bit strings help out e.g. they let us fit more trees within a single word of memory and, if we are silly enough to do so, fit multiple tiny trees within a single word of memory.

> Future John here (from 202504272330): the shortener doesn't work in the case where there is a one, a zero, and the first occurrence of a one is the last occurrence of a zero e.g. '00000111111'.
> You can fix this by checking for this exact degeneracy and then keep whichever block of identical digits is shorter.

Next I'll come up with a way to showcase how each bit string or binary tree locates chuncks, down to words, of memory.
After that it will probably be time for some bit string and binary tree arithmetic.
No promises!

Here's all the code I used to write this whole note:
```
let pair=(leftPart,rightPart)=>({leftPart,rightPart})
, leftPartOf=pair=>pair.leftPart
, rightPartOf=pair=>pair.rightPart;
let nil={get leftPart(){return this}, get rightPart(){return this}};
let run=code=>{console.log(code,'\n  ',eval(code));}
run('nil == leftPartOf(nil) && nil == rightPartOf(nil)');

let atom=x=> x==leftPartOf(x) && x==rightPartOf(x);
run('atom(nil)');

let isNil=x=> x==nil;
let zero='.'
, one=','
, concatenate=(...strings)=> strings.length ? strings[0]+concatenate(...strings.slice(1)) : '';
run('zero != one');
run('concatenate(zero,one,zero,one,one)');

let decode= tree => isNil(tree) ? zero 
: concatenate(decode(leftPartOf(tree)), decode(rightPartOf(tree)), one);
run('decode(nil)');
run('decode(pair(nil,nil))');
run('decode(pair(nil,pair(pair(nil,nil),nil)))');

let push= (stack,item) =>pair(stack,item)
, pop= stack => leftPartOf(stack)
, peek = stack => rightPartOf(stack);

let error = message => console.log(message)
, emptyPair=[]
, isEmpty = pair => pair.length == 0
, purePop = stack => isEmpty(stack) ? emptyPair : leftPartOf(stack)
, purePeek= stack => isEmpty(stack) ? error('empty stack') : rightPartOf(stack);

let emptyString=''
, isEmptyString = string => string==emptyString
, isZero = string => string == zero
, isOne = string => string == one
, firstOf = string => isEmptyString(string) ? emptyString: string[0]
, restOf = string => isEmptyString(string) ? emptyString : string.slice(1);
run("isEmptyString(emptyString)");
run("isEmpty(concatenate(zero, emptyString, one, one))");
run("isZero(firstOf(concatenate(zero, one, one, emptyString)))");
run("isOne(firstOf(restOf(concatenate(zero, one, one, emptyString))))")
run("concatenate(zero, one, zero) == restOf(concatenate(one, zero, one, zero))");

let pushNil=stack=>push(stack,nil);
let emptyStack = nil
, isEmptyStack = stack => stack==emptyStack
, topOf = stack => peek(stack)
, secondOf= stack => peek(pop(stack))
, pop2 = stack => pop(pop(stack))
, pairUp = stack => push(pop2(stack), pair(secondOf(stack), topOf(stack)));
let encodeHelperBeta = (string, stack) =>
  isEmptyString(string) ? topOf(stack)
  : isZero(firstOf(string)) ? encodeHelperBeta(restOf(string), pushNil(stack))
  : isOne(firstOf(string)) ? encodeHelperBeta(restOf(string), pairUp(stack))
  : encodeHelperBeta(restOf(string),stack)
, encodeBeta = string => encodeHelperBeta(string, emptyStack);
run("decode( encodeBeta( concatenate(zero, zero, one)))");
run("decode( encodeBeta( decode( encodeBeta( concatenate(zero, zero, one)))))")
run("decode( encodeBeta( concatenate(one, one)))")
run("decode( encodeBeta(concatenate(zero, zero, zero, zero)))");

let pairUpEverything = stack =>
  isEmptyStack(pop(stack)) ? topOf(stack)
  : pairUpEverything(pairUp(stack))
, encodeHelper = (string, stack) =>
  isEmptyString(string) ? pairUpEverything(stack)
  : isZero(firstOf(string)) ? encodeHelper(restOf(string), pushNil(stack))
  : isOne(firstOf(string)) ? encodeHelper(restOf(string), pairUp(stack))
  : encodeHelper(restOf(string),stack)
, encode = string => encodeHelper(string,emptyStack);
run("decode( encode( concatenate(zero, zero, one)))")
run("decode( encode( concatenate(zero, zero, zero, zero)))")
run("decode( encode( concatenate(one, zero, one, zero, zero, zero)))");
run("decode(encode(concatenate(zero, zero, one, zero, one, zero, zero, zero, one, one, one)))");

let occursIn = (string, item) => string.includes(item)
, trimLeftZeros = string => string.slice(string.indexOf(one))
, trimRightOnes = string => string.slice(0,1+string.lastIndexOf(zero))
, trim = string => trimLeftZeros(trimRightOnes(string))
, shorten = string =>
  isEmptyString(string) ? '.'
  : occursIn(string,zero) && occursIn(string,one) ? trim(string)
  : string;
run("shorten(concatenate(zero, zero, one, zero, one, zero, zero, zero, one, one, one))");
run("decode(encode(shorten(concatenate(zero, zero, one, zero, one, zero, zero, zero, one, one, one))))");
```

## 2025 0412

### 2025 0412 2335

It is late and it feels like there is a fat tire tightening around skull.
My temples throb and my eyes hurt.
Nothing comes from closing my eyes but the feeling of pains marching across my face.
If I don't tell myself to relax then I'll soon be clenching my teeth until my ears start to ring.
A few deep breaths may loosen my neck and shoulders, but my forehead remains scrunched.

Despite these unfavorable conditions, I have added some more to the [2025 0412 1422 Paper on Logic](#2025-0412-1422-paper-on-logic).

### 2025 0412 1619
I'm making a note here because I am so happy to have a single text document that I can easily copy and paste onto a single monolithic website.
This is the closest I have come to explaining myself to others in a way that is mostly accurate and mostly accessible.

For now there are only a list of timestamped entries to show you.
Soon these shall evolve into more substantial blocks of templates, contemplations, and conclusions.
As what I have to say reaches a critical mass, it bubbles up into a more solid and long standing brick in the foundation of my principles and practices.
I am glad to share these steps in the evolution fo my behavior.

### 2025 0412 1422
It has been a while since I've worked directly on my paper on logic.
In the past I have had to shove what little I can into a thin thread of tweets.
Now I am not stuck in a tight corner and can let my explanations relax into the more casual style that my friends and family are subjected to when I trap them in a "Johnversation".

My outlook on logic is now almost entirely that of Quine's (amended by Skinner's outlook on verbal behavior).
His "Methods of Logic 4th Edition" (and you need the 4th edition!) has almost split in half from how many times I've read it.
It doesn't help that Quine's main method of proof for predicate logic is about half way through the book and that it is the one section I've probably read and reread the most.

Whereas Quine's "Methods of Logic 4th Edition" narrowly focuses on the mechanics of logical practices, his "From Stimulus to Science" broadly applies those mechanics to, what Quine called, a "Breast Pocket Theory of the World".
As much as I disagree with the details of Quine's theory, e.g. he mistakes something like Skinner's experimental analysis of behavior for a dispositional stimulus-response theory, I agree with his methods.

Where Quine goes wrong, Skinner often goes right, and where Skinner goes wrong, Quine is there to clearly identify the problems if not also to provide compact solutions.

That is enough on where I'm coming from when I talk about logic.
Now for parts of my paper on logic.

***

There are a few unique things about my methods of logic.
The most important is that I aim to present predicate functor logic not as the elimination of variables from predicate logic, but as an autonomous system independent of predicate logic.

Whereas predicate logic is a consequence of grammar and truth, predicate functor logic is a consequence of grammar and denotation.
Closed sentences are true, predicates denote, and closed sentences are a degenerate form of predicate: they are zero place predicates.
Correspondingly, truth is a degenerate form of denotation.
In predicate functor logic we can define truth of a zero place predicate as the universal (or existential closure) of the quotation of its padding denoting.

> The phrases "existential closure", "quotation", "padding", and "denoting" are all technical terms and I have used them as such with full knowledge that they are foreign to most people e.g. "padding" is most likely unknown to anyone who is not yet familiar with Quine's predicate functors.

The first sentence of the paper, so far, is little more than the opening of a mystery:

> Logic is the science of validity as a consequence of grammar and denotation.

It is a sad sentence, and I am unhappy with it.
Simpler alternatives occur to me e.g.

> Logic is a consequence of grammar and denotation.

or perhaps

> Logic is that science which is the consequence of grammar and denotation.

but neither of those, nor the first, really clear the way for what is to come.
Such introductory sentences tend to come much later in my writing of a paper.
They are a summary of what is to come, and when what is to come has not yet come, there is nothing to concretely summarize.

The key components to the introductory sentence shall remain 'grammar' and 'denotation'.
They are an echo of Quine's "Philosophy of Logic 2nd Edition" which clearly shows the ways in which logic is the sum of grammar and truth.
He has a beautiful metaphor:

> Logic chases truth up the tree of grammar. [pg. 35]

and

> The logician talks of sentences only as a means of achieving generality along a dimension that he cannot sweep out by quantifying over objects.
> The truth predicate then preserves his contact with the world, where his heart is. [pg. 35]

Together these give a clear hint as to where Quine is coming from when he talks about the methods of logic.
The problem of "achieving generality" by talking of sentences is a critical part of Quine's method of semantic ascent:

> The move is what I call *semantic ascent*: mentioning an expression by name instead of using it as a component clause. [pg. 92 of From Stimulus to Science]

In "From Stimulus to Science" Quine does a better job of showing how quotation is just one potential method of semantic ascent by showing how it can be extracted from 'that' clauses e.g. 

- perceives that
- thinks that
- it occurred to him that
- believes that
- doubts that
- expects that
- hopes that
- fears that
- regrets that
- says that
- denies that
- predicts that
- strives that

[pg. 90 of From Stimulus to Science].
For most people familiar with Quine, semantic ascent is intertwined with his theory of propositional attitudes (as the examples amply indicate).
But, there is nothing so grand in the method: it is a technical solution to a technical complication:

> We can generalize on 'Tom is mortal', 'Dick is mortal', and so on, without talking of truth or of sentences; we can say 'All men are mortal'.
> We can generalize similarly on 'Tom is tom.', 'Dick is Dick', '0 is 0', and so on, saying 'Everything is itself'.
> When on the other hand we want to generalize on 'Tom is mortal or Tom is not mortal', 'Snow is white or snow is not white', and so on, we ascend to talk fo truth and of sentences, saying "Every sentence of the form 'p or not p' is true', or 'Every alternation of a sentence with its negation is true'.
> What prompts semantic ascent is not that 'Tom is mortal or Tom is not mortal' is somehow about sentences while "Tom is mortal' and 'Tom is Tom' are about Tom.
> All three are about Tom.
> We ascend only because of the oblique way in which the instances over which we are generalizing are related to one another. [pg. 11 Philosophy of Logic]

Quotations as a method of semantic ascent are haunted by philosophies of the past which are unable to distinguish between schematics and the items they purport to schematize.
For example, "of the form 'p or not p'" is usually written without the interior quotes, that is as "of the form p or not p", and this is otherwise excused as an expedient to effective writing where the reader is expected to know where the invisible quotations are to be put.
This has continued to confuse people who then take the occurrence of the letter pee in "of the form p or not p" as a pronoun referring to some otherwordly item called a proposition that somehow is *the meaning* of what a sentence like "Tom is Tom" says.

To be trapped down such dark allys of past mistakes is as silly as being stuck doing arithmetic with roman numerals.
We can look back and marvel at our mistakes and laugh at the labors we now save by teaching better methods.

All of this is to say that the single sentence

> Logic is the sciene of validity and validity is a consequence of grammar and denotation.

which is the first in my paper on logic is woefully inadequate as a summary of even what little I have said above and what little I have yet to say on the rest of logic.

***

Perhaps the proper "first sentence" is the second one

> Compounding is (denotative) functional when, exclusively, each like compound denotes or each like compound does not denote, where and only where (waow), exclusively, each like component denotes or each like component does not denote.
> Chains are compounds compounded functionally.

The phrase "denotative functional" generalizes to predicates what is said as "truth functional" of sentences.
The definition of denotational compound is clumsy with its "exclusively"s and "like"s, but the alternatives are woefully inaccurate and undermine the task at hand.
Logic does not have the benefit of its fruits before they have ripened.
As much as I would love to say that whether a compound denotes or not is a function of whether its components denote or not, there is no thing that can yet be spoken of as "a function".

It is only from the methods of logic that we come to pin down the items of the world.
While there are many who say that they are in touch with the world in some direct way, e.g. Russell repeatedly leaves room for singular terms so that they can be used to designate particulars of immediate experience, there is not yet a way that I have come to be in touch with the world as such others describe.
It is only through their reports that I am even confronted with the problems of such seemingly personalized revelations.

There is another problem that sometimes occurs when making such definitions: do I not appear to be speaking of "exclusive alternations" when I have not said what it means to say an instance of the sentence schema "exclusively p or not p"?
Is there not a circularity to my methods?

Quine was confronted with the same problem so often in the first publication of his early text "Mathematical Logic" that he had to add an appendix to help the anxious reader.
There he explains the difference between "theorem" and "metatheorem" as well as "deduction" and "formal deduction".
There (and in his Methods of Logic) he is confined to philosophic methods and only indirectly mentions in Methods of Logic the scientific origins of logical practices.

I have gone out of my way to use the phrase 'where and only where' or 'when and only when' to distinguish the preformal/informal or preschematic methods from the schematic methods of logic.
To explain the origin of verbal behavior with the forms "Exclusively Tom is wet or Tom is dry" is to leave the science of logic for the science of verbal behavior more generally.
Just as humans spoke grammatically before speaking of grammar, they spoke logically before speaking of logic.

At this time I do not have a better explanation as to why I can use phrases that appear to have the same form as those said to be defined e.g. 

> Joint denials denote waow each of their components do not.
> Negations are self joint denials: they denote waow their component does not.
> Alternations are negations of joint denials: they denote waow some of their components do.
> Conjunctions are joint denials of negations: they denote waow each of their components do.

***

With all that said, I have only introduced the following sentences from my paper on logic

> Logic is the science of validity and validity is a consequence of grammar and denotation.
>
> Compounding is (denotative) functional when, exclusively, each like compound denotes or each like compound does not denote, where and only where (waow), exclusively, each like component denotes or each like component does not denote.
> Chains are compounds compounded functionally.
>
> Joint denials denote waow each of their components do not.
> Negations are self joint denials: they denote waow their component does not.
> Alternations are negations of joint denials: they denote waow some of their components do.
> Conjunctions are joint denials of negations: they denote waow each of their components do.

Here is all that I have so far:

#### 2025 0412 1422 Paper on Logic

Logic is the science of validity and validity is a consequence of grammar and denotation.

##### Functional Compounding and Chains

Compounding is (denotative) functional when, exclusively, each like compound denotes or each like compound does not denote, where and only where (waow), exclusively, each like component denotes or each like component does not denote.
Chains are compounds compounded functionally.

##### Example Chains: Joint Denials, Negations, Alternations, and Conjunctions

Joint denials denote waow each of their components do not.
Negations are self joint denials: they denote waow their component does not.
Alternations are negations of joint denials: they denote waow some of their components do.
Conjunctions are joint denials of negations: they denote waow each of their components do.

##### Subcompounds and Functional Substitutions

Subcompounds of compounds are their self or those of their components.
Substitutions of like compounds for like nonchain subcompounds are (denotative) functional.
Functional substitutions of functional substitutions of compounds are functional substitutions of their self.

##### Functional Validity, Consistency, Implication, and Equivalence

Compounds are (functionally)
* valid waow each of their functional substitutions denote,
* consistent waow their negation is nonvalid (i.e. soem of their functional substitutions denote),
* implied by others waow the conjunction of their self (the conclusion) with the negation of the other (the premise) is nonconsistent (i.e. each of their functional substitutions denotes where the same of the other does), and
* equivalent to others waow they are mutually implicative (i.e. each of their functional substitutions denotes waow the same of the other does).
[See pg. 36 of POL]

##### Example Validities and (Non)consistencies: Laws of Excluded Middle, Contradiction, Self Implication, and Self Equivalence

Alternations of compounds with their negations are valid (they denote waow the compound does or its negation does, i.e. waow it does or does not, so, each functional substitution denotes).
Conjunctions of compounds with their negations are nonconsistent (they denote waow their compound does and its negation does i.e. waow it does and does not, so, each functional substitution does not denote).
Compounds are implied by and equivalent to their self.

##### Functional Substitutions Keep Validity, Nonconsistency, Implication and Equivalence

Functional substitutions in
* validities are validities (each functional substitution of the functional substitution of the validity is a functional substitution of the validity and hence denotes),
* nonconsistencies are nonconsistencies (each function substitution of the negation of the functional substitution of the nonconsistency is a functional substitution of the negation of the nonconsistency i .e. is a functional substitution of a validity and hence the negation of the functional substitution of the nonconsistency is valid so that the function substitution of the nonconsistency is nonconsistent),
* implications are implications (the conjunction of the conclusion with the negation of the premise is nonconsistant and hence its functional substitution is nonconsistent and identical to the conjunction of the function substitution of the conclusion with the negation of the functional substutituion of the premise), and
* equivalences are equivalences (functional substitutions of mutual implications are mutual implications).

##### Interchanges of Equivalents are Equivalent
Interchanges of equivalents in a compound are equivalent to that compound (each functional substitution of a compound matches the same of its interchange, except perhaps for the same of the equivalents which otherwise denote in tandem, so each denotes waow the other does i.e. they are equivalent).

##### Interchnage of Equivalents Keeps Validity, Nonconsistency, Implication, Equivalence, Nonvalidity, Consistency, Nonimplication, and Nonequivalence

Interchanges of equivalents in
* validities are validities (each functional substitution of the interchange denotes waow the same of the validity does),
* nonconsistencies are nonconsistent (their negation is a validity and so the interchange in the negation is a validity),
* implications are implications (interchange into the nonconsistency is a nonconsistency),
* equivalents are equivalents (interchange of mutual implications are mutual implications),
* nonvalidities are nonvalidities (a compound is nonvalid waow some functional substitution does not denote, i.e. some functional substitution of its negation denotes, i.e. its negation is consistent, and since the negation of the interchange is identical tot he interchange of the negation which is consistent and consistency is kept by interchange then the negation is consistent i.e. it is nonvalid)
* consistencies are consistencies (the negation of the interchange is identical to the interchange of the negation which is nonvalid hence it is nonvalid),
* nonimplications are nonimplications (nonimplication is consistency of the conjunction ...)
* nonequivalences are nonequivalences (one is a nonimplication ...).

##### Equivalents of Identity
Compounds are equivalent to
* their double negation (which denotes waow the negation of the compound does not, i.e. waow it does, so, each functional substituion of it denotes waow the same of its double negation does),
* their self alternation/conjunction (which denotes waow some/each of its components does i.e. waow the compound does), and 
* their alternation/conjunction with nonconsistencies/validities.

##### Equivalents of Distributivity of Conjunctions and Alternations



### 2025 0412 1349
I have found that it is easier to be wrong than it is to be right and that it is easier to be right after having been wrong.
If there was a way to avoid being wrong then I would only act in that way.
Logic helps but is slow.
Sorting through the wrong bits takes more than what logic has to offer.

I see logic as part of science and see science as picking up what logic fumbles and drops.
Logic works on the theoretical side of science and laboratories work on the experimental side.
As I have said many times before "Libraries select theoretical practices and laboratories select experimental practices".
Science is compounded of theoretical and experimental practices.

One of the reasons I've allowed myself to write in this longer form is to make it easier for other people to see that I am often wrong, and that it is only from first being wrong that I get anywhere near being right.
The definitions, the natures, or the essences of what is right and what is wrong are not easy to suss out.

Many people have spent their whole lives trying and failing to establish what is right and what is wrong.
As much as I have a sort of optimism that if I work hard enough then I may one day uncover some piece of a complete theory of right and wrong, I am not so foolish to think that I shall do anything like what the best already have.

With that in mind, I offer these writings that are most certainly mistaken for no other reason than that they occurred to me.

***

I talk about behavior a lot.
Does that imply that my behavior is under my careful control?
No.
It is one thing to know a science and another thing to have a technology in hand.
The things of technology are the consequences of science, more or less.
But, the knowledge of science lacks the concrete dimensions of levers, pullies, medicines, and computers.
All that goes into scinece is often ephemeral: here for but a moment and gone forever, lost to our remote contact with the past through fragmentary records that soon fall apart.

Those who manufacture products, be they assemblies of machines, or chemicals, or organisms, or programs, or etc., work in ways often mistaken as scientific.
They manufacture with the controls offered by technologies i.e. with the descendants of scientific practices.

The difference between science and such manufactureres is the difference between sensitivity and insensitivity.
The manufacturer is deliberately insensitive to the strange variations responsible for science.
This is not what most people think or say: they say that business is as much a science and an art as physics or biology.
They insist that it is so in large part because the best businesses are almost always the result of some new technological consequence from science.

This does not always seem to be the case.
Some businesses appear to survive despite science.
Much that is "made for TV" objects to being called pseudoscience.
It sells, and there is a science to sales!

These are all problems of control: can you make a person buy your product?
Some say you can't, others say you can.
Some say that people only buy what is an intrinsically good product: you can't trick your way into becoming a good business when your product is technologically sophistocated.

***

What's with people who make a living as oracles?
They make predictions, often asserted with the authority of a proclamation, and profit, one way or another, from betting that they're right and ultimately being right.
But, are they betting on what they think they are?
Are they right when their bet pays off?
There seems to be some difference between constructing bets that are verly likely to pay off and making predictions like those people assume come from our best sciences.

How do we know the future?
When we speak of the future, as in predictions, what is it that we are speaking of?
Can speach actually be said to be about anything?
Certainly the speaking occurrs, but what good reasons do we have for saying that such speach is about this, that, or the future?

### 2025 0412 0119
[Matios Berhe](https://www.twitter.com/MatiosTV) suggests I read
- Kant's Critique of Pure Reason, chapter 2, section 12
- Kant's Critique of Pure Judgement, Critique of the Teleological Judgement
- Locke's Essay Concerning Human Understanding

I have now added these to my long list of "stuff to read".
Some day, sooner rather than later, I shall provide a complete list of texts in my library and the schedule on which I read them.
There are two main reasons to do so:
1. to point up the origins of what I have to say (so that others do not suspect me of originating, initiating, or creating my verbal behavior)
2. to better scheduling my reading and writing.

I already have a copy of Kant's Critique of Pure Reason that I read first when I was in high school.
There are a few texts from that time which have survived long enough to find themselves on my shelves.
One of the treasures from that time is Russell's "Problems of Philosophy".
In no way did I learn all that Russell and Kant had to teach in their books when I first read them so very long ago.
There is something special though about reading things that you do not yet understand.

The more I read the more I don't at first understand, and yet, at the same time, the more I learn about the world as a whole.
There is something about reading which makes the world bigger and smaller than it once seemed.
When I'm not in such a metaphorical mood, I am quick to say the world is no bigger nor no smaller than it is: to say otherwise is to plunge such a theory into contradiction from which no logic prevents each and every imaginable conclusion.

People are usually scared by contradiction for the wrong reasons: this is somewhat corrected by phrases like "law of explosion".
The problem with contradiction is not really a problem: when you have a contradiction you find yourself without the benefits of a consistent theory.
Some people are ok with that.

### 2025 0412 0102
I finally have a place to put all my daft drafts without chopping them into little tweet size pieces.
Why has it taken me so long to return to this long form method of writing?
It's mostly that strange feeling I get that this is like a message in a bottle sent out into an enormous sea of nothing.

Once I figured out that it wasn't sending the message that mattered most, I warmed up to the practice of writing from my little island on the internet.

## 2025 0411

### 2025 0411 2248
I'm implementing a little LISP.
Like the LISPs of yore, there are two basic items: pairs and atoms.

Pairs split into left and right parts.
Paul Graham wrote of "halves" when explaining pairs in [Bel](https://www.paulgraham.com/bel.html).

Whether you talk about the anatomy of a pair by "breaking it into pieces", "splitting it into halves", or "decomposing it into components", there is one rule that governs the logic of pairs: 

> pairs are identical where and only where their left components are identical and their right components are identical.

This is otherwise know as "the law of extensionality of ordered pairs".
It is a fancy way of mentioning the precise conditions upon which to count two items in a universe of ordered pairs as identical.
The single premise "each item is (a,b,c,x,y, and z such that a pairs b with c, x pairs y with z, and a is identical to x, if and only if b is identical to y and c is identical to z)" logically implies each conclusion of *any* theory of ordered pairs.

Now is not the time or place to get distracted by problems of logic, theories, and ordered pairs.
As much as I would enjoy a full digression on the origins of logic, the extensionality of effective theories, and ordered pairs as a paradigm of philosophic investigation, those are not the problems that I am presented with here and that I have to solve now.

Back to my little LISP.

Pairs are identical where and only where their parallel components are.
For now, "(x,y)" is short for "the pair whose left part is x and whose right part is y".
Thus, (x,y) is identical to (a,b) where and only where x is identical a and y is identical to b.

Since I'm coding up my little LISP in javascript I'm using the following definitions:

```
let cons=(x,y)=>[x,y]
, car=x=>x[0]
, cdr=x=>x[1]
```

These just say that 'cons' is the name of a function that takes two arguments and returns an array whose zeroth component is the first argument and whose first component is the second argument (note this confusing sentence is why some people prefer different methods of indexing into arrays of data).
The function named 'car' takes one argument and returns the item at index zero of it.

> If you do not already know how javascript arrays work, and how indexing into javascript arrays works, then these explanations are not helpful.
> Thankfully, if you do not already know how javascript arrays work, you get the joy of learning that first before coming back here.
>
> I have not yet found a faster way to introduce programming than to present it after teaching logic up to and including Quine's main method (of proof for predicate logic).
> Since more people are familiar with javascript than they are with Quine's main method--- or with the methods of logic--- I shall continue to avoid what might otherwise appear as an entirely roundabout way of teaching programming from scratch.

The function named 'cdr' takes one argument and returns the item at index one of it.

Both 'car' and 'cdr' are names inherited from working with programmable machines (aka "computers") during the late 1950s and early 1960s.
If you want to learn more about where 'car' and 'cdr' come from, and perahsp even how to pronounce 'cdr' (which I say as "could-er"), then track down [John McCarthy's 1979 "History of Lisp"](https://justine.lol/sectorlisp/lisp-history.pdf) and give it a compassionate read.

With 'cons', 'car', and 'cdr' we can make pairs, get their left part, and their right part respectively.
There are algebraic or equational ways of introducing these basic functions e.g.

```
car(cons(x,y))==x
cdr(cons(x,y))==y
'car(x)==car(y) and cdr(x)==cdr(y)' is equivalent to 'x==y'
```

but again this is just a distraction from the problem to solve (implementing a little lisp like language).
Note, such purportedly algebraic or equational theories appear to follow entirely from pure logic when Quine's method of schematic theories is extended to schematic theories of functional predicates (see Quine's 'Philosophy of Logic' for a schematic theory of identity).

Following the conventions established in the wonderful book "The Little Schemer" by Friedman and Felleisen, cdr and car are only used in contexts where their arguments are pairs with left and right parts.
There is one pair without any parts.
It is unique (in more ways than one).

```
let nil=[]
, id=(x,y)=>x==y
, empty=x=>id(x,nil)
```

The item named 'nil' is an empty javascript array.
Not all empty arrays are equal in javascript.
Equality, identity, and indiscernibility are hard to deal with when you don't start with logic.
Surprisingly, 'id' works almost like it does in a language like [Bel](https://www.paulgraham.com/bel.html) even though it is not obvious that it should.
Remember the 'law of extensionality of ordered pairs'?
Well, it is violated by almost all programming languages.

There is a silent assumption that is among the many hidden away behind any explanation of a foreign programming language: they actually run on real machines.
On most machines there are places and objects at those places.
Objects in different places can't be identical!
(Think that over: identical things can't differ in place apparently.)
And yet, sometimes they are!

Pairs may have parallel components that are identical without being identical themselves.
This happens when two pairs are silently assumed to be in different places but each of them places their components *in the same places*!
There are lots of solutions to this problem of identity.
Most of them are needlessly complex and overburdened with crusty philosophies.
My solution is as sad as it is simple: just go with what javascript gives you.

My real solution is to go with logic: identity is indiscernibility in that any two predicates which fill out each instance of the premises of identity are coextensive and indiscernibiltiy is one such predicate (in a logical theory with a finite lexicon).
For more on this see Quine's "Philosophy of Logic".

Atoms are not pairs (and pairs are not atoms).

```
let pair=x=>Array.isArray(x)
, atom=x=>!pair(x)
```

Is nil an atom?
No, not here.
Is nil an atom elsewhere?
Sometimes.
Early LISPs only had pairs and symbols.
Symbols are often a large part of LISPs.
They are the workhorses of reference: they end up sometimes playing the part of proper nouns and sometimes playing the part of pronouns.
Here again, there is much to be learned from predicate logic.
Here again, I must avoid dipping into such unfamiliar wells of simplicity.

The general problem of atomic items provides ample opportunity to make use of degenerate cases of common objects e.g. Quine provides for atoms in his "Set Theory and its Logic" by picking out atoms with the reflection of membership i.e. 'x is atomic' is short for 'x belongs to x'.
Most people avoid making convenient use of alarming degeneracies: they're really missing out on some good fun.

Atomic items and empty items have a long and sordid history.
Atoms are not supposed to have parts, and yet we are often forced to say that they do.
Empty items always end up behaving like atoms in many respects--- e.g. they are empty and presumably "don't have parts"--- but they almost always tend to be unique where as there tend to be many many atoms in any given theory.

The only place I have found clarity on these boundary problems is, you guessed it, logic.
In a theory of ordered pairs, it is enough to provide for the existence of an ordered pair that neither has a left part nor has a right part.
It is then unique since any two purportedly distinct empty pairs have identical parallel components (vacuously).
Atoms are then accomodated by one of a few methods, each of which match up with Quine's atomic methods in "Set Theory and its Logic".
An item of a theory of ordered pairs is atomic when it is identical to its components (or, just one of its components, in which case the other components allows us to distinguish between atoms with, e.g., identical left components: are these even atoms?).

Sadly neither of these principles are used in LISPs.
Again, this is because there is that silent assumption working in the background of most explanations: the programming language runs on machines in our shared material world.

This is enough thinking on this for now.

### 2025 0411 2217
I can not guarentee that there will be no spelling errors in what I write.
Rarely do I spell well, and rarely do I take the time to check my spelling.
Rather than check my spelling I would keep on thinking.
Spell checking stops thinking except when looking up the word to be spelled spurs some better bits of writing.
Definitions have the power to prompt better writing when they present short and sticky explanations.
Those sensitive to spelling errors will be greatly disappointed (that is until I go back through and spell check this document, that is, if I ever get around to doing that).

### 2025 0411 2158
I do not have much familiarity with using this software and can not be certain how what I've written here will appear.
I am also not familiar with markdown.
Will there be new lines in the final document if I put them here?
The answer is no.

Does this start a new paragraph? The answer is yes.

Does this [20250411](#2025-0411) automatically.

ok, rather than get lost in the details of editing I will just go on and start writing.
