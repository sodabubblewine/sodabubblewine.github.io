Discover, predict, and control changes in counts, rates, and accelerations as selections from variations on physical, chemical, biological, behavioral, and cultural scales by making and maintaining strong practices mediated by strong people marked by strong principles from the sciences of logic (denotative, Boolean, and functor), mathematics (calculi, collections, and categories), physics (quantum field theory, statistical thermodynamics, gravity), chemistry (phyiscal, biophysical, and biological), biology (oranelles, organisms, environments), behavior (biological, biosocial, social), and culture (history, technology, survival).

## 2025 0412

### 2025 0412 2335

It is late and it feels like there is a fat tire tightening around skull.
My temples throb and my eyes hurt.
Nothing comes from closing my eyes but the feeling of pains marching across my face.
If I don't tell myself to relax then I'll soon be clenching my teeth until my ears start to ring.
A few deep breaths may loosen my neck and shoulders, but my forehead remains scrunched.

Despite these unfavorable conditions, I have added some more to the [2025 0412 1422 Paper on Logic](#2025-0412-1422-paper-on-logic).

### 2025 0412 1619
I'm making a note here because I am so happy to have a single text document that I can easily copy and paste onto a single monolithic website.
This is the closest I have come to explaining myself to others in a way that is mostly accurate and mostly accessible.

For now there are only a list of timestamped entries to show you.
Soon these shall evolve into more substantial blocks of templates, contemplations, and conclusions.
As what I have to say reaches a critical mass, it bubbles up into a more solid and long standing brick in the foundation of my principles and practices.
I am glad to share these steps in the evolution fo my behavior.

### 2025 0412 1422
It has been a while since I've worked directly on my paper on logic.
In the past I have had to shove what little I can into a thin thread of tweets.
Now I am not stuck in a tight corner and can let my explanations relax into the more casual style that my friends and family are subjected to when I trap them in a "Johnversation".

My outlook on logic is now almost entirely that of Quine's (amended by Skinner's outlook on verbal behavior).
His "Methods of Logic 4th Edition" (and you need the 4th edition!) has almost split in half from how many times I've read it.
It doesn't help that Quine's main method of proof for predicate logic is about half way through the book and that it is the one section I've probably read and reread the most.

Whereas Quine's "Methods of Logic 4th Edition" narrowly focuses on the mechanics of logical practices, his "From Stimulus to Science" broadly applies those mechanics to, what Quine called, a "Breast Pocket Theory of the World".
As much as I disagree with the details of Quine's theory, e.g. he mistakes something like Skinner's experimental analysis of behavior for a dispositional stimulus-response theory, I agree with his methods.

Where Quine goes wrong, Skinner often goes right, and where Skinner goes wrong, Quine is there to clearly identify the problems if not also to provide compact solutions.

That is enough on where I'm coming from when I talk about logic.
Now for parts of my paper on logic.

***

There are a few unique things about my methods of logic.
The most important is that I aim to present predicate functor logic not as the elimination of variables from predicate logic, but as an autonomous system independent of predicate logic.

Whereas predicate logic is a consequence of grammar and truth, predicate functor logic is a consequence of grammar and denotation.
Closed sentences are true, predicates denote, and closed sentences are a degenerate form of predicate: they are zero place predicates.
Correspondingly, truth is a degenerate form of denotation.
In predicate functor logic we can define truth of a zero place predicate as the universal (or existential closure) of the quotation of its padding denoting.

> The phrases "existential closure", "quotation", "padding", and "denoting" are all technical terms and I have used them as such with full knowledge that they are foreign to most people e.g. "padding" is most likely unknown to anyone who is not yet familiar with Quine's predicate functors.

The first sentence of the paper, so far, is little more than the opening of a mystery:

> Logic is the science of validity as a consequence of grammar and denotation.

It is a sad sentence, and I am unhappy with it.
Simpler alternatives occur to me e.g.

> Logic is a consequence of grammar and denotation.

or perhaps

> Logic is that science which is the consequence of grammar and denotation.

but neither of those, nor the first, really clear the way for what is to come.
Such introductory sentences tend to come much later in my writing of a paper.
They are a summary of what is to come, and when what is to come has not yet come, there is nothing to concretely summarize.

The key components to the introductory sentence shall remain 'grammar' and 'denotation'.
They are an echo of Quine's "Philosophy of Logic 2nd Edition" which clearly shows the ways in which logic is the sum of grammar and truth.
He has a beautiful metaphor:

> Logic chases truth up the tree of grammar. [pg. 35]

and

> The logician talks of sentences only as a means of achieving generality along a dimension that he cannot sweep out by quantifying over objects.
> The truth predicate then preserves his contact with the world, where his heart is. [pg. 35]

Together these give a clear hint as to where Quine is coming from when he talks about the methods of logic.
The problem of "achieving generality" by talking of sentences is a critical part of Quine's method of semantic ascent:

> The move is what I call *semantic ascent*: mentioning an expression by name instead of using it as a component clause. [pg. 92 of From Stimulus to Science]

In "From Stimulus to Science" Quine does a better job of showing how quotation is just one potential method of semantic ascent by showing how it can be extracted from 'that' clauses e.g. 

- perceives that
- thinks that
- it occurred to him that
- believes that
- doubts that
- expects that
- hopes that
- fears that
- regrets that
- says that
- denies that
- predicts that
- strives that

[pg. 90 of From Stimulus to Science].
For most people familiar with Quine, semantic ascent is intertwined with his theory of propositional attitudes (as the examples amply indicate).
But, there is nothing so grand in the method: it is a technical solution to a technical complication:

> We can generalize on 'Tom is mortal', 'Dick is mortal', and so on, without talking of truth or of sentences; we can say 'All men are mortal'.
> We can generalize similarly on 'Tom is tom.', 'Dick is Dick', '0 is 0', and so on, saying 'Everything is itself'.
> When on the other hand we want to generalize on 'Tom is mortal or Tom is not mortal', 'Snow is white or snow is not white', and so on, we ascend to talk fo truth and of sentences, saying "Every sentence of the form 'p or not p' is true', or 'Every alternation of a sentence with its negation is true'.
> What prompts semantic ascent is not that 'Tom is mortal or Tom is not mortal' is somehow about sentences while "Tom is mortal' and 'Tom is Tom' are about Tom.
> All three are about Tom.
> We ascend only because of the oblique way in which the instances over which we are generalizing are related to one another. [pg. 11 Philosophy of Logic]

Quotations as a method of semantic ascent are haunted by philosophies of the past which are unable to distinguish between schematics and the items they purport to schematize.
For example, "of the form 'p or not p'" is usually written without the interior quotes, that is as "of the form p or not p", and this is otherwise excused as an expedient to effective writing where the reader is expected to know where the invisible quotations are to be put.
This has continued to confuse people who then take the occurrence of the letter pee in "of the form p or not p" as a pronoun referring to some otherwordly item called a proposition that somehow is *the meaning* of what a sentence like "Tom is Tom" says.

To be trapped down such dark allys of past mistakes is as silly as being stuck doing arithmetic with roman numerals.
We can look back and marvel at our mistakes and laugh at the labors we now save by teaching better methods.

All of this is to say that the single sentence

> Logic is the sciene of validity and validity is a consequence of grammar and denotation.

which is the first in my paper on logic is woefully inadequate as a summary of even what little I have said above and what little I have yet to say on the rest of logic.

***

Perhaps the proper "first sentence" is the second one

> Compounding is (denotative) functional when, exclusively, each like compound denotes or each like compound does not denote, where and only where (waow), exclusively, each like component denotes or each like component does not denote.
> Chains are compounds compounded functionally.

The phrase "denotative functional" generalizes to predicates what is said as "truth functional" of sentences.
The definition of denotational compound is clumsy with its "exclusively"s and "like"s, but the alternatives are woefully inaccurate and undermine the task at hand.
Logic does not have the benefit of its fruits before they have ripened.
As much as I would love to say that whether a compound denotes or not is a function of whether its components denote or not, there is no thing that can yet be spoken of as "a function".

It is only from the methods of logic that we come to pin down the items of the world.
While there are many who say that they are in touch with the world in some direct way, e.g. Russell repeatedly leaves room for singular terms so that they can be used to designate particulars of immediate experience, there is not yet a way that I have come to be in touch with the world as such others describe.
It is only through their reports that I am even confronted with the problems of such seemingly personalized revelations.

There is another problem that sometimes occurs when making such definitions: do I not appear to be speaking of "exclusive alternations" when I have not said what it means to say an instance of the sentence schema "exclusively p or not p"?
Is there not a circularity to my methods?

Quine was confronted with the same problem so often in the first publication of his early text "Mathematical Logic" that he had to add an appendix to help the anxious reader.
There he explains the difference between "theorem" and "metatheorem" as well as "deduction" and "formal deduction".
There (and in his Methods of Logic) he is confined to philosophic methods and only indirectly mentions in Methods of Logic the scientific origins of logical practices.

I have gone out of my way to use the phrase 'where and only where' or 'when and only when' to distinguish the preformal/informal or preschematic methods from the schematic methods of logic.
To explain the origin of verbal behavior with the forms "Exclusively Tom is wet or Tom is dry" is to leave the science of logic for the science of verbal behavior more generally.
Just as humans spoke grammatically before speaking of grammar, they spoke logically before speaking of logic.

At this time I do not have a better explanation as to why I can use phrases that appear to have the same form as those said to be defined e.g. 

> Joint denials denote waow each of their components do not.
> Negations are self joint denials: they denote waow their component does not.
> Alternations are negations of joint denials: they denote waow some of their components do.
> Conjunctions are joint denials of negations: they denote waow each of their components do.

***

With all that said, I have only introduced the following sentences from my paper on logic

> Logic is the science of validity and validity is a consequence of grammar and denotation.
>
> Compounding is (denotative) functional when, exclusively, each like compound denotes or each like compound does not denote, where and only where (waow), exclusively, each like component denotes or each like component does not denote.
> Chains are compounds compounded functionally.
>
> Joint denials denote waow each of their components do not.
> Negations are self joint denials: they denote waow their component does not.
> Alternations are negations of joint denials: they denote waow some of their components do.
> Conjunctions are joint denials of negations: they denote waow each of their components do.

Here is all that I have so far:

#### 2025 0412 1422 Paper on Logic

Logic is the science of validity and validity is a consequence of grammar and denotation.

##### Functional Compounding and Chains

Compounding is (denotative) functional when, exclusively, each like compound denotes or each like compound does not denote, where and only where (waow), exclusively, each like component denotes or each like component does not denote.
Chains are compounds compounded functionally.

##### Example Chains: Joint Denials, Negations, Alternations, and Conjunctions

Joint denials denote waow each of their components do not.
Negations are self joint denials: they denote waow their component does not.
Alternations are negations of joint denials: they denote waow some of their components do.
Conjunctions are joint denials of negations: they denote waow each of their components do.

##### Subcompounds and Functional Substitutions

Subcompounds of compounds are their self or those of their components.
Substitutions of like compounds for like nonchain subcompounds are (denotative) functional.
Functional substitutions of functional substitutions of compounds are functional substitutions of their self.

##### Functional Validity, Consistency, Implication, and Equivalence

Compounds are (functionally)
* valid waow each of their functional substitutions denote,
* consistent waow their negation is nonvalid (i.e. soem of their functional substitutions denote),
* implied by others waow the conjunction of their self (the conclusion) with the negation of the other (the premise) is nonconsistent (i.e. each of their functional substitutions denotes where the same of the other does), and
* equivalent to others waow they are mutually implicative (i.e. each of their functional substitutions denotes waow the same of the other does).
[See pg. 36 of POL]

##### Example Validities and (Non)consistencies: Laws of Excluded Middle, Contradiction, Self Implication, and Self Equivalence

Alternations of compounds with their negations are valid (they denote waow the compound does or its negation does, i.e. waow it does or does not, so, each functional substitution denotes).
Conjunctions of compounds with their negations are nonconsistent (they denote waow their compound does and its negation does i.e. waow it does and does not, so, each functional substitution does not denote).
Compounds are implied by and equivalent to their self.

##### Functional Substitutions Keep Validity, Nonconsistency, Implication and Equivalence

Functional substitutions in
* validities are validities (each functional substitution of the functional substitution of the validity is a functional substitution of the validity and hence denotes),
* nonconsistencies are nonconsistencies (each function substitution of the negation of the functional substitution of the nonconsistency is a functional substitution of the negation of the nonconsistency i .e. is a functional substitution of a validity and hence the negation of the functional substitution of the nonconsistency is valid so that the function substitution of the nonconsistency is nonconsistent),
* implications are implications (the conjunction of the conclusion with the negation of the premise is nonconsistant and hence its functional substitution is nonconsistent and identical to the conjunction of the function substitution of teh conclusion with the negation of the functional substutituion of the premise), and
* equivalences are equivalences (functional substitutions of mutual implications are mutual implications).

##### Interchanges of Equivalents are Equivalent
Interchanges of equivalents in a compound are equivalent to that compound (each functional substitution of a compound matches the same of its interchange, except perhaps for the same of the equivalents which otherwise denote in tandem, so each denotes waow the other does i.e. they are equivalent).

##### Interchnage of Equivalents Keeps Validity, Nonconsistency, Implication, Equivalence, Nonvalidity, Consistency, Nonimplication, and Nonequivalence

Interchanges of equivalents in
* validities are validities (each functional substitution of the interchange denotes waow the same of the validity does),
* nonconsistencies are nonconsistent (their negation is a validity and so the interchange in the negation is a validity),
* implications are implications (interchange into the nonconsistency is a nonconsistency),
* equivalents are equivalents (interchange of mutual implications are mutual implications),
* nonvalidities are nonvalidities (a compound is nonvalid waow some functional substitution does not denote, i.e. some functional substitution of its negation denotes, i.e. its negation is consistent, and since the negation of the interchange is identical tot he interchange of the negation which is consistent and consistency is kept by interchange then the negation is consistent i.e. it is nonvalid)
* consistencies are consistencies (the negation of the interchange is identical to the interchange of the negation which is nonvalid hence it is nonvalid),
* nonimplications are nonimplications (nonimplication is consistency of the conjunction ...)
* nonequivalences are nonequivalences (one is a nonimplication ...).

##### Equivalents of Identity
Compounds are equivalent to
* their double negation (which denotes waow the negation of the compound does not, i.e. waow it does, so, each functional substituion of it denotes waow the same of its double negation does),
* their self alternation/conjunction (which denotes waow some/each of its components does i.e. waow the compound does), and 
* their alternation/conjunction with nonconsistencies/validities.

##### Equivalents of Distributivity of Conjunctions and Alternations



### 2025 0412 1349
I have found that it is easier to be wrong than it is to be right and that it is easier to be right after having been wrong.
If there was a way to avoid being wrong then I would only act in that way.
Logic helps but is slow.
Sorting through the wrong bits takes more than what logic has to offer.

I see logic as part of science and see science as picking up what logic fumbles and drops.
Logic works on the theoretical side of science and laboratories work on the experimental side.
As I have said many times before "Libraries select theoretical practices and laboratories select experimental practices".
Science is compounded of theoretical and experimental practices.

One of the reasons I've allowed myself to write in this longer form is to make it easier for other people to see that I am often wrong, and that it is only from first being wrong that I get anywhere near being right.
The definitions, the natures, or the essences of what is right and what is wrong are not easy to suss out.

Many people have spent their whole lives trying and failing to establish what is right and what is wrong.
As much as I have a sort of optimism that if I work hard enough then I may one day uncover some piece of a complete theory of right and wrong, I am not so foolish to think that I shall do anything like what the best already have.

With that in mind, I offer these writings that are most certainly mistaken for no other reason than that they occurred to me.

***

I talk about behavior a lot.
Does that imply that my behavior is under my careful control?
No.
It is one thing to know a science and another thing to have a technology in hand.
The things of technology are the consequences of science, more or less.
But, the knowledge of science lacks the concrete dimensions of levers, pullies, medicines, and computers.
All that goes into scinece is often ephemeral: here for but a moment and gone forever, lost to our remote contact with the past through fragmentary records that soon fall apart.

Those who manufacture products, be they assemblies of machines, or chemicals, or organisms, or programs, or etc., work in ways often mistaken as scientific.
They manufacture with the controls offered by technologies i.e. with the descendants of scientific practices.

The difference between science and such manufactureres is the difference between sensitivity and insensitivity.
The manufacturer is deliberately insensitive to the strange variations responsible for science.
This is not what most people think or say: they say that business is as much a science and an art as physics or biology.
They insist that it is so in large part because the best businesses are almost always the result of some new technological consequence from science.

This does not always seem to be the case.
Some businesses appear to survive despite science.
Much taht is "made for TV" objects to being called pseudoscience.
It sells, and there is a science to sales!

These are all problems of control: can you make a person buy your product?
Some say you can't, others say you can.
Some say that people only buy what is an intrinsically good product: you can't trick your way into becoming a good business when your product is technologically sophistocated.

***

What's with people who make a living as oracles?
They make predictions, often asserted with the authority of a proclamation, and profit, one way or another, from betting that they're right and ultimately being right.
But, are they betting on what they think they are?
Are they right when their bet pays off?
There seems to be some difference between constructing bets that are verly likely to pay off and making predictions like those people assume come from our best sciences.

How do we know the future?
When we speak of the future, as in predictions, what is it that we are speaking of?
Can speach actually be said to be about anything?
Certainly the speaking occurrs, but what good reasons do we have for saying that such speach is about this, that, or the future?

### 2025 0412 0119
[Matios Berhe](https://www.twitter.com/MatiosTV) suggests I read
- Kant's Critique of Pure Reason, chapter 2, section 12
- Kant's Critique of Pure Judgement, Critique of the Teleological Judgement
- Locke's Essay Concerning Human Understanding

I have now added these to my long list of "stuff to read".
Some day, sooner rather than later, I shall provide a complete list of texts in my library and the schedule on which I read them.
There are two main reasons to do so:
1. to point up the origins of what I have to say (so that others do not suspect me of originating, initiating, or creating my verbal behavior)
2. to better scheduling my reading and writing.

I already have a copy of Kant's Critique of Pure Reason that I read first when I was in high school.
There are a few texts from that time which have survived long enough to find themselves on my shelves.
One of the treasures from that time is Russell's "Problems of Philosophy".
In no way did I learn all that Russell and Kant had to teach in their books when I first read them so very long ago.
There is something special though about reading things that you do not yet understand.

The more I read the more I don't at first understand, and yet, at the same time, the more I learn about the world as a whole.
There is something about reading which makes the world bigger and smaller than it once seemed.
When I'm not in such a metaphorical mood, I am quick to say the world is no bigger nor no smaller than it is: to say otherwise is to plunge such a theory into contradiction from which no logic prevents each and every imaginable conclusion.

People are usually scared by contradiction for the wrong reasons: this is somewhat corrected by phrases like "law of explosion".
The problem with contradiction is not really a problem: when you have a contradiction you find yourself without the benefits of a consistent theory.
Some people are ok with that.

### 2025 0412 0102
I finally have a place to put all my daft drafts without chopping them into little tweet size pieces.
Why has it taken me so long to return to this long form method of writing?
It's mostly that strange feeling I get that this is like a message in a bottle sent out into an enormous sea of nothing.

Once I figured out that it wasn't sending the message that mattered most, I warmed up to the practice of writing from my little island on the internet.

## 2025 0411

### 2025 0411 2248
I'm implementing a little LISP.
Like the LISPs of yore, there are two basic items: pairs and atoms.

Pairs split into left and right parts.
Paul Graham wrote of "halves" when explaining pairs in [Bel](https://www.paulgraham.com/bel.html).

Whether you talk about the anatomy of a pair by "breaking it into pieces", "splitting it into halves", or "decomposing it into components", there is one rule that governs the logic of pairs: 

> pairs are identical where and only where their left components are identical and their right components are identical.

This is otherwise know as "the law of extensionality of ordered pairs".
It is a fancy way of mentioning the precise conditions upon which to count two items in a universe of ordered pairs as identical.
The single premise "each item is (a,b,c,x,y, and z such that a pairs b with c, x pairs y with z, and a is identical to x, if and only if b is identical to y and c is identical to z)" logically implies each conclusion of *any* theory of ordered pairs.

Now is not the time or place to get distracted by problems of logic, theories, and ordered pairs.
As much as I would enjoy a full digression on the origins of logic, the extensionality of effective theories, and ordered pairs as a paradigm of philosophic investigation, those are not the problems that I am presented with here and that I have to solve now.

Back to my little LISP.

Pairs are identical where and only where their parallel components are.
For now, "(x,y)" is short for "the pair whose left part is x and whose right part is y".
Thus, (x,y) is identical to (a,b) where and only where x is identical a and y is identical to b.

Since I'm coding up my little LISP in javascript I'm using the following definitions:

```
let cons=(x,y)=>[x,y]
, car=x=>x[0]
, cdr=x=>x[1]
```

These just say that 'cons' is the name of a function that takes two arguments and returns an array whose zeroth component is the first argument and whose first component is the second argument (note this confusing sentence is why some people prefer different methods of indexing into arrays of data).
The function named 'car' takes one argument and returns the item at index zero of it.

> If you do not already know how javascript arrays work, and how indexing into javascript arrays works, then these explanations are not helpful.
> Thankfully, if you do not already know how javascript arrays work, you get the joy of learning that first before coming back here.
>
> I have not yet found a faster way to introduce programming than to present it after teaching logic up to and including Quine's main method (of proof for predicate logic).
> Since more people are familiar with javascript than they are with Quine's main method--- or with the methods of logic--- I shall continue to avoid what might otherwise appear as an entirely roundabout way of teaching programming from scratch.

The function named 'cdr' takes one argument and returns the item at index one of it.

Both 'car' and 'cdr' are names inherited from working with programmable machines (aka "computers") during the late 1950s and early 1960s.
If you want to learn more about where 'car' and 'cdr' come from, and perahsp even how to pronounce 'cdr' (which I say as "could-er"), then track down McCarthy's history of LISP and give it a compassionate read.

With 'cons', 'car', and 'cdr' we can make pairs, get their left part, and their right part respectively.
There are algebraic or equational ways of introducing these basic functions e.g.

```
car(cons(x,y))==x
cdr(cons(x,y))==y
'car(x)==car(y) and cdr(x)==cdr(y)' is equivalent to 'x==y'
```

but again this is just a distraction from the problem to solve (implementing a little lisp like language).
Note, such purportedly algebraic or equational theories appear to follow entirely from pure logic when Quine's method of schematic theories is extended to schematic theories of functional predicates (see Quine's 'Philosophy of Logic' for a schematic theory of identity).

Following the conventions established in the wonderful book "The Little Schemer" by Friedman and Felleisen, cdr and car are only used in contexts where their arguments are pairs with left and right parts.
There is one pair without any parts.
It is unique (in more ways than one).

```
let nil=[]
, id=(x,y)=>x==y
, empty=x=>id(x,nil)
```

The item named 'nil' is an empty javascript array.
Not all empty arrays are equal in javascript.
Equality, identity, and indiscernibility are hard to deal with when you don't start with logic.
Surprisingly, 'id' works almost like it does in a language like [Bel](https://www.paulgraham.com/bel.html) even though it is not obvious that it should.
Remember the 'law of extensionality of ordered pairs'?
Well, it is violated by almost all programming languages.

There is a silent assumption that is among the many hidden away behind any explanation of a foreign programming language: they actually run on real machines.
On most machines there are places and objects at those places.
Objects in different places can't be identical!
(Think that over: identical things can't differ in place apparently.)
And yet, sometimes they are!

Pairs may have parallel components that are identical without being identical themselves.
This happens when two pairs are silently assumed to be in different places but each of them places their components *in the same places*!
There are lots of solutions to this problem of identity.
Most of them are needlessly complex and overburdened with crusty philosophies.
My solution is as sad as it is simple: just go with what javascript gives you.

My real solution is to go with logic: identity is indiscernibility in that any two predicates which fill out each instance of the premises of identity are coextensive and indiscernibiltiy is one such predicate (in a logical theory with a finite lexicon).
For more on this see Quine's "Philosophy of Logic".

Atoms are not pairs (and pairs are not atoms).

```
let pair=x=>Array.isArray(x)
, atom=x=>!pair(x)
```

Is nil an atom?
No, not here.
Is nil an atom elsewhere?
Sometimes.
Early LISPs only had pairs and symbols.
Symbols are often a large part of LISPs.
They are the workhorses of reference: they end up sometimes playing the part of proper nouns and sometimes playing the part of pronouns.
Here again, there is much to be learned from predicate logic.
Here again, I must avoid dipping into such unfamiliar wells of simplicity.

The general problem of atomic items provides ample opportunity to make use of degenerate cases of common objects e.g. Quine provides for atoms in his "Set Theory and its Logic" by picking out atoms with the reflection of membership i.e. 'x is atomic' is short for 'x belongs to x'.
Most people avoid making convenient use of alarming degeneracies: they're really missing out on some good fun.

Atomic items and empty items have a long and sordid history.
Atoms are not supposed to have parts, and yet we are often forced to say that they do.
Empty items always end up behaving like atoms in many respects--- e.g. they are empty and presumably "don't have parts"--- but they almost always tend to be unique where as there tend to be many many atoms in any given theory.

The only place I have found clarity on these boundary problems is, you guessed it, logic.
In a theory of ordered pairs, it is enough to provide for the existence of an ordered pair that neither has a left part nor has a right part.
It is then unique since any two purportedly distinct empty pairs have identical parallel components (vacuously).
Atoms are then accomodated by one of a few methods, each of which match up with Quine's atomic methods in "Set Theory and its Logic".
An item of a theory of ordered pairs is atomic when it is identical to its components (or, just one of its components, in which case the other components allows us to distinguish between atoms with, e.g., identical left components: are these even atoms?).

Sadly neither of these principles are used in LISPs.
Again, this is because there is that silent assumption working in the background of most explanations: the programming language runs on machines in our shared material world.

This is enough thinking on this for now.

### 2025 0411 2217
I can not guarentee that there will be no spelling errors in what I write.
Rarely do I spell well, and rarely do I take the time to check my spelling.
Rather than check my spelling I would keep on thinking.
Spell checking stops thinking except when looking up the word to be spelled spurs some better bits of writing.
Definitions have the power to prompt better writing when they present short and sticky explanations.
Those sensitive to spelling errors will be greatly disappointed (that is until I go back through and spell check this document, that is, if I ever get around to doing that).

### 2025 0411 2158
I do not have much familiarity with using this software and can not be certain how what I've written here will appear.
I am also not familiar with markdown.
Will there be new lines in the final document if I put them here?
The answer is no.

Does this start a new paragraph? The answer is yes.

Does this [20250411](#2025-0411) automatically.

ok, rather than get lost in the details of editing I will just go on and start writing.
