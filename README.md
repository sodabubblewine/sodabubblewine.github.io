# What I Must Do Before I Die
Discover, predict, and control changes in counts, rates, and accelerations as selections from variations on physical, chemical, biological, behavioral, and cultural scales by making and maintaining strong practices mediated by strong people marked by strong principles from the sciences of logic (denotative, Boolean, and functor), mathematics (calculi, collections, and categories), physics (quantum field theory, statistical thermodynamics, gravity), chemistry (phyiscal, biophysical, and biological), biology (oranelles, organisms, environments), behavior (biological, biosocial, social), and culture (history, science, technology).

# Notes

## 2025 0610

### 2025 0610 1648
Notes on "the brain is not a computer".


The brain is not a computer: it is not a consequence of centuries of science and logic.
It evolved, like any other organ, by natural selection by consequences from variations.

It evolved without anything like the deliberate designs which brought about modern digital computers and their analogue predecessors.
We can say that the brain evolved as part of a behaving organism and that the practices which brought about computers evolved as social environments brought more and more of their deferred consequences under scientific and logical control.

A common mistake, especially among brain scientists, is to say that the mind is what the brain does.
That different parts of the mind are the computations which run on different parts of the brain as biochemical computers.

What I finally wrote (and I regret sending it):

> The brain is not a computer in the same way that the physical world is not an equation. Computers are a consequence of centuries of science and logic, but the brain is a consequence of some 4 billion years of natural selection. Zeno's paradoxes teach how such metaphors miscarry.


### 2025 0610 1606

#### HOW TO CONTROL YOUR BEHAVIOR

Your behavior is under the control of its consequences.
The more control you have over the consequences of your behavior, the more (self) control you have over your behavior:
1. Find the controls you have.
2. Build better controls from them.
3. Go to 1.


#### HOW TO MAKE A THEORY LOGICAL

1. List the nouns and verbs you use when you talk about it.
2. Define as many of them as you can from as few as you can.
3. List sentences made from those few that are true.
4. Conclude as many of them as you can from as few as you can.
5. Go to 1.


#### HOW TO ACT

Conclude from contemplation on templates:
1. Notes link templates to conclusions.
2. Memos link contemplations to templates.
3. Rules link conclusions to contemplations.
4. Good notes accurately report.
5. Good memos accelerate change.
6. Good rules adaptively govern.

If memos, notes, and rules were lights of a traffic signal they would be colored red, yellow, and green.

Logic is the axis on which the lever arm of language turns.
Without logic, all the work in the world is without fixed purpose.


## 2025 0608

### 2025 0608 1515
This continues work on my paper on logic from [2025 0521 1232](#2025-0521-1232).

In order to better show how old notation gives way to better methods by way of the substitutional definition of validity, and to take a big step towards finishing my paper on logic, I shall write out my thinking as I prove the theorems which, at first, appear to decidedly take truth functional (denotative functional) logic into truth functional and existential closure logic of one place predicates (Boolean closure logic).

This corresponds to the arguments from chapter 18 of Quine's "Methods of Logic 4th edition".
First a review of terms:

> Compounding is (denotative) functional when, exclusively, each like compound denotes or each like compound does not denote where and only where (waow), exclusively, each like component denotes or each like component does not denote. Chains are compounds compounded functionally e.g. joint denials denote waow each of their components do not, negations are self joint denials (they denote waow their component does not), alternations are negations of joint denials (they denote waow some of their components do), and conjunctions are joint denials of negations (they denote waow each of their components do).
>
> Subcompounds of compounds are their self or those of their components. Subsitutions of like compounds for like nonchain subcompounds are (denotative) functional. Compounds are (functionally) valid waow each of their functional substitutions denotes everywhere, consistent waow their negation is nonvalid (some of their substitutions denote somewhere), implied by others waow the conjunction of their self (the conclusion) with the negation of the other (the premise) is nonconsistent (each of their functional substitutions denotes where the same of the other does), and equivalent to others waow they are mutually implicative (each of their functional substitutions denotes waow the same of the other does).

The last addition made to the evolving paper was the definition of (existential) closures and what I call Boolean chains and what Quine called "Boolean statement schemata".
Quine could call them statements, or, as he tended to in his later work, sentences because they were built directly from the truth functional combinations of sentences as instances of truth functional schema of sentence letters e.g. 'It is raining or it is cold' is an instance of the truth functional schema 'p or q' where 'p' and 'q' are sentence letters and 'p' is substituded by the sentence 'it is raining' and 'q' is substituted by the sentence 'it is cold'.

Here, there are only predicates, and the shift from 'is true' to 'is true of' is coupled with a disavowal of schematic methods entirely for purely grammatical ones e.g. 'rain or cold' is the alternation of the predicates 'rain' and 'cold'.
Thus, the alternation of 'rain' and 'cold' is denotes where and only where 'rain' denotes and 'cold' denotes i.e. 'rain or cold' is true where and only where 'rain' or 'cold' is.
To be absolutely clear in traditional terms: there is somewhere, call it x, such that 'rain or cold' denotes x if and only if 'rain' denotes x or 'cold' denotes x.
I am hesitant to put things in these terms because they do not properly carry the methods at hand.

They posit some item (here called 'x') which poses as an individuated or reified object, but what I have in mind is much more like an undifferentiated occasion or, now speaking with overt metaphors, a global stimulation or a kind of passing show or, unhappily, a unitary essence (though I would never agree to anything like this last blatently metaphysical description).

The plan is to attack the problem of individuation and reification through the abstract items known as ordered pairs and subject to the single rule that ordered pairs are identical if and only if their corresponding components are.
This simple plan is actually much more difficult to execute than it would seem because the problem of individuation and reification are difficult.
They are difficult in that individuation and reification are remote from talk of them: they are not easy to examine by speaking and listening alone.
The wedge of simplicity that I plan on pushing through this problem is that talk of the left and right part of an ordered pair is sufficiently similar to what is called for by individuation that it can sturdy any drift into metaphysical speculation.

This long prelude to what is ultimately a report on my discovery expedition into a proof about Boolean chains is warrented by the problem posed by introducing the principle to prove.
Quine was able to give the law as follows

> If a Boolean term schema is consistent, then in any universe containing a given object there is an interpretation fo term letters that makes the schema come out true of that object. [pg. 115]

A Boolean term schema is just a truth functional composition of one place predicates which Quine then paraphrases into truth functional sentence schema by distributing the name of an object, e.g. 'x', down the truth functional connectives until it is predicated of each one place predicate.
The steps are given by the first paragraph of chapter 18:

> To say of an object x that it is an F, we write 'Fx'. Here then is a new sort of sentence schema: 'Fx', 'Gx', etc. These may be compounded by truth functions; e.g. 'Fx and not Gx, or Gx only if Hx'. We may conveniently abbreviate such compounds by extracting the 'x' everywhere and putting it at the end, thus: '[F and not G, or G only if H]x'. We arrive in this way at schematic representations of certain complex terms: schemat such as 'not G', 'F and G', 'G only if H', 'F and not G, or G only if H', etc. They will be called *Boolean term schemata*. [pg. 114]

Since I begin with predicates rather than sentences, I have no recourse to the logic of sentences and their truth functional structure.
I also do not have access to methods of truth value analysis that Quine uses throughout his text.
The consequence of these restrictions is a prestine simplicity: what comes out as truth functional analysis in Quine's text comes out as equivalence to succesive alternational developments of subcomponents.
I've not gone through the details of this development in the paper yet because it would break the law of premature optimization.

Most importantly, I do not have names for things: variables play no part at all nor do singular terms nor do singular descriptions.
All of these carry over into singular predicates which denote one and only one item: all methods of designation reduce to this use.
Predication is achieved by, e.g., an existential cropping of a conjunction of a singular predicate with a one place predicate.
Existential croppings are straight out of Quine's presentations of predicate functor logic.
Croppings do not crop up anywhere in the Boolean parts of my paper on logic.

Returning now to the law at hand in Quine's words.
The consistency of the Boolean term schema of the law guarentees that there is some object of some universe under some interpretation that it is true of.
What the law says is something more than what consistency directly implies: pick an object in any universe whatsoever and there is an intrepretation that makes that consistent Boolean term schema true of that object in that universe.
Quine's argument is short and simple in ways that are not accessible directly by my methods:

> For, being consistent, the schema resolves to the true under some substitution of the true and the false for the term letters. We have merely to interpret each term letter as true or false of the given object according as its substitute was the true or the false. [pg. 115]

One note on my quotation of Quine's words: I've replaced quotations of a special bold letter tee and its upside down reflection with the phrases 'the true' and 'the false' even though Quine otherwise mentioned these notations as short for 'truth' and 'falsity' as truth values.
This is primarly to draw a closer connection between Frege's outlook and Quine's (it is mostly to remind my future self that there is more to say about this later).

> Anywhere is denoted by some functional substitution of a consistency.

That is the problem to solve.
It is not solved here because I stopped here writing before I solved it.

## 2025 0605

### 2025 0605 1541

Henkin's theorem is presented in Ebbinghaus, Flum, and Thomas as 

> Let capital phi be a consistent set of formulas which is negation complete and contains witnesses. Then, for all phi, 
> > the term interpretation of capital phi satisfies phi if and only if capital phi proves phi.

This reveals one of the weaknesses of Ebbinghaus, Flum, and Thomas' book (and a lot of books on mathematical logic): a needlessly complex sequence of symbols and definitions are lofted atop each other in haste to establish a narrow statement of an otherwise general result.

There are a number of things that I have to get out of the way in order to write on this:

1. Almost all books on mathematical logic (and some on logic) make a grand dichotomy between syntax and semantics as if these are two complementary and comprehensive classifications of the methods of logic i.e. as if a method of logic is syntactic or semantic (or perhaps both, although usually only by some equivalence).
This yields the myopic view that there are two classifications of logical methods: model theoretic and proof theoretic.
In EFT model theoretic methods are a kind of mathematical semantics and proof theoretic methods are a kind of matheamtical syntax.
But this is not really just a pecularity of EFT's book on mathematical logic, it can be found throughout books on mathematical logic e.g. the classic by Joseph R. Shoenfield.

    This is a mistake of history and says more about the insensitivity of mathematical logicians to logicians than it does anything about logic or mathematical logic.

2.


Well, there's a fragment of writing that I'll never finish.




## 2025 0602

### 2025 0602 1615 The Ethics of Logic
There is a grand misconception that since logic is not about anything in particular then logic is not about the world and hence logic, as with science, is without ethical or moral values.
Logic is part of the world.
It "deals with" or "is about" the world through each of its primary origins: grammar and denotation (as 'is true of' and not as 'designates').

Ethical and moral contingencies (as consequences from responses on occasions e.g. a door opens from a push on a lever, an electron emits from a photon on an atom, etc.) are as much a consequence of logic as they are grammar and denotation.
For most people, it is easier to note the part played by denotation in ethics than it is the part played by grammar.
Grammar is traditionally dealt with as something secondary to denotation as if denotation covered meaning rather than meaning following as a trivial kind of reference accomodated entirely by denotation of a special sort (just as designation is a special case of denotation e.g. denotation by a one place singular predicate).

The origins of grammatical and denotative verbal behavior dispell such traditional fictions and fictious explanations leaving a concrete link between logic and the rest of the world: the autoclitics of Skinner's "Verbal Behavior".

...

The ethics of logic works much like the grammar of logic in that the logic of grammar strengthes the grammar of logic and so on like so many other human practices.
Just as the logic of grammar is a consequence of the gramamr of logic, so to is the logic of ethics a consequence of the ethics of logic.

...

Logic is the fulcrum of languages.

the ethics of extensionality

the ethics of validity

the predicates of ethical theories

## 2025 0529

### 2025 0529 1511
This continues my work on my little lisp from way back in [2025 0514 2226](#2025-0514-2226).

It has been a while since I last worked on my little lisp, and, as is so often the case when I return to what I left incomplete, I must begin by collecting the entirety of what was done.
As a reminder: I do my programming in a browser by first going to the url "about:blank" and then opening the developer tools on it.
In Google's Chromium, script snippets are my programming environment.
This is not because it is "the best programming environment", but because this happens to be the easiest way I can program on whatever computer happens to be available to me.


```
// pairs
let theEmptyPair={}
, isEmpty = it => it == theEmptyPair
, pairOf = (it, that) => [it, that]
, leftOf = it => isEmpty(it) ? it : it[0]
, rightOf = it => isEmpty(it) ? it : it[1];

// sequences as pairs
let theEmptySequence = theEmptyPair
, isEmptySequence = isEmpty
, singletonSequenceOf = it => pairOf(it, theEmptySequence)
, headOf = leftOf
, restOf = rightOf
, concatOf = (it, that) => isEmptySequence(it) ? that 
  : pairOf(headOf(it), concatOf(restOf(it), that))
, prependSingletonOf = (it, that) =>
   concatOf(singletonSequenceOf(it), that)
, appendSingletonOf = (it, that) =>
   concatOf(that,singletonSequenceOf(it));

// stacks as pairs
let theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, singletonStackOf = it => pairOf(theEmptyStack, it)
, pushOf = pairOf
, dropOf = leftOf
, topOf = rightOf
, secondOf = stack => topOf(dropOf(stack))
, drop2Of = stack => dropOf(dropOf(stack))

, prependOf = stack => pushOf(drop2Of(stack)
  , prependSingletonOf(topOf(stack), secondOf(stack)))
, appendOf = stack => pushOf(drop2Of(stack)
  , appendSingletonOf(topOf(stack), secondOf(stack)));

// letters, strings, concatenations and runes
let isConcatenationOf = (x,y,z) => x == y.concat(z)
, isEmptyConcatenation = x => isConcatenationOf(x,x,x)
, theEmptyConcatenation = ''
, isIdenticalConcatenation = (x,y) => 
   isConcatenationOf(x,y,theEmptyConcatenation)
, theConcatenationOf = (x,y) => x.concat(y)

, stringOf = (...letters) => theEmptyConcatenation.concat(...letters)
, firstLetterOf = letters => isEmptyConcatenation(letters) ? theEmptyConcatenation : letters[0]
, restLettersOf = letters => isEmptyConcatenation(letters) ? theEmptyConcatenation : letters.slice(1)

, theAlphabet = '() 0123456789abcdefghijklmnopqrstuvwxyz'

, runeHelpOf = (it, abc) =>
 isEmptyConcatenation(abc) || isIdenticalConcatenation(it, firstLetterOf(abc)) ? theEmptyPair
 : pairOf(theEmptyPair, runeHelpOf(it, restLettersOf(abc)))
, runeOf = it => runeHelpOf(it, theAlphabet)

, letterHelpOf = (it, abc) => isEmptyConcatenation(abc) ? theAlphabet
 : isEmpty(it) ? firstLetterOf(abc)
 : letterHelpOf(rightOf(it), restLettersOf(abc))
, letterOf = it => letterHelpOf(it,theAlphabet)

, runesOf = letters => isEmptyConcatenation(letters) ? theEmptySequence
  : prependSingletonOf(runeOf(firstLetterOf(letters))
    , runesOf(restLettersOf(letters)))

, lettersOf = runes => isEmptySequence(runes) ? theEmptyConcatenation
  : stringOf(letterOf(headOf(runes)), lettersOf(restOf(runes)))

// read and print sequences
let theOpenRune = theEmptyPair
, isOpenRune = isEmpty
, readOpenRuneOf = (stack, runes) => 
   readSequenceOf(pushOf(stack,theEmptySequence), restOf(runes))

, theCloseRune = pairOf(theEmptyPair, theOpenRune)
, isCloseRune = it => !isEmpty(it) && isEmpty(leftOf(it)) && isOpenRune(rightOf(it))
, readCloseRuneOf = (stack, runes) =>
   readSequenceOf(appendOf(stack), restOf(runes))

, readDefaultRune = (stack, runes) =>
   readSequenceOf(appendOf(pushOf(stack,headOf(runes))), restOf(runes))

, readSequenceOf = (stack, runes) => 
  isEmptySequence(runes) ? headOf(topOf(stack))
  : isOpenRune(headOf(runes)) ? readOpenRuneOf(stack,runes)
  : isCloseRune(headOf(runes)) ? readCloseRuneOf(stack, runes)
  : readDefaultRune(stack, runes)
, readOf = runes => readSequenceOf(theEmptyStack, runes);

let parenOf = runes => prependSingletonOf(theOpenRune
    , appendSingletonOf(theCloseRune, runes))

, printSequenceOf = sequence =>
  isEmptySequence(sequence) ? theEmptySequence
  : concatOf(printOf(headOf(sequence))
    , printSequenceOf(restOf(sequence)))

, printOf = sequence =>
  isEmptySequence(sequence) ? parenOf(theEmptySequence)
  : parenOf(printSequenceOf(sequence))   

, read = letters => readOf(runesOf(letters))
, print = sequence => lettersOf(printOf(sequence));
```

The definition of 'isConcatenationOf' aspires to more than it can reach in Javascript: a three place predicate in the lexicon of a theory of concatenation.
The definition of 'theConcatenationOf' most clearly reveals the weakness (to anyone who wasn't already confused by the invocation of '.concat' in the definition of 'isConcatenationOf'): it is not short for a singular description.
The three place functional predicate 'is the concatenation of' is constructed with the help of singular descriptions as

> x, y, and z are such that x is identical to the (w such that w concatenates y with z)

where 'is identical to' is coextensive with 'is indistinguishable from' as in a schematic thoery of identity.
This is the basic context within which Russell's elimination of singular descriptions occurs:

> x, y, and z are such that some item is (w such that x is identical to w and each item is (u such that u is identical to w if and only if u concatenates y with z)).

I shall leave that behind and go on to what occurs to me in response to copying the latest iteration here.

I've been looking for a better way to weave together the work so that it is cumulative without imposing additional constraints which are extraneous (as was done in an older iteration which built stacks from sequences rather than from pairs).

The problem remains printing and reading, as much as these are likely to stick around in later iterations.
A new solution occurred to me which is no more foreign to me than any other solutions that have occurred to me (that is they were already present in some work or another that I was exposed to and can not even remotely be attributed to me except perhaps for passing through me).

By including a default function which catches what the defined reader doesn't, i.e. as in the default case of a case statement in a standard imperitive programming language, readers and printers can be combined like LEGOs to accomidate evolving occassions that must be dealt with e.g. as when the items of a sequence are dealt with as something other than sequences.

The names of functions defined are already far longer than they would ever be if I was programming privately, perhaps this is for the better or perhaps this is for the worst, I can not yet tell.
I'm inclined to introduce the following obtuse names "readStringAsRune", "readStringAsRunes", etc. so that then all is readers e.g. "readPairAsString" is the new name of the old function "printAsString".
It may even be more convenient then to just name the functions "letterAsPair", "lettersAsPair", "lettersAsSequence", etc. where, e.g., "lettersAsSequence" happens to be identical to the composition of the functions designated by "lettersAsSequence" and "sequenceAsPair".
But this doesn't make sense.

The distinction between reading and writing is an io problem.
So, "inputAsSequence" or "readAsSequence" makes sense because everthing after "inputAs" is, by explicit design, a pair.

Aha, ignore all that.
I uncovered a potential solution that simplifes and unites.
Instead of creating a seperate set of functions for letters, strings, and the like, I can just go straight to a sequence of runes.

Darn, I'll have to come back and fill this out later.
It has been very hard to find time to write out what I have to say as it occurs to me, even with respect to these little steps along this programming project.

The following incomplete and uncommented code is left here as a record of what was done but not as what was explained.

```
, where = (is, seq) => isEmptySequence(seq) 
  || is==headOf(seq) ? theEmptySequence 
  : prependSingletonOf(theEmptySequence, where(is, restOf(seq)))
, here = (at, seq) => isEmptySequence(at) ? headOf(seq)
  : itemAt(restOf(seq), restOf(at))
, each = (f, seq) => isEmptySequence(seq) ? theEmptySequence
  : prependSingletonOf(f(headOf(seq)), each(f, restOf(seq)))
, over = (f, f0, seq) => isEmptySequence(seq) ? f0
  : f(headOf(seq), over(f, f0, restOf(seq)))

, til = (stop, base, step, part, rest, it) => stop(it) ? base
  : step(part(it), til(stop, base, step, part, rest, rest(it)))

, sequenceOf = items => til(x=>x.length==0, theEmptySequence, prependSingletonOf, x=>x[0], x=>x.slice(1), items)
, here = (at, seq) => til(isEmptySequence, headOf, x=>x )

, theAlphabet = sequenceOf('() 0123456789abcdefghijklmnopqrstuvwxyz')
, readRune = rune => where(rune, theAlphabet)
, printRune = pair => here(pair, theAlphabet)

, readRunes = runes => each(readRune, sequenceOf(runes))
, printRunes = pair => 
```

It is unclear if taking up these methods of functions on functions is sufficiently smooth to warrent admission into this discovery expedition.
It occurs to me that since I have only recently begun sharing public notes that those reading these may not know how familiar I am with the scope and limits of different methods of programming and their mathematical theories.
Ultimately this is of no concern to me because if I can avoid burdening others with all that I have learned and subsequently passed over, then they will be left with that much more energy to learn all that I never shall in my life.

### 2025 0529 1419 The Philosophy of Composition by Edgar Allan Poe
R.P. shared a delightful paper with me upon reading my last entry on Paul Graham's "Good Writing":
[The Philosophy of Composition by Edgar Allan Poe](https://www.poetryfoundation.org/articles/69390/the-philosophy-of-composition).
It is to be compared to B. F. Skinner's talk "On Having a Poem" a recording of which can be heard here: <https://www.bfskinner.org/bf-skinners-lecture-on-having-a-poem/>

Standards of records required to provide explanations for the origins of poetic behavior, or any verbal behavior whatsoever, are to be found in Skinner's ["Cumulative Record Definitive Edition"](https://www.bfskinner.org/wp-content/uploads/2015/02/CUMULATIVE_RECORD.pdf) on page 115 titled "A Case History of Scientific Method" which appeared originally in around 1955.
The details mentioned by Skinner in that paper are not sufficient, but his larger work "[Schedules of Reinforcement](https://www.bfskinner.org/wp-content/uploads/2015/05/Schedules_of_Reinforcement_PDF.pdf)" with Ferster provides sufficient conditions for explaning the behavior of pigeons under controlled laboratory conditions.
Similar methods are at least required to explain human behavior (however inconvenient they might be).

I do not have time to write on all these things now, and have left this note here in the off chance that I may return to it later.

But, here is what I have to say of what I have already read of Poe's paper.
The meat of the matter is found in the eigth paragraph.
The earlier paragraphs simply set the reader next to Poe as if all that was read had been just penned before you.
It is a delightful trick, and much of the paper itself follows the same methods as those who purport to be only comedians and yet have surprisingly strong political effects.

The eigth paragraph begins after all the essential features of a behavior analysis would otherwise have ended:

> Let us dismiss, as irrelevant to the poem, per se, the circumstance—or say the necessity—which, in the first place, gave rise to the intention of composing a poem that should suit at once the popular and the critical taste.

Such a dismissal is perhaps the most profound deception in the entirety of Poe's paper.
The 'per se' pricks the vail of deception and provides Poe with permission, from the reader who continues to read, to be as sly as is his want.
This is a perfect example of what I was talking about when I said in my paper on Paul Graham's "Good Writing" that

> the initial ambiguity and the consequent incompatible methods could have been charitably explained away as nonrhetorical literary language e.g. as a linguistic ignis fatuus.

Where Graham failed, Poe has instructively succeeded.

I very much appreciate R.P. bringing this beautiful example of good writing to my attention.

## 2025 0526

### 2025 0526 2141 The First Two Words of Paul Graham's Paper "Good Writing" 
Here I sincerely analyze Paul Graham's latest paper on ["Good Writing"](https://www.paulgraham.com/goodwriting.html) (I transcribe the snarky analysis I made on twitter at the end for completeness).

The first sentence

> There are two senses in which writing can be good: it can sound good, and the ideas can be right.

begins with the ambiguous phrase "There are".
Vernacular expands it to either "there are at least" or "there are exactly" (there are exactly three ways "there are" can be taken and the third is as "there are at most" as in "there are at most three Stooges" but this is not carried by the vernacular).
Logicians, having noted this ambiguity, regiment "there are" as "there are at least" e.g. "There are at least two senses in which writing can be good".
This regimentation is not merely a convenience.
It is the difference between existence and uniqueness e.g. "there is exactly one even prime number" and "there is at least one even prime number".
Lawyers also noted this ambiguity and regiment "there are" as "there are exactly" under the rule "Expressio unius est exclusio alterius".
Between logicians and lawyers the language of the home leaves the first sentence of Paul Graham's essay woefully underdetermined.

The strong reader rallies with contextual clues.
But, the first clue is very far from the proximate context.
In the eleventh paragraph the lawyers finally win: "So yes, the two senses of good writing are connected".
But then, before the sentence can end, everyone looses.
The full sentence is "So yes, the two senses of good writing are connected in at least two ways." and it spits on the languages of lawyers, logicians, and locals.
For, "the two senses" implies the lawyers rule but "in at least two ways" breaks it since two and only two connections are given.

Had Graham's "Good Writing" been on something other than good writing, then the initial ambiguity and the consequent incompatible methods could have been charitably explained away as nonrhetorical literary language e.g. as a linguistic ignis fatuus.
But, charity extends to the paper as an example of good writing in exactly those ways explained: "it can sound good, and the ideas can be right."
In other words, the ambiguous and incompatible methods are covert rather than accidental.
The deferred cosequences of covert methods are rarely helpful i.e. the ninth item of Russell's decalogue, "Be scrupulously truthful, even if the truth is inconvenient, for it is more inconvenient when you try to conceal it." 

The etymology of 'sense' likely goes to the PIE root \*sent- for "to go" and is the "source also of Old High German sinnan "to go, travel, strive after, have in mind, perceive," German Sinn "sense, mind," Old English sið "way, journey," Old Irish set, Welsh hynt 'way'" (<https://www.etymonline.com/word/sense>).
Replacing 'sense' with 'way' as in "There are two ways in which writing can be good" uncovers the metaphorical role of 'sense' in any sentence in which it occurs.
But, a reason to avoid "way" in Graham's sentence is because "two ways" is near "diverging paths" and the point is made later in the essay that these ways are not independent.
Alternatively, different ways can just as well diverge and yet later intersect and in general, the nearer a pair of people are to a destination the fewer ways there are to finish their trip i.e. the more likely they are to take the same way.

This negative outlook can be countered with the German's "sinn" which is most famous as a single word from Frege's "Über Sinn und Bedeutung".
It is clear from the remainder of this essay that Graham is committed to a mentalistic outlook, one that follows the German 'sinn' through 'sense' to 'mind'.
Thus, the occurrence of 'sense' primes talk of mental things.
Ideas, as in the vernacular (or that special part of the vernacular called folk psychology), are objects of the mind (charitably, the mind is what the brain does), expressed by words.
Then, writing is the a process by which ideas are expressed through words.
This is the same story as in hoodoo rootwork where a spell is written on a slate and washed into a glass of water which somehow expresses the cure and is absorbed by the drinker.

More charitably, the mind is what the body does, and the body as much has ideas as it has behaviors.
In fact, the intermediate talk of ideas which are later expressed by behaviors then dissolves and writing as a repertoire is said to be good or bad in as far as it is reinforced by prevailing cultural practices.
Then, to identify the key features of good writing is to identify the reinforcing practices which build and maintain one repertoire of writing rather than some other.
This is not the same as concluding with a kind of cultural relativism nor with a kind of cultural darwinism ("survival of the fittest" is so often interpreted as "'fit' is equivalent to 'survive'" that any such short story on the process of selection by consequences from variation on cultural scales is bankrupt).

Good verbal behavior presumably contributes to the survival of the culture within which it occurs.
It presumably takes into account the deferred consequences of its appearence in a culture sensitive to contributions of the deferred consequences of its practices to its survival.
It no more helps to supplant these descriptions by "sounds good and gets ideas right" than it does to supplant the principles of aerospace engineering with Kelly Johnson's "If it looks good, it will fly well" (which I was unable to find a record of from a search with DuckDuckGo or Google).

The result of my snarky analysis is copied here from Twitter (one of the worse places to write on the internet):

> The whole paper is paraphrased as follows:
>
> Good writing is sound, salient, and, essentially, slick. Why essentially? Lo, Paul Graham knows it is, and thou shalt not show Him otherwise. Soundness and salience are essential too. Why? Hark! Thou shalt know it as He does for thou art made as Him. He shakes the heavens and the earth with his grace and all becomes beautiful in its perfection: "In the beginning was the Word, and the Word was with God, and the Word was God."


### 2025 0526 1619 Books on my Desk and Reading Shelf
Books on my reading shelf:

* 1935 "The Story of Civilization: Volume I. Our Oriental Heritage" by Will and Ariel Durant
* 2017 "Understanding Behaviorism: Behavior, Culture, and Evolution 3rd Edition" by William M. Baum
* 1985 "Behaviorism: A conceptual Reconstruction" by G. E. Zuriff
* 1999 "Cumulative Record Definitive Edition" by B. F. Skinner
* 1973 "I.Q. in the Meritocracy" by R. J. Hernnstein
* 1996 "The Bell Curve: Intelligence and Class Structure in Americal Life" by Hernnstein and Murray
* 2020 "Human Diversity: They Biology of Gender, Race, and Class" by Murray
* 1993 "Cognitive Foundations of Natural History: Towards an Anthropology of Science" by Scott Atran
* 2022 "The Structure of Scientific Revolutions: 50th Anniversary Edition" by Kuhn and Hacking
* 2016 "Classical Philosophy: A history of philosophy without any gaps, Volume 1" by Peter Adamson
* 1984 "The Presocratic Philosophers: A Critical History with a Selection of Texts 2nd Edition" by G. S. Kirk, J. E. Raven, M. Schofield
* 1755 "Discourse on Inequality" by Jean-Jacques Rousseau
* 2011 "Introduction to Reliable and Secure Distributed Programming 2nd ed." by Christian Cachin, Rachid Guerraoui, Luís Rodrigues
* 1985 "Communicating Sequential Processes" by C. A. R. Hoare
* 2005 "Finite Model Theory 2nd Edition" by Ebbinghaus and Flum
* 2019 "Geometric Algebra for Electrical Engineers: Multivector electromagnetism" by Joot
* 2011 "Linear and Geometric Algebra" by Alan Macdonald 
* 2012 "Vectora and Geometric Calculus" by Alan Macdonald
* 2024 "Projective and Geometric Algebra Illuminated" by Eric Lengyel
* 1943 "The Glass Bead Game" by Hermann Hesse
* 1920 "The Mysterious Affair at Styles" by Agatha Christie
* 2013 "Hercule Poirot: The Complete Short Stories: A Hercule Poirot Mystery: The Official Authorized Edition" by Agatha Christie
* 2018 "Hey Mom: Stories for My Mother, But You Can Read Them Too" by Louie Anderson 

Books on my desk:
* 1950 "The Story of Art" by EH Gombrich and Leonie Gombrich
* 1952 "Anatomy Lessons From the Great Masters" by Robert Beverly Hale and Terence Coyle
* 1979 "Tage Frid Teaches Woodworking: Three Step-by-Step Guidebooks to Essential Woodworking Techniques" by Tage Frid
* 1967 "From Frege to Gödel: A Source Book in Mathematical Logic, 1879-1931" edited by Jean van Heijenoort
* 1997 "The Frege Reader" edited by Michael Beaney
* 1960 "Word and Object" by Quine
* 1995 "Selected Logic Papers" by Quine
* 1971 "Set Theory and its Logic Revised Edition" by Quine
* 1994 "Mathematical Logic 2nd ed." by Ebbinghaus, Flum, Thomas
* 2023 "The Earth and Its Peoples 8th ed." by Richard Bulliet, Pamela Crossley, Daniel Headrick, Steven Hirsch, and Lyman Johnson
* 1973 "Essentials of Earth History: An Introduction to Historical Geology 3rd ed" by William Lee Stokes
* 2017 "The Little Book of Big History" by Ian Crofton and Jeremy Black
* 1973 "A History of American Law 1st ed." by Friedman

> the 4th edition was published in 2019 (maybe get that)

* 2009 "History of the Ojibway People" by William W. Warren
* 2014 "History of the World 6th ed." by J. M. Roberts and Odd Arne Westad
* 1992 "History of the World 3rd ed." by J. M. Roberts (this one has pictures)
* 1993 "The Harper Encyclopedia of Military History" by R. Ernest Dupuy and Trevor N. Dupuy
* 2013 "The Encyclopedia of Warfare" by Dennis E. Showalter
* 2011 "The Rise and Fall of the Third Reich 50th anniversary edition" by William L. Shirer

> get Richard J. Evan’s “Third Reich Trilogy”:
> * “The Coming of the Third Reich”  2003
> * “The Third Reich in Power” 2005
> * “The Third Reich at War” 2008

* 2023 "The Economic Government of the World: 1933-2023" by Martin Daunton
* 1969 "The Rise of Anthropological Theory: A History of Theories of Culture" by Marvin Harris

> the updated edition was published in 2001

* 1977 "Cannibals and Kings" by Marvin Harris 
* 1968 "The Sacred and the Profane: The Nature of Religion" by Mircea Eliade
* 1902 "The Varieties of Religious Experience" by William James
* 1999 "JPS Hebrew-English Tanakh, 2nd Edition" by the Jewish Publication Society
* 2007 "The Tibetan Book of the Dead: First Complete Translation (Penguin Classics Deluxe Edition)" by Coleman, Padmasambhava, et al.
* 2002 "The Holy Quran with English Translation and Commentary" by Maulana Muhammad Ali
* 2007 "The Bhagavad Gita" translated by Eknath Easwaran
* 1611 "The Holy Bible" commissioned by King James I of England
* 1999 "Selected Writings (Penguin Classics)" by Thomas Aquinas and edited by Ralph McInerny
* 426 "The City of God" by Augustine of Hippo
* 397-400 "The Confessions" by Augustine of Hippo
* 1321 "The Divine Comedy" by Dante Alighieri
* 1990 "The Victorian Fairy Tale Book" edited by Michael Patrick Hearn
* 1998 "Legends and Tales of the American West" by Richard Erdoes.
* 1997 "Yiddish Folktales" by Beatrice Weinreich translated by Wolf
* 1982 "Norwegian Folk Tales" by Peter Christen Asbjørnsen and Jørgen Moe
* 1970 "Asimov's Guide to Shakespeare" by Isaac Asimov
* 1961 "Call for the Dead" by John le Carré
* 1999 "The Well-Trained Mind: A Guide to Classical Education at Home" by Susan Wise Bauer and Jessie Wise
* 2015 "The Reading Teacher's Book of Lists, 6th Edition" by Jacqueline E. Kress and Edward B. Fry
* 2011 "The New York Times Guide to Essential Knowledge" by The New York Times staff
* 1989 "Madrigal's Magic Key to Spanish" by Margarita Madrigal
* 1786 "The Diversions of Purley" by John Horne Tooke
* 1905 "The Elements of Psychology" by Edward L. Thorndike
* 1896 "An Outline of Psychology" by Edward Bradford Titchener
* 1957 "Verbal Behavior" by B.F. Skinner
* 1971 "Beyond Freedom and Dignity" by B.F. Skinner
* 1974 "About Behaviorism" by B.F. Skinner
* 1996 "Modern Political Thought: Readings from Machiavelli to Nietzsche" edited by David Wootton
* 1963 "History of Political Philosophy" by Leo Strauss and Joseph Cropsey
* 2006 "Introduction to Philosophy: Classical and Contemporary Readings, Fourth Edition" edited by John Perry, Michael Bratman, and John Martin Fischer
* 2019 "The History of Philosophy" by A. C. Grayling
* 1926 "The Story of Philosophy" by Will Durant
* 1946 "A History of Western Philosophy" by Bertrand Russell
* 1912 "The Problems of Philosophy" by Bertrand Russell
* 1898 "Pragmatism: a new name for some old ways of thinking" by William James
* 1516 "Utopia" by Thomas More
* 1626 "New Atlantus" by Sir Francis Bacon
* 1623 "The City of the Sun" by Tommaso Campanella
* 1996 "Structure and Interpretation of Computer Programs 2nd Edition" by Harold Abelson, Gerald Jay Sussman, and Julie Sussman
* 1995 "The Little Schemer, 4th Edition" by Daniel P. Friedman and Matthias Felleisen
* 1995 "The Seasoned Schemer 2nd edition" by Daniel P. Friedman and Matthias Felleisen
* 2018 "The Reasoned Schemer 2nd Edition" by Daniel P. Friedman, William E. Byrd, Oleg Kiselyov, and Jason Hemann.

## 2025 0523

### 2025 0523 1401 What would I tell my younger self?
I ask myself "What would I tell my younger self?" as a way of checking on how I am doing and how I might do better sooner rather than later.
For example, would I tell my younger self to work on the practices of logic as much as I have been over these past months?
How would I even explain logic to my younger self?
Also, what does my younger self really have to do with my present self or my future self?
These are just some questions which flow from me like an endless stream of unstoppable curiosity.
Do I have answers for most of my questions?
No.
Do I have answers for a few of them?
Only very few.

So, what would I tell my younger self about logic?
Would telling my younger self to do something different with respect to logic really change his behavior in any way?
Are counterfactual conditionals like these even helpful?
They are relative to a theory of the world and it is relative to such a theory that logic would have the greatest impact, even for my younger self.

Do we say that young people have theories of the world, ones that evolve over time as they are tested and as trials and errors pile up for or against this or that variation on some working theory of the world?
That's a long question.
Long questions rarely have short answers that are also self contained i.e. answers that encapsulate the question without posing it.

Rather than start with how logic would help my younger self I'll start with how logic has helped me, the older self to my younger self.
Logic helps me deal with the world and all the problems it presents to me.
Putting a problem into words is the first step and paraphrasing those words into a logical theory helps me factor some large theory of the world into tiny parts that I can see at a glance.
It focuses me on the predicates of a theory and the potential premises whose implied conclusions are so often fated to be overthrown by experiment.
None of this is what a young me would care about or notice.

When I help my nephews with their math homework I almost always do it over the phone so that we have to talk out a solution without simply sending each other pictures of things we've written down.
The first thing I tend to teach them is to read out the problem aloud.
To most people this may seem like a tiny thing: don't people know how to read aloud?
No, a lot of people don't.
They were barely taught to read selections from books aloud to each other, and are rarely asked to read math problems aloud.
How do you read most of the notation anyway?

That there are effective ways of reading mathematical notation aloud makes all the difference between noticing a parenthesis and skipping it (and messing up the rest of the problem).
What does this have to do with logic?
Logic takes our exposure to our native tongue and shapes it up into a verbal machine that introduces mechanical advantages to a language.
But, what does that mean to a youngling?

There are different ways you can talk about the world.
Some of them are helpful and some of them are hurtful.
If you want the way that you talk about the world to help you and not hurt you then speak logically.
Logic is made for dealing with what is true.
As much as dealing with what is true helps you to deal with the world, logic helps.

How do you speak logically?
First you have to notice that the way you talk about the world affects how you deal with it.
Lots of teachers struggle to teach this.
We know they struggle because so many kids say "Will this be on the test?" or "What does this have to do with the real world?".
If teachers were honest they would tell kids "It has nothing to do with the real world. This is a game that we made. We make you play it because we don't know what else to do. We don't know how else to help you deal with the world."
Instead, they threaten students with bad grades.
Bad grades are not what education avoids: they are just side effects of teachers not knowing what to do.
It is the bad consequences of dealing with the world poorly that actually threaten the young and old alike.

Education is not overtly presented as "the way to avoid dealing poorly with the world and the pains that follow from doing so".
It is presented as some sort of obligation or insurance or self contained arena of competition.
As if the educational environment is of independent interest relative to the rest of the world.
The practices of education have survived because they just barely make the educated person more effective than the uneducated person in almost everything that they do.
"Just barely" is all it takes: those who talk up 10x or 100x multipliers are credit/debit shills.
The snowball of fundamental productivity plows through the ups and downs of short sighted accelerationists (who now, more than ever, invoke the "long term" as nothing more than a verbal hedge or buzzword).

So what have I actually said of logic itself?
There is little to say, especially now that we have Quine's main method of proof for quantificational logic: a sentence is inconsistent if when put into prenex form a truth functional inconsistency is accumulated from instantiation so that existentials always receive new variables.
Logic links truth with grammar through validity either as that which can be derived from Quine's main method (as inconsistency of negation) or that which is true under all interpretations (where interpretation can be taken semantically or syntactically if at least the premises of placevalue notation are present).

From these technical conclusions there are vernacular conventions e.g. very few sentences are of any consequence what so ever since validity is no kind of honorific but rather an expedient of verbal linkage.
Where logic links up the essential sentences of a theory, it binds their fate in any practical experiment.
Deferred consequences of logical links come out in surprising ways that are fatal to a fledgling theory.
Those insensitive to logic are more easily spotted as its practices become more and more familiar.
Such logical deviants break up logical links either out of careless insensitivity or out of coercive intervention.


## 2025 0522

### 2025 0522 2202
Yesterday I finally got out about five sentences that open up the world of existential closures to my methods of logic.
I also carried through the subtle shift from "denotes" in the definition of validity to "denotes everywhere".

Much self reflection occurred yesterday and today, but it is not written here.
Most of it was silent.


## 2025 0521

### 2025 0521 1232

Today is the day that I make a big step with the continued work on my logic paper from [2025 0514 1345](#2025-0514-1345).

Sometime last week I compressed eight cards of outline on my logic paper into three cards.
Those new notes do not include the serious edit of adding "denotes everywhere" wherever I had written "denotes" and carrying through the rest of the paper with the corresponding predicate "denotes somewhere".
Here I shall write out the new notes with new edits and follow through the consequences of editing the definition of denotative functional validity as "each functional substitution denotes everywhere" rather than the old version "each functional substitution denotes".
I shall add comments on what is written or what is to be written (as a record of my thinking at the time of making the revisions) using the standard markdown notation for quotations (on most sites this is displayed as text in a box of a different color).

#### Science, Logic, Functional Compounding, and Chains
Logic is the science of validity and validity is a consequence of grammar and denotation.

Compounding is (denotative) functional when, exclusively, each like compound denotes or each like compound does not denote where and only where (waow), exclusively, each like component denotes or each like component does not denote.
Chains are compounds compounded functionally e.g. joint denials denote waow each of their components do not, negations are self joint denials (they denote waow their component does not), alternations are negations of joint denials (they denote waow some of their components do), and conjunctions are joint denials of negations (they denote waow each of their components do).

> The phrase "where and only where" is critical here: it links up the practices of logic with talk of the rest of the world.
> This turns on a geometric interpretation of spacetime which I do not mention except through the occurrence of 'where' and 'when'.
> A deferred consequence of this spacetime trick is that when I change "this" from a pronoun to a predicate it is easier to navigate how individuation is bound up with logic and the rest of the world.
> In general, I take 'it' as designating and 'this' as denoting chunks of spacetime.
> That does not imply that I take spacetime as prior to or primitive relative to what I write on logic.
> It is an outrider to my logical methods: it is on the periphery and only gently touches the outer rim of logical methods.

#### Subcompounds, Functional Substitutions, Validity, Consistency, Implication, Equivalence, and Laws
Subcompounds of compounds are their self or those of thier components.
Subsitutions of like compounds for like nonchain subcompounds are (denotative) functional.
Compounds are (functionally) valid waow each of their functional substitutions denotes everywhere, consistent waow their negation is nonvalid (some of their substitutions denotes somewhere), implied by others waow the conjunction of their self (the conclusion) with the negation of the other (the premise) is nonconsistent (each of their functional substitutions denotes where the same of the other does), and equivalent to others waow they are mutually implicative (each of their functional substitutions denotes waow the same of the other does).

> This is where the big change occurs.
> The addition of the single word "everywhere" to the definition of validity, which now reads "Compounds are (functionally) valid waow each of their functional substitutions denote everywhere", carries on into the definition of consistency as "Compounds are (functionally) consistent waow their negation is nonvalid" the fundamental consequence of which is that "Compounds are consistent waow some of their functional substitutions denote somewhere."
> This is key to making arguments about the validity or consistency of an existential (or univeral) closure of a chain as defined.
> I've stuck to the spatiotemporal "where"s and occasional "when"s so that there is a constant reminder that in explaining the practices of logic, there is nothing like the logic explained which is yet available.
> This is hard to get out in a way that doesn't churn up more confusion than the verbal constructions can bear.
>
> I had planned to avoid this early mention of "somewhere" and "everywhere" by getting away with the connectives "where" and "only where" as well as the conspicuous "each" and "some" which carry over firmly from this language to logic.
> The phrases "somewhere" and "everywhere" are sometimes adverbs and sometimes nouns in their native language of English.
> Since my aim is on logic and not English and its grammar, I've not done as Quine did in "word and object" and gone out of my way to link up the elementary school grammar of English with the technical grammar of Logic.
> But, here I am confronting the problem head on in this comment.
>
> The "somewhere"s and "everywhere"s as they occur in these definitions are picked to play a grammatical part in English which is between adverb and noun.
> It is from Logic and not from English that I got this usage: the phrase "everywhere" is an implicit universal quantification and the phrase "somewhere" is an implicit existential compound.
> The method of implicit quantification, in this form, is Dreben's: the construction of implicit existential compounds from their implicit universal components depends, with Dreben's method, upon the scope of purported quantifiers.
> Quine's method of functional normal forms makes the simplifying assumption that all problems of scope are established by assuming prefix normal form.
> In logic programming, the historical origin of both Quine and Dreben's method is emphasized by referring directly to either implicitly universally quantified variables or to Skolem constants, or to Skolem functions of implicitly universally quantified variables and Skolem constants, and so on.
> 
> In the definition of validity then "valid where and only where each of their functional substitutions denote everywhere" there are only universals, and carry over to predicate abstract notation as
>
> > each {xyz : x is valid if and only if y is a functional substitution of x and y denotes z}
>
> under Quine's interpretation, and as 
>
> > each {x: x is valid if and only if each {y : y is a functional substitution of x and each {z : y denotes z}}}
>
> under Dreben's (more or less).
> But, in rendering this definition as a sentence in predicate abstract logic it is important to note that the letter to the right of 'denotes' does not effectively carry the complex whereabouts which ultimately come through the later development.
> In an implicit quantifier notation of some logical programming languages, this definition carries over to the sentence
>
> > ?x is valid if and only if ?y is a functional substitution of ?x and ?y denotes ?z
>
> but none of these examples shows the notation for implicit existential compounds.
>
> The carry over of the fundamental consequence of the definition of consistency "consistent waow some functional substitutions denote somewhere" does.
> Quine's and Dreben's interpretations are both then
> > each {x : x is consistent waow y_(x) is a functional substitution of x and y_(x) denotes z_(y_(x))}
> which emphasizes with the underscore that Quine gives the notation with subscripts rather than with additional notation like underscores and parentheses.
>
> Since neither variables nor quantifiers are part of predicate logic (they are part of quantificational logic), none of this actually concerns us here.

Alternations of compounds with their negations are valid (they denote waow the compound does or its negation does, i.e. waow it does or does not, so, each functional substitution denotes everywhere).
Conjunctions of compounds with their negations are nonconsistent (they denote waow their compound does and its negation does, and, hence, waow it does and does not, so, each functional substitution of the conjunction does not denote everywhere i.e. the negation of the conjunction is valid).
Compounds are implied by and equivalent to their self.

> I'm uncertain if I should include these supporting arguments in something more than the parenthetical.
> That compounds are implied by and equivalent to their self is shown by the same step by step reduction to previous and accumulating results that have been built up thus far.
> A compound implies itself when the conjunction of it with its negation is nonconsistent and that is precisely what was just shown and the same conjunction occurs twice in the case of self equivalence.

#### Functional Substitutions Keep Validity, Nonconsistency, Implication and Equivalence
Functional substitutions in
* validities are validities (each functional substitution of the functional substitution of the validity is a functional substitution of the validity and hence denotes everywhere),
* nonconsistencies are nonconsistencies (each functional substitution of the negation of the functional substitution of the nonconsistency is a functional substitution of the negation of the nonconsistency i.e. is a functional substitution of a validity and hence the negation of the functional substitution of the nonconsistency is valid so that the function substitution of the nonconsistency is nonconsistent),
* implications are implications (the conjunction of the conclusion with the negation of the premise is nonconsistant and hence its functional substitution is nonconsistent and identical to the conjunction of the functional substitution of the conclusion with the negation of the functional substutituion of the premise), and
* equivalences are equivalences (functional substitutions of mutual implications are mutual implications).

#### Interchanges of Equivalents are Equivalent
Interchanges of equivalents in a compound are equivalent to that compound (each functional substitution of a compound matches the same of its interchange, except perhaps for the same of the equivalents which otherwise denote in tandem, so each denotes waow the other does i.e. they are equivalent).

#### Interchnage of Equivalents Keeps Validity, Nonconsistency, Implication, Equivalence, Nonvalidity, Consistency, Nonimplication, and Nonequivalence

Interchanges of equivalents in
* validities are validities (each functional substitution of the interchange denotes waow the same of the validity does),
* nonconsistencies are nonconsistent (their negation is valid and so the interchange in the negation is a validity),
* implications are implications (interchange into the nonconsistency is a nonconsistency),
* equivalents are equivalents (interchange of mutual implications are mutual implications),
* nonvalidities are nonvalidities (a compound is nonvalid waow some functional substitution does not denote somewhere, i.e. some functional substitution of its negation denotes somewhere; since the negation of the interchange is identical to the interchange of the negation then the interchange of the negation is equivalent to the negation, and hence the negation of the interchange denotes waow the negation does which denotes somewhere from some functional substituion i.e. the interchange is nonvalid)
* consistencies are consistencies (the negation of the interchange is identical to the interchange of the negation which is equivalent to the negation and hence nonvalid),
* nonimplications are nonimplications (nonimplication is consistency of the conjunction ...)
* nonequivalences are nonequivalences (one is a nonimplication ...).

> I still don't feel quite right about the argument for nonvalidities.
> I just fixed it by distinguishing between the step of equivalence and the step of the negation denoting somewhere from some functional substitution (so that the interchange of the negation, being equivalent to the negation, denotes somewhere from the same functional substitution).
> Why avoid notation though?
> Notation undermines the sensitivities conditioned by carefully designed sentences.
> There are very likely better sentences than the ones I've made thus far, but these are the ones I have and they work well enough.

#### Equivalents of Identity
Compounds are equivalent to
* their double negation (which denotes waow the negation of the compound does not, i.e. waow it does, so, each functional substituion of it denotes waow the same of its double negation does),
* their self alternation/conjunction (which denotes waow some/each of its components does i.e. waow the compound does), and 
* their alternation/conjunction with nonconsistencies/validities.

#### Equivalents of Distributivity of Conjunctions and Alternations
* Alternations of a component with an alternation are equivalent to the alternation of the alternations of the component with each of the others.
* Alternations of a component with a conjunction are equivalent to the conjunction of the alternations of the component with each of the others.
* Conjunctions of a component with an alternation are equivalent to the alternation of the conjunctions of the component with each of the others.
* Conjunctions of a component with a conjunction are equivalent to the conjunction of the conjunctions of the component with each of teh others.

#### Equivalents of Development: Alternational and Conjunctional
* Compounds are equivalent to their alternations with nonconsistencies (equivalents of identity), and, in particular, with conjunctions of other compounds with their negation (by the law of contradiction) which are themselves equivalent to the conjunction of their alternations with the other compound and its negation (by distributivity of alternation over conjunction) i.e. conjunctive development of the one compound with respect to the other.
* The dual for conjunction.

#### Equivalents of Associativity
* The conjunction of the first component with the conjunction of the second and third is equivalent to the conjunction of the conjunction of the first and second with the third.
* The alternation of the first component with the alternation of the second and third is equivalent to the alternation of the alternation of the first and second with the third.

#### Iterated Alternations and Conjunctions
The equivalents of associativity yield the many component alternations and conjunctions which are equivalent to iterated nestings of alternations or conjunctions down their left or right components.

#### Equivalents of Commutativity
* The alternation of the left component with the right component is equivalent to the alternation of the right component with the left.
* The conjunction of the left component with the right component is equivalent to the conjunction of the right component with the left.

#### Equivalents of Distributivity of Negations
* Negations of alternations are equivalent to the conjunctions of the negations of their components.
* Negations of conjunctions are equivalent to the alternations of the negations fo their components.



#### Laws of Validity
Validity is
* inconsistency of negation (which is not consistency of negation, and, hence, not nonvalidity of negation of negation i.e. validity of negation of negation which, by equivalents of identity, is validity)
* nonimplication of negation,
* negational nonimplication
* nonequivalence of negation
* negational nonequivalence, and

#### Laws of Consistency
* Nonconsistency is validity of negation.
* Nonconsistency implies nonvalidity.

#### Laws of Implication
* One chain implies an other and the other a third only where the one implies the third.
* Chains imply their self.
* Chains imply validities.
* Validities do not imply nonvalidities.
* Validities do not imply nonconsistencies.
* Validities only imply validities (each functional substitution of the former that denotes everywhere is one where the latter denotes everywhere)
* Nonconsistencies imply chains.
* Consistencies do not imply nonconsistencies.
* Nonconsistencies only imply nonconsistencies.

#### Laws of Equivalence
* One chain is equivalent to an other and the other a third only where the one is equivalent to the third.
* Chains are equivalent to their self.
* One chain is equivalent to an other waow the other is equivalent to the one.
* Validities are equivalent to and only equivalent to validities.
* Nonconsistencies are equivalent to and only equivalent to nonconsistencies.

#### Conditionals, Biconditionals, Exclusive Alternations, and Sequents
* Conditionals are alternations of the negation of their (antecedent) left component with their (consequent) right component.
* Biconditionals are conjunctions of the distinct conditionals of their components.
* Sequents are conditionals whose antecedent is the conjunction of their antecdent components and whose consequent is the alternation of their consequent components.
* Exclusive alternations are negations of the biconditionals of their components.
* Implication is validity of conditionals.
* Equivalence is validity of biconditionals.
* Nonequivalence is validity of exclusive alternations.

#### Relays, Literals, and Clauses
* Relays are their component or its negation.
* Literals are relays of nonchain compounds.
* Each component of a clausal chain is a literal.

#### Disjoint, Full, and Empty Chains
* No nonchain subcompounds of different components of *disjoint* chains match.
* The nonchain subcomponents of each component of *full* chains are the same.
* Empty chains have no components: often they are replaced by a relevant validity or inconsistency so as to carry an empty method into a nonempty one.

#### Laws of Equivalent Grammatical Categories
Each clause is equivalent to a disjoint or empty clause.
Each chain is equivalent to
* one with only joint denials
* one with only negations and conjunctions
* one with only negations and alternations
* one with only negations and conditionals
* one without conditionals and biconditionals
* one where only nonchain components are negated
* a conjunction of negations of clausal conjunctions
* the alternational dual of the above
* an alternation of disjoint clausal conjunctions (alternational normal form)
* a conjunction fo disjoint clausal alternations (conjunctive normal form)
* a full alternation of unique disjoint clausal conjunctions
* a full conjunction of unique disjoint clausal alternations

#### Existential Closures and Closed Chains
(Existential) closures denote waow there is somewhere denoted by their component.
Closed chains are closures of (open) chains.

#### Boolean Chains and Triviality
Boolean chains are closed or are chains of Boolean chains.
Without whereabouts closures denote nowhere since there is nowhere denoted by their component i.e. they are nonconsistent.
Only nontrivial results are contemplated here.
Check the trivial case by interchanging each closure with a nonconsistency.

The alternation of the closure of a compound with the closure of the negation of that compound is (nontrivially) valid and trivially nonconsistent.

#### Closure Conditionals
Consequences of closure conditionals are closed chains and their antecedents are conjunctions of them.
Consequences or antecedents of degenerate closure conditionals can be empty.


## 2025 0518

### 2025 0518 1857
This is a little experiment where I try to get out the distinction between the two specializations of 'denotes' which are 'denotes somewhere' and 'denotes everywhere'.
Usually when I write on something like this, the context is already available to me in that I write after having looked exactly where the problem I am solving actually occurred.
In this case, I have not looked at my notes on my paper on logic.
Thus, I am without context other than that which I bring to this keyboard unsupplemented by recent exposure to the problem.

The way the paper is designed now, it introduces denotative functional methods of compounding components in the same way that truth functional methods of compounding components is introduced along classical lines except it avoides the most familiar appeal to truth values.
The definition of truth functional compounding is usually given in the following way: a method of compounding components is truth functional if the truth value of the compound is a function of the truth values of the components.
So, in a sentence like "It is red and it is round." the compound is the whole sentence and it is compounded, from a logical grammar of which 'and' is an atom, of the components 'it is red' and 'it is round' and the truth value of 'it is red and it is round' is a function of the truth values of 'it is red' and 'it is round'.
Assigning truth values to the components is one way of interpreting them, and calculating the truth value of the compound with reference to a truth table is one way of specifying the truth function which carries the truth values of the components to the truth value of the compound.

Whether 'it is red' is assigned the value of truth or falsity is often confronted as a problem of interpretation, though when the sentence is given directly and not through the indirect methods of schematisms, e.g. 'x is F' or 'it is F' or just 'Fx', then the problem of interpretation is seen in its more familiar form e.g. selecting a specific item which 'x' or 'it' designates and specifying a particular predicate which is substituted for 'F', in this case substituting 'red' for 'F' in 'it is F' gives 'it is red' and taking 'it' as designating a red thing results in the assignment of truth to 'it is red'.
If it so happens that under this same interpretation 'it' designates a round thing then the compound 'it is red and it is round', which is an instance of the schemata 'it is F and it is G' or 'Fx and Gx' where 'F' is substituted by 'red' and 'G' is substituted by 'round', then since both the components 'it is red' and 'it is round' are assigned truth and since the connective 'and' marks the compound as receiving the value of truth where and only where each of its components are assigned the value truth, then the compound 'it is red and it is round' is interpreted as true.

Under these methods, the truth functional methods that go out of their way to introduce abstract objects called truth values and functions there of, each assignement of truth values to the truth functional components of a compound fixes the truth value of the compound.
Now, having gone out of my way to describe this method, I am left saying that this is not the path taken here.
Nor, and this is the crux of the matter, is it the strongest path that can be taken through the methods of logic.
In fact, taking this path has come to weaken logical and mathematical practices in ways which are largely unseen but which are of great consequence: they've all but broken the most powerful links between language and logic.
This is unexpected for many reasons, but the most relevant reason is this: those who are most familiar with the links between language and logic are precisely the ones who are most likely to break up the links between them and least likely to point up the most powerful links.

How this has come to pass is a pressing problem and one whose solution is well within my sights, but it reaches beyond the scope of language and logic and is best put aside as I have just as well put aside how it has come that different philosophers go in and out of style.

So what is the alternative that I submit is so much more powerful and important?
It is well known but is not carefully dealt with in the same systematic way that any mathematical logician would quickly demand of an introduction to model theory or proof theory (or, beyond an introduction, to the concerns of mathematical logic as a whole).
It is the method explicated by Quine in his "Philosophy of Logic" which can be briefly and artfully stated as "Logic chases truth up the tree of grammar." and "Logic is the sum of two components: grammar and truth." 

The predicates of grammar are coupled with the predicates of truth (that I must always explain myself when I say "predicates of truth" as if it was not shown almost a century, now, ago that there is no consistenty theory of a predicate of truth which satisfies the minimal requirement of disquotation, or that I have to go through the way in which disquotation is an instance of the maxim of minimal mutilation, should be alarming enough to those who know better).
When presenting logic, i.e. when teaching it to a fresh learner, there is nothing but the verbal communities with which that learner has been in contact from which to teach them.
Logical behavior is taught to those without any logical behavior: this is far more controversial than it looks.

That logical behavior can be taught to an organism without any logical behavior often comes as a surprise to most people, especially logicians and mathematicians.
This is largely because of the historical principles which guide both philosophers and logicians away from anything like the experimental analysis of behavior.
Logic, and its methods, are to transcend our material world through some metaphysical mist, or so it is said.
That we, as mere mortals, are in touch with some otherworldly essence of logic is merely an aftereffect or perhaps even an uninteresting collateral effect of some more fundamental metaphysical principles.

This is something which must be explained, but which would take me too far from this path.
To be clear, those who hold to metaphysical practices must be accounted for in an effective theory of the world, though, like most effective theories, it may not account for them in the way they might hope.

I shall also not go into the methods by which a pigeon can be conditioned to behave logically without, in the case of a pigeon, clearly having any logical behavior like that so often attributed to even the most isolated human (that a pigeon can be made to "do logic" is often astounding to people who are mostly unfamiliar with what little there ultimately is to logic in itself since they are unable to seperate logic from its application, which is another one of the key problems caused by traditional methods).

The grammatical part of logic can be introduced with some basic predicates, though they are not introduced as the application of logic so they can not be relied upon to bring appropriate behavior of the learner under control without being a tiny variation on some nonlogical behavior.
The nonlogical behavior often relied upon when introducing logic is that of the vernacular.
An example of a minimally viable repertoire is that what is written here can be read: much more work must be done to make this as precise as it is, e.g., in a text like Skinner's "verbal behavior" (for all that many have said to extinguish its control over scientific practice).

Sentences are taken, tentatively, as nothing more than quotations i.e. as concatenations of letters as in the familiar strings of computer science.
This is displeasing for many people who hold the form of a given response as sacred and who are unaware of the contributions of the consequences and the occasions upon which a response of a given form is emitted.
I can not go out of my way to explain that here now.
As far as the following description goes, I make no commitment to quotations and you can pick your favorite stand in items.

A single grammatical predicate does the work of the truth functional part of a language: x jointly denies y with z.
Joint denial is not the only such predicate e.g. 'x alternatively denies y with z' is another such.
I prefer joint denial because 'nor' is a sentence connective in my native tongue i.e. those who speak in a language with 'nor' are already that much closer to behaving logically with respect to it.

With this grammatical predicate a predicate of truth can be combined and a tiny example of Quine's outlook can be demonstrated in a familiar way.
In general, it is supposed that the predicate 'is true' is an abbreviation for the highest of the truth predicates among those in the inductive hierarchy of Tarski's analysis which belongs to the lexicon of our present language.

The following is what I call the synthetic categorical of joint denial (this is not quite correct, as the categoricals are to be distinguished from the universal closures of conditionals, but I have assumed that logic is being taught to a learner without a history of exposure to a community of logicians and hence if they can read this sentence at all then it will bring them under the control characteristic of categoricals):

> each item is (x, y, and z such that x jointly denies y with z, y is not true, and z is not true, only if x is true)

The analytic categorical is 

> each item is (x, y, and z such that x jointly denies y with z and x is true, only if y is not true and z is not true)

The bicategorical is

> each item is (x, y, and z such that x jointly denies y with z, y is not true, and z is not true, if and only if x is true)

The synthetic categorical chases truth up the tree of grammar as Quine said, and the analytic categorical chases truth down the tree, and the bicategorical chases it both ways.

Looking back on the traditional way of introducing joint denial, it is seen that no method of truth values or truth functions or models or proofs have been used to explain joint denial.
Using the method of predicate abstracts, which I've mentioned many times before and which can be quickly found with a simple text search among previous entries, it can easily be explained how it comes that Frege began talk of "truth functions".

The predicate (abstract)

> (x, y, and z such that x jointly denies y with z, y is not true, and z is not true, only if x is true)

is *almost* functional in its first place.
Note, schematically

> 'Functional F' is short for "each item is (w,x,y, and z such that Fxyz and Fwyz, only if x is indiscernible to w)" where indiscernibility is given schematically (I'm not sure if I've explained that elsewhere or not, but I'm pretty sure I have).

The predicate 'x jointly denies y with z' is where functionality goes faulty since there may be many different ways that a compound may be formed from its components into a joint denial of its components e.g. "neither Tom loves Dick nor Dick loves Tom" or perhaps "Tom loves Dick nor Dick loves Tom" or perhaps "not Tom loves Dick or Dick loves Tom".

This is where the all important methods of paraphrasing are revealed as of foundational importance.
Logical practices restrict the predicate '(x,y, and z such that x jointly denies y with z)' so that it is functional and then the predicate

> (x, y, and z such that x jointly denies y with z, y is not true, and z is not true, only if x is true)

is functional.
Note, without the early constraint that the learner of logic have some language (as the reinforcing practices of a verbal community) in which they speak, there is little that can be done from simply presenting the synthetic and analytic conditionals.

Also, note, that paraphrasing is linked with indiscernibility by the logicians insistence of paraphrasing so that "Functional (xyz : x jointly denies y with z)" is true.

Sadly I must end here without going from 'is true' to 'is true of'.

### 2025 0518 1403 Scott Atran's Cognitive foundations of natural history
Another great recommendation from R.P. arrived yesterday: Scott Atran's 1990 "Cognitive foundations of natural history: towards an anthropology of science".

The introduction unveils the inspiration for the rest of the text: the theories of Chomsky and Piaget are nonconsistent.
A nonconsistency is constructed of science from Chomsky's premise that 'each fundamental type of human-knowledge arises from a specialized cognitive aptitude" (pg. x Artran 'cognitive foundations') and Piaget's premise that "the innate and universal foundations of human thinking reduce to an undifferentiated intelligence, which is responsible in the same way for all cognitive operations" (pg. ix) from which talk of an intelligent scientist goes sour.

It is already clear from teh introduction that those items of Chomsky's theory which Atran keeps upon rejecting the contrasting principle from Paiget's theory are those which root themselves in teh behaviorist tradition i.e. the contingencies of the world, and Earth in particular, are such that global uniformities are selected without any centralized coordination so that the practices of science are as much a consequence of the evolved tendancy of organisms to bring the world (of which other members of their species are a part) under their control.
This permits a diversity among practices of presciences, and admits of evolution therein, while explaining those features across cultures that prompt those like Piaget to speak of a unifying intellect.

It is the contingencies of the world which select uniformities (for reasons that may be initially capricious) and which maintain them: not some ephemeral intellect.
The problem for Chomsky, and hence for Atran in support of him, shall be looking beyond the individual to the contingencies of natural, operant, and social selection and variation that explain how bits of behaving biology build and maintaining scientific communities.

The first chapter points to the mistake which is easily corrected by the radical behaviorist: it is not some essence of common sense that haunts the brains of the masses, but rather the vernacular as the language of the household where language is nothing more, nor less, than teh reinforcing practices of a verbal community.
When a person is said to be "using common sense" we can look at their speach and trace its origins through the listeners that selected it from its past variations.
Correspondance between common sense as speaking in the vernacular is easily established by examining the world upon which we speak.
It is one that is filled with other members of our species and which has been sufficiently stable so as to not only permit the survival of the species but also the survival of those verbal practices which compose cohesive cultures.

The practices between cultures do not always carry over perfectly because the world is not so uniform, e.g., as to select exactly the same form of response across great distances.
Geography is not sufficiently uniform so as to strengthen vernacular which is diverse in some places, e.g. as when it was established in 2010 in [Chapter 16 entitled "Franz Boas and Inuktitut Terminology for Ice and Snow: From the Emergence of the Field to the 'Great Eskimo Vocabulary Hoax' by Igor Krupnik and Ludger Müller-Wille](https://gwern.net/doc/psychology/linguistics/2010-krupnik.pdf) from the 2010 book ["SIKU: Knowing Our Ice Documenting Inuit Sea Ice Knowledge and Use" ](https://link.springer.com/book/10.1007/978-90-481-8587-0) that there are a larger number of root words for their world of ice and snow than in other languages of the world.

Vocal musculature is a critical uniformity among organisms of a given species that permits verbal communities to acquire new members whether they are old or young.

As I mentioned in my last entry, there are only tiny chunks of writing that I can do right now and though that frustrates me I am at least glad to have gotten this bit out.


### 2025 0518 1344 Copleston's History of Philosophy
Last night I watched more of "The Great Philospohers" from 1987 which I mentioned in [Bryan Magee’s “Men of Ideas” and “The Great Philosophers”](#2025-0505-1709-bryan-magees-men-of-ideas-and-the-great-philosophers).
Frederick Copleston spoke on Schopenhauer and Magee mentioned Copleston's series "A History of Philosophy" which is now eleven volumes but which was published originally as nine between 1946 and 1975.
Here is the link to the wikipedia page: <https://en.wikipedia.org/wiki/A_History_of_Philosophy_(Copleston)>.

It will take some time for me to get through it all, as it takes me so very long to read even one book in the way that I do (like a loom weaving threads from dozens of books at a time).
In anticipation I shall do as I do and write out the volumes titles and dates of publication:

1. 1946 Greece and Rome
2. 1950 Augustine to Scotus
3. 1953 Ockham to Suarez
4. 1958 Descartes to Leibniz
5. 1959 Hobbes to Hume
6. 1960 Wolff to Kant
7. 1963 Fichte to Nietzsche
8. 1966 Bentham to Russell
9. 1975 Maine de Biran to Sartre
10. 1986 Russian Philosophy
11. 1956 Logical Positivism and Existentialism

They are sold as a series of eleven books though they were originally published as a series of nine.
The last two were added into the series in 2003 for reasosn that I do not yet know.


### 2025 0518 1335
Another scrapped entry from yesterday that I was unable to complete and which went unpublished.
It continues work on my logic paper, but sadly rests as a mere record of my past about which I know so very little even this single day later.
Recently, I have struggled to find the large chunks of time which tend to release the greatest potential from any writing that I might do.
As much as there is a temporary moment of frustration it is so common that it almost goes without saying except in these peculiar circumstances where detailed records of responses are demanded.

>2025 0517 1625
> A lot of work on my paper on logic is occuring at once and I have not been able to complete it all.
> I am doing my best to make a record of all the responses which appear with respect to it.
> This continues the last entry on the paper [2025 0516 1352](#2025-0516-1352).
>
> Quantificational logic, as opposed to predicate (functor) logic, defines validity with respect to the truth of its sentences (relative to provisions for grammatical constructions with free variables e.g. treating them as dummy singular terms or as abbreviations of their universal closures).
> Predicate logic defines validity with respect to the denotation of its predicates: there are no variables and hence no provisions for free variables (and, happily, no complex rules of substitution for bound variables).
>
> Quantificational logic seems to predicate the quotation of a sentence of the predicate 'is true' as in "'it is raining or it is not raining' is true".
> By Tarski's analysis, this sentence is equivalent to "'it is raining' is true or 'it is not raining' is true" which is itself equivalent to, by the same analysis, "'it is raining' is true or 'it is raining' is not true" whihc, as an instance of a truth functionally valid schema, is true (we also know from Tarski's analysis that the predicate 'is true', which occurs repeatedly throughout the quoted equivalents, is not coextensive with the predicate occuring right before this parenthetical).
> Note, the use of 'predicate' and 'sentence' picks out nothing but what is spelled out by a quotation e.g. 'is true' is a predicate and 'it is raining' is a sentence.
>
> The sentence "it is raining or it is not raining" is itself an instance of a truth functionally valid schema and hence it is true without recourse to the intermediate steps of the argument by equivalents in the previous paragraph.
> On the other hand, the sentence "it is raining" is not an instance of a valid schema (either when 'it' is taken as a dummy singular term or when the whole is taken as an abbreviation for "each item is it such that it is raining") i.e. whether, exclusively, 'it is raining' is true or 'it is not raining' is true, or, directly, whether, exclusively, it is raining or it is not raining (exclusivity has nothing in particular to do with this example, whether 'or' is read exclusively or not in the previous paragraph).
> These two examples demonstrate the difference between a triviality and the potential urgency of an umbrella.
>
> Predicate logic, at first, seems to predicate the quotation of a predicate and some chunk of the world of the two place predicate 'is true of', aka 'denotes', as in "'rain or not rain' denotes this" which is equivalent to "'rain' dentoes this or 'not rain' denotes this".
> Equivalently, "'rain' denotes this or 'rain' does not denote this"
> Now, instead of being an instance of a truth-functionally valid schema, the compound predicate "'rain' denotes this or 'rain' does not denote this"


## 2025 0516

### 2025 0516 1352
This is an incomplete and, it would appear, incoherent note that continues work on my paper on logic from [2025 0514 1345](#2025-0514-1345).

> Logic is the leverarm of language, and those who speak in it can lift the weight of the world with their words.
> There are multiple ways of specifying valid verbal behavior, but only two are mentioned to most modern students: the model theoretic and the proof theoretic.
> Under appropriate assumptions, these methods are shown to be equivalent.
> In most cases, both methods are presented schematically so that upon establishing one or the other validity there is a secondary step to projecting one or the other definition of validity to the verbal behavior that occurs in logical practice.
>
> It is with great sadness that the model theoretic methods are called out as semantic and the proof theoretic methods are called out as syntactic.
> Neither the word 'semantic' nor the word 'syntactic' are worthy of a place in logic except from its application e.g. in formalisms, perhaps in those special formalisms called "axiomatic systems", of model theories (set theories within which the semantic methods are derived by interpretation) or of syntactic theories (concatenation theories within which the syntactic methods are derived by calculation).
>
> Logic is linguistic like grammar.
> It tells us something about the reinforcing practices of verbal communities.
> Principles aimed at verbal communities generally are said to be transcendent, and those aimed at a single community are said to be immanent.
>
> On a given occasion the response "Red" and the response "Round" are reinforced by a listener, or punished, 
> 
> To say of a denotative functional compound that it denotes somewhere or that it denotes everywhere is to do something like the implicit quantifier notation used in logic programming 
>
> * <https://www.youtube.com/watch?v=MsuaWozZowg>
>
> The definition of validity of a functional compound must be ammended to explicitly mention "denotes everywhere" where "denotes" was always short for in previous editions.
> The reason for this is that "consistency" as "some functional substitutions denote" must be altered to "some functional substitutions denote somewhere" so that problems of existential closure can be dealt with uniformly through what Quine calls "Boolean Term Schema".
>
> Compound Whereabouts and Quine's Functional Normal Form
> x_(y_(z)uv_(ws)) becomes ((z),u,(w,s))
> this is important to Hilbert and Bernay's completeness, especially as presented in Kleene's "Introduction to Metamathematics".
write on quine's explanation of hilbert and bernay's completeness in "selected papers on logic"

### 2025 0516 1348

This fragment of a note was written yesterday as I began to read the 1994 book "The Bell Curve" by Richard J. Herrnstein and Charles Murray.
I stopped writing on it because it no longer presented itself as scientific literature.
It became obvious that it is a part of nationlistic literature more so than anything scientific.
Thus, a deep reading has been delayed for when I address the collection of nationalistic writers e.g. Thomas Sowell, Milton Friedman, and Francis Fukuyama.

Though their writings contain copious references to what clearly purports to be scientific literature, there is nothing in the design of their documents which brings up science as the primary source of control over what they have to say.
Much of what is written is of this sort, and even much of what is written here by me is of this sort.

We, as humans, speak under the control of a wide variety of social environments, and, through them, have contact with a wide variety of contingencies of the world.
Not all of them are scientific.
Not all of them purport to be under scietific control.
Some do, and among those, there are some which do so to share, by association, in the strengths of scientific practice.
Whether they are the result of scientific practices, as in a theoretical analysis of the social consequences of a science of behavior, is important to me because of the control that scientific practices have over my own behavior as a reader and writer.

A copy of Richard Herrnstein's 1973 "I.Q. in the Meritocracy" is arriving soon and it purports to be, like "The Bell Curve", under the control of scientific practices.
Until it arrives I have stopped writing on "The Bell Curve" and have simply been reading it as time has allowed.

Here is the fragment:

> 2025 0515 1911 The Bell Curve
> For much of the past year I have avoided digging into Herrnstein's work because there is so much of it which is incompatible with the science of behavior as a science of behavior rather than a science of cognition (which is nothing more than a reincarnation of a science of mind but with that ever present seasoning of "computational" or "informational" sprinkled about).
> But, the genetic control of behavior continues to be poorly dealt with, as in Charles Murray's 2020 book "Human Diversity: The biology of gender, race, and class", and the less I read the less I'm likely to know.
> 
> The general problem is simple: there is no room in scientific practices for events happening in other dimensions other than those under investigation.
> People are no more haunted by more or less intelligent minds than houses are haunted by more or less malevolent ghosts.
To say so is to already 


## 2025 0514

### 2025 0514 2226
This continues my work on my little lisp from [2025 0512 1400](#2025-0512-1400).

Much as you do when you are solving a problem in physics, I shall write down the basic programs that have evolved thus far.
This note adds a layer to the onion of grammars growing out the primitive parenthetical language.

```
// pairs
let theEmptyPair={}
, isEmpty = it => it == theEmptyPair
, pairOf = (it, that) => [it, that]
, leftOf = it => isEmpty(it) ? it : it[0]
, rightOf = it => isEmpty(it) ? it : it[1];

// sequences as pairs
let theEmptySequence = theEmptyPair
, isEmptySequence = isEmpty
, singletonSequenceOf = it => pairOf(it, theEmptySequence)
, headOf = leftOf
, restOf = rightOf
, concatOf = (it, that) => isEmptySequence(it) ? that 
  : pairOf(headOf(it), concatOf(restOf(it), that))
, prependSingletonOf = (it, that) =>
   concatOf(singletonSequenceOf(it), that)
, appendSingletonOf = (it, that) =>
   concatOf(that,singletonSequenceOf(it));

// stacks as pairs
let theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, singletonStackOf = it => pairOf(theEmptyStack, it)
, pushOf = pairOf
, dropOf = leftOf
, topOf = rightOf
, secondOf = stack => topOf(dropOf(stack))
, drop2Of = stack => dropOf(dropOf(stack))

, prependOf = stack => pushOf(drop2Of(stack)
  , prependSingletonOf(topOf(stack), secondOf(stack)))
, appendOf = stack => pushOf(drop2Of(stack)
  , appendSingletonOf(topOf(stack), secondOf(stack)));
```
Eventually these basic functions will settle down and an instructive sequence of tiny steps will lead the novice through these methods of programming.
What follows are the new basic functions on concatenations as javascript strings.
A small change in initial defintions is given to better follow the theory of concatenations like those of Tarski.
```
// letters, strings, concatenations and runes
let isConcatenationOf = (x,y,z) => x == y.concat(z)
, isEmptyConcatenation = x => isConcatenationOf(x,x,x)
, theEmptyConcatenation = ''
, isIdenticalConcatenation = (x,y) => 
   isConcatenationOf(x,y,theEmptyConcatenation)
, theConcatenationOf = (x,y) => x.concat(y)

, stringOf = (...letters) => theEmptyConcatenation.concat(...letters)
, firstLetterOf = letters => isEmptyConcatenation(letters) ? theEmptyConcatenation : letters[0]
, restLettersOf = letters => isEmptyConcatenation(letters) ? theEmptyConcatenation : letters.slice(1)
```
Now, the alphabet is introduced as a special concatenation.
```
let theAlphabet = '() 0123456789abcdefghijklmnopqrstuvwxyz';
```
The following new definitions for making runes and letters skip over the intermediate steps of working with tallies.
Even though such methods otherwise would be instructive, the definitions given are basic methods of "finding where a thing is at" and are more than good enough.
```
let runeHelpOf = (it, abc) =>
 isEmptyConcatenation(abc) || isIdenticalConcatenation(it, firstLetterOf(abc)) ? theEmptyPair
 : pairOf(theEmptyPair, runeHelpOf(it, restLettersOf(abc)))
, runeOf = it => runeHelpOf(it, theAlphabet)

, letterHelpOf = (it, abc) => isEmptyConcatenation(abc) ? theAlphabet
 : isEmpty(it) ? firstLetterOf(abc)
 : letterHelpOf(rightOf(it), restLettersOf(abc))
, letterOf = it => letterHelpOf(it,theAlphabet)

, runesOf = letters => isEmptyConcatenation(letters) ? theEmptySequence
  : prependSingletonOf(runeOf(firstLetterOf(letters))
    , runesOf(restLettersOf(letters)))

, lettersOf = runes => isEmptySequence(runes) ? theEmptyConcatenation
  : stringOf(letterOf(headOf(runes)), lettersOf(restOf(runes)))
```
Then an example, which deviates from the prior example by including a 'not':
```
lettersOf(runesOf("(()())() this should not be ignored")) 
 (()())() this should not be ignored
```
Which is a good sign (sadly, not a proof).

The change in alphabet has led to a slight change in the placement and form of the prior definitions of "theOpenRune" and "theCloseRune": it is now with the definitions of reading and printing sequences.

```
// read and print sequences
let theOpenRune = theEmptyPair
, isOpenRune = isEmpty
, readOpenRuneOf = (stack, runes) => 
   readSequenceOf(pushOf(stack,theEmptySequence), restOf(runes))

, theCloseRune = pairOf(theEmptyPair, theOpenRune)
, isCloseRune = it => !isEmpty(it) && isEmpty(leftOf(it)) && isOpenRune(rightOf(it))
, readCloseRuneOf = (stack, runes) =>
   readSequenceOf(appendOf(stack), restOf(runes))
```
The default method of reading runes which were not open or close runes was to skip over them.
Now any nonparenthetical runes are added to the end of the sequence under construction.
This makes for minor alterations of the method of reading sequences.
```
let readDefaultRune = (stack, runes) =>
   readSequenceOf(appendOf(pushOf(stack,headOf(runes))), restOf(runes))

, readSequenceOf = (stack, runes) => 
  isEmptySequence(runes) ? headOf(topOf(stack))
  : isOpenRune(headOf(runes)) ? readOpenRuneOf(stack,runes)
  : isCloseRune(headOf(runes)) ? readCloseRuneOf(stack, runes)
  : readDefaultRune(stack, runes)
, readOf = runes => readSequenceOf(theEmptyStack, runes);
```
The rest is as it was.
```
let parenOf = runes => prependSingletonOf(theOpenRune
    , appendSingletonOf(theCloseRune, runes))

, printSequenceOf = sequence =>
  isEmptySequence(sequence) ? theEmptySequence
  : concatOf(printOf(headOf(sequence))
    , printSequenceOf(restOf(sequence)))

, printOf = sequence =>
  isEmptySequence(sequence) ? parenOf(theEmptySequence)
  : parenOf(printSequenceOf(sequence))

, read = letters => readOf(runesOf(letters))
, print = sequence => lettersOf(printOf(sequence));
```
The examples let it be shown that nothing has changed from when they were last run.
```
lettersOf(parenOf(runesOf('(()()())'))) 
 ((()()()))
print(read('()')) 
 ()
print(read('(()(())())')) 
 (()(())())
print(read('(())(()(())())')) 
 (())
print(read('(()(())())((()))')) 
 (()(())())
print(read('))))((')) 
 ()
```
None of these include a space rune so that has yet to be treated as any special case.
But, the next example with other than close or open runes paves the way for the next layer of the onion mentioned at the beginning of this note.
```
print(read('(this is a test)')) 
 ((()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()())(()()()()()()()()()()()()()()()()()()()())(()()()()()()()()()()()()()()()()()()()()())(()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()())(()())(()()()()()()()()()()()()()()()()()()()()())(()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()())(()())(()()()()()()()()()()()()())(()())(()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()())(()()()()()()()()()()()()()()()()())(()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()())(()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()))
```
The method of making runes is now revealed as a way of reinforcing the parenthetical reader and printer: nonparenthetical runes are read as sequences of of empty sequences.
This opens up the following more sophistocated methods without entailing further alterations to the parenthetical reader and printer.
```
letterOf(headOf(read('(this is a test)'))) 
 t
letterOf(headOf(restOf(read('(this is a test)')))) 
 h
lettersOf(read('(this is a test)')) 
 this is a test
```
This took far longer to finish than I had aimed at, but I'm happy to be this far.
Though it is easy to go back and see that I've been near a similar target in past iterations, they relied more on peculiarities of javascript than on the basic logic of a design of a lisp like language which does not depend on its implementation.

### 2025 0514 1420

Quine's main method corresponds, in a significant way that I have not fully grasped, to Aristotle's principle of noncontradiction: this also happens to be the crux of the methods of logical programming.
The demonstration of the relevant inconsistency constructs, by conjunctive description that comes out to a generalization of 'the primitive recursive value of a the functional predicates of a given theory', the item whose existence is presumably implied by noncontradiction. 

### 2025 0514 1345
This entry ended up contributing to the work on my paper on logic from [2025 0510 1238](#2025-0510-1238).

The past two days have been filled with private conversations and wayward contemplations that were sadly ungoverned by the work done here.
It happens, and yet, the more often it happens, the less often I stumble into a tiny success in any of my little endevours.

For example, I've failed to note my responses to watching more of the series I mentioned in [Bryan Magee’s “Men of Ideas” and “The Great Philosophers”](#2025-0505-1709-bryan-magees-men-of-ideas-and-the-great-philosophers).
I've also failed to note the arrival of Kir, Raven, and Schofield's "The Presocratic Philosophers", or the multiple rereadings of key parts of Quine's fourth edition of "Methods of Logic" (for all the times I've read the book from cover to cover there is still more than I can carry with me without referencing it).

I see each of these unnoted events as failures to hit where I have aimed.
Thankfully, a few bits of minor productivity do carry me onward if not wayward.

As mentioned in my last note on the evolution of my paper on logic, I can not wait to finish the sections covering denotative functional duality prior to starting on the new grammatical category of existential closures.
Yet, such duality is more a labor saving device (it multiplies the consequences of any result on functional compounding by two) than a milestone.
The milestone, under the grammatical definition of valdity as substitution on supplementation of lexicon (by some finite list of previously unintroduced predicates), is the equivalence of each funtional compound to one in the grammatical category of alternational normal forms and conjunctive normal forms.
It yields a decision procedure for validity: a n component chain is valid waow its full alternational normal form is compounded of n to the power of two clauses.
This retreives, without oblique references to items purported by the phrase "denotative values" or "truth values", the breadth and depth of methods of truth tables and, much more importantly, truth trees, though, to be clear, the law of validity just stated is the carry over of truth tables, and the grammatical import of truth tree methods has yet to be described conspicuously as such (by anyone).

The grammatical import of truth trees are a consequence of alternational and conjunctive development which are already presented in the parts of the paper on logic that I've produced thus far.
Such methods are generalizations of Gentzen's sequents along the lines of Quine's method of existential conditionals which is a decision procedure for Boolean statement schemata.
I introduce sequents as special conditionals long before reaching out from Quine's main method to those of natural deduction, which I expect to show are nothing more than the application of Quine's main method to sequents so intorduced (for me, a sequent is to be a denotative functional compound whose antecedents components are conjoined as the antecedent of a conditional whose consequent is the alternation of the consequent components of the sequent).
The parallels between implications and conditionals are the smooth passage from Quine's main method applied to sequents and the methods of natural deduction.


## 2025 0512

### 2025 0512 1400
This continues work on my little lisp from [2025 0509 1722](#2025-0509-1722).

It occurred to me in the shower that I can use the defined operations for manipulating concatenations of letters as javascript strings to give some interesting and important definitions of basic predicates and functions on concatenations.
It also occurred to me that this fits perfectly into both Quine's "Concatenation as a Basis for Arithmetic" as well as my outlook on bit strings and binary trees.

The first function which hit me was one that checks whether a letter is in a concatenation of letters.
This led me to a definition that either constructs directly the appropriate rune, constructed step by step, and then to Quine's definition of tallies in a theory of concatenation with at least two distinct atoms.

Since there can be, and are, many distinct atoms in the concatenations of letters as javascript strings, a basic predicate of identity of letters is unavoidable, or at least unadvisable at this moment.
The following sketch of code appeard.

```
let isIdenticalLetter = (it, that) => it == that
, isLetterOf = (it, that) => !isEmptyLetter(that) 
  && (isIdenticalLetter(it,firstLetterOf(that))
     || isLetterOf(it,restLettersOf(that)))
, theAlphabet = '() 0123456789abcdefghijklmnopqrstuvwxyz'
, isLetter = it => isLetterOf(it, theAlphabet)
, isIdenticalLetters = (it, that) =>
  (isEmptyLetter(it) && isEmptyLetter(that))
  || (isIdenticalLetter(firstLetterOf(it), firstLetterOf(that))
     && isIdenticalString(restLettersOf(it), restLettersOf(that)))
```

The alphabet is no longer a javascript array each item of which is a javascript string: it is now just a javascript string.

There are two next steps that appeared: 1) start with the basic predicate 'x concatenates y with z' in javascript and build the rest from there so that it is closer to traditional theories of concatenation (the newer theories focus on a family of functions that prepend or appened a single atom/letter to its argument, and while something like that is easier with modern programming langauges to set up, it does not leverage the power of pure predicate logic like the older techniques which have their origins through Tarski's theories of concatenation), and 2) define the extended versions 'letterOf' and 'lettersOf' and 'tallyOf' etc.

The predicate 'x concatenates y with z' is defined as
```
let isConcatenationOf = (x,y,z) => x == y.concat(z);
```
then, the empty string, or empty letter as I've been calling it, is the item denoted by the following predicate.
```
let isEmptyString = x => isConcatenationOf(x,x,x);
```
Somewhere while I was coding these things up it finally occurred to me that 'Of' is actually just a mark of a relative predicate, and not a good indicator of what is classically taken as a function value: that is done by 'the' as in singular descriptions, but these are not to be confused with functions and I am reluctant to conflate matters further.
But I shall, just to see how it goes.

```
let theConcatenationOf = (x,y) => x.concat(y)
, theEmptyConcatenation = '';
```

Social commitments ended this entry prior to any conclusions.


## 2025 0511

### 2025 0511 1844
The work that must be done on my paper on logic can not be done here.
It has done best when worked on with B. F. Skinner's Thinking Aid.
The thinking aid is nothing more nor less than a way of arranging index cards onto a sturdy flap of a folder with masking tape so that each part of a paper can be flipped to so as to cover up all others and leave labels for each section visible.
Items can be easily moved around or replaced.

Right now I am doing a grand compression of those elements that have settled down and not undergone many edits e.g. definitions and examples of denotative functional compounds.
Another change is in the method of arranging the cards.
In my first experiment with Skinner's thinking aid, I only put the cards on the outside of a folder whose contents were loose notes which primed and prompted the cards I made.
Now I am arranging the cards on the inside of the folder so that I can get two full pages when laid flat.

Upon getting eight cards down to three I couldn't stand writing anymore.

## 2025 0510

### 2025 0510 1238
This continues work on my paper on logic from [2025 0507 1421](#2025-0507-1421).

Upon reading my latest work on my logic paper I made the following notes:
1. relays, literals, clauses as well as disjoint, full, and empty chains can go right before or as some part of "Laws of Equivalent Grammatical Categories" as they are even more specific classifications of grammatical categories.
2. Functional completeness is also a special problem of equivalent grammatical categories: it deals with the problem of grammatical supplementation.
For example, the current presentation takes advantage of the purported equivalence between conjunction as a denotative functional method of compounding components and a compound of joint denials of the same components to introduce conjunction as a compound of joint denials.
Each denotative functional method of compounding components is introduced as some compound of joint denials.
This is not the same as showing that joint denials are functionally complete.
Functional completeness contemplates a grammar with a method of denotative compounding which is newly called 'conjunction' and the compounds of which must be shown to be functionally equivalent to some compounding of joint denials.

    The problem of functional completeness is the one of indefinite supplementation and subsequent equivalence of some one grammatical category (such as that of pure negations and conjunctions) with an other (such as that of joint denial).
A general method can be introduced which puports to demonstrate how, under any supplementation of the grammar of a language by a functional method of compounding, a corresponding compound in a canonical category (such as that of pure joint denials) can be constructed.
Wittgenstein mistook the indefiniteness for limitlessness when he failed to establish quantificational sentences as unending truth functional compounds in the Tractatus.

    There is an other way of approaching the problem of functional completeness: translation or paraphrasing.
This solves the problem in that it disolves it: the foreigner is now the responsible party in that the fruits of the grammar of the native is open to them only in so far as they pick out their method of functional compounding among those of the native.
The native is concerned by such problems of paraphrasing in the same way as the foreigner: if the native is to enjoy the advantages of some foreign grammar then the foreign must be superimposed upon the native.
This is the line that I've taken when, e.g., negations are given as self joint denials.

    That there are only finitely many n component methods of functional composition and that the defining characteristic of a functional composition is that its compounds exclusively denote or do not denote and that whether the compound does or does not denote is implied by whether each of its components exclusively do or do not gives the basis for any satisfactory method of paraphrasing from a foreign grammar or proving equivalence of a supplemented grammatical category to some initial and cononical category.

3. A few of the sentences at the beginning of some sections, e.g. laws of validity, consistency, implication, and equivalence, are now redundant since they repeat what was paraphrased immediately following the introduction of the relevant term.

4. The problem of individuation is likely to be case as the transition from existential closures 'some*'as "there is somewhere" to existential croppings 'some' as "there is something".
The ultimate resolution of 'some*' as "some items are" or perhaps 'some*' as a specific iteration of 'some's depends significantly upon the specific recombic functors that are introduced.
It occurred to me yesterday that my [Stack Notation for Predicate (Functor) Logic ](#a-stack-notation-for-predicate-functor-logic-2025-0414-1626) solves a great problem with smoothing the transition from those unfamiliar with predicate functor logic and yet familiar with quantificational logic e.g. 'xFy' a instance of which is 'x is a member of y' or, in short, 'x in y' is familiar for having a lowercase (variable) letter on the left and on the right, and it is this basic division between left and right which not only paves the way (following Russell's "Human Knowledge: scope and limits") for some method of individuation while also smoothly supporting the analysis of "somewhere" as a binary tree of "somethings" which extends the ordered tuple methods which are already familiar in the carrying over of n place predicates to n place relations and which shall come to be the carry over of tree shaped predicates to tree shaped relations: this is already a standard method in many programming langauges, e.g. Paul Graham's [bel](https://www.paulgraham.com/bel.html), and is called 'destructuring assignment'.

5. If I wait to get to existential closures until I am done with denotative functional duality, then I may never get on with the more pressing matter of presenting Quine's main method in its autonomous predicate functor form.

6. The fact that existential closures of denotative functional compounds imposes homogeneity upon the nonchain components of the functional compound may not be as obvious or inevitable as it seems: there may be other alien methods of grammatical analysis that do not fit this rubric.

---
#### Logic, Science, and Validity

Logic is the science of validity and validity is a consequence of grammar and denotation.

#### Functional Compounding and Chains

Compounding is (denotative) functional when, exclusively, each like compound denotes or each like compound does not denote, where and only where (waow), exclusively, each like component denotes or each like component does not denote.
Chains are compounds compounded functionally.

#### Example Chains: Joint Denials, Negations, Alternations, and Conjunctions

Joint denials denote waow each of their components do not.
Negations are self joint denials: they denote waow their component does not.
Alternations are negations of joint denials: they denote waow some of their components do.
Conjunctions are joint denials of negations: they denote waow each of their components do.

#### Subcompounds and Functional Substitutions

Subcompounds of compounds are their self or those of their components.
Substitutions of like compounds for like nonchain subcompounds are (denotative) functional.
Functional substitutions of functional substitutions of compounds are functional substitutions of their self.

#### Functional Validity, Consistency, Implication, and Equivalence

Compounds are (functionally)
* valid waow each of their functional substitutions denote,
* consistent waow their negation is nonvalid (i.e. soem of their functional substitutions denote),
* implied by others waow the conjunction of their self (the conclusion) with the negation of the other (the premise) is nonconsistent (i.e. each of their functional substitutions denotes where the same of the other does), and
* equivalent to others waow they are mutually implicative (i.e. each of their functional substitutions denotes waow the same of the other does).
[See pg. 36 of POL]

#### Example Validities and (Non)consistencies: Laws of Excluded Middle, Contradiction, Self Implication, and Self Equivalence

Alternations of compounds with their negations are valid (they denote waow the compound does or its negation does, i.e. waow it does or does not, so, each functional substitution denotes).
Conjunctions of compounds with their negations are nonconsistent (they denote waow their compound does and its negation does i.e. waow it does and does not, so, each functional substitution does not denote).
Compounds are implied by and equivalent to their self.

#### Functional Substitutions Keep Validity, Nonconsistency, Implication and Equivalence

Functional substitutions in
* validities are validities (each functional substitution of the functional substitution of the validity is a functional substitution of the validity and hence denotes),
* nonconsistencies are nonconsistencies (each function substitution of the negation of the functional substitution of the nonconsistency is a functional substitution of the negation of the nonconsistency i .e. is a functional substitution of a validity and hence the negation of the functional substitution of the nonconsistency is valid so that the function substitution of the nonconsistency is nonconsistent),
* implications are implications (the conjunction of the conclusion with the negation of the premise is nonconsistant and hence its functional substitution is nonconsistent and identical to the conjunction of the function substitution of the conclusion with the negation of the functional substutituion of the premise), and
* equivalences are equivalences (functional substitutions of mutual implications are mutual implications).

#### Interchanges of Equivalents are Equivalent
Interchanges of equivalents in a compound are equivalent to that compound (each functional substitution of a compound matches the same of its interchange, except perhaps for the same of the equivalents which otherwise denote in tandem, so each denotes waow the other does i.e. they are equivalent).

#### Interchnage of Equivalents Keeps Validity, Nonconsistency, Implication, Equivalence, Nonvalidity, Consistency, Nonimplication, and Nonequivalence

Interchanges of equivalents in
* validities are validities (each functional substitution of the interchange denotes waow the same of the validity does),
* nonconsistencies are nonconsistent (their negation is a validity and so the interchange in the negation is a validity),
* implications are implications (interchange into the nonconsistency is a nonconsistency),
* equivalents are equivalents (interchange of mutual implications are mutual implications),
* nonvalidities are nonvalidities (a compound is nonvalid waow some functional substitution does not denote, i.e. some functional substitution of its negation denotes, i.e. its negation is consistent, and since the negation of the interchange is identical tot he interchange of the negation which is consistent and consistency is kept by interchange then the negation is consistent i.e. it is nonvalid)
* consistencies are consistencies (the negation of the interchange is identical to the interchange of the negation which is nonvalid hence it is nonvalid),
* nonimplications are nonimplications (nonimplication is consistency of the conjunction ...)
* nonequivalences are nonequivalences (one is a nonimplication ...).

#### Equivalents of Identity
Compounds are equivalent to
* their double negation (which denotes waow the negation of the compound does not, i.e. waow it does, so, each functional substituion of it denotes waow the same of its double negation does),
* their self alternation/conjunction (which denotes waow some/each of its components does i.e. waow the compound does), and 
* their alternation/conjunction with nonconsistencies/validities.

#### Equivalents of Distributivity of Conjunctions and Alternations
* Alternations of a component with an alternation are equivalent to the alternation of the alternations of the component with each of the others.
* Alternations of a component with a conjunction are equivalent to the conjunction of the alternations of the component with each of the others.
* Conjunctions of a component with an alternation are equivalent to the alternation of the conjunctions of the component with each of the others.
* Conjunctions of a component with a conjunction are equivalent to the conjunction of the conjunctions of the component with each of teh others.

#### Equivalents of Development: Alternational and Conjunctional
* Compounds are equivalent to their alternations with nonconsistencies (equivalents of identity), and, in particular, with conjunctions of other compounds with their negation (by the law of contradiction) which are themselves equivalent to the conjunction of their alternations with the other compound and its negation (by distributivity of alternation over conjunction) i.e. conjunctive development of the one compound with respect to the other.
* The dual for conjunction.

#### Equivalents of Associativity
* The conjunction of the first component with the conjunction of the second and third is equivalent to the conjunction of the conjunction of the first and second with the third.
* The alternation of the first component with the alternation of the second and third is equivalent to the alternation of the alternation of the first and second with the third.

#### Iterated Alternations and Conjunctions
The equivalents of associativity yield the many component alternations and conjunctions which are equivalent to iterated nestings of alternations or conjunctions down their left or right components.

#### Equivalents of Commutativity
* The alternation of the left component with the right component is equivalent to the alternation of the right component with the left.
* The conjunction of the left component with the right component is equivalent to the conjunction of the right component with the left.

#### Equivalents of Distributivity of Negations
* Negations of alternations are equivalent to the conjunctions of the negations of their components.
* Negations of conjunctions are equivalent to the alternations of the negations fo their components.



#### Laws of Validity
Validity is
* inconsistency of negation (which is not consistency of negation, and, hence, not nonvalidity of negation of negation i.e. validity of negation of negation which, by equivalents of identity, is validity)
* nonimplication of negation,
* negational nonimplication
* nonequivalence of negation
* negational nonequivalence, and
* kept by functional substitutions.

#### Laws of Consistency
* Chains are nonconsistent when each of their functional substitutions does not denote.
* Nonconsistency is validity of negation.
* Nonconsistency implies nonvalidity.

#### Laws of Implication
* One chain implies an other and the other a third only where the one implies the third.
* Chains imply their self.
* Chains imply validities.
* Validities do not imply nonvalidities.
* Validities do not imply nonconsistencies.
* Validities only imply validities (each functional substitution of the former that denotes is one where the latter denotes)
* Nonconsistencies imply chains.
* Consistencies do not imply nonconsistencies.
* Nonconsistencies only imply nonconsistencies.

#### Laws of Equivalence
* One chain is equivalent to an other and the other a third only where the one is equivalent to the third.
* Chains are equivalent to their self.
* One chain is equivalent to an other waow the other is equivalent to the one.
* Validities are equivalent to and only equivalent to validities.
* Nonconsistencies are equivalent to and only equivalent to nonconsistencies.

#### Conditionals, Biconditionals, Exclusive Alternations, and Sequents
* Conditionals are alternations of the negation of their (antecedent) left component with their (consequent) right component.
* Biconditionals are conjunctions of the distinct conditionals of their components.
* Sequents are conditionals whose antecedent is the conjunction of their antecdent components and whose consequent is the alternation of their consequent components.
* Exclusive alternations are negations of the biconditionals of their components.

* Implication is validity of conditionals.
* Equivalence is validity of biconditionals.
* Nonequivalence is validity of exclusive alternations.

#### Relays, Literals, and Clauses
* Relays are their component or its negation.
* Literals are relays of nonchain compounds.
* Each component of a clausal chain is a literal.

#### Disjoint, Full, and Empty Chains
* No nonchain subcompounds of different components of *disjoint* chains match.
* The nonchain subcomponents of each component of *full* chains are the same.
* Empty chains have no components: often they are replaced by a relevant validity or inconsistency so as to carry an empty method into a nonempty one.

#### Laws of Equivalent Grammatical Categories
Each clause is equivalent to a disjoint or empty clause.
Each chain is equivalent to
* one with only joint denials
* one with only negations and conjunctions
* one with only negations and alternations
* one with only negations and conditionals
* one without conditionals and biconditionals
* one where only nonchain components are negated
* a conjunction of negations of clausal conjunctions
* the alternational dual of the above
* an alternation of disjoint clausal conjunctions (alternational normal form)
* a conjunction fo disjoint clausal alternations (conjunctive normal form)
* a full alternation of unique disjoint clausal conjunctions
* a full conjunction of unique disjoint clausal alternations

---

I may need to work more on this with Skinner's Thinking Aid.


## 2025 0509

### 2025 0509 1722
This continues work on the little lisp from the last entry [2025 0509 1429](#2025-509-1429).

I was tempted to introduce functions that took a letter and turned it into the corresponding rune as a slower introduction to the method of encoding letters by their numeric index in a given alphabetization and then encoding that number as a tally (where a tally is the proper name for a sequence of empty pairs which is equivalent to the iterated pairing of the empty pair on the left of the empty pair).
Now that I write of that temptation I am prepared to indulge it rather than leap to the 'index of' methods.
These grammatical methods, though they may seem trivial to someone already familiar with the breadth and depth of algorithms from a classical education in computer science, are of great consequence in the traditional methods of arithmetizing syntax.

```
let run=code=>console.log(code,'\n',eval(code)) // for examples

// pairs
let theEmptyPair={}
, isEmpty = it => it == theEmptyPair
, pairOf = (it, that) => [it, that]
, leftOf = it => isEmpty(it) ? it : it[0]
, rightOf = it => isEmpty(it) ? it : it[1];

// sequences as pairs
let theEmptySequence = theEmptyPair
, isEmptySequence = isEmpty
, singletonSequenceOf = it => pairOf(it, theEmptySequence)
, headOf = leftOf
, restOf = rightOf
, concatOf = (it, that) => isEmptySequence(it) ? that 
  : pairOf(headOf(it), concatOf(restOf(it), that))
, prependSingletonOf = (it, that) =>
   concatOf(singletonSequenceOf(it), that)
, appendSingletonOf = (it, that) =>
   concatOf(that,singletonSequenceOf(it));

// stacks as pairs
let theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, singletonStackOf = it => pairOf(theEmptyStack, it)
, pushOf = pairOf
, dropOf = leftOf
, topOf = rightOf
, secondOf = stack => topOf(dropOf(stack))
, drop2Of = stack => dropOf(dropOf(stack))

, prependOf = stack => pushOf(drop2Of(stack)
  , prependSingletonOf(topOf(stack), secondOf(stack)))
, appendOf = stack => pushOf(drop2Of(stack)
  , appendSingletonOf(topOf(stack), secondOf(stack)));

// letters and Runes
let theEmptyRune = theEmptyPair
, isEmptyRune = isEmpty
, theOpenRune = pairOf(theEmptyPair, theEmptyRune)
, isOpenRune = it => !isEmpty(it) && isEmpty(leftOf(it)) && isEmptyRune(rightOf(it))
, theCloseRune = pairOf(theEmptyPair, theOpenRune)
, isCloseRune = it => !isEmpty(it)&&isEmpty(leftOf(it))&&isOpenRune(rightOf(it))

, theEmptyLetter=''
, isEmptyLetter = it => it == theEmptyLetter
, theOpenParen = '('
, isOpenParen = it => it == theOpenParen
, theCloseParen = ')'
, isCloseParen = it => it == theCloseParen

, stringOf = (...letters) => 
   letters.length ? letters.shift() + stringOf(...letters) : theEmptyLetter
, firstLetterOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters[0]
, restLettersOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters.slice(1)

, runeOf = letter => isOpenParen(letter) ? theOpenRune
  : isCloseParen(letter) ? theCloseRune
  : theEmptyRune

, letterOf = rune => isOpenRune(rune) ? theOpenParen
  : isCloseRune(rune) ? theCloseParen
  : theEmptyLetter

, runesOf = letters => isEmptyLetter(letters) ? theEmptySequence
  : prependSingletonOf(runeOf(firstLetterOf(letters)), runesOf(restLettersOf(letters)))

, lettersOf = runes => isEmptySequence(runes) ? theEmptyLetter
  : stringOf(letterOf(headOf(runes)), lettersOf(restOf(runes)))

run('lettersOf(runesOf("(()())() this should be ignored"))');

// read and print sequences
let readOpenRuneOf = (stack, runes) => 
   readSequenceOf(pushOf(stack,theEmptySequence), restOf(runes))

, readCloseRuneOf = (stack, runes) =>
   readSequenceOf(appendOf(stack), restOf(runes))

, readSequenceOf = (stack, runes) => 
  isEmptySequence(runes) ? headOf(topOf(stack))
  : isOpenRune(headOf(runes)) ? readOpenRuneOf(stack,runes)
  : isCloseRune(headOf(runes)) ? readCloseRuneOf(stack, runes)
  : readSequenceOf(stack, restOf(runes))
, readOf = runes => readSequenceOf(theEmptyStack, runes);

let parenOf = runes => prependSingletonOf(theOpenRune
    , appendSingletonOf(theCloseRune, runes))

, printSequenceOf = sequence =>
  isEmptySequence(sequence) ? theEmptySequence
  : concatOf(printOf(headOf(sequence))
    , printSequenceOf(restOf(sequence)))

, printOf = sequence =>
  isEmptySequence(sequence) ? parenOf(theEmptySequence)
  : parenOf(printSequenceOf(sequence))   

, read = letters => readOf(runesOf(letters))
, print = sequence => lettersOf(printOf(sequence));

run("lettersOf(parenOf(runesOf('(()()())')))")
run("print(read('()'))")
run("print(read('(()(())())'))")
run("print(read('(())(()(())())'))")
run("print(read('(()(())())((()))'))")
run("print(read('))))(('))")
```

This appears to have simplified definitions while also pointing to general methods e.g. the definitions of empty, open, and closed runes are ripe for inductive generalization as was done in past versions that gave the alphabet as a javascript array on which functions of search were invoked.


### 2025 0509 1429

Here is the completed code from yesterday's work  [2025 0508 2207](#2025-0508-2207) on my little lisp.
I also put the appropriate printer code that was missing from the last note into that note.

```
let run=code=>console.log(code,'\n',eval(code)) // for examples

// pairs
let theEmptyPair={}
, isEmpty = it => it == theEmptyPair
, pairOf = (it, that) => [it, that]
, leftOf = it => isEmpty(it) ? it : it[0]
, rightOf = it => isEmpty(it) ? it : it[1];

// sequences as pairs
let theEmptySequence = theEmptyPair
, isEmptySequence = isEmpty
, singletonSequenceOf = it => pairOf(it, theEmptySequence)
, headOf = leftOf
, restOf = rightOf
, concatOf = (it, that) => isEmptySequence(it) ? that 
  : pairOf(headOf(it), concatOf(restOf(it), that))
, prependSingletonOf = (it, that) =>
   concatOf(singletonSequenceOf(it), that)
, appendSingletonOf = (it, that) =>
   concatOf(it,singletonSequenceOf(that));

// stacks as pairs
let theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, singletonStackOf = it => pairOf(theEmptyStack, it)
, pushOf = pairOf
, dropOf = leftOf
, topOf = rightOf
, secondOf = stack => topOf(dropOf(stack))
, encatOf = stack => pushOf(dropOf(dropOf(stack))
  , appendSingletonOf(secondOf(stack), topOf(stack)));

// letters
let theEmptyLetter=''
, isEmptyLetter = it => it == theEmptyLetter
, stringOf = (...letters) => 
   letters.length ? letters.shift() + stringOf(...letters) : theEmptyLetter
, firstLetterOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters[0]
, restLettersOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters.slice(1)

, theOpenParen = '('
, isOpenParen = it => it == theOpenParen
, theCloseParen = ')'
, isCloseParen = it => it == theCloseParen

, runesOf = letters => isEmptyLetter(letters) ? theEmptySequence
  : isOpenParen(firstLetterOf(letters)) ? prependSingletonOf(theOpenRune, runesOf(restLettersOf(letters)))
  : isCloseParen(firstLetterOf(letters)) ? prependSingletonOf(theCloseRune, runesOf(restLettersOf(letters)))
  : runesOf(restLettersOf(letters))

, lettersOf = runes => isEmptySequence(runes) ? theEmptyLetter
  : isOpenRune(headOf(runes)) ? stringOf(theOpenParen, lettersOf(restOf(runes)))
  : isCloseRune(headOf(runes)) ? stringOf(theCloseParen, lettersOf(restOf(runes)))
  : lettersOf(restOf(runes));
run('lettersOf(runesOf("(()())() this should be ignored"))');

// read and print sequences
let theOpenRune = theEmptyPair
, isOpenRune = isEmpty
, readOpenRuneOf = (stack, runes) => 
   readSequenceOf(pushOf(stack,theEmptySequence), restOf(runes))

, theCloseRune = pairOf(theEmptyPair,theEmptyPair)
, isCloseRune = it => !isEmpty(it)&&isEmpty(leftOf(it))&&isEmpty(rightOf(it))
, readCloseRuneOf = (stack, runes) =>
   readSequenceOf(encatOf(stack), restOf(runes))

, readSequenceOf = (stack, runes) => isEmptySequence(runes) ? headOf(topOf(stack))
  : isOpenRune(headOf(runes)) ? readOpenRuneOf(stack,runes)
  : isCloseRune(headOf(runes)) ? readCloseRuneOf(stack, runes)
  : readSequenceOf(stack, restOf(runes))
, readOf = runes => readSequenceOf(theEmptyStack, runes);
run("isEmpty(read('()'))")

let parenOf = runes => 
  prependSingletonOf(theOpenRune, appendSingletonOf(runes, theCloseRune))
, printSequenceOf = sequence =>
  isEmptySequence(sequence) ? theEmptySequence
  : concatOf(printOf(headOf(sequence))
    , printSequenceOf(restOf(sequence)))
, printOf = sequence =>
 isEmptySequence(sequence) ? parenOf(theEmptySequence)
 : parenOf(printSequenceOf(sequence))   

, read = letters => readOf(runesOf(letters))
, print = sequence => lettersOf(printOf(sequence));

run("lettersOf(parenOf(runesOf('(()()())')))")
run("print(read('()'))")
run("print(read('(()(())())'))")
run("print(read('(())(()(())())'))")
run("print(read('(()(())())((()))'))")
run("print(read('))))(('))")
```

## 2025 0508

### 2025 0508 2207
This entry shall be different than prior ones.
Some time in the near future I shall look back on what I have done and select what has worked from what has not.
The only way I know how to carry out that task effectively is to print out what I have written and go over it with a pen in my hand and some note paper near by, but this will be the first time that I attempt to do this in public.

There are a lot of firsts among what is written here.
This is another first not entirely divorced from the more detailed review of what I have done that is yet forthcoming.
Originally I had aimed at scheduling a public review at the end of last month.
As the days go by I shall miss the mark more and more, but I do not expect to be that far off.
Perhaps I am simply being optimistic.

This entry is a response to what I have to say about what I have done that I recall without looking back over a print out of what is recorded here.
There are three threads that are woven between my writings:

1. logic
2. programming
3. philosophy.

For all the philosophy that is here, I have never been a big fan of it.
The moment I was exposed to the science of behavior, as outlined in any one of B. F. Skinner's books, I left philosophy as anything more than a narrow science of a narrow kind of verbal behavior (what I have since come to call "the science of smooth dialogue").

To my surprise, the science of behavior has yet to take up much space here.
As much as I've tried to keep all my episodes of reading on the record here, I have failed to do so on many occasions.
In particular, I have read selections from Skinner's "Cumulative Record" without noting what my responses to them were here.
Even when I carry around a notepad and a pen in my pocket, I still read without making any record of what occurs to me while reading.
There is an excuse looming; something about thinking without writing being faster and easier than writing without thinking.

I've spent far more time working on my little lisp than I ever planned.
This is partially becuase I am frequently reminded of the missing link between Feferman's Finitary Inductively Presented Logics and something like LISP.
It is also because I must make sure that there is nothing I have missed about each step in the construction and conventions of LISP that might later leave me blind when I finally come to compare and contrast it with FORTH or some other concatenative language.

Another reason I've spent so much time on it is because I see each obstacle at some later stage of any longer work as an opportunity to introduce a foundation whose deferred consequences entirely avoid such obstacles.
The more obstructions I confront the more likely I am to examine the link between whatever premises I've overtly identified and how presently obstructing conclusions follow from them.
This is rarely a matter of formal deduction, but almost always a result of methods with a strong familial similarity to logic.
Almost always, major problems are the result of a poorly selected lexicon.
The basic predicates with which a problem is stated and in which its solutions are proposed almost always decide my fate.

In the past LISP has been saddled with lambda calculi.
No such theories are likely to turn up here: lambda calculi are nonconsistent without some crippling and complex system of typing.
Furthermore, the methods of lambda calculi are merely the theoretical echos of better logical methods e.g. predicate abstraction and concretion are the logical operations that permit talk of lambda abstraction and application.
This has been known since Quine and Church traded reviews of each others works in the early half of the 20th century.
The moment Quine revealed that application could be reinterpreted as the collective relate (an operation on relations uncovered by Russell among his short summaries of work he and Whitehead had done), it should have been clear that no result obtained from the lambda calculus should be taken as special to it.
Little did either Quine or Church know that Quine's later methods of predicate abstraction and concretion revealed the logically universal way of dealing with the multitude of theories mathematicians may propose (be they lambda calculi, relational calculi, set theories etc.)

Thus, I've spent much time fussing over tiny distinctions between various attempts at getting out something like LISP: the methods are to be closer to those of Goodstein's equation calculi than to any lambda calculus.
But, it is more than that.
Ordered pairs are the most modest abstract objects that may be admitted to any theory and which release most of the grammatical methods that have found greatest use in the analysis of carefully designed languages.
This is the premise not only revealed at the end of Quine's "Word and Object" but also a turning point in Feferman's F.I.P.L.s.

Up until very recently this was the path I planned on following in order to reach out from logic to the practices of programming.
It is only upon uncovering "Logic Programming" that the entirety of programming practice can be reduced to the predicate "is valid", or, as the case often is, the kindred predicate "is inconsistent".
Had this been shown to me earlier I may have avoided much of my work on primitive recursive functions of ordered pairs.
But, I'm still stuck in a world of Forths and Lisps rather than a world of logic.

What little I have learned of Prolog has no promise to me.
It works on a limited slice of logical methods and Datalog doesn't appear to do much better.
The rush to monetize logic programming and its fundamental relation to database systems generally ruined the logic.
The modern world, one where large masses of capital are committed to the matrix methods of machine learning, is technically one where logic programming is primed to reign supreme for the following simple reason: any path which leads to inconsistency works but some paths are faster than others for no reason in particular.
Pro- and data- log aimed to only permit constructions which admit short chains of relevant implications.
The historic obsession with implication as in formal deduction has blinded modern computer scientists and logicians to the role of logical methods prior to any science e.g. mathematical or computational.
It is only a matter of time before the window for logical programming closes once again for the same reason it did in the 80s.

Through my little lisp I still plan on exploring the methods of logic programming.
This shall be done in parallel with my other programming language which is much like uhdForth.

The problem I'm posed with upon writing these paragraphs is this: can I juggle more while also building a new experimental culture?

### 2025 0508 1650
This continues my work on my little lisp from [2025 0507 2029](#2025-0507-2029).

I'm hopeful that whatever logical errors in the code I produced yesterday will be solved today: I am more awake now than I was when I was working on this last night.
It seems that I forgot about the output at the end of the reader: instead of just taking the top of the stack as the appropriately constructed sequence, I must actually take the head of the sequence atop the stack.
This is due to how a close parenthesis (or close rune) works in the degenerate case where there are less than two items on the stack.
In this degenerate case the last parenthesis (of a sequence of runes with balanced parenthesis, as I have not checked the unbalanced cases yet) actually encats the singleton sequence of the top of the list to what it takes as an empty sequence, but which is really the result of a degenerate case of taking the top of the empty stack: it returns the empty sequence.

As I went to copy the code over here I again fell prey to coding rather than copying.
It occurred to me that there are errors that have slipped in because I have once again not properly factored out the boundaries between languages that are under construction.

Thanks to my prior work I have happily seperated the defining features of sequences from those of lists.
Though it is not a traditional distinction, it is consistent with Feferman's Finitary Inductively Presented Logics use of sequences as special ordered pairs and lists as a sequence grammatically tagged as a sequence.
The analysis of a LISP list as a sequence tagged grammatically as a sequence is key to general grammatical methods at work in modern programming practices and which are sadly conflated with problems of types as an abstract concern.

My aim now is to simplify the reader and printer to work entirely with sequences in order to sus out any logical errors at the level of a purely parenthetical language rather than one that also induces the complexity of runes and strings.

The new starting point is this code:

```
// pairs
let theEmptyPair={}
, isEmpty = it => it == theEmptyPair
, pairOf = (it, that) => [it, that]
, leftOf = it => isEmpty(it) ? it : it[0]
, rightOf = it => isEmpty(it) ? it : it[1]

// sequences as pairs
, theEmptySequence = theEmptyPair
, isEmptySequence = isEmpty
, singletonSequenceOf = it => pairOf(it, theEmptySequence)
, headOf = leftOf
, restOf = rightOf
, concatOf = (it, that) => isEmptySequence(it) ? that 
  : pairOf(headOf(it), concatOf(restOf(it), that)) 

// stacks as pairs
, theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, singletonStackOf = it => pairOf(theEmptyStack, it)
, pushOf = pairOf
, dropOf = leftOf
, topOf = rightOf
, secondOf = stack => topOf(dropOf(stack))
, encatOf = stack => pushOf(dropOf(dropOf(stack))
  , concatOf(secondOf(stack), singletonSequenceOf(topOf(stack))));
```

The only part that still needs a bit of explanation is what the function designated by 'encatOf' actually does and why it keeps occurring in the code (or why some function like it continues to reoccur).
My explanation for now is that it is the least intrusive way to take advantage of the power of stack based methods without bringing every defined operation on pairs and sequences to stacks.
To do so would be to create a stack based language like FORTH, also called a concatenative language.
That is not the aim here.

What the function designated by 'encatOf' does is to help with how to interpret a closed parenthesis.
This gets us to the new function being defined.
For now it is designated by 'readSequenceOf'.
It aims to take a sequence of runes, each of which is an open or close rune, and construct the corresponding iterated sequences of empty sequences.
The open rune begins a new sequence by pushing the empty sequence onto the stack and the close rune adds the item at the top of the stack to the end of the sequence that is second from the top of the stack.

Here is my first attempt based on all that I've learned so far.

```
// read sequences from a sequence of open and closed runes
let theOpenRune = theEmptyPair
, isOpenRune = isEmpty
, readOpenRuneOf = (stack, runes) => 
   readSequenceOf(pushOf(stack,theEmptySequence), restOf(runes))

, theCloseRune = pairOf(theEmptyPair,theEmptyPair)
, isCloseRune = it => !isEmpty(it)&&isEmpty(leftOf(it))&&isEmpty(rightOf(it))
, readCloseRuneOf = (stack, runes) =>
   readSequenceOf(encatOf(stack), restOf(runes))

, readSequenceOf = (stack, runes) => isEmptySequence(runes) ? headOf(topOf(stack))
  : isOpenRune(headOf(runes)) ? readOpenRuneOf(stack,runes)
  : isCloseRune(headOf(runes)) ? readCloseRuneOf(stack, runes)
  : readSequenceOf(stack, restOf(runes))

, readOf = runes => readSequenceOf(theEmptyStack, runes);
```

Now only a small number of specialty operations on javascript strings appear to be needed to interface with these functions as defined.

```
// letters
let theEmptyLetter=''
, isEmptyLetter = it => it == theEmptyLetter
, stringOf = (...letters) => 
   letters.length ? letters.shift() + stringOf(...letters) : theEmptyLetter
, firstLetterOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters[0]
, restLettersOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters.slice(1)

, theOpenParen = '('
, isOpenParen = it => it == theOpenParen
, theCloseParen = ')'
, isCloseParen = it => it == theCloseParen;
```

They are the same old same old, except for the last two that single out two particular letters.
It finally occurred to me to add the following two definitions to the end of the definitions of functions for treating ordered pairs as sequences:

```
let prependSingletonOf = (it, that) =>
   concatOf(singletonSequenceOf(it), that)
, appendSingletonOf = (it, that) =>
   concatOf(it,singletonSequenceOf(that));
```

Then, the definition of the function designated by 'encatOf' becomes just as follows.

```
let encatOf = stack => pushOf(dropOf(dropOf(stack))
 , appendSingletonOf(secondOf(stack), topOf(stack)));
```

Construction of a sequence of runes from a string of letters of which only the parenthesis are of concern proceeds as follows.

```
let runesOf = letters => isEmptyLetter(letters) ? theEmptySequence
  : isOpenParen(firstLetterOf(letters)) ? prependSingletonOf(theOpenRune, runesOf(restLettersOf(letters)))
  : isCloseParen(firstLetterOf(letters)) ? prependSingletonOf(theCloseRune, runesOf(restLettersOf(letters)))
  : runesOf(restLettersOf(letters))
```

Then the function that gets us back from a sequence of runes to letters is as follows.

```
let lettersOf = runes => isEmptySequence(runes) ? theEmptyLetter
  : isOpenRune(headOf(runes)) ? stringOf(theOpenParen, lettersOf(restOf(runes)))
  : isCloseRune(headOf(runes)) ? stringOf(theCloseParen, lettersOf(restOf(runes)))
  : lettersOf(restOf(runes))
```

A brief example demonstrates that the functions from letters to runes and vice versa work.

```
lettersOf(runesOf("(()())() this should be ignored")) 
 (()())()
```

This assures us that when we send a sequence of runes to the reader it corresponds to the parentheses in the given string.

Next the sequence printer.
It should print out a sequence of runes that, when put back into the reader and printed should output an identical sequence of runes.
The notation for sequences adopted in the reader has an open and close rune around its contents.

```
let parenOf = runes => prependSingletonOf(theOpenRune, appendSingletonOf(runes, theCloseRune))
```
An example:
```
lettersOf(parenOf(runesOf('(()()())'))) 
 ((()()()))
```

The sequence printer assumes that the pair it is passed is to be printed as a pure sequence.
If asked to print the empty sequence it prints an open rune followed by a closed rune.
Otherwise it encloses the print of each item in the sequence, one after the other, in open and closed runes.

```
let parenOf = runes => 
  prependSingletonOf(theOpenRune, appendSingletonOf(runes, theCloseRune))
, printSequenceOf = sequence =>
  isEmptySequence(sequence) ? theEmptySequence
  : concatOf(printOf(headOf(sequence))
    , printSequenceOf(restOf(sequence)))
, printOf = sequence =>
 isEmptySequence(sequence) ? parenOf(theEmptySequence)
 : parenOf(printSequenceOf(sequence))   

, read = letters => readOf(runesOf(letters))
, print = sequence => lettersOf(printOf(sequence));
```

The following examples test out the reader and the printer.
```
isEmpty(read('()')) 
 true
print(read('()')) 
 ()
print(read('(()(())())')) 
 (()(())())
print(read('(())(()(())())')) 
 (())
print(read('(()(())())((()))')) 
 (()(())())
```
The last two examples indicate conspicuously that the case in the function denoted by 'readSequenceOf' where the rune sequence is empty and returns the head of the top of the stack on which the corresponding sequence was being constructed.

This parenthesis notation is the infix version of the postfix notation from [Bit Strings and Binary Trees](#2025-0413-1513-bit-strings-and-binary-trees) but instead of interpreting everything as binary trees they are interpreted as pure sequences i.e. iterated sequences of the empty sequence.


This is the basic method upon which my earlier attempts at constructing a reader and printer for past versions of my list are based.


## 2025 0507

### 2025 0507 2029
This continues the work on my little lisp from [2025 0505 1725](#2025-0505-1725).

Last note left on a question: were all these distinctions between stacks as LISP lists as sequences and sequences as pairs helpful?
What I uncovered upon further reflection is that the answer to this question is yes and no.
These distinctions are still of critical importance, but not in the straight line from pairs up through stacks as LISP lists.
What occurred to me is that there are two languages that I've built upon native javascript and that I've failed to untangle them from each other.

The lowest language is the programmable part of the arithmetic of ordered pairs, and the language above that is my little LISP.
Thus there are two readers: one that constructs LISP lists and the one for my little LISP (which is more like a calculator or evaluator).

Sequences and stacks are then implemented as before the last entry: atop the arithmetic of ordered pairs.

```
// pairs
let theEmptyPair={}
, isEmpty = it => it == theEmptyPair
, pairOf = (it, that) => [it, that]
, leftOf = it => isEmpty(it) ? it : it[0]
, rightOf = it => isEmpty(it) ? it : it[1]

// sequences as pairs
, theEmptySequence = theEmptyPair
, isEmptySequence = isEmpty
, singletonSequenceOf = it => pairOf(it, theEmptySequence)
, headOf = leftOf
, restOf = rightOf
, concatOf = (it, that) => isEmptySequence(it) ? that 
  : pairOf(headOf(it), concatOf(restOf(it), that)) 

// stacks as pairs
, theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, singletonStackOf = it => pairOf(theEmptyStack, it)
, pushOf = pairOf
, dropOf = leftOf
, topOf = rightOf
, secondOf = stack => topOf(dropOf(stack))
, encatOf = stack => pushOf(dropOf(dropOf(stack))
  , concatOf(secondOf(stack), singletonSequenceOf(topOf(stack))));
```

All is back to how it was a few notes ago but with a few better names for the different operations.
These name changes also introduce some later distinctions that will come as the result of grammatical (type) analysis.
I've gone back to not focusing on a specific alphabetization of runes.
The reader is now assembled from simpler functions for reading the special open, close, and space runes.

There are a few other tricks of the trade that have been included e.g. javascript defaults to giving negative one when it does not find the index of an item you are looking for in an array, but it greatly simplifies a theory, in almost all cases, when it returns the length of the array.
Another very helpful trick is to return the entire alphabet when you give it an index that is outside the range of the alphabet.
Both of these are examples of degenerate cases that can be made to play nicely with the rest of a theory.

```
// letters
, theEmptyLetter=''
, isEmptyLetter = it => it == theEmptyLetter
, stringOf = (...letters) => 
   letters.length ? letters.shift() + stringOf(...letters) : theEmptyLetter
, firstLetterOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters[0]
, restLettersOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters.slice(1)

// letter as number
, abc =[...'() 0123456789abcdefghijklmnopqrstuvwxyz']
, alphabeticalIndexOf = letter => 
   abc.includes(letter) ? abc.indexOf(letter) : abc.length
, alphabeticalLetterOf = index => 
   0 <= index && index < abc.length ? abc[index] : abc

// number as tally (as pair)
, tallyOf = n => n>0 ? pairOf(theEmptyPair, tallyOf(n-1)) : theEmptyPair
, countOf = x => isEmpty(x) ? 0 : 1 + countOf(rightOf(x))

// rune as tally
, runeOf = letter => tallyOf(alphabeticalIndexOf(letter))
, letterOf = rune => alphabeticalLetterOf(countOf(rune))

// letters as sequences of runes
, runesOf = letters => 
   letters.length ? concatOf(
    singletonSequenceOf(runeOf(firstLetterOf(letters)))
    , runesOf(restLettersOf(letters)))
   : theEmptySequence
, lettersOf = runes =>
   isEmptySequence(runes) ? theEmptyLetter
   : stringOf(letterOf(headOf(runes)), lettersOf(restOf(runes)))

// recursive definition of identity of pairs
, id = (it, that) =>
  (isEmpty(it) && isEmpty(that))
  || (!(isEmpty(it) || isEmpty(that))
     && id(leftOf(it), leftOf(that))
     && id(rightOf(it), rightOf(that)))

// reader
, theOpenRune = runeOf('(')
, isOpenRune = it => id(it, theOpenRune)
, readOpenRuneOf = (stack, runes) => 
   readerOf(pushOf(stack, theEmptySequence), restOf(runes))

, theCloseRune = runeOf(')')
, isCloseRune = it => id(it, theCloseRune)
, readCloseRuneOf = (stack, runes) => 
   readerOf(encatOf(stack), restOf(runes))

, theSpaceRune = runeOf(' ')
, isSpaceRune = it => id(it, theSpaceRune)
, readSpaceRuneOf = (stack, runes) => !isEmptySequence(topOf(stack))   
  ? readerOf(pushOf(encatOf(stack), theEmptySequence), restOf(runes))
  : isEmptySequence(secondOf(stack)) ? readerOf(stack, restOf(runes))
  : readerOf(pushOf(stack, theEmptySequence), restOf(runes))

, isRune = it => countOf(it) < abc.length
, readRuneOf = (stack, runes) => 
   readerOf(encatOf(pushOf(stack, headOf(runes))), restOf(runes))

, readerOf = (stack, runes) =>
  isEmptySequence(runes) ? topOf(stack)
  : isOpenRune(headOf(runes)) ? readOpenRuneOf(stack, runes)
  : isCloseRune(headOf(runes)) ? readCloseRuneOf(stack, runes)
  : isSpaceRune(headOf(runes)) ? readSpaceRuneOf(stack, runes)
  : isRune(headOf(runes)) ? readRuneOf(stack, runes)
  : readerOf(stack, restOf(runes)) 

, readOf = runes => readerOf(theEmptyStack, runes)
```
This reader is increadibly powerful.
Here is a tiny printer that anticipates the method of lists as sequences which begin with the symbol "list".
```
// printer
, printSequenceOf = sequence => isEmptySequence(sequence) ? theEmptySequence
 : concatOf(printOf(headOf(sequence))
   , concatOf(singletonSequenceOf(theSpaceRune)
     , printSequenceOf(restOf(sequence))))
, parenOf = runes => concatOf(singletonSequenceOf(theOpenRune)
   , concatOf(runes, singletonSequenceOf(theCloseRune)))
, printParenSequenceOf = sequence => parenOf(concatOf(singletonSequenceOf(theSpaceRune), printSequenceOf(sequence)))

, isList = sequence => id(headOf(sequence), runesOf('list'))
, printListOf = list => printParenSequenceOf(list)

, printOf = item => isList(item)? printListOf(item) : item

// external read and print
, read = letters => readOf(runesOf(letters))
, print = item => lettersOf(printOf(item))
```

Neither the reader nor the printer seem to be working as expected e.g. reading a space rune goes wrong when there are lots of parentheses bunched up.


### 2025 0507 1421
This continues my work on my paper on logic from [2025 0412 1422](#2025-0412-1422).

The following presentation of logic is unique in a number of ways.
Primarily it is quantificational logic.
It does away with quantifiers as in "(each item is x such that)(x is human only if x is mortal)" by skipping over the intermediate methods of Quine's predicate abstracts as in "(each item is)(x such that x is human only if x is mortal)" which clearly factors out the predicate functor 'each item is' from the predicate 'x such that x is human only if x is mortal' by doing away entirely with the classical role of variables as in 'each human only if mortal'.

Predicate functor logic is a foreign language to those who speak natively with all the cross referential power of pronouns, whether they be free and taken as dummy singular terms or bound and taken as dummies of qunatification.
Though the grammars of (the unhappily named) natural languages are burdened with only indirect analyses of cross reference, the carefully designed language of predicate functor logic eliminates all referential oddities by explaining their cross referential operations as the result of the (logical) equivalence of a predicate with one in a special grammatical category (those constructed from the so called recombic predicate functors).

Although at most three predicate functors are sufficient to assemble all the compound predicates from the lexicon of a logical langauge, the predicate functor foreigner would find themselves lost in construction.
Thus, full predicate functor logic is introduced on a schedule which is sufficiently similar to classical treatments to admit little or no alarm among those already familiar with the path to quantificational logic from truth functional logic and through the Boolean closure logics of many place predicates (this is an urgent reminder that Boolean logic is the logic of existential closures of truth functional compounds of basic predicates, and that Boolean logic is not to be confused, as it so often is, with the algebra of truth functions as functions on the set whose sole members are the true and the false).

In addition to the unique methods of predicate functor logic, there is an additional and equally profound difference in this presentation of logic: it follows from the grammatical definition of validity as, broadly, substitution for lexicon.
Just as Quine failed to provide an autonomous proof procedure for predicate functor logic (one that did not depend on translation to quantificational logic and back from the same), so too did Quine fail to bring his grammatical definition of validity as truth of substitution under supplementation of lexicon to predicate logic.
The substitutional basis for the grammatical definition of validity is established by carefully combining the works of Hilbert and Bernay's in their proof of completeness of quantificational logic in the second book of their Grundlagen as a strengthening of Tarski's model theoretic definition of validity originally from his analysis of truth in his 1933 paper “The concept of truth in the languages of the deductive sciences” but conspicuously from his 1956 paper "Arithmetical extensions of relational systems" with Robert Vaught, and the completeness theorems of Skolem, Herbrand, and Godel.
For more details on the delicate combination of these fundamental components of logical practice see Quine's "Philosophy of Logic second edition" section entitled "Adequacy of Substitution".

So the distinguishing features of this presentation are predicate functors and substitutional validity.
There are secondary features which are perhaps just as important, but which are not so easy to explain.
Traditional logical methods deal with schema and functions of truth values by building up schematic sentence letters with connectives parallel to functions from truth values to truth values as in truth tables, or, much better, as truth trees.
Following the guidence of grammatical methods, no such methods are found here.
They are secondary in that each branch of a truth tree is an otherworldly mirror on the grammatical structure of their alternational development and reduction by equivalents of identity.
This stricture has the benefit of eliminating confusion caused by schematic letters e.g. they are sometimes taken as more than dummies and made to refer or mean in one or another mysterious way.

I am not full convinced of the utility of these self contained grammatical methods, but shall push them as far as I can without losing track of how well the schedule upon which quantificational practices are built is carried over.

What follows is merely an outline.
It charts out the path that has not yet been taken based on the best available reports that have come back from those who have survived their trip into the jungle of logic without having been too harshly damanged by its wilds.
Russell cleared much of the way, but was left with unhappy impressions that, thankfully, do not seem to have harmed Quine into a frightful silence: Quine wrote on logic till his final days.

These notes are terse and a not inconsiderable amount of effort on the part of the reader is required to effectively navigate them: many of the later conclusions lack supporting arguments.
The key to taking each step, e.g. from your native tongue to the truth functional part of predicate functor logic or from the truth functional part to the Boolean closure part, is Quine's method of paraphrasing inwards.
No matter what the inner components of a sentence may be we can analyze their outermost grammatical structure while leaving their inner parts as yet unanalyzed.
But, now we must take our sentences as those of a predicate functor community i.e. as one where primitives and compounds are predicates as integral words or phrases such as "father of", "is man", or just "man".

I submit that the sentences of a predicate functor community are like those we attribute to early humans, e.g. "each human only if mortal" rather than the aristotelian universal "All humans are mortal" or the quantificational "Everything is such that it is human only if it is mortal", and that this is no mere accident of expedient analysis.

A final note on my methods of logic: I have tried to defer the introduction of as many distinguishing features as far as possible.
For example, conditionals and biconditionals are introduced far after functional equivalence.
This method of spreading out concerns is invaluable in any endevour to simplify and elucidate without adding comments of simplification adn elucidation.

 
#### Logic, Science, and Validity

Logic is the science of validity and validity is a consequence of grammar and denotation.

#### Functional Compounding and Chains

Compounding is (denotative) functional when, exclusively, each like compound denotes or each like compound does not denote, where and only where (waow), exclusively, each like component denotes or each like component does not denote.
Chains are compounds compounded functionally.

#### Example Chains: Joint Denials, Negations, Alternations, and Conjunctions

Joint denials denote waow each of their components do not.
Negations are self joint denials: they denote waow their component does not.
Alternations are negations of joint denials: they denote waow some of their components do.
Conjunctions are joint denials of negations: they denote waow each of their components do.

#### Subcompounds and Functional Substitutions

Subcompounds of compounds are their self or those of their components.
Substitutions of like compounds for like nonchain subcompounds are (denotative) functional.
Functional substitutions of functional substitutions of compounds are functional substitutions of their self.

#### Functional Validity, Consistency, Implication, and Equivalence

Compounds are (functionally)
* valid waow each of their functional substitutions denote,
* consistent waow their negation is nonvalid (i.e. soem of their functional substitutions denote),
* implied by others waow the conjunction of their self (the conclusion) with the negation of the other (the premise) is nonconsistent (i.e. each of their functional substitutions denotes where the same of the other does), and
* equivalent to others waow they are mutually implicative (i.e. each of their functional substitutions denotes waow the same of the other does).
[See pg. 36 of POL]

#### Example Validities and (Non)consistencies: Laws of Excluded Middle, Contradiction, Self Implication, and Self Equivalence

Alternations of compounds with their negations are valid (they denote waow the compound does or its negation does, i.e. waow it does or does not, so, each functional substitution denotes).
Conjunctions of compounds with their negations are nonconsistent (they denote waow their compound does and its negation does i.e. waow it does and does not, so, each functional substitution does not denote).
Compounds are implied by and equivalent to their self.

#### Functional Substitutions Keep Validity, Nonconsistency, Implication and Equivalence

Functional substitutions in
* validities are validities (each functional substitution of the functional substitution of the validity is a functional substitution of the validity and hence denotes),
* nonconsistencies are nonconsistencies (each function substitution of the negation of the functional substitution of the nonconsistency is a functional substitution of the negation of the nonconsistency i .e. is a functional substitution of a validity and hence the negation of the functional substitution of the nonconsistency is valid so that the function substitution of the nonconsistency is nonconsistent),
* implications are implications (the conjunction of the conclusion with the negation of the premise is nonconsistant and hence its functional substitution is nonconsistent and identical to the conjunction of the function substitution of the conclusion with the negation of the functional substutituion of the premise), and
* equivalences are equivalences (functional substitutions of mutual implications are mutual implications).

#### Interchanges of Equivalents are Equivalent
Interchanges of equivalents in a compound are equivalent to that compound (each functional substitution of a compound matches the same of its interchange, except perhaps for the same of the equivalents which otherwise denote in tandem, so each denotes waow the other does i.e. they are equivalent).

#### Interchnage of Equivalents Keeps Validity, Nonconsistency, Implication, Equivalence, Nonvalidity, Consistency, Nonimplication, and Nonequivalence

Interchanges of equivalents in
* validities are validities (each functional substitution of the interchange denotes waow the same of the validity does),
* nonconsistencies are nonconsistent (their negation is a validity and so the interchange in the negation is a validity),
* implications are implications (interchange into the nonconsistency is a nonconsistency),
* equivalents are equivalents (interchange of mutual implications are mutual implications),
* nonvalidities are nonvalidities (a compound is nonvalid waow some functional substitution does not denote, i.e. some functional substitution of its negation denotes, i.e. its negation is consistent, and since the negation of the interchange is identical tot he interchange of the negation which is consistent and consistency is kept by interchange then the negation is consistent i.e. it is nonvalid)
* consistencies are consistencies (the negation of the interchange is identical to the interchange of the negation which is nonvalid hence it is nonvalid),
* nonimplications are nonimplications (nonimplication is consistency of the conjunction ...)
* nonequivalences are nonequivalences (one is a nonimplication ...).

#### Equivalents of Identity
Compounds are equivalent to
* their double negation (which denotes waow the negation of the compound does not, i.e. waow it does, so, each functional substituion of it denotes waow the same of its double negation does),
* their self alternation/conjunction (which denotes waow some/each of its components does i.e. waow the compound does), and 
* their alternation/conjunction with nonconsistencies/validities.

#### Equivalents of Distributivity of Conjunctions and Alternations
* Alternations of a component with an alternation are equivalent to the alternation of the alternations of the component with each of the others.
* Alternations of a component with a conjunction are equivalent to the conjunction of the alternations of the component with each of the others.
* Conjunctions of a component with an alternation are equivalent to the alternation of the conjunctions of the component with each of the others.
* Conjunctions of a component with a conjunction are equivalent to the conjunction of the conjunctions of the component with each of teh others.

#### Equivalents of Development: Alternational and Conjunctional
* Compounds are equivalent to their alternations with nonconsistencies (equivalents of identity), and, in particular, with conjunctions of other compounds with their negation (by the law of contradiction) which are themselves equivalent to the conjunction of their alternations with the other compound and its negation (by distributivity of alternation over conjunction) i.e. conjunctive development of the one compound with respect to the other.
* The dual for conjunction.

#### Equivalents of Associativity
* The conjunction of the first component with the conjunction of the second and third is equivalent to the conjunction of the conjunction of the first and second with the third.
* The alternation of the first component with the alternation of the second and third is equivalent to the alternation of the alternation of the first and second with the third.

#### Iterated Alternations and Conjunctions
The equivalents of associativity yield the many component alternations and conjunctions which are equivalent to iterated nestings of alternations or conjunctions down their left or right components.

#### Equivalents of Commutativity
* The alternation of the left component with the right component is equivalent to the alternation of the right component with the left.
* The conjunction of the left component with the right component is equivalent to the conjunction of the right component with the left.

#### Equivalents of Distributivity of Negations
* Negations of alternations are equivalent to the conjunctions of the negations of their components.
* Negations of conjunctions are equivalent to the alternations of the negations fo their components.

#### Relays, Literals, and Clauses
* Relays are their component or its negation.
* Literals are relays of nonchain compounds.
* Each component of a clausal chain is a literal.

#### Disjoint, Full, and Empty Chains
* No nonchain subcompounds of different components of *disjoint* chains match.
* The nonchain subcomponents of each component of *full* chains are the same.
* Empty chains have no components: often they are replaced by a relevant validity or inconsistency so as to carry an empty method into a nonempty one.

#### Laws of Validity
Chains are nonvalid waow they are not valid and, hence, not where each of their functional substitutions denotes i.e. where some of its functional substitutions do not denote.

Validity is
* inconsistency of negation (which is not consistency of negation, and, hence, not nonvalidity of negation of negation i.e. validity of negation of negation which, by equivalents of identity, is validity)
* nonimplication of negation,
* negational nonimplication
* nonequivalence of negation
* negational nonequivalence, and
* kept by functional substitutions.

#### Laws of Consistency
* Chains are consistent when some of their functional substitutions denote.
* Chains are nonconsistent when each of their functional substitutions does not denote.
* Nonconsistency is validity of negation.
* Nonconsistency implies nonvalidity.

#### Laws of Implication
* Chains are implied by others waow each functional substitution of the one denotes where the same of the other denotes.
* One chain implies an other and the other a third only where the one implies the third.
* Chains imply their self.
* Chains imply validities.
* Validities do not imply nonvalidities.
* Validities do not imply nonconsistencies.
* Validities only imply validities (each functional substitution of the former that denotes is one where the latter denotes)
* Nonconsistencies imply chains.
* Consistencies do not imply nonconsistencies.
* Nonconsistencies only imply nonconsistencies.

#### Laws of Equivalence
* Chains are equivalent waow each of their functional substitutions denotes or not together.
* One chain is equivalent to an other and the other a third only where the one is equivalent to the third.
* Chains are equivalent to their self.
* One chain is equivalent to an other waow the other is equivalent to the one.
* Validities are equivalent to and only equivalent to validities.
* Nonconsistencies are equivalent to and only equivalent to nonconsistencies.

#### Conditionals, Biconditionals, Exclusive Alternations, and Sequents
* Conditionals are alternations of the negation of their (antecedent) left component with their (consequent) right component.
* Biconditionals are conjunctions of the distinct conditionals of their components.
* Sequents are conditionals whose antecedent is the conjunction of their antecdent components and whose consequent is the alternation of their consequent components.
* Exclusive alternations are negations of the biconditionals of their components.

* Implication is validity of conditionals.
* Equivalence is validity of biconditionals.
* Nonequivalence is validity of exclusive alternations.

#### Laws of Equivalent Grammatical Categories
Each clause is equivalent to a disjoint or empty clause.
Each chain is equivalent to
* one without conditionals and biconditionals
* one where only nonchain components are negated
* a conjunction of negations of clausal conjunctions
* the alternational dual of the above
* an alternation of disjoint clausal conjunctions (alternational normal form)
* a conjunction fo disjoint clausal alternations (conjunctive normal form)
* a full alternation of unique disjoint clausal conjunctions
* a full conjunction of unique disjoint clausal alternations

#### Laws of Functional Completeness
Each chain is equivalent to a compound compounded only by
* alternative denial
* joint denial
* negation and conjunction
* negation and alternation
* negation and conditionalization

-----

That's all I have the strength to do for now.


## 2025 0505

### 2025 0505 1725
This continues my work on my little lisp from [2025 0504 0140](#2025-0504-0140).

In the last entry it occurred to me that the only distinction between types of items that is implicit in LISP is that between lists and nonlists (where an example of a nonlist is a symbol).
Consequently, I introduced the following definitions:

```
// pairs as lists
, isList = item => !isEmpty(item) && isEmpty(carOf(item))
, singletonListOf = item => consOf(theEmptyPair, item)
, theEmptyList = singletonListOf(the)
, prependHelpOf = (x,y) => isEmpty(x) ? (isList(y) ? cdrOf(y) : y)
 : consOf(carOf(x), prependHelpOf(cdrOf(x),y))
, prependOf = (x,y) => isList(x) ? prependHelpOf(x,y)
 : prependHelpOf(singletonListOf(x),y)
```

The problem with this is that there is a big difference between a binary tree (as an iterated pairing of the empty pair) as a sequence and as a LISP list.
This difference broke the beauty of the symmetry of definitions that take binary trees as stacks on the one hand and sequences on the other.
While I was reading Feferman's "Finitary Inductively Presented Logics" it occurred to me that by distinguishing between sequences and lists I can further factor the underlying arithmetic of binary trees out from the implementation details of my little lisp.

The empty sequence is identical to the empty pair and a pair is the empty sequence when it is the empty pair.
The left part of a pair is called the head of the sequence.
It is also called the first item of the sequence.
The right part of a pair is called the rest of the sequence.
Sequences are constructed from other sequences by concatenation.

The code which corresponds to this specification is as follows.

```
// basic operations on pairs
let theEmptyPair={}
, isEmpty = it => it == theEmptyPair
, pairOf = (it, that) => [it, that]
, leftOf = it => isEmpty(it) ? it : it[0]
, rightOf = it => isEmpty(it) ? it : it[1]

// sequences as pairs
, theEmptySequence = theEmptyPair
, isEmptySequence = isEmpty
, singletonSequenceOf = it => pairOf(it, theEmptySequence)
, headOf = leftOf
, restOf = rightOf
, concatOf = (it, that) => isEmptySequence(it) ? that 
  : pairOf(headOf(it), concatOf(restOf(it), that)) 
```

A sequence whose head is the empty pair is now a LISP list.
Thus a pair is a LISP list if it is not the empty pair and if its left part is.
Promote a sequence to a list by concatenating the singleton sequence whose sole item is the empty pair with the sequence.
So, the empty list is the list of the empty sequence, and the singleton list of an item is the list of the singleton sequence of that item.
The first item of a list is called its 'car' and the list of the rest of the items in the sequence of the list is called the 'cdr'.
Lists shall be constructed by consing rather than concatenating so that nonlist items are automatically promoted to the appropriate singleton list.

```
// LISP lists as sequences
, isList = it => !isEmpty(it) && isEmpty(leftOf(it))
, listOf = sequence => pairOf(theEmptyPair, sequence)
, theEmptyList = listOf(theEmptySequence)
, isEmptyList = it => isList(it) && isEmptySequence(rightOf(it))
, singletonListOf = it => listOf(singletonSequenceOf(it))
, carOf = list => headOf(rightOf(list))
, cdrOf = list => listOf(restOf(rightOf(list)))
, consOf = (it, that) => 
  !isList(it) ? consOf(singletonListOf(it),that) 
  : isList(that) ? listOf(concatOf(rightOf(it),rightOf(that))
  : consOf(it, singletonListOf(that)))
```
In the past, I built stacks directly from pairs, but now I shall build them from LISP lists since they are needed as part of my little lisp and not as part of the arithmetic of pairs.

The empty stack is the empty list, an item is pushed onto a stack by consing it to the stack, the top of the stack is got with car, and the top is dropped from the stack with cdr.
The second from the top of the stack is the top of the drop of that stack.
Finally, the top two items of the stack are "enconsed" by replacing them with the cons of the second from top with the singleton list of the top: this is used mostly for implementing the reader.

```
// stacks as LISP lists
, theEmptyStack = theEmptyList
, isEmptyStack = isEmptyList
, pushOf = consOf
, dropOf = cdrOf
, topOf = carOf
, secondOf = stack => topOf(dropOf(stack))
, enconsOf = stack => pushOf(dropOf(dropOf(stack))
  , consOf(secondOf(stack), topOf(stack)))
```

Is this a good way of doing things though?
The old way clung to the arithmetic of binary trees as iterated ordered pairs of the empty pair.
This way goes back to the earlier method of distinguishing between runic lists and nonrunic lists, or runic symbols and nonrunic symbols.
The difference is that here, symbols are pairs rather than javascript strings.
This doesn't eliminate the problem of dealing with javascript strings, we still need to carry them over into some sequences of symbols.

Has this accomplished anything or is it like when you make a substitution into an algebra problem and end up with 'zero equals zero'.

### 2025 0505 1709 Bryan Magee's "Men of Ideas" and "The Great Philosophers"
My interests in philosophy are largely historic: the literature of philosophers tells us something about what was going on from its compatibility and incompatibility with cultural practices uncovered by archeology.
Bryan Magee (1930-2019) presented two series of television shows which gave the public a instructive look into the work of philosophers past and present:

1. "Men of Ideas" 1978
2. "The Great Philosophers" 1987

The first is composed of the following speakers and topics:
1. Isaiah Berlin: An Introduction to Philosophy
2. Charles Taylor: Marxist Philosophy
3. Herbert Marcuse: Marcuse and the Frankfurt School
4. William Barrett: Martin Heidegger and Modern Existentialism
5. Anthony Quinton: The Two Philosophies of Wittgenstein
6. A. J. Ayer: Logical Positivism and its Legacy
7. Bernard Williams: The Spell of Linguistic Philosophy
8. R. M. Hare: Moral Philosophy
9. Willard Van Orman Quine: The Ideas of Quine
10. John Searle: The Philosophy of Language
11. Noam Chomsky: The Ideas of Chomsky
12. Hilary Putnam: The Philosophy of Science
13. Ronald Dworkin: Philosophy and Politics
14. Iris Murdoch: Philosophy and Literature
15. Ernest Gellner: Philosophy, The Social Context

The second is composed of the following speakers and philosophers:
1. Myles Burnyeat: Plato
2. Martha Nussbaum: Aristotle
3. Anthony Kenny: Medieval Philosophy
4. Bernard Williams: Descartes
5. Anthony Quinton: Spinoza and Leibniz
6. Michael R. Ayers: Locke and Berkeley
7. John Passmore: Hume
8. Geoffrey Warnock: Kant
9. Peter Singer: Hegel and Marx
10. Frederick Copleston: Schopenhauer
11. J. P. Stern: Nietzsche
12. Hubert Dreyfus: Husserl, Heidegger and Modern Existentialism
13. Sidney Morgenbesser: The American Pragmatists
14. A. J. Ayer: Frege, Russell and Modern Logic
15. John Searle: Wittgenstein

Both series can be easily found on YouTube and are well worth a long watch, especially when the speaker is speaking on their own work e.g. Quine.

## 2025 0504 

### 2025 0504 1517
This begins my read of Peter Adamson's "Classical Philosophy: a history of philosophy without gaps" from 2014.

It was recommended to me by R.P. when he noticed that I was reading some [other histories of philosophy](#2025-0415-1915) in addition to Durant's "Story of Philosophy" which I last wrote about in [2025 0430 2324](#2025-0430-2324).

As the preface explains, this is one in a series of books that was written from a podcast on the history of philosophy.
Unlike the other histories of philosophy that I have on hand, this one purports to exhaust the nooks and crannies of philosophers throughout the world, hence the "without gaps".
More than anything, I look forward to the list of references promised: a good reading list can save a lifetime of woe.

Classical philosophy is very narrow in its scope, and the book is broken into the traditional three parts:

1. Early Greek Philosophy
2. Socrates and Plato
3. Aristotle.

The 'Early Greeks' are sometimes called the "presocratics" as if Socrates was the saviour of philosophy.
Each chapter of the part on early greek philosophy deals with a specific philosopher and, in a few places, with groups of them.
The last two parts deal exclusively with Socrates, Plato, and Aristotle.

Andamson says he links up the philosophers to their social environments and that he doesn't see a clean seperation between the concerns of the philosopher and those of the culture of which they were a part.
His claim that philosophy does not occur without affluence or some kind of social saftey net is common enough to be somewhat beyond question, but I do question it.
I submit that the philosophy which has survived to our day, e.g. as a corpus of literature if I am pressed to give a concrete account of philosophy as a part of our social environment, is a collateral effect of any verbal community and that what has survived is simply that.
Said humorously: the absense of evidence is not evidence of absense.

Philosophy is a consequences of the reinforcing effects of a verbal community which practices a kind of self control i.e. the deferred consequences of the reinforcing practices of the verbal community are taken into account by it.
Any culture which comes to teach its people to care for their own survival and the survival of their culture produces philosophers in that there are those sensitive to the contribution that verbal behavior can make to that survival.

When Socrates passes through Plato it is ethics that reigns supreme as " the study of ideal conduct; the highest knowledge, said Socrates, is the knowledge of good and evil, the knowledge of the wisdom of life” [pg. 3 Durant "The Story of Philosophy"].
Here, ideal conduct is that which does more than avoid the threats and damages which doom the practices of a culture: it is that which contributes most to its survival for whatever reason no matter how consistent or inconsistent it may be with prevailing verbal practices.

The history of philosophy is, to me, the history of a conspicuous kind of cultural self control.
It is no surprise that the sciences descend from philosophy and that philosophy can be, in my measure, best taken as a kind of science.
In the past I have suggested that it is the science of smooth discourse in as much as it eeks out where and when verbal behavior works and where and when it does not.

---

The first chapter is "Everything is Full of Gods: Thales".
The Dorians invaded mainland Greece around 1100 BCE and those Athenians who escaped eastward over the Aegean Sea landed under the leadership of Ion.
Born c. 626/623, Thales grew up in the southern Ionian colony of Miletus.
He is seen by many as the first Greek philosopher.

As when I read Durant's description of Athens in the "Context of Plato" in [2025 0420 2247](#2025-0420-2247), Miletus was a nexus of cultural practices from its eastward and westward neighbors.
Thales may have visited Egypt, picked up some Babylonian astronomy, and used it to predict the solar eclipse of 585 BCE.
Reports on astronomical phenomena give us ways of calculating the ordering of various events in addition to the timing of the report itself (assuming such reports are not manufactured post hoc).

As with my conviction that the stories of Socrates are to be taken as I take most of history, that is as folklore, perhaps even that stronger folklore called just 'lore', so are the stories of Thales taken by me.
This outlook is also echoed in the next few pages of Adamson's sketch.

It is dawning on me that for many of these philosophers there are simply a list of basic stories that we can list and cling to without having to commit ourselves to their historic accuracy.
But, my interest in world history (and the role of archeology in setting up and taking down the histories we have inherited) prevents me from resting easy with such foggy commitments.
The urgency of writing on world history continues to press itself upon me.

The main stories of Thales are these
1. born c. 626/623 and died c. 548/545 BCE
2. he predicted a solar eclipse in 595 BCE
3. he may have visited Egypt
4. he was one of the Seven Sages of early Greece
5. his lobby for the unification of Ionian communities against eastern threats, failed
6. he foresaw a large olive harvest and cornered the olive press market
7. Plato says he fell into a hole because he was looking at the sky
8. he broke an unpassable river into two passable rivers
9. he wrote a book on navigation at sea
10. he thought water was of great consequence (in ways that are lost to history). Aristotle says Thales 
    * "beleived the world floats upon water, like a piece of wood"[pg.7 Adamson "Classical Philosophy"]
    * "thought that water was a cosmic *principle*"[pg.7]
    * claimed magnets and amber have a soul
    * "all things are full of gods"

Adamson contrasts Thales with 

1. Homer: *Iliad* and *Odysse* and
2. Hesoid: *Theogony*

as we do scientific behavior with literary behavior.

The chapter ends with a tiny argument assembled by Adamson as an example of a possible argument Thales may have made:

> "everything is full of gods, and I'll show you this using the example of the magnet. It seems to be lifeless, but it must have a soul, because it can initiate motion. So, by extension, you should at least be open to the idea that everything has a soul, which is divine."[pg. 7-8]

He then admits the evidence for this argument is weaker than the argument is.

### 2025 0504 0140
This continues my work on my little lisp from [2025 0502 2048](#2025-0502-2048).

An alternate design decision occurred to me: the only implicit distinction betwen teh part played by ordered pairs in a lisp is that between list and nonlist.
Thus, rather than checking if a pair is a symbol or not, it is more general to check if it is a list or not.
Then the rest of the distinctions can be made however the programmer desires.

This also has the benefit of, in the simple case where an item is either a list or a symbol, of identifying a symbol by its first rune which can never be empty because of the alphabetization adopted.
This also allows space to serve its traditional function without adding greatly to the complexity of the definition of the reader.
It also allows for a more uniform presentation of pairs as lists followed by pairs as stacks.

```
// basic operations on pairs
let theEmptyPair={}
, isEmpty = x => x == theEmptyPair
, consOf = (x,y) => [x,y]
, carOf = x => isEmpty(x) ? x : x[0]
, cdrOf = x => isEmpty(x) ? x : x[1]

// left and right singletons
, enlistOf = x => consOf(x,theEmptyPair)
, enstackOf = x => consOf(theEmptyPair,x)

// pairs as lists
, isList = item => !isEmpty(item) && isEmpty(carOf(item))
, singletonListOf = item => consOf(theEmptyPair, item)
, theEmptyList = singletonListOf(the)
, prependHelpOf = (x,y) => isEmpty(x) ? (isList(y) ? cdrOf(y) : y)
 : consOf(carOf(x), prependHelpOf(cdrOf(x),y))
, prependOf = (x,y) => isList(x) ? prependHelpOf(x,y)
 : prependHelpOf(singletonListOf(x),y)

// pairs as stacks
, theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, pushOf = consOf
, popOf = carOf
, topOf = cdrOf
, secondOf = stack => topOf(popOf(stack))
, drop2 = stack => popOf(popOf(stack)) 
, enpendOf = stack => 
   pushOf(drop2(stack)
   ,prependOf(secondOf(stack)
    ,enlistOf(topOf(stack))))

// reader
, isOpenParen = isEmpty
, isCloseParen = x => !isEmpty(x)&&isEmpty(carOf(x))&&isEmpty(cdrOf(x))
, isSpace = x => !isEmpty(x)&&isEmpty(carOf(x))&&isCloseParen(cdrOf(x))
, readHelperOf = (stack,runes) => isEmpty(runes) ? carOf(topOf(stack))
  : isSpace(carOf(runes)) ?
   ( isEmpty(topOf(stack)) ? readHelperOf(stack,cdrOf(runes)) 
     : readHelperOf(pushOf(enpendOf(stack),theEmptyPair), cdrOf(runes)))
  : isCloseParen(carOf(runes)) ? readHelperOf(enpendOf(stack), cdrOf(runes))
  : isOpenParen(carOf(runes)) ? readHelperOf(pushOf(stack,theEmptyList), cdrOf(runes))
  : readHelperOf(enpendOf(pushOf(stack,carOf(runes))), cdrOf(runes))
, readOf = runes => readHelperOf(theEmptyStack, runes)
```

I'll test this code and figure out the printer next time.

### 2025 0504 0139
Failure of cross reference does not require a referent.

## 2025 0503

### 2025 0503 1757
Today is a Bob Ross and Bosch day.
While I do not seem to enjoy reading Michael Connelly, I do enjoy watching his stories on screen.
They are that perfect drip of mid.
Law & Order is another.

### 2025 0503 1750
It is better to have gone out of the way to note the source of a quote than to go out of your way to find where you sourced it.

### 2025 0503 1531
This continues the work on [my paper on logic](#2025-0412-1422).
It also contains more on my philosophy of logic as well as a careful analysis of Quine's proof that no logical theory of the world is beyond supplementation (which itself contains an alternate presentation of Quine's analysis of denotation as a generalization of Tarski's analysis of truth).

My methods of logic are unique in that they present Quine's predicate functor methods independently of quantificational logic.
Quantificational logic is then constructed from predicate functor logic by way of Quine's predicate abstracts.
The difference between predicate functor logic and quantificational logic is the difference between 'is true' and 'is true of'.

A word on the words 'denote' and 'predicate' from Quine's "From Stimulus to Science":
>"For what follows we must come squarely to terms with 'denote'. Since it is often used interchangeably with 'designate', and a singular term normally designates one and only one object, readers are apt to think of denotation as relating a predicate likewise to a single object, namely the class of all thoe things it is true of, or a property shared by them. In my use of 'denote', as in John Stuart Mill's [Mill, ChapterII $5], a predicate denotes rather each separate thing of which it is true. The class or property is not involved.
>
>For years, to obviate confusion, I avoided 'denote' altogether in favor of 'true of'; but that evasion woudl be impracticable in these pages where denotation is becoming the center of action. Unlike Mill, I still withhold the word from singular terms; they are well served by 'designation'.
>
>A word of caution is in order regarding 'predicate' too. Some logicians take a predicate as a *way* of building a sentence aroudn a singular term or, more concretely, as what Peirce called a *rheme* [Volume 2, paragraph 95], a sentence with blanks in it, these being distinctively marked in the case of a many-place predicate. This version covers, implicitly, the potential output of predicate abstraction or predicate functors. But a predicate in my sense is always an integral word, phrase, or claus, grammatically a noun, adjective, or verb. Some are generated from others by grammatical constructions, notably the relative clause or, formally, predicate abstraction and predicate functors."[pg. 60-61 Quine "From Stimulus to Science"]

One of the virtues of this method is that less is spoken of than ever before.
This has the benefit of eliminating explanations that are ultimately offshoots of predicate logic itself.
In the past, and perhaps even in these passages, there are parts of the world which intruded upon logic and muddied its methods.
The most famous of which is mathematics.
There is much to be recommended from mathematical methods in matheamtical logic, but there is less to say about its ultimate contributions to logic itself.
For example, formalism continues to confuse those unable to seperate logic from mathematical logic as if the formalisms were the subject of logic itself.
One wonders by what methods such formalists make conclusions about their formalisms.

Logic no more has formalisms as its subject matter than it has anything in particular as its items of interest.
Some, like Frege, have mistaken the schematics which fascilitate the articulation of logical principles and practices as part of logic itself rather than as a technical tool for fascilitating an exacting analysis of the same.

> Specifically, Frege mistakes the mathematical metaphor of functions for functional predicates (and he never actually deals with the functional predicates relevant to his analysis of logic e.g. the predicate '{xy: some item is {uv: x is the truth value of u, y is the truth value of v, and u negates v}}' is confused with its purported extension).

Other times, like Frege, logic was so far removed from schematic methods that it bled into thoughts, judgements, senses, meanings, and other possessions of the minds which haunt human bodies and perhaps even that amazing organ we call the brain.

I submit that all these mistakes are not mistakes: they are each a piece in a larger puzzle which has yet to be pictured as a complete whole.
That practice which has done the most to put together all these pieces as if they fit into a larger whole is science.
It is only recently that the science of philosophy was so far divorced from the other sciences.
The philosophical revolt against psychology as a science of mind is justified in that it was easy to critique and its conclusions were in stark contrast with those of our stronger sciences.
Cognative scientists continued to pluck what they could from philosophy and any failures they were confronted with were attributed to weaknesses not in their own science but in some philosophical or logical method: the philosophers were punished for the past transgressions against psychology as the science of mind.

The science of behavior and the philosophy of radical behaviorism mends many such mistakes by revealing not only a sharper picture of what science has assembled but also an inclusive explanation of how such mistakes were so easy to make:

>"Mentalism kept attention away from the external antecedent events which might have explained behavior, by seeming to supply an alternative explanation. Methodological behaviorism did just the reverse: by dealing exclusively with external antecedent events it turned attention away from self-observation and self-knowledge. Radical behaviorism restores some kind of balance. It does not insist upon truth by agreement and can therefore consider events taking place in the private world within the skin. It does not call these events unobservable, and it does not dismiss them as subjective. It simply questions the nature of the object observed and the reliability of those observations.
>
>The position can be stated as follows: what is felt or introspectively observed is not some nonphysical world of consciousness, mind, or mental life but the observer's own body. This does not mean, as I shall show later, that introspection is a kind of physiological research, nor does it mean (and this is the heart of the argument) that what are felt or introspectively observed are the causes of behavior. An organism behaves as it does because of its current structure, but most of this is out of reach of introspection. At the moment we must content ourselves, as the methodological behaviorist insists, with a person's genetic and environmental histories. What are introspectively observed are certain collateral products of those histories."[pg. 18-19 Skinner "About Behaviorism"]

Under this rubric truth quickly distinguishes itself from validity:

>"*Truth*. The truth of a statement of fact is limited by the sources of the behavior of the speaker, the control exerted by the current setting, the effects of similar settings in the past, the effects upon the listener leading to precision or to exaggeration or falsification, adn so on. There is no way in which a verbal description of a setting can be absolutely true. A scientific law is derived from possibly many episodes of this sort, but it is similarly limitied by the repertoires of the scientists involved, The verbal community of the scientist maintains special sanctions in an effort to guarentee validity adn objectivity, but again, there can be no absolute. No deduction from a rule or law can therefore be absolutely true. Absolute truth can be found, if at all, only in rules derived from rules and here it is mere tautology."[pg.150 Skinner "About Behaviorism"]

The word "tautology" descends through Wittgenstein's failed attempt to reduced quantificational logic to a kind of never ending truth-functional logic (sometimes unhappily called "sentence logic").
The popularity of the word is unfortunate because it reinforces defective distinctions between clear cut definitions of validity which are very rarely mentioned and yet when mastered frequently simplify the methods of logic.
They are described by Quine as follows

>"It was remarked at the end of Chapter 7 that validity may be ascribed not only to truth-functional schemata but also, by extension, to the sentences whose forms those schemata depict; but that it is well then to add the qualifier 'truth-functional'. COrrespondingly a sentence obtainable by substitution in a valid quantificational schema is *quantificationally* valid. Such a sentence is true, or true for all values of its free variables. But it may or may not be truth-functionally valid; its truth may depend solely on its truth functional structure, or it may depend partially on how the quantifiers are arranged.
>
>We may also note an intermediate grade, monadic validity. A sentence is quantificationally valid if it can be got by substitution in a valid quantificational schema; it is monadically valid, more particularly, if it can be got by substitution in a valid quantificational schema which is monadic; and it is truth functionally valid if it can be got by substitution in a valid truth-functional schema." [pg. 176 Quine "Methods of Logic 4th Edition"].

A warning for those who are tempted, like Frege or Russell, to occasionally take schemata as the items of logic: all this talk of substition into valid schema to obtain valid sentences can be skipped by resorting directly to sentences and their grammatical structure.
The grammatical structure with which validity is concerned is specifically designed to avoid the miscarriage of conclusions.

> "Sentences have the same grammatical structure when they are interconvertible by lexical substitutions. Our definition of logical truth, then, can also be put thus: *a logical truth is a truth that cannot be turned false by substituting for lexicon. When for its lexical elements we substitute any other strings belonging to the same grammatical categories."[pg. 58 Quine "Philosophy of Logic Second Edition"]

There are two things of note here:

1. what Quine calls 'logicl truth', and what others have called the same, is what is called 'quantificationally valid' in Quine's "Methods of Logic 4th edition"; and 
2. substitution no longer leads from schema to sentences but from sentences to sentences by way of the lexicon of the langauge.

Whatever promise there is to this grammatical definition of quantificaitonal validity is lost when the grammatical part of variables is reflected upon as Quine does from having read Gilbert Harman's 1971 review in *Metaphilosophy*.
Quine's corrective is to contemplate only substitution for predicates and not for variables which are otherwise part of the lexicon of logical languages (Harman suggests taking variables as belonging to atomic grammatical elements which are more like logical connectives than items of the langauges lexicon).

Such restricted substitution no longer leads from one sentence to an other of the same grammatical structure: they may differ in the variables which they contain and no substitution for predicates alone can change one variable into an other.
There are further troubles that, when confronted, lead to a stronger definition of quantificational validity by grammatical method:

> "Our definition of logical truth in terms of substitution of lexicaon confronts also another and graver difficulty: it appeals only to substitution of predicates for predicates, as against substitution of sentences for simple sentences. We saw early in this chapter how the sentential approach coudl issue in a stronger, narrower concept of logical truth, screening out some cases that woudl have slipped through if only predicate substitution had been called for. IT can even be shown that the version in terms of predicate substitution will inevitably be too weak, as long as our stock of predicates is finite [This was pointed out to me by Mark L. Wilson]. The natural remedy, then is to exploit the indefiniteness of our category of predicates: to admit substitution not only of the predicates in some imagined list, but also of any predicates that might be added. Thus adjusted, then, the abstract version rusn as follows: *a logical truth is a sentence that cannot be turned false by substitution for lexicon, even under supplementation of lexical resources*."[pg. 59 Quine "Philosophy of Logic 2nd Edition"]

Here then is where predicate functor logic reigns supreme and reveals the intimate relationship between validity (or what Quine and so many others speak of as "logical truth"), grammar, and truth.
Since the only items of the lexicon of a predicate functor logic are predicates then there is no question of how substitution is to covey one sentence to an other with the same grammatical structure.
It is also clear that supplementation of lexicon is required from contemplation of a lexicon of only one place predicates as Quine did in "Word and Object":

> "But may we not still aspire to the discovery of some fundamental lexicon on the basis of which all traits and states of everything could in principle be formulated? No; we can prove that opennes is unavoidable, as long anyway as the predicates of a theory are included as items of the theory.
> 
>For, let S1, S2, ... be the sentences in the notation of a theory that have 'x' as their sole free variable. Each of them is, for each object of the universe as value of 'x', true or false of that object; hence each of them, being also an object of the universe, is true or false of itself. We easily show that no general term definiable in the theory is true of exactly those of S1, S2, ... that are false of themselves. (For, if 'F' were such a term, then 'Fx' would be true of itself if and only if false of itself. [This argument is in principle Cantor's. The form I have given it is reminiscent also fo Grelling's paradox, and the use made of it is reminiscent of Tarski.]) Such a term can be added, irreducibly supplementing the theory."[pg. 231-232 Quine "Word and Object"]

Note, I have changed a few words in the above quote to more closely bring what Quine said to bear on terminology used here.
It falls well within Quine's principles of paraphrasing.

This argument puzzled me at first, and as a result of the following analysis I was able to produce an even more surprising argument that directly reveals the immediate limitations on any consistent theory of denotation.
It also brings together the principles of prediate abstraction, predicate concretion, disquotation and (to my surprise) the deduction theorem while also just giving the exact predicate that is missing.

Under the assumption that among the items of the theory are the predicates themselves it is possible, following Quine's generalization of Tarski's analysis of truth to denotation (which I shall state), to define a predicate which, in anticipation of the definition to follow, shall be written out as 'degree 0 denotes'.
It is defined just as Quine specifies on pg.63 of "From Stimulus to Science" but with predicate functors as predicate abstracts in a quantificational logic as described in [An Incomplete Sketch of My Philosophy of Logic](#2025-0422-2322-an-incomplete-sketch-of-my-philosophy-of-logic)):

1. Where 'F' designates an n place primitive predicate define "{..x : 'F' degree 0 denotes ..x}" for 'F',
2. Where 'F' and 'G' designate n place predicates define
   * "{..x : 'not F' degree 0 denotes ..x}" for "{..x : not ('F' degree 0 denotes ..x)}"
   * "{..x : 'F and G' degree 0 denotes ..x}" for "{..x : F degree 0 denotes ..x} and {..x : G degree 0 denotes ..x}"
   * "{..y : 'some F' degree 0 denotes ..y}" for "{..y : some item is {z : 'F' degree 0 denotes ..yz}"
   * "{..xy : 'pad F' degree 0 denotes ..xy}" for "{..xy: 'F' degree 0 denotes ..x}"
   * "{..uv : 'refl F' degree 0 denotes ..uv}" for "{..uv: 'F' degree 0 denotes .uvv}"
   * "{a..cb: 'perm F' degree 0 denotes a..cb}" for "{a..cb: 'F' degree 0 denotes ab..c}".

> A detailed explanation of the construction of predicates which permit the manipulation of quotations as above can be found in the last chapter of Quine's "Mathematical Logic" on his theory of protosyntax.
> Feferman's Finitary Inductively Presented Logics are perhaps a simpler alternative but the presentation does not address this construction in particular.

Now, a predicate coextensive with 'degree 0 denotes' as defined above (rather than taking it as an abbreviation as has been done with the phrase 'for' these are changed to sentences of coextension) can not belong to such a theory i.e. such a predicate must be added as a primtive to "irreducibly supplement the theory" (the case where it is already among the primitives is covered after this).
For, assume it does, then so does the predicate '{x: not (x degree 0 denotes x)}'.

Suppose, '{x: not (x degree 0 denotes x)}' degree 0 denotes  '{x: not (x degree 0 denotes x)}'.
By disquotation (which is the defining principle given above), {x: not (x degree 0 denotes x)}'{x: not (x degree 0 denotes x)}'.
By concretion, it is not the case that '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}'.
Hence, by the deduction theorem, '{x: not (x degree 0 denotes x)}' degree 0 denotes  '{x: not (x degree 0 denotes x)}' only if it is not the case that '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}'.

Now, suppose it is not the case that '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}'.
By disquotation, it is not the case that {x: not (x degree 0 denotes x}'{x: not (x degree 0 denotes x)}'.
By concretion, it is not the case that it is not the case that (yes, the same phrase is repeated twice there) '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}' i.e. '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}'.
Hence, by the deduction theorem, it is not the case that '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}' only if '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}' i.e. '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}' if it is not the case that '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}'.

Therefore, '{x: not (x degree 0 denotes x)}' degree 0 denotes  '{x: not (x degree 0 denotes x)}' if and only if it is not the case that '{x : not (x degree 0 denotes x)}' degree 0 denotes '{x: not (x degree 0 denotes x)}', a contradiction.

Though that mays seem like a mouthfull of nit picking steps, it gives each principle that, step by step, permits the given contradiction.

The above contradiction followed by assuming that 'degree 0 denotes' was not a part of the lexicon of the theory but was coextensive with some compound constructed from the lexicon.
What's interesting is that there is nothing wrong with supposing 'degree 0 denotes' is a basic predicate, now added to the theory, which is defined as above (that is each instance of the corresponding coextensive schema) with respect to each OTHER basic predicate except for itself!
Read that very carefully because it is very slipery.

So, in that case, the case where you have 'degree 0 denotes' as stipulated, we are left again where Quine's argument starts, but now, we contemplate a new predicate 'degree 1 denotes' which is defined from the basic predicates, one of which is 'degree 0 denotes', and which, by the same argument, can not be coextensive with any compound of the basic predicates, one of which is 'degree 0 denotes'.
Thus, 'degree 1 denotes' can be added as a basic predicate defined from the other basic predicates by the inductive definition above.

Now, for any theory, there is a finite lexicon, and thus there is some smallest 'n' for which 'degree n denotes' does not belong to the lexicon i.e. there is no end to how a theory might be supplemented.

Quine's argument involving one place predicate glosses over the problem of denotation, as Quine frequently did, with the phrase "is true of", but it is precisely on the analysis of this predicate that Quine's argument (and Frege's work, for 'is true' is a degenerate case of 'is true of' i.e. 'denotes') hinges.

When I started this not I had an aim of typing up the rest of my sketch of my paper on logic i.e. the concrete consequences of arguments like those above.
That did not happen because now I'm going to switch to work on my little lisp before I forget what occurred to me with respect to it earlier today.


## 2025 0502

### 2025 0502 2048
This continues my work on my little lisp from [2025 0501 1754](#2025-0501-1754).

I made great progress in the last entry and between the end of the last note on this project and now I was able to solve the problem of going back and forth between lisp and forth.
I wish there was an easy way to write while sleepy or tired or whatever happened last night while I failed to put together the pieces as in the simple example that occurred to me while I was watching a show called "The Sticky".
It is common for me to work until exhaustion.
Sadly, the exhaustion which comes from reading, writing, and thinking does not look like it does when it is from lifting.
You are not a sweaty mess nor are your muscles fatigued in that way that makes it hard to walk up or down steps or get out of a chair (which, even writing about them brings that feeling too me).

My lingering on this problem of exhaustion is important: it is probably the biggest problem to solve when you must solve so many problems before you die.
There is no clear indication that I shall do all that I must before I die and the bottleneck to most of what I must do is largely a matter of dealing with the pains of weak thinking.
Perhaps the biggest part of this big problem is encapsulated in one of my most important principles: "Stop at the sign."

It can be hard to notice when thinking has gone weak.
This seems to be the result of prescientific practices around thinking, reading, and writing generally.
Rather than seeing a schedule of reinforcement that maintains the behaviors that have led to the most profound of scientific and logically discoveries, prescientific peoples see the individual as a willing agent or some other such mentalistic haunties of the brain by the mind.
But, if I waste more time now solving this problem then I will not get out the solutions to problems that I have already solved.
It is difficult to say both that "here we have the biggest problem and the biggest bottleneck to solving problems" and then "but now I will leave that to work on lesser problems."
It is precisely in being able to say both these things that schedules of reinforcement resolve what appears to be a contradiction.

Here is an exact replica of what occurred to me while watching "The Sticky" earlier today and which leads to the radical simplification of any reader for any lisp like language while also explaining exactly how it is that lisp is a forth:

```
((a)(bc))
0 | (a)(bc))
0,0 | a)(bc))
0,(a,0) | )(bc))
((a,0),0) | (bc))
((a,0),0),0 | bc))
((a,0),0),(b,0) | c))
((a,0),0),(b,(c,0)) | ))
((a,0),((b,(c,0)),0)) | )
(((a,0),((b,(c,0)),0)),0) |
```

Believe it or not, this shows that my sleepy claim that a simple alphabetization radically simplifies the reader of any language that purports to be some sort of parenthetical or tree like form, but which can be immediately interpreted as a string of stack operations.
This exactly generalizes the methods from [Bit Strings and Binary Trees](#2025-0413-1513-bit-strings-and-binary-trees).

Recall that the reader was previously composed of steps involving lexing and parsing: this new method goes straight from a list of runes to the list they purport to designate.
All of that complexity is eliminated by a change alphabatization (I've gone ahead and reproduced all the code for the current state of this project).

Ooops!
Once I opened the tab to bring the code over here I started coding and totally forgot about writing out what I was doing here.

Here's where I'm starting from:
```
let run=code=>console.log(code,'\n',eval(code)) // for examples

// basic operations on pairs
let run=code=>console.log(code,'\n',eval(code)) // for examples

// basic operations on pairs
let theEmptyPair={}
, isEmpty = x => x == theEmptyPair
, consOf = (x,y) => [x,y]
, carOf = x => isEmpty(x) ? x : x[0]
, cdrOf = x => isEmpty(x) ? x : x[1]

// left and right singletons
, singletonListOf = x => consOf(x,theEmptyPair)
, singletonStackOf = x => consOf(theEmptyPair,x)

// to and from tallies
, tallyOf = n => 
  n>0 ? singletonStackOf(tallyOf(n-1)) : theEmptyPair
, countOf = x => isEmpty(x) ? 0 : 1 + countOf(cdrOf(x))

// the alphabet
, abc =[...'()0123456789abcdefghijklmnopqrstuvwxyz ']
, alphabeticalIndexOf = letter => abc.indexOf(letter)
, alphabeticalLetterOf = index => abc[index]

// to and from rune
, runeOf = letter => tallyOf(alphabeticalIndexOf(letter))
, letterOf = x => alphabeticalLetterOf(countOf(x))

// basic string operations
, theEmptyLetter=''
, isEmptyLetter = x => x == theEmptyLetter
, concatenationOf = (...strings) => 
   strings.length ? strings.shift() + concatenationOf(...strings) : theEmptyLetter
, firstLetterOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters[0]
, restLettersOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters.slice(1)

// to and from runes
, runesOf = letters => 
   letters.length ? consOf(runeOf(firstLetterOf(letters))
    , runesOf(restLettersOf(letters)))
   : theEmptyPair
, lettersOf = x =>
   isEmpty(x) ? theEmptyLetter
   : concatenationOf(letterOf(carOf(x)),lettersOf(cdrOf(x)))

// prepending pairs as lists
, prependedListOf = (x,y) =>
  isEmpty(x) ? y : consOf(carOf(x), prependedListOf(cdrOf(x),y))

// pairs as stacks
, theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, pushOf = consOf
, popOf = carOf
, topOf = cdrOf
, secondOf = stack => topOf(popOf(stack))
, drop2 = stack => popOf(popOf(stack)) 
, enpendOf = stack => 
   pushOf(drop2(stack)
   ,prependedListOf(secondOf(stack)
    ,singletonListOf(topOf(stack))))

// recursive definition of identity
, id = (x,y) =>
  (isEmpty(x) && isEmpty(y))
  || (!(isEmpty(x) || isEmpty(y))
     && id(carOf(x),carOf(y))
     && id(cdrOf(x),cdrOf(y)))

// reader
, isOpenParen = x => id(runeOf('('),x)
, isCloseParen = x => id(runeOf(')'),x)
, isSpace = x => id(runeOf(' '),x)
, readHelperOf = (stack,runes) =>
  isEmpty(runes) ? carOf(topOf(stack))
  : isCloseParen(carOf(runes)) ? readHelperOf(enpendOf(stack), cdrOf(runes))
  : isOpenParen(carOf(runes)) ? readHelperOf(pushOf(stack,theEmptyPair), cdrOf(runes))
  : isSpace(carOf(runes)) ? readHelperOf(stack,cdrOf(runes))
  : readHelperOf(enpendOf(pushOf(stack,carOf(runes))), cdrOf(runes))
, readOf = letters => readHelperOf(theEmptyStack, runesOf(letters))

// printer
, parentheticalOf = x => isEmpty(x) ? runesOf('()') : prependedListOf(runesOf('('),prependedListOf(x,runesOf(')')))
, printListOf = list =>
 isEmpty(list) ? theEmptyPair
 : prependedListOf(printHelperOf(carOf(list)),printListOf(cdrOf(list)))
, printHelperOf = item =>
 isEmpty(item) ? runesOf('()')
 : isEmpty(carOf(item))? parentheticalOf(prependedListOf(runesOf('()'),cdrOf(item)))
 : parentheticalOf(printListOf(item))
, printOf = item => lettersOf(printHelperOf(item));
```

Everything is simpler than ever, but it is no longer clear that the specific alphabetization is as important as I thought, but there is still some opportunity.

I did so much that it is hard to go back and write through it.
Since there is still more that I must do I'm going to just start writing from here and hopefully get back to where I started.

It looks like the reader and the printer are still more complex than they need to be and that they can be simplified if I can figure out the appropriate stack operations.
As it stands the reader interprets each rune as a stack operation:

* if the first rune is an open parenthesis it pushes an empty pair onto the stack and reads the rest of the runes with that stack
* if the first rune is a close parenthesis it "enpends" the top two items on the stack and then goes on
* if the first rune is a space then it does nothing and goes on
* otherwise it pushes the first rune onto the stack and "enpends" it.

Enpending is something that happens often but which doesn't often get its own name: it's when you enlist an item, i.e. put it into a singleton list, and then append it to some other list of items.
There are distinctions for operations like this in the J programming language, but that is for another time.

Well this is one heck of a fragmentary note.

Surprisingly, I was able to entirely defer the introduction of the identity function and finally push off all the work that goes into external reading and printing so that everything comes together quickly.
Here it does end up being helpful to have a specific alphabetization: it is key to the simplicity of the internal reader and printer.
This is all quite exciting and there is so much more to come.
```
// basic operations on pairs
let theEmptyPair={}
, isEmpty = x => x == theEmptyPair
, consOf = (x,y) => [x,y]
, carOf = x => isEmpty(x) ? x : x[0]
, cdrOf = x => isEmpty(x) ? x : x[1]

// left and right singletons
, singletonListOf = x => consOf(x,theEmptyPair)
, singletonStackOf = x => consOf(theEmptyPair,x)

// prepending pairs as lists
, prependOf = (x,y) =>
  isEmpty(x) ? y : consOf(carOf(x), prependOf(cdrOf(x),y))

// pairs as stacks
, theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, pushOf = consOf
, popOf = carOf
, topOf = cdrOf
, secondOf = stack => topOf(popOf(stack))
, drop2 = stack => popOf(popOf(stack)) 
, enpendOf = stack => 
   pushOf(drop2(stack)
   ,prependOf(secondOf(stack)
    ,singletonListOf(topOf(stack))))

// reader
, isOpenParen = isEmpty
, isCloseParen = x => !isEmpty(x)&&isEmpty(carOf(x))&&isEmpty(cdrOf(x))
, isSpace = x => !isEmpty(x)&&isEmpty(carOf(x))&&isCloseParen(cdrOf(x))
, readHelperOf = (stack,runes) => isEmpty(runes) ? carOf(topOf(stack))
  : isSpace(carOf(runes)) ? readHelperOf(stack,cdrOf(runes))
  : isCloseParen(carOf(runes)) ? readHelperOf(enpendOf(stack), cdrOf(runes))
  : isOpenParen(carOf(runes)) ? readHelperOf(pushOf(stack,theEmptyPair), cdrOf(runes))
  : readHelperOf(enpendOf(pushOf(stack,carOf(runes))), cdrOf(runes))
, readOf = runes => readHelperOf(theEmptyStack, runes)

// printer
, theOpenParen = theEmptyPair
, theCloseParen = consOf(theEmptyPair,theEmptyPair)
, isSymbol = item => !isEmpty(item)&&isEmpty(carOf(item))
, theSymbolPair = theEmptyPair
, parenOf = runes => consOf(theOpenParen,prependOf(runes,singletonListOf(theCloseParen)))
, printListOf = list => isEmpty(list) ? theEmptyPair
 : prependOf(printOf(carOf(list)),printListOf(cdrOf(list)))
, printOf = item => isSymbol(item)? parenOf(prependOf(printOf(theSymbolPair),cdrOf(item)))
 : parenOf(printListOf(item))

// the alphabet
, abc =[...'() 0123456789abcdefghijklmnopqrstuvwxyz']
, alphabeticalIndexOf = letter => abc.indexOf(letter)
, alphabeticalLetterOf = index => abc[index]

// to and from tallies
, tallyOf = n => 
  n>0 ? singletonStackOf(tallyOf(n-1)) : theEmptyPair
, countOf = x => isEmpty(x) ? 0 : 1 + countOf(cdrOf(x))

// to and from rune
, runeOf = letter => tallyOf(alphabeticalIndexOf(letter))
, letterOf = x => alphabeticalLetterOf(countOf(x))

// basic javascript string operations
, theEmptyLetter=''
, isEmptyLetter = x => x == theEmptyLetter
, concatenationOf = (...strings) => 
   strings.length ? strings.shift() + concatenationOf(...strings) : theEmptyLetter
, firstLetterOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters[0]
, restLettersOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters.slice(1)

// to and from runes
, runesOf = letters => 
   letters.length ? consOf(runeOf(firstLetterOf(letters))
    , runesOf(restLettersOf(letters)))
   : theEmptyPair
, lettersOf = x =>
   isEmpty(x) ? theEmptyLetter
   : concatenationOf(letterOf(carOf(x)),lettersOf(cdrOf(x)))

// external read and print
, read = letters => readOf(runesOf(letters))
, print = item => lettersOf(printOf(item))

// recursive definition of identity
, id = (x,y) =>
  (isEmpty(x) && isEmpty(y))
  || (!(isEmpty(x) || isEmpty(y))
     && id(carOf(x),carOf(y))
     && id(cdrOf(x),cdrOf(y)));
```


## 2025 0501

### 2025 0501 2151
For reasons that escape me I've had a few different youtube playlists queued up in the background the past few days.
The first, and perhaps the most important is Groucho Marx's "You Bet Your Life".
There is nothing like Groucho.
He is certinaly one of my idols in that I mention him as often to those near me as I do Skinner and Quine to everyone else.

* [You Bet Your Life: Complete Episodes by Air Date](https://youtube.com/playlist?list=PLHaioNpr_GDbvsTj_taM-jO6C1658N1PC&si=gvhBN2wJP49ioY4W)

I don't know how accurate the title is.
Next, of all things, Marathon lore:

* [Marathon - Reviews, History & Lore (Bungie)](https://youtube.com/playlist?list=PLT1yGPRy2oPrXlQ730iOZ32eMI_-FhXS3&si=Tv62twCdgZVX8Mi4)

Last, and most certainly least, a playthrough of an old game I recall from my childhood:

* [Let's Play Lands of Lore Guardians of Destiny](https://youtube.com/playlist?list=PL2sIyFzFMWstwPV2XKG_uwoNolVaOSX0s&si=9pRtEwUv2As8rzuO)

Such things are part of modern folklore, though they very often lack the warmth that I associate with well worn folklore.
Perhaps such stories from games are nothing more than the folklore of *my* life.

Sometimes I need silence in order to solve problems.
Other times I need there to be something going on in the background.
I need to at least see that someone else at some other time was making progress on something they set out to do even if I might not be at this exact moment.

### 2025 0501 1754
This continues the work on my little lisp from [2025 0426 1851](#2025-0426-1851-purifying-my-little-lisp).

The basic definitions are now

```
let run=code=>console.log(code,'\n',eval(code)) // for examples
// basic operations on pairs
let theEmptyPair={}
, isEmpty = x => x == theEmptyPair
, consOf = (x,y) => [x,y]
, carOf = x => isEmpty(x) ? x : x[0]
, cdrOf = x => isEmpty(x) ? x : x[1]
// recursive definition of identity
, id = (x,y) =>
  (isEmpty(x) && isEmpty(y))
  || (!(isEmpty(x) || isEmpty(y))
     && id(carOf(x),carOf(y))
     && id(cdrOf(x),cdrOf(y)))
// left and right singletons
, singletonListOf = x => consOf(x,theEmptyPair)
, singletonStackOf = x => consOf(theEmptyPair,x)
// to and from tallies
, tallyOf = n => 
  n>0 ? singletonStackOf(tallyOf(n-1)) : theEmptyPair
, countOf = x => isEmpty(x) ? 0 : 1 + countOf(cdrOf(x))
// the alphabet
, abc =[...'0123456789abcdefghijklmnopqrstuvwxyz ()']
, alphabeticalIndexOf = letter => abc.indexOf(letter)
, alphabeticalLetterOf = index => abc[index]
// basic string operations
, theEmptyLetter=''
, isEmptyLetter = x => x == theEmptyLetter
, concatenationOf = (...strings) => 
   strings.length ? strings.shift() + concatenationOf(...strings) : theEmptyLetter
, firstLetterOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters[0]
, restLettersOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters.slice(1)
// to and from rune
, runeOf = letter => tallyOf(alphabeticalIndexOf(letter))
, letterOf = x => alphabeticalLetterOf(countOf(x))
// to and from runes
, runesOf = letters => 
   letters.length ? consOf(runeOf(firstLetterOf(letters))
    , runesOf(restLettersOf(letters)))
   : theEmptyPair
, lettersOf = x =>
   isEmpty(x) ? theEmptyLetter
   : concatenationOf(letterOf(carOf(x)),lettersOf(cdrOf(x)))
// prepending and appending pairs as lists
, prependedListOf = (x,y) =>
  isEmpty(x) ? y : consOf(carOf(x), prependedListOf(cdrOf(x),y))
, appendedListOf = (x,y) => prependedListOf(y,x);
```

Symbols, which I once took as native javascript strings, are now to be pairs whose left part is the empty pair:
```
let isSymbol = x => isEmpty(carOf(x));
```
Then, as a matter merely of convention, the right part of a symbol is taken as runes which spell out the name of the symbol.
Recall, way back in the first entry on this line of work, I mentioned that a candidate way of defining atoms was by making them identical to their left part.
After that I asked what of the right part?
To which I answered: this might help us to more easily, as humans, differentiate between symbols.
But, note, here, symbols are not atoms in this sense, they are just a special case of ordered pair.

This method of distinguishing symbols from nonsymbols is itself a convention imposed upon the particular designs of my pure little lisp: an other may come along and devise a method which behaves nothing like the symbols of LISPs past.
Such an eample is not as far off as it might seem to those wedded to LISP e.g. I've already mentioned in [my list of links on concatenative languages](#2025-0417-2020) that there is a calculus of trees which purports to do more with less from a foundation of ordered pairs as that contemplated here by me.
That, like so many other things that I mention among these notes, is for another time.

The introduction of symbols as special pairs interferes in one place with the broader convention of taking pairs as lists.
What of the difference between a pair playing the part of a list whose first item (its left part) is itself the empty list?
It is by answering this question that we find the first use of symbols: the symbol symbol.
```
let theSymbolSymbol = consOf(theEmptyPair, theEmptyPair);
```
But, "the symbol symbol" is a mouthfull and there is a name I am more fond of which is short and which came to me originally through my study of the J programming language:
```
let ace = theSymbolSymbol;
```
A more incremental presentation of the above and one that rids the design of its commitment to a particular pair as the symbol pair is as follows.

First, add a letter for the symbol pair to the alphabet.
Here I pick '$' as that letter because it resembles the letter 'S' in 'Symbol':
```
abc =[...'0123456789abcdefghijklmnopqrstuvwxyz ($)'];
```
Then the symbol rune *is* the symbol pair
```
let theSymbolRune = runeOf('$');
```
or just
```
let $ = theSymbolRune;
```
so that the symbol symbol, 'ace' for short, is
```
ace = consOf($,$); 
```
But, this too breaks the convention that the right part of a symbol is to be taken as spelling out the name of a particular occurrence of the symbol.
This can be remedied directly with
```
ace = consOf($,singletonListOf($));
```
This is unsatisfactory for many reasons, the least of which is that it makes it hard to see the convention being enforced (i.e. that 'ace' designates the symbol symbol and that the name of the symbol symbol is designated by "runesOf('$')".
So it is that there is one last alternation that can be made now and which not only satisfies this convention but eases reading and printing: it is taking the symbol pair as the runes of the item designated by "'$'" (and yes the double quotes enclosing the single quoted occurrence of '$' is needed).
```
let theSymbolPair = runesOf('$');
isSymbol = pair => id(pair, theSymbolPair);
theSymbolSymbol = consOf(theSymbolPair, theSymbolPair);
$ = theSymbolPair;
ace = theSymbolSymbol;
```
There is some temptation to designate the symbol symbol by '$$' but I'll avoid that for now.

Surprisingly, the way in which this convention aids in reading and printing imposes its own convention on the grammar of programs in this little lisp of mine.
It also happens to be the grammar that helps to bring LISP and FORTH closer together: they are already extremely close because LISP is a FORTH that is ashamed of its stacks.

The convention is this: programs are space seperated spellings of symbols.
Said another way: the names of the symbols of a program are spelled out by whatever is seperated by spaces.
There is more, because we have said that the symbol symbol is marked as a symbol by having the symbol pair as its left part, we have introduced the convention of marking the part of speech played by a list with its left part.
It shall be shown that the distinctino between symbols and lists is enough to mark out any grammatical distinctions that may later be used to the benefit of the programmer.

But, this is not quite right either because it is the job of the reader to build lists and symbols.
Thus, the program grammar is actually the result of further contemplation on what the reader must do and how it must do it.

Since this is a LISP and not a FORTH, parenthesis play a special part in the reader from the beginning.
The reader can be taken as a tiny interpreter/compiler (there is no distinction really between these things other than how we speak of them when we speak of them asperationally).
It reads a list of runes and builds a symbol or a list of symbols or a list each item of which is either a symbol or a list of symbols, and so on, from it.

The commands that go to the reader are the ones that build the pairs upon which the lisp works, whether it directly evaluates them or alters them by way of some grammatical rule of transformation (commonly known as a macro).
As said before, the commands are seperated by spaces, so there is actually another even tinier interpreter/compiler that sits between a list of runes and the reader (that is when you discard the native javascript transformation from a javascript string into a list of runes!)

This tinier interpreter/compiler is called a tokenizer or a lexer.
But, this is only appropriate when it is spoken of in relation to some other language.
Otherwise, it is just another programming language that takes a pair as a list of runes and builds a list of lists of runes from it!
The name "lexer" is the most suggestive of what is to be done: it is to break a complex compound into its lexical atoms.
The lexicon of a language is what you get if you grab that language by its grammatical trees and shake it until all its words fall out.

The alphabet of a language is what you get when you grab its lexemes, i.e. items of its lexicon, and shake them until its letters fall out.
It is from thes letters, or runes, that the lexer builds lexemes.
So, to recap, the lexer builds lexemes, and the rest of the reader builds symbols, lists of symbols, and so on from the list of lexemes made by the lexer.

So, on to the lexer!
Our convention of spelling lexemes by seperating their spellings with spaces divides the lexer into two operations: skipping spaces and making words from their spellings.
First, a method of detecting spaces:
```
let theSpaceRune = runeOf(' ')
, isSpace = x => id(x, theSpaceRune);
```
Next a way of taking a list of runes and trimming off any spaces from the front.
```
let trimSpacesOf = x => isEmpty(x) ? x : isSpace(carOf(x)) ? trimSpacesOf(cdrOf(x)) : x;
```
This is the easy step.
The hard step is making words from their spellings and putting them all together as a list of runes.
There are two paths that are before us: one where stacks are formally introduced or one where continuations are formally introduced.
The simpler path is stacks, but they are both the same thing (continuations are stacks in disguise, and this part of the "lisp is a forth ashamed of its stacks" thing I said earlier).
Also, stacks are the best, better even than lists (which are just the mirror of stacks, or stacks in reverse, which is itself foreshadowing).

The empty pair is taken as the empty stack, pushing an item onto a stack is making an ordered pair whose left part is the old stack and whose right part is the new top of stack, popping the empty stack gives the empty stack and popping a nonempty stack gives its left part, and, finally, peeking, or looking at the top of a stack returns its right part or the empty pair if you peek the empty pair.
```
let theEmptyStack = theEmptyPair
, isEmptyStack = isEmpty
, pushOf = consOf
, popOf = carOf
, topOf = cdrOf;
```
Notice, these stack operations are all just different names for the same old operations on pairs that were introduced in the beginning.
Stacks are simply a different perspective on pairs, lists are another, and there are others still.
Ordered pairs are probably the most important abstract objets that humans have introduced into the world.

The plan for the lexer is to make a function that takes three arguments:

1. a stack of lexemes accumulated thus far
2. the lexeme currently being constructed
3. the list of runes being lexed.

Thus the lexer will just be a way of setting up the helper function with an empty stack, an empty list, and the list of runes to be lexed.
The first step is to trim off any spaces that happen to be at the front of the list.
So, the helper can assume that the list of runes to be lexed is already trimmed: this must now be enforced by the design of the helper itself.
One option is to make a helper for the helper, one that does nothing more than take the same three arguments as the helper and simply trim the third and then pass them onto the helper.
This seems excessive and can be accomplished by just making sure to trim any initial spaces from the third argument once a space is confronted.
At the same time, confronting a space is a command that the current lexeme under construction is no longer under construction and can be pushed onto the stack of lexemes lexed thus far.
All that remains is to take the first rune of the list to be lexed, check if it is a space or not, if it is not then append it to the lexeme under construction, and if it is then push the constructed lexeme onto the stack of lexemes lexed and trim the list of runes to be lexed before passing everything onto the helper again.

Oh!
It just occurred to me that I do not even need to worry about all that stack stuff!
I can just append lexemes to the list of lexemes because I already gave the definitions for append and prepending.
So, forget all that stack stuff for now.

```
let lexerHelperOf = (lexemes, lexeme, runes) =>
 isEmpty(runes) ? (
  isEmtpy(lexeme) ? lexemes 
  : prependedListOf(lexemes,singletonListOf(lexeme)))
 : isSpace(carOf(runes)) ? 
  lexerHelperOf(prependedListOf(lexemes,singletonListOf(lexeme))
  ,theEmptyPair,trimSpacesOf(runes))
 : lexerHelperOf(lexemes
  ,prependedListOf(lexeme,singletonListOf(carOf(runes))),cdrOf(runes))
, lexOf = runes => 
 lexerHelperOf(theEmptyPair,theEmptyPair,trimSpacesOf(runes));
```

The rest of the reader is usually called the parser for very good reason:

>"parse(v.): 1550s, in grammar, "to state the part of speech of a word or the words in a sentence," a verbal use of Middle English pars (n.) "part of speech" (c. 1300), from Old French pars, plural of part "a part," from Latin pars "a part, piece" (from PIE root *pere- (2) "to grant, allot") in the school question, Quae pars orationis? "What part of speech?" Transferred (non-grammatical) use is by 1788. Pars also was a common plural of part (n.) in early Middle English. Related: Parsed; parsing." <https://www.etymonline.com/word/parse>

From the list of lexemes the parser deduces the part of speach to which each lexeme belongs and, from doing so, constructs the grammatical tree from which they fell.
There are only two parts of speech for the items of my little LISP: symbol or list.
So the parser either builds lists or symbols based on the lexemes it is fed, one by one.
An open parenthesis starts the construction of a list, a closing parenthesis ends the construction of a list under construction, and everything else is a symbol.
The conventions established so far make it so that the any pair whose left part is not the symbol pair is a list.
This can be made overt rather than covert by introducing a general convention that the way of dealing with the right part of a pair is to be determined from its left part.

This is a general method e.g. FORTH's dictionaries are designed precisely in this way so that native code on the left tells the machine how to deal with the stuff on the right.
Since it appears as if symbols can be used to defer the introduction of this general method (so that the programmer of my little lisp can set such grammatical concerns up as they see fit), I shall keep with the implicit way of taking pairs as lists.

So, there are two things the parser must do: build symbols and build lists.
Parsing a symbol is easy: just make a pair whose left part is the symbol pair and whose right part is the lexeme which names the symbol being parsed.
```
let parseSymbolOf = lexeme => consOf($,lexeme);
```
Note that this automatically produces the symbol symbol when the lexeme is the symbol pair.
This is an example of one of the conveniences which follows from the conventions evolved here.

Parsing a list is only slightly harder.
Now, all that stuff about stacks can be unforgotten.
When the first lexeme of the list of lexemes to be parsed is an open parenthesis then whatever we were parsing up to that point goes on the stack of things we have to go back to parsing.

It's slightly easier to grasp this when you start parsing with an open parenthesis.
This tells you to start with an empty list, i.e. the empty pair, and build up the rest of the list based on the remaining lexemes.
Next suppose there is neither a closed parenthesis or an open parenthesis as the next lexeme.
Then it must be a symbol.
So you parse that symbol and then append it to the list under construction.
This is all fine and good until you run into another open parenthesis.
You need to start making a new list, but, when you're done making it, you have to append what you've made onto the end of the last list you were working on and continue parsing with whatever lexemes are left over.

If you're lucky, you have a helper that has an argument for the list you're going to get back to constructing, an argument for the list currently under construction, and the rest of the lexemes you have to go through to finish everything up.
But, since you can run into an unknown number of open parenthesis before reaching your first closed one, you can't just keep hopping you have helpers for helpers.
For me, stacks are the obvious answer.
They occur to me so readily because I deal with the world all the time with stacks.
There are stacks and stacks of things that I'm going to come back to after I finish working with the thing that is at the top of the stack.

Now, when you run into an open parenthesis you push whatever list is under construction onto the stack of lists under construction, and start with the empty list.
If the next lexeme is neither an open or closed parenthesis you append its parsed symbol to the list under construction.
If it's a closed parenthesis then you are done constructing the current list and append it to the list atop the stack of lists under construction.
```
let isOpenParen = x => id(runesOf('('),x)
, isCloseParen = x => id(runesOf(')'),x)
, parseHelperOf = (stack, list, lexemes) =>
 isEmpty(lexemes) ? list
 : isOpenParen(carOf(lexemes)) ? 
  parseHelperOf(pushOf(stack,list),theEmptyPair,cdrOf(lexemes))
 : isCloseParen(carOf(lexemes)) ?
  parseHelperOf(popOf(stack)
  , prependedListOf(topOf(stack),singletonListOf(list))
  , cdrOf(lexemes))
 : parseHelperOf(stack
   , prependedListOf(list
    , singletonListOf(parseSymbolOf(carOf(lexemes))))
   , cdrOf(lexemes))
, parseOf = lexemes => parseHelperOf(theEmptyPair, theEmptyPair, lexemes);
```
There are a few things that seem as if they are not going to work out well based on the above design:

* if the lexeme is just a list of symbols then what happens?
* does there really need to be a helper function for "parsing Symbols"?
* is there a simpler parser that just works with parenthesis, spaces, and everything else?

To answer that last question is to get to the heart of the matter I submit.
So, rather than parsing symbols and such I'll just take every lexeme that is not a parenthesis directly over to the list under construction:
```
parseHelperOf = (stack, list, lexemes) =>
 isEmpty(lexemes) ? list
 : isOpenParen(carOf(lexemes)) ? 
  parseHelperOf(pushOf(stack,list),theEmptyPair,cdrOf(lexemes))
 : isCloseParen(carOf(lexemes)) ?
  parseHelperOf(popOf(stack)
  , prependedListOf(topOf(stack),singletonListOf(list))
  , cdrOf(lexemes))
 : parseHelperOf(stack
   , prependedListOf(list
    , singletonListOf(carOf(lexemes)))
   , cdrOf(lexemes));
```
By simply dropping the step that parses symbols something important has been revealed: this is almost the same form of program as the primtiive encoder and decoder cooked up in [Bit Strings and Binary Trees](#2025-0413-1513-bit-strings-and-binary-trees).

The following supplementary functions make it clearer:
```
let secondOf = stack => topOf(popOf(stack))
, drop2 = stack => popOf(popOf(stack)) 
, appendOf = stack => pushOf(drop2(stack)
  , prependedListOf(secondOf(stack), topOf(stack)));
parseHelperOf = (stack, lexemes) =>
 isEmpty(lexemes) ? topOf(stack)
 : isOpenParen(carOf(lexemes)) ? parseHelperOf(pushOf(stack,theEmptyPair), cdrOf(lexemes))
 : isCloseParen(carOf(lexemes)) ? parseHelperOf(appendOf(stack), cdrOf(lexemes))
 : parseHelperOf(appendOf(pushOf(stack,carOf(lexemes))), cdrOf(lexemes));
parseOf = lexemes => parseHelperOf(theEmptyPair, lexemes);
```
Here, the open parenthesis plays the part of the zero of the encoder, the close parenthesis plays the part of the one of the encoder, and the default case appends the first of the lexemes onto the list at the top of the stack.

This suggests a certain ordering to the alphabet in order to make the similarities stronger along with a complete overhall of method.
It seems like it is worth it and that it also brings the similarities between lisp and forth even closer together while simplifying going between one and the other (and without eliminating lisp as a language).
It also suggests a pretty obvious alphabetization.

I was unable to see through the rest of what I had seen to my self.
Meditation and sleep are the most likely solutions to this problem.

## 2025 0430

### 2025 0430 2324
This continues my read of Durant's "Story of Philosophy" from [2025 0420 2247](#2025-0420-2247).

Last time I cracked open the spine of this old book I had just read that "Critias was a pupil of Socrates, and an uncle of Plato".
Critias is worthy of his own story from what little I read of his history outside of what less Durant mentioned.
He is also surprisingly relevant to the world we are presently faced with: the Sophists were selling aristocracy by bashing democracy and Critias ended up the very brief head of Athen's Oligarchical Party.
There is not enough written by Durant or that I have already read as part of my work on the history of the world to deduce to what extent "democracy" was anything that existed strongly enough to even be denounced e.g. what written records we have are not even enough to make too many commitments to the stories that happen to have survived to our day.

Archeology is a countercontrol to the division of the past into history and prehistory where prehistory is simply a weak stand in for "unrecorded" or "having no surviving records written in a language as the reinforcing practices of a verbal community".
It is quite clear that there is more to be got from simulating past environmental conditions and watching how humans navigate such contrived worlds in many cases than to take the word of a textual record that just so happens to have found its way to our modern world.
The validity of such methods depends on the difference between the primary reinforcers of the past and those now.
As far as I can tell they are still the same old same old: sex, food, water, violence.
There may be a few others, but these are the primary reinforcers from which most of the organisms are likely to evolve or condition the much larger collection of secondary reinforcres that come to be called social because they involve an other organism (perhaps of the same species) in some way key to the conditioning contingency.

Alas, archeology is not what I am reading, though I would love to know more about the archeological work that has been done on Athens during the times of Socrates and Plato.
That is for a different time.

The second section of the first part is titled "Socrates".
His bust bursts onto the scene as if a wart on his philosophic reputation: bald, round,

>"it was rather the head of a porter than that of the most famous of philosophers."[pg. 7]

There are various sculptures of Socrates purported from his time and their accuracy is probably more asperational than scientific e.g.

* <https://digital.library.cornell.edu/catalog/ss:172717>
* <https://www.ancientworldmagazine.com/articles/a-portrait-of-socrates/>
* <https://www.mfa.org/article/2021/portrait-head-of-socrates>
* <https://www.worldhistory.org/image/4425/socrates-bust-palazzo-massimo/>

and, last but not least, a collection of them as part of an article on the big question "Who was Socrates?"

* <https://guides.library.duq.edu/c.php?g=605283&p=4214778>

One bust or the other is a concrete connection with the ever ephemeral Socrates.

He spoke and listened in ways that differed remarkably from those of his time, and, presumably, of our time as well.
Just as the young and curious of our day are apt to ask "why? why? why?", whether as inquiries or objections, Socrates queried his fellow Athenians in search of something beyond question.

Perhaps this is all that was different about Socrates' verbal behavior and perhaps this was all that was required to bring a cross section of young and old back to his arena of discourse.
What is known of Socrates is second hand: no writing of his survives if ever it was made.
We shall never have the records we might need to recreate what made his verbal behavior so remarkable that descriptions of it have made their way to us as something beyond myth and history.

The stories of Socrates are, for me, folklore.
Most of what is called history is folklore in my measure.
There's little beyond the dutiful work of archeologists that allows me to speak with any certainty on the intersection of history and folklore.
When taken as folklore it becomes easier to see that there are certain stories that are told with a character called Socrates and, just as knowing the stories of Shakespeare can alert you to fitting surprises, knowing the stories of Socrates can strengthen otherwise remote cultural connections.

I for one cherish Shakespeare far more than the stories of Socrates, but can not doubt the effect that Plato's tales of Socrates have had on European philosophy, just as there is even less doubt of the effect Shakespeare has had on European literature.

### 2025 0430 2309
Two delightful links (from a friend) on a ternary stack machine:
 * <https://concatenative.org/wiki/view/DSSP>
 * <https://web.archive.org/web/20230405010717/http://brokestream.com/daf.txt>

### 2025 0430 1905 Frege Metaphorically Taken
My first reading of Begriffsschrift was a while ago when I first got the invaluable text "From Frege To Godel" Third Printing edited by van Heijenoor.
At that time I was mainly interested in von Neumann's outlook on set theory and logic.
But, it was already clear that Frege had made his Begriffsschrift and all else was commentary upon it.

R.P. and I have disagreed on this matter.
Last Friday I received my copy of "The Frege Reader" edited by Michael Beaney and began a close and careful analysis of Frege's writings without them being of secondary interest as they were when I was studying Quine's "Set Theory and its Logic" and was led to von Neumann's 1925 "An axiomatization of set theory".

Not only do I have a new respect for Frege.
I am more certain than ever that he shared my own outlook on his commentaries.
Today, I finally started reading Frege's 1891 "Function and Concept" and was delighted to see that he had this to say

> "There can be no question of setting forth my *Begriffsschrift* in its entirety, but only of elucidating some fundamental ideas."[pg. 131 Frege "The Frege Reader" edited by Michael Beaney]

So that the result of his comments upon Begriffsschrift can be taken merely as elucidation and nothing more.
The etymology of the root is here from <https://www.etymonline.com/word/*leuk->
> "*leuk-. Proto-Indo-European root meaning "light, brightness."
>
>It might form all or part of: allumette; elucidate; illumination; illustration; lea; leukemia; leuko-; light (n.) "brightness, radiant energy;" lightning; limn; link (n.2) "torch of pitch, tow, etc.;" lucent; lucid; Lucifer; luciferase; luciferous; lucifugous; lucubrate; lucubration; luculent; lumen; Luminal; luminary; luminate; luminescence; luminous; luna; lunacy; lunar; Lunarian; lunate; lunation; lunatic; lune; lunette; luni-; luster; lustrum; lux; pellucid; sublunary; translucent.
>
>It might also be the source of: Sanskrit rocate "shines;" Armenian lois "light," lusin "moon;" Greek leukos "bright, shining, white;" Latin lucere "to shine," lux "light," lucidus "clear;" Old Church Slavonic luci "light;" Lithuanian laukas "pale;" Welsh llug "gleam, glimmer;" Old Irish loche "lightning," luchair "brightness;" Hittite lukezi "is bright;" Old English leht, leoht "light, daylight; spiritual illumination," German Licht, Gothic liuhaþ "light."

We can see this is consistent with Frege's prior sentence:

>"Today I should like to throw light upon the subject from another side, and tell you about some supplementations and new conceptions, whose necessity has occurred to me since then."[pg. 130-131 Frege "The Frege Reader" edited by Michael Beaney]

This sentence appears, to some, to say that there are necessary changes that must be made to Begriffsschrift in light of his work on the forthcoming "Grundgesetze der Arithmetik, Volume I" published in 1893.
But, a careful reading of Begriffsschrift or a careful reading of the introduction or a careful reading of the rest of the paper reveals this can not be the case and that Frege is simply doing as he says e.g. supplementing and newly conceptualizing.

>"It is my intention, in the near future, as I have indicated elsewhere, to explain how I express the fundamental definitions of arithmetic in my Begriffsschrift, and how I construct proofs from these solely by means of my symbols. For this purpose it will be useful here to be able to refer to this lecture so as not to be drawn then into discussions which many might condemn as not directly relevant, but which others might welcome."[pg.130 Frege "The Frege Reader" edited by Michael Beaney]

His supplements and conceptualizations do not alter the Begriffsschrift.
It is besides the point if they are largely mistaken when they are taken literally.

Thus, where his elucidations are incompatible with his Begriffsschrift we can, by his own admission, take his elucidations in jest, error, or, what is perhaps most valuable, as metaphor.

But there is more!
Frege has already in the introduction of Begriffsschrift explained his metaphorical methods and their contribution to his designs!
He repeats the same in "Function and Concept".
We are, from the very introduction of Begriffsschrift, told of its arithematical origins and of the exact metaphorical extensions which smooth the steps from the arithmetic of numbers to that of thought or concept.

For those who have not yet undertaken an endevor like Frege's it may seem strange to treat his latter writings with such apparent disregard.
But, the disregard is only apparent, as it is otherwise familiar to those of us who have come to understand (or, as the case may be, to misunderstand) our younger self.

So it is that Frege can be taken as metaphor where he is incompatible with the strictures of Begriffsschrift, and all his elucidations remain as elucidating as ever, but now as metaphor.
Said another way, if later commentaries were to be given as foundational alternatives to Begriffsschrift then the entirety of the new foundation would be required as part of subsequent publications: this is known from the fact that Frege did not present Begriffsschrift as an alteration of his earlier attempts at such notation which are merely mentioned at the end of section three of Begriffsschrift. 



### 2025 0430 1830
I am once again drawn to entry [2025 0416 2358](#2025-0416-2358) in which I finally made a public record of being unable to keep up with my self.
There is more I have written than I have been able to write out here.
Thankfully, I am less likely to lose what it is I wrote than I ever was thanks to a social experiment I did some four or so years ago.

Together with two friends I designed a tiny culture around a collection of strict contingencies set up with the help of a somewhat elaborate sequence of index cards and index card holders.
The strict method was evolved until it was eventually dropped, but it had done its work on my friends and my self in ways which were mostly within the reach of my designs e.g. the often complex contingencies linking time, reading, and writing into a social environment where collective action was a major conclusion to any such work were revealed more clearly than any of us had ever seen before.

Whatever was dropped of the evolved practices of reading and writing continued to control all our subsequent interactions.
We had all come to keep a notebook in our pockets and to bring them out whenever we sat down to work with an other (especially when the other was one of the members of our tight knit group.

Somewhere along the way we landed on "The Pocket Notebook" by the Portage company as they appeared to, at the time, have the right price point.
Previously I had been working mostly with the beautiful "Field Notes" by the company of the same name: John Gruber had mentioned them more than once and they were sturdy and simple in those ways that I cherish.

There are still many of those pocket notebooks around and I occasionally buy replacements when I run out of them.
Spiral bound is important when you're really "on the go" but it is not effective when you're building cumulative records.
The choice between one or the other is largely a matter of "where am I?"
Pages can be torn from a spiral notebook and displayed as was once a major part of the tiny culture run via index cards.
They can also be kept in place.
When it comes to long form bound notes that are written as in a cumulative record in an experimental environment, I go with hardback books bound for artists.

Recently I've experimented with binding my own books as a further way of simplifying.
There is still promise in such methods if not just because I haven't tried them out in detail.


## 2025 0429

### 2025 0429 1424 A Preliminary Outline of The Method of Bringing Frege Through Quine
The first concrete hint of the transition from sense to schematism is in the first section (again I keep my etymological principles by presenting both translations of this selection from the books which I have in hand):
> "*I adopt this fundamental idea of distinguishing two kinds of symbols*, which unfortunately is not strictly carried through in the theory of magnitude, [consider 1, log, sin, Lim], *in order to make it generally applicable in the wider domain of pure thought*.
>I therefore divide all the symbols I use into *those by means of which one can represent different things* and *those that have a quite determinate sense*.
> The first are the *letters*, and these should serve primarily to express *generality*.
> For all their indeterminancy, it must be insisted that a letter *retain* in the same context, the meaning once given to it."[pg. 52 Frege 'The Frege Reader' edited by Michael Beaney]

> "*I adopt this basic idea of distinguishing two kinds of signs*, which unfortunately is not strictly observed in the theory of magnitudes [consider 1, log, sin, lim], *in order to apply it in the more comprehensive domain of pure thought in general*. I therefore divide all signs that I use into *those by which we may understand different objects* and *those that have a completely determinate meaning*. The former are *letters* and they will serve chiefly to express *generality*. But, no matter how indeterminate the meaning of a letter, we must insist that throughout a given context the letter *retain* the meaning once given to it."[pg. 10-11 Frege 'From Frege to Godel' Third Printing edited by van Heijenoor]

The former come immediately over to the variable letters as a grammatical category perhaps constructed from a single lowecase letter to which further members are added by iterated accentuation e.g. "x'" and "x''''".

Although it is somewhat out of place here, I shall mention overtly that there are many details to the problems of grammatical analysis which are better explaiend in Quine's "Philosophy of Logic 2nd Edition" than in any explanation I may give.
In general, the methods of the grammarian can be applied to the methods of the logician and have been so with great effect.
The result is that the methods of logic can then be applied to the methods of the grammarian and so on with ever greater effect (although there is presumably some kind of diminishing return).
Thus it is that the grammar of logic can be spoken of logically.
This is what is out of place here, but it is not so far out of place as to block me from further comment.

Talk of "the grammatical category of variables" can be dropped with all of its commitments of a theory of grammatical categories by giving a logical theory one predicate of which is 'is a variable'.
Here a warning is given about reading "predicate" which I repeat from Quine:

>"A word of caution is in order regarding 'predicate' too. Some logicians take a predicate as a *way* of building a sentence aroudn a singular term or, more concretely, as what Peirce called a *rheme* [Volume 2, paragraph 95], a sentence with blanks in it, these being distinctively marked in the case of a many-place predicate. This version covers, implicitly, the potential output of predicate abstraction or predicate functors. But a predicate in my sense is always an integral word, phrase, or claus, grammatically a noun, adjective, or verb. Some are generated from others by grammatical constructions, notably the relative clause or, formally, predicate abstraction and predicate functors."[pg. 61 Quine "From Stimulus to Science"]

This is given in the chapter "Denotation and Truth".
Note, for later reference, this restriction includes the results of predicate abstraction and predicate functors e.g. '{x : x loves Dick}' and 'Refl love' (which goes into 'love oneself') are predicates.
A reminder that an explanation of predicate abstraction from quantificational logic and predicate functors from predicate abstraction is to be found in my [2025 0422 2322 An Incomplete Sketch of My Philosophy of Logic](#2025-0422-2322-an-incomplete-sketch-of-my-philosophy-of-logic).

Under the control of this convention the predicate 'loves Dick' is coextensive with the predicate abstract '{x : x loves Dick}' which is to be distinguished from 'x loves Dick' which may be taken in one of three ways:

1. 'x loves Dick' is like the sentence 'He loves Dick' were 'He' is a pronoun which purports to designate one and only one item.
It is then for the rest of the sentences in a given theory to establish the existence (or non-existence) of the item purportedly designated by 'x' or 'he'.

    This is what has been called a Free Logic, but under the methods imposed by Quine in his paper "Free Logic and Virtual Classes" of 1994 and more carefully integrated into the methods of logic in his 1982 "Methods of Logic 4th edition".
Leblanc and Schock had different methods for dealing with what they called free logics which are clumsy compared to Quine's e.g. rather than simply alter the definition of 'traditional logic' to demand first contemplating a nonempty universe of discourse and only as a special case going through and marking existentials as false and universals as true in the degenerate case of an empty universe, they would demand traditional logic distinguish between those where an instance of a schematic premise such as "each item is {x : Fx only if Fx}" be introduced at every turn lest the theory fall to the degenerate case.

    The thrust of Quine's arguement for his method is two fold: 1) the rules of passage do not hold in the empty case but are part and parcel of traditional logic, 2) admiting singular terms which merely purport to designate one and only one item clears the way for a careful analysis of the descriptional premises which are so often invisible to those who invoke singular terms as if they could not even be so invoked without designating some unspecified item.

    This is my favorite method and it is also a delightful way of noticing how smoothly singular terms disolve into singular descriptions: a more detailed explanation of this dissolution and its combination with predicate abstracts as purported designations of their extensions shall not be found in this note.

2. 'x loves Dick' is short for its universal closure 'each item is {x such that x loves Dick}' with predicate functors and predicate abstracts or, in crusty quantificational languages, '(each item is x such that)(x loves Dick)'.
It is this way of taking 'x loves Dick' that is most familiar from the practices of arithmetic where an equation with variables such as 'x+x = 2x' is said to be "an identity" in that "each item is {x : x +x = 2x}" is true.

    It is also this method which is commonly used in logic programming where question marks are prepended to the variable letters that are under universal quantification e.g. '?x loves Dick' or '?x loves ?y'.
This is because of the way that some logic programming langauges take such sentences as querries upon which they generate substitutions that satisfy the sentence based on the contents of the database being querried.

    In general, this is the beginning of the method of Skolemization or its notational variation as Quine's functional normal form (which does away with Skolem functions while retaining their logical import).
Quine's functional normal form introduces compound variable-like letters which are either a classic variable letter or are a classical varible letter subscripted with a list of compound variable-like letters.
Then the variable letters which have no subscript are implicitely universal quantified and those with subscripts are implicitely existentially quantified and the order of these quantifications can be constructed from the structure of the subscripting.
At a probably much later time I shall give the algorithm for converting to and from functional normal form since it greatly simplifies logic programming with Quine's main method.

3. 'x loves Dick' is not a sentence nor a predicate nor an unmarked universal closure but rather a kind of sentence under construction. For example, the place marked by 'x' in 'x loves Dick' is waiting to be replaced by a proper singular term e.g. replace 'x' by 'Tom' in 'x loves Dick' to get 'Tom loves Dick' which is a complete sentence.
This is my least favorite method.

I've taken so much time to describe these different ways of taking 'x loves Dick' because it is the multitude of incompatible methods that were presented to Frege and which he had to navigate in order to arrive at his Begriffsschrift as something coherent, systematic, and, above all, extensional.
His inspiration was arithmetic, and in that way he went with the second method whenever he said that a sentence like 'x loves Dick' is used to express a generality like that expressed from arithmetic in 'x(y+z) = xy + xz', and it is in Quine's functional normal form that there are the most similarities between Frege's methods and those of arithmetic e.g. as when he says

> "The first are the *letters*, and these should serve primarily to express *generality*. For all their indeterminancy, it must be insisted that a letter *retain* in teh same context, the meaning once given to it."[pg. 52 Frege 'The Frege Reader' edited by Michael Beaney]

>"The former are *letters* and they will serve chiefly to express *generality*. But, no matter how indeterminate the meaning of a letter, we must insist that throughout a given context the letter *retain* the meaning once given to it."[pg. 10-11 Frege 'From Frege to Godel' Third Printing edited by van Heijenoor]

But, alas, none of these three methods actually coincide with Frege's usage or his own explanations of his own usage which is consistent throughout Begriffsschrift and wherever Begriffsschrift is directly invoked.

The methods of Frege are schematic and it is the way in which they are schematic which leads to sentences in Frege's texts that, if read carelessly or in isolation, can lead to a great deal of confusion at many different times e.g. at some times the schematism is part of a theory of schematisms and at other times there is no such theory under investigation and the schematic methods are purely logical (as in talk of 'valid schema' through which validities are got by substitution but with which no items or objects are concerned).




### 2025 0429 1407 An Analysis of "At Plataea the Persians were defeated by the Greeks"
The subject of "At Plataea the Persians were defeated by the Greeks" is "the Persians".
The predicate is "{x: At Plataea x were defeated by the Greeks}" via Quine's predicate abstract notation.
> Here is my single sentence summary of Quine's predicate abstract notation (for more on its definition in a quantificational logic see [2025 0422 2322 An Incomplete Sketch of My Philosophy of Logic](#2025-0422-2322-an-incomplete-sketch-of-my-philosophy-of-logic)):
>
>The English relative clause 'who loves Dick' and the pidgin 'x such that x loves Dick' are uniformly paraphrased by the *predicate abstract* '{x: x loves Dick}' which *abstracts* 'Tom' from 'Tom loves Dick' by *binding* the *free* occurrence of 'x' in the *open* sentence 'x loves Dick' with the prefix 'x:' so that the *predication* '{x:x loves Dick}Tom' *concretes* to 'Tom loves Dick': whatever can be said of a thing can be said by predicating a predicate of it i.e. *predicational completeness*.

The object of "At Plataea the Persians were defeated by the Greeks" is "the Greeks".
The most general predicate is "{x,y,z: At x y were defeated by z}" from which the sentence can be factored conspiucously into its predicate, subject, and objects as "{x,y,z: At x y were defeated by z} Plataea, the Persians, the Greeks".
This analysis is of major consequence to explaining Frege's main paragraph in Begriffsschrift which I first mentioned [here](#2025-0428-1517-frege-thru-quine).

### 2025 0429 1403
Rules of inference are the manufactured products of logical methods: they mark the end of logical heavy lifting and the beginning of logical economy for all.
Those obsessing over the by products of this or that collection of rules of inference are only indirectly engaged in logical methods.
Inference rules are the mechanical proxies of logical work.

## 2025 0428

### 2025 0428 1517 Frege Thru Quine
The translation of Frege (1848–1925) through Quine (1908-2000) begins with John Horne Tooke's (1736-1812) suggestion that

> "you substitute the composition, &c, of *terms* wherever he has supposed a composition of ideas"[pg.6 Quine "From Stimulus to Science"].

(Note, Tooke's "The Diversions of Purely" played a part of mention in Skinner's (1904 –1990) theoretical analysis of verbal behavior.)

This technique evolved through Bentham's (1748–1832) contextual fictions (I have not been able to yet identify if Bentham read Tooke), then Boole's (1815–1864) concrete differential notation, and directly on through Russell's (1872–1970) triumphant contextual definition of singular description where 'the {x such that Fx}' is elimianted (with major restrictions like those in Quine's "Free Logic and Virtual Classes" of 1994 and their projection back and forth through Quine's "Methods of Logic 4th edition" of 1982) as follows:

> 'G the {x such that Fx}' for 'some item is {y such that Gy and each item is {x such that x is identical to y if and only if Fx}}'.

Frege, Peirce (1839-1914), Peano (1858-1932), Dedekind (1831-1916), and Cantor (1845-1918) are in the firmament from which Russell's theory of descriptions appeared (I am simply using it here as a concrete checkpoint).

As an aside, Russell and Frege dreamed of deducing mathematical truth from logical truth where their logic contained set membership and is what we would today call a mathematical logic: it was Gödel (1906-1978) in 1931 who finally severed the methods of mathematics from the methods of logic (Quine submits that Russell's paradox of 1902 is an earlier component of the full severence).
It is the analysis of the ancestral of a relation and its confusion with the transitive closure of a predicate that prompted Frege and Russell to easily mix mathematics and logic: they mixed copuli of predication with predicates of membership. 
The confusion between mathematical logic and (predicate) logic can be forgiven when carrying Frege through Quine because it is a confusion which almost all mathematicians and logicians continue to make either out of laziness or ignorance: the lazy maintain the ignorant.

Quine continues the Tookeian tradition through his method of semantic ascent, and it is with such methods that the bulk of Frege can be happily translated.
It is not effective to go straight to semantic ascent first though because there are a number of distinctions which Frege makes which are interdependent with each other in ways that are factored out by Quine in different ways that don't depend on semantic ascent.

First I shall collect the relevant selections from Quine and then I shall collect and collate the relevant sections from Frege.
But, it is impossible not to allude to Frege for there is so much which he already noticed and which is only slightly obscure relative to Quine's distinctions.
In fact, I shall already break my plan here at the beginning because there is a beautiful distinction which Frege makes and which is otherwise obscure to me but in light of the following key distinction by Quine:

>“A context is extensional if its truth value cannot be changed by supplanting a component sentence by another of the same truth value, nor by supplanting a component predicate by another with all the same denotata, nor by supplementing a singular term by another with the same designatum. Succinctly, the three requirements are substitutivity of covalence, of coextensiveness, and of identity, salva veritate. A context is intensional if it is not extensional.”[pg. 90 Quine “From Stimulus to Science”].

The division between extensional and intentsional context is given by Frege in the third section of the first part of Begriffsschrift.
Since I know of no way of seperating Frege's overlapping concerns I must present the section in its whole each time I explain one dimension of distinction that Quine makes which is relevant to it.
I also have access to two different translations of Begriffsschrift and must put both of them right after each other because of my etymological principles.

>"A distinction between *subject* and *predicate* finds *no place* in my representation of a judgement. To justify this, I note that the contents of two judgements can differ in two ways: either the conclusions that can be drawn from one when combined with certain others also always follow from the second when combined with the same judgements, or else this is not the case. The two propositions 'At Plataea the Persians were defeated by the Greeks' and 'At Plataea the Persians were defaeted by the Greeks' differ in the first way. Even if a slight difference in sense can be discerned, the agreement predominates. Now I call that part of the content that is the *same* in both, the *conceptual content*. Since *only this* has significance for the Begriffsschrift, no distinction is needed between propositions that have the same conceptual content. If it is said, 'The subject is the concept with which the judgement is concerned', then this applies also to the object. It can therefore only be said: "The subject is the concept with which the judgement is primarily concerned'. The linguistic significance of the position of the subject in the word-order lies in its *marking* the place where what one particularly wants to draw the attention of the listener to is put. (See also $9.) This can have the purpose, for example, of indicating a relation between this judgement and others, thereby fascilitating the listener's grasp of all the interconnections. Now all those features of langauge that result only from the interaction of speaker and listener--- where the speaker, for example, takes the listener's expectations into account and seeks to put them on the right track even before a sentence is finished--- have no counterpart in my formula language, since here the only thing that is relevant in a judgement is that which influences its *possible consequences*. Everything that is necessary for a valid inference is fully expressed; but what is not necessary is mostly not even indicated; *nothing is left go guessing*. In this I closely follow the example of the formula language of matheamtics, in which subject and predicate can also be distinguished only by violating it. Imagine a language in which the proposition 'Archimedes was killed at the capture of Syracuse' is expressed in the following way: 'The violent death of Archimedes at the capture of Syracuse is a fact'. Even here, if one wants, subject and predicate can be distinguished, but the subject contains the whole content, and the predicate serves only to present it as a judgement. *Such a language would have only a single predicate for all judgements, namely 'is a fact'*. It can be seen that there is no question here of subject and predicate in teh usual sense. *Our Begriffsschrift is such a language and the symbol \|--- is its common predicate for all judgements.*
>
> In my first draft of a formula language I was misled by the example of ordinary language into constructing judgements out of subject and predicate. But I soon convinced myself that this was an obstacle to my particular goal and only led to useless proxlixity."[pg. 54 Frege 'The Frege Reader' edited by Michael Beaney]

I have read this section multiple times, and have written it out twice now: it is probably one of the most important paragraphs in human history.
It combines practically every major distinction which separates the practices of logic from all other human practices.
Here is the same selection from a different translation (note the difference in spelling from the last's 'judgement' to 'judgment'):

>"A distinction between *subject* and *predicate* does *not occur* in my way of representing a judgment. In order to justify this I remark that the contents of two judgments may differ in two ways: either the consequences derivable from the first, when it is combined with certain other judgments, always follow also from the second, when it is combined with these same judgements, [[and conversely,]] or this is not the case. The two propositions "The Greeks defeated the Persians at Plataea" and "The Persians were defeated by the Greeks at Plataea" differ in the first way. Even if one can detect a slight difference in meaning, the agreement outweighs it. Now I call that part of the content that is the *same* in both the *conceptual content*. Since *it alone* is of significance for our ideography, we need not introduce any distinction between propositions having the same conceptual content. If one says of the subject that it "is the concept with which the judgment is concerned", this is equally true of the object. We can therefore only say that the subject "is the concept with which the judgment is chiefly concerned". In orderinary language, the place of the subject in the sequence of words has the significance of a *distinguished* place, where we put that to which we wish especially to direct the attention of the listener (see als $9). This may, for example, have the purpose of pointing out a certain relation of the given judgment to others and thereby making it easier for the listener to grasp the entire context. Now, all those peculiarities of ordinary language that result only from the interaction of speaker and listener--- as when, for example, the speaker takes the expectations of the listener into account and seeks to put them on the right track even before the complete sentence is enunciated--- have nothing that answers to them in my formula language, since in a judgment I consider only that which influences its *possible consequences*. Everything necessary for a correct inference is expressed in full, but what is not necessary is generally not indicated: *nothing is left to guesswork*. In this I faithfully follow the example of the formula language of mathematics, a language to which one would do violence if he were to distinguish between subject and predicate in it. We can imagine a language in which the proposition "Archimedes perished at the capture of Syracuse" would be expressed thus: "The violent death of archimedes at the capture of Syracuse is a fact". To be sure, one can distinguish between subject and predicate here, too, if one wishes to do so, but the subject contains the whole content, and the predicate serves only to turn the content into a judgement. *Such a language would have only a single predicate for all judgments, namely "is a fact".* We see that there cannot be any question herer of subject and predicate in the ordinary sense. *Our ideography is a language of this sort, and in it the sign \|--- is the common predicate for all judgments*.
>
> In the first draft of my formula langauge I allowed myself to be misled by the example of ordinary language into constructing judgments out of a subject and predicate. But I soon became convinced that this was an obstacle to my specific goal and led only to useless prolixity.[pg.12-13 Frege 'From Frege to Godel' Third Printing edited by van Heijenoor]

Another reason I have presented this paragraph twice is becuase it can only really be taken as a whole the way that Frege presents it, and to factor it into its component parts, which is what can be done once each of Quine's methods is in hand, would be to dismantle the innerworkings of vintage Cortébert pocket watch!

To make the concrete connection then between Quine's extensional and nonextension contexts and Frege's distinction between conceptual content and nonconceptual content of a judgement:

> "the contents of two judgements can differ in two ways: either the conclusions that can be drawn from one when combined with certain others also always follow from the second when combined with the same judgements, or else this is not the case." [pg. 54 Frege 'The Frege Reader' edited by Michael Beaney]

> "the contents of two judgments may differ in two ways: either the consequences derivable from the first, when it is combined with certain other judgments, always follow also from the second, when it is combined with these same judgements, [[and conversely,]] or this is not the case."[pg.12 Frege 'From Frege to Godel' Third Printing edited by van Heijenoor]

I did not grasp the distinction that Frege was making without interpreting what he wrote through Quine.
First, the distinction is between the complementary ways that the contents of a judgement can differ.
That is, he distinguishes between different (yet complementary) differences: this is why it is a bit confusing.
We take a pair of judgements, say there is a judgement to your left and a different judgement to your right, and look at each conclusion that is implied by the content of the left judgement under a potentially empty collection of supplementary judgements and look at each conclusion that is implied by the content of the right judgement under the same collection of supplementary judgements.
Then, if there is for each conclusion from the left judgement the same conclusion from the right judgement under the same collection of supplementary judgements, the different contents of the left and right judgements are different _only in_ their _non_-conceptual content.

The example Frege gives for the different contents of two judgements that differ only in their non-conceptual content is as follows:

> "The two propositions 'At Plataea the Persians were defeated by the Greeks' and 'At Plataea the Persians were defaeted by the Greeks' differ in the first way."[pg. 54 Frege 'The Frege Reader' edited by Michael Beaney]

> "The two propositions "The Greeks defeated the Persians at Plataea" and "The Persians were defeated by the Greeks at Plataea" differ in the first way."[pg.12 Frege 'From Frege to Godel' Third Printing edited by van Heijenoor]

This example, as given, does not emphasize that the contents are to be of a judgement: this is because Frege has said he shall only contemplate contents which are judgeable.
Following through the example in detail: 1) the two quotations are spelled differently i.e. they are not identical as concatenations of letters, 2) thus they differ in at least one way (as contents) and the question is how they differ with respect to the distinction Frege has made between conceptual and nonconceptual content, 3) Frege claims that there is no conclusion that can be got from the one that can not be got from the other with respect to the same supplementary judgements, 4) therefore they have the same conceptual content and differ only in their non-conceptual content, one part of which is, presumably, their spelling as quotations.

Now note that Frege, in this example, goes far beyond the difference in content that I have mentioned as a difference in the spelling of their quotations: he wishes to go beyond propositions as sentences that may differ in spelling or, that complex difference in spelling called a difference in phrasing.
This is seen in the sentence following the sentence with the examples:

> "Even if a slight difference in sense can be discerned, the agreement predominates."[pg. 54 Frege 'The Frege Reader' edited by Michael Beaney]

> "Even if one can detect a slight difference in meaning, the agreement outweighs it."[pg.12 Frege 'From Frege to Godel' Third Printing edited by van Heijenoor]

Here was also see our first divergence between translations with "sense" in about the same place as "meaning" is in the other.
I do not own the German editions (yet), but a bit of etymology obviates any problems of translation from there (I can not get into this in greater detail without become derailed entirely in Skinner's analysis of verbal behavior).

When I worked through Frege's example I neither mentioned the sense nor the meaning of a sentence: I stuck with the difference in phrasing and took that to be slight in that, following Frege, there is not a conclusion from the one phrasing that can not be got from the other phrasing under identical supplementation of auxiliary judgements.

This radical translation is the hallmark of my presentation here.
Though I may not land on any singular method of radical translation from Frege's words and sentences, I shall demonstrate how, through Quine and Skinner, nothing like what Frege or most philosophers expect, as in senses and meanings, is actually at work throughout Frege's writings: so much so that nothing need be edited within his texts besides the already well known slips and such.
I see this as being in the same vein as Tooke, Bentham, Boole, Russell, and Quine, but wish to make it clear that I feel no affinity to a sort of nominalism (it shall be hard for some to accept this last point but the only way I could make this clear is by going over Quine's early stint as a nominalist with Goodman in 1947 which comes a cropper in ways parallel to Carnap's Aufbau).

Thus I mention now Quine's outlook on propositions in 1995:

> "So it is in standing sentences that the notion of meaning goes shaky. But this is the very locus of the philosophic notion of *proposition*: the meaning of a sentence of fixed truth value. Many philosophers have seen propositions as abstract objects that statements served to express. They have seen them as the bearers of truth values; sentences were true and false only in the sense of expressing true or false propositions.
>
> There is indeed a usage of 'proposition' that is useful and unobjectionable. It can be construed as denoting the sentences themselves, rather than their meanings, but it is used instead of 'sentence' when we are concerned with the sentences as an object of belief (as we shall be in Chapter VIII) rather than with its morphology and syntax. I deny myself this convenient usage, for fear of beclouding issues; but it carries no commitment to sentence meanings."[pg.77 Quine "From Stimulus to Science"]

I make no such commitment as there outlined at this time, but give it as an example of a principle governing my contemplation.

It is in the next sentence, where Frege defines the conceptual content of a proposition, that the quote from Quine, where he defines extensional contexts, activates:

>"Now I call that part of the content that is the *same* in both, the *conceptual content*. Since *only this* has significance for the Begriffsschrift, no distinction is needed between propositions that having the same conceptual content."[pg. 54 Frege 'The Frege Reader' edited by Michael Beaney]

>"Now I call that part of the content that is the *same* in both the *conceptual content*. Since *it alone* is of significance for our ideography, we need not introduce any distinction between propositions have the same conceptual content."[pg.12 Frege 'From Frege to Godel' Third Printing edited by van Heijenoor]

This is another way of saying that the contexts of Frege's propositional conceptual contents are extensional: conceptual content is defined by its invarience over implication which is sufficient to guarentee extensionality in such contexts.
While Carnap is known for his principle of extensionality (later demoted to the mere conjecture that intensional contexts can be paraphrased by extensional contexts), it is Quine's "Word and Object" that I have in mind when I say that invarience over implication guarentees extensionality.

It is because the contexts of Frege's propositional conceptual contents are extensional that I can present the following elaboration of transition from sense/meaning to schematics.

The next fragmentary step of my view of Frege is here where the examples are further analyzed [An Analysis of “At Plataea the Persians were defeated by the Greeks”](#2025-0429-1407-an-analysis-of-at-plataea-the-persians-were-defeated-by-the-greeks)

Or you can jump over that and just go here: [2025 0429 1424 A Preliminary Outline of The Method of Bringing Frege Through Quine](#2025-0429-1424-a-preliminary-outline-of-the-method-of-bringing-frege-through-quine).


## 2025 0427

### 2025 0427 2333
First editorial comment added to an older note: the shortener at the end of [Bit Strings and Binary Trees](#2025-0413-1513-bit-strings-and-binary-trees) does not work in a degenerate case that is now described there!

### 2025 0427 2230 A Quick Response to Frege's "Thought"
I skimmed Frege's paper on thought.
Almost everything he says is covered by B. F. Skinner's theoretical analysis in "Verbal Behavior" 1957.

A theory of behavior may admit abstract items designated by a description of a contingency of reinforcement e.g. "a door opens from a push on a lever" can be made to designate an abstract ordered pair whose first component is a concrete chunk of doorish spacetime and whose second component is an ordered pair whose first component is a slice of spacetime which intersects the doorish chunk and the lever chunk through the chunk of behaving organism (perhaps narrowed by setting up a mechanical device called a detector or cumulative recorder) and whose second component is the leverish chunk.
This ordered pair is abstract in that its existence does not depend on a mental or physical theory: the components are singular descriptions and not singular terms.
That is, the ordered pair exists whether the components do or not.

You can do all of this and bring Skinner's analysis of verbal behavior into the abstract realm of orderd pairs to get something beyond 'x from y on z' with descriptive premises for 'x', 'y', and 'z' (as in Quine's "Free Logic and Virtual Classes" of 1994).
I set this up to shift from propositions to ordered pairs where the theory of ordered pairs is clear: they are identical when their components are.
What both Quine and Skinner do is to make all these excursions to simulate some abstract object templated on a mental or physical analysis moot.
You can skip them entirely and nothing sacred disappears.

It may continue to feel as if the sacred has been paraphrased away, but if you stick with Frege then that would be letting psycholgoy intrude upon philosophy.

### 2025 0427 1326
The word 'sadness' contains 'adn' which is also one of my common mispellings of 'and'.

### 2025 0427 1304 My Concrete Outlook on Frege
I am forgoing my traditional methods of reading Frege for the following reasons.

I can state my outlook on Frege concretely: it is the interpretation of Frege by Quine in "On Frege's Way Out" of 1955 in light of all that Quine later worked out in, e.g., "From Stimulus to Science" of 1996.
What I can do is paraphrase all that can be brought through Frege without breaking our best logical practices.
I have already noted such latter day methods on Twitter when I wrote of schematic theories of predicates of extensionality.
I will do my best to write that up and read from Frege.

The key to this transition was provided by B. F. Skinner's theoretical analysis in "Verbal Behavior" of 1957 which ends with chapters on logic, science, and thinking.
Whereas Quine indulges the attitudinists with their properties, propositions, concepts, information, attributes, and other nonextensional contexts long enough to dismiss them, he does not explain them away (as many have subsequently griped).
Skinner provides the details of such an explanation and there is some path from Quine's method of semantic ascent and Skinner's analysis of autoclitic verbal behavior which covers past efforts by Frege, Russell, Whitehead, and the moderns like Per Martin-Löf primed by their ill fated allegiance to their fellow attitudenists.

Quine's paper "On Frege's Way Out" points up Frege's position as one where attributes/concepts/propositions/information/essences/properties (and whatever new nonextensional contexts have been cooked up while I was writing this sentence) are secondary to extensional contexts: it is Russell and whitehead who based their theory of extensional contexts of classes on the nonextensional contexts of attributes.

The transition is one from sense to schematics.
It is the one Quine promoted and which, I submit, is also promoted by Frege, though I accept that it is perhaps only through Quine's later work that the paraphrase can be judged as satisfactory and read as perfectly parallel to Frege in Begriffsschrift.
Frege is to be judged solely on Begriffsschrift: the rest of his work is commentary and is like barnacles that must be cleaned from the hull of humanity's greatest achievement.

There is one last reason for the urgency of this all: as verbal machines become more articulate they shall make it hard for most people to cling to nonextensional contexts.
Furthermore, those who dispense with them sooner rather than later, as prescribed by Skinner, are more likely to survive: they can skip intermediate ruminations on attributes.
This saves time and extends the reach of extensioanl contexts which are among our greatest verbal technologies.

---
For those who may not be familiar with the distinction between extensional and nonextensional contexts here is Quine's definition:
>“A context is extensional if its truth value cannot be changed by supplanting a component sentence by another of the same truth value, nor by supplanting a component predicate by another with all the same denotata, nor by supplementing a singular term by another with the same designatum. Succinctly, the three requirements are substitutivity of covalence, of coextensiveness, and of identity, salva veritate. A context is intensional if it is not extensional.”[pg. 90 Quine “From Stimulus to Science”].

## 2025 0426

### 2025 0426 1851 Purifying My Little Lisp
This continues my work on my little lisp from [202504211546](#2025-0421-1546).
If you haven't been following along then this is a good place to start, but, like the rest of the notes I've made thus far, it assumes that you know basic javascript and how to define functions by recursion.

Simplification does not defer design decisions: it pushes them out of the problem space entirely.
Upon approaching the design of my little lisp's reader, such simplifications occurred to me and, hence, solved major problems by disolving them rather than resolving them into simpler problems with simpler solutions.
This is a halmark of factoring as a problem elimination process rather than a problem solving process.
A problem can very often be factored out of existence faster than a solution can be found to it.

A pure lisp is one without any atoms.
The empty pair takes over the part played by the item previously designated by 'nil'.
The empty pair shall here be designated by 'theEmptyPair'.
Though I have a theory of ordered pairs in the works, I am committed here to javascript as the lingua franca of the internet.

```
let theEmptyPair={}
, isEmpty = x => x == theEmptyPair
, consOf = (x,y) => [x,y]
, carOf = x => isEmpty(x) ? x : x[0]
, cdrOf = x => isEmpty(x) ? x : x[1]; 
```
Identity is no longer a primitive:
```
let id = (x,y) =>
  (isEmpty(x) && isEmpty(y))
  || (!(isEmpty(x) || isEmpty(y))
     && id(carOf(x),carOf(y))
     && id(cdrOf(x),cdrOf(y)));
```
There is no longer the troublesome distinction between proper and dotted lists, between symbols or runes, and we are free to build such distinctions, if we so desire, from this simpler starting point.

Note, I was reluctant to start with a pure LISP because I am already familiar with the theory of ordered pairs upon which it may be based and had hoped to learn something new from the purported simplicity of more familiar implementations of LISP.
The design of the reader made it clear that there is nothing to be gained from mixing the construction of native and foreign items (the items of javascript and lisp respectively).

This switch to a simpler design also brings my little pure lisp into closer contact with my little concatenative language designed from simplifying Charles H. Moore's uhdForth (it is sad that so far there is no one who has written on uhdForth but me: the reason for this is that almost everything about uhdForth has to be deduced from short yet logically complete presentations in garbled youtube videos).

Under this simplification every item is a rune, a runic list, a proper list, and a number of dotted lists (no greater than the length of the proper list).
Thus, there is no longer a printer, but rather a collection of printers to be used depending on the context of the program as written.
This simplification also switches the emphasis from the printer to the reader: why?

There are no strings that cross the boarder from javascript to the pure lisp: strings are left behind!
Rather than wrestling with problems of peculiar implementations, we are left to establish whatever expedient conventions work for now.
They may be changed later, but such conventions do not change the lisp: they change how we set it up.
It shall later become clear that these conveniences are not about lisp at all, but rather the theory of ordered pairs.
An example of one such theory is [Finitary inductively presented logics](https://math.stanford.edu/~feferman/papers/presentedlogics.pdf) by Solomon Feferman.

There is one kind of item which crosses the boarder between javascript and my lisp which I have not mentioned all this time: the boolean values designated by 'true' and 'false' in javascript.
I've mentioned them in descriptions of past functions, but not as naturalized citizens of my lisp.
The traditional line is to take the empty pair as either the mark of truth or the mark of falsehood.
This can not be done with javascript without digging into late manglings of the language (though it is hard to tell when something that is already mangled is mangled more).
The reason is one which I have also not mentioned overtly, but which I shall mention now: I've been using the javascript ternary conditional notations 'p ? t : f' and hidden versions of the same in 'p && q' and 'p || q'.

In javascript, such expressions control the execution of their subparts e.g. if 'p' designates the same as 'true' then 't' is executed in 'p ? t : f' and 'f' is executed otherwise.
Mixing 'executed' and 'designated' and all such things here has gone against my better principles, but these are not the things I aimed at addressing here so I'm moving on without further comment.

The recursive definition of the function designated by 'id' above is not simple, but the following functions are and they establish a key convention:
```
let singletonListOf = x => consOf(x,theEmptyPair)
, singletonStackOf = x => consOf(theEmptyPair,x);
```
Almost all of the conventions that I shall adopt here are from my concatenative language that I have yet to present here.

The singleton functions provide a simple way of tallying, and hence transforming a native javascript number into a foreign tally:

```
let tallyOf = n => 
  n>0 ? singletonListOf(tallyOf(n-1)) : theEmptyPair;
```

Then, to read an external character is to find its index in some alphabet and take the tally of that:

```
let abc =[...'0123456789abcdefghijklmnopqrstuvwxyz ()']
, alphabeticalIndexOf = letter => abc.indexOf(letter)
, alphabeticalLetterOf = index => abc[index];
```

There is an even more devilish way of going back and forth acrosst he boarder between javascript and my little lisp.
Rather than use tallies, a bit based method can be introduced e.g. only a bit at a time crosses the boarder one way or the other.
I leave that for a few code fragements later.

The old names for functions can now be used anew (after introducing the familiar string functions under slightly different names):

```
let theEmptyLetter=''
, isEmptyLetter = x => x == theEmptyLetter
, concatenationOf = (...strings) => 
   strings.length ? strings.shift() + concatenationOf(...strings) : theEmptyLetter
, firstLetterOf = letters => isEmptyLetter(letters) ? theEmptyLetter : letters[0]
, restLettersOf = letters => isEmptyLetter(letters) ? theEmptyLetter : string.slice(1);
```
Oops! Forgot the inverse function to the one that makes tallies:
```
let countOf = x => isEmpty(x) ? 0 : 1 + countOf(cdrOf(x));
```
Now old names can be used anew!
```
let runeOf = letter => tallyOf(alphabeticalIndexOf(letter))
, runicListOf = letters => 
   letters.length ? consOf(runeOf(firstLetterOf(letters))
    , runicListOf(restLettersOf(letters)))
   : theEmptyPair
, letterOf = x => letterAtIndexOf(countOf(x))
, stringOf = x =>
   isEmpty(x) ? theEmptyString
   : concatenationOf(letterOf(carOf(x)),stringOf(cdrOf(x)));
```
Some much needed examples:
```
countOf(tallyOf(3)) 
 3
letterOf(runeOf('a')) 
 a
lettersOf(runesOf('this is a test')) 
 this is a test
```
Now for some strange examples that more closely reveal what I hinted at long ago about the joys of letting degenerate cases flourish:
```
letterOf(tallyOf(3)) 
 3
lettersOf(runeOf('a')) 
 0000000000
countOf(runesOf('this is a test'))
 14
```
For those who may have been put off by the definitions of the functions designated by 'tallyOf' and 'countOf' since they seemed to favor singleton stacks over singleton lists, I hope the last example calms your nerves: these conventions are selected by their native logic in javascript (and in any other language which purports to cover arithmetic and concatenation for that matter).
Said poetically: economize on externalities.

A further consequence of these simplifications is that I can bring together the work done from [2025 0413 1513 Bit Strings and Binary Trees](#2025-0413-1513-bit-strings-and-binary-trees) and explain how to cross the boarder between langauges bit by bit without leaving our native language:

```
let theEmptyStack = theEmptyPair 
, isEmptyStack = isEmpty
, pushOf= (stack,item) => consOf(stack,item)
, popOf = carOf
, peekOf = cdrOf
, topOf = stack => peekOf(stack)
, secondOf= stack => peekOf(popOf(stack))
, pop2Of = stack => popOf(popOf(stack))
, pairUpOf = stack => pushOf(pop2Of(stack),consOf(secondOf(stack),topOf(stack)))
, pairUpEachOf = stack =>
  isEmpty(popOf(stack)) ? topOf(stack)
  : pairUpEachOf(pairUpOf(stack))
, encodeHelper = (digits, stack) =>
  isEmptyLetter(digits) ? pairUpEachOf(stack)
  : isZeroDigit(firstLetterOf(digits)) ?
     encodeHelper(restLettersOf(digits), pushOf(stack,theEmptyPair))
  : isOneDigit(firstLetterOf(digits)) ?
     encodeHelper(restLettersOf(digits), pairUpOf(stack))
  : encodeHelper(restLettersOf(digits),stack)
, encode = digits => encodeHelper(digits,theEmptyStack);
```
with some examples (notice the last one)
```
decode(encode('001')) 
 001
decode(encode('0000')) 
 0000111
decode(encode('10100')) 
 001010011
decode(encode('00101011')) 
 000101011
decode(runeOf('a')) 
 000000000001111111111
lettersOf(encode(decode(runesOf('this is a test')))) 
 this is a test
```

That's enough for today.

### 2025 0426 1845 To and From Predicate Functor Logic
This uses notation from [A Stack Notation for Predicate Functor Logic](#a-stack-notation-for-predicate-functor-logic-2025-0414-1626).

Here I give a few quick examples of how to translate a sentence of quantificational logic to one of predicate functor logic.
As I finish up my little lisp I'll write out mechanical algorithms for accomplishing each of these steps and look for a few shortcuts.

As an aside, it is possible to reduce all of predicate logic to just three predicate functors:

1. '..xy(drop F)..z' for '..xF..z'
2. '..xyz(hem F)..a' for '..xyzFy..a'
3. '..x(F huh G)a..b' for 'some item is {y such that not (..xayF..b and ..xayG..b)}'

I'll save that reduction for another time maybe.

The basic predicate functors in this presentation of homogenization will be then

1. '...xy(drop F)...z' for '...xF...z'
2. '...wxy(hem F)...z' for '...wxyFx...z'
3. '...x(push F)y...z' for '...xyF...z'
4. '...x(not F)...y' for 'not ...xF...y'
5. '...x(F and G)...y' for '...xF...y and ...xG...y'
6. '...x(some F)...y' for 'some item is (z such that ...xzF...y)'.

In the linked note I show how to get 

7. '..xy(dup F)..a' for '..xyyF..z'
8. '..xy(pop F)..a' for '..xFy..a'
9. '..xyz(swap F)..a' for '..xzyF..a'

from those basic six starting functors.
With the appropriate combination of swaps, pops, and pushes, it is possible to move the variables of any predicate into any position:

10. '..xyz(swop F)..a' for '..xyz(swap pop F)..a' i.e. '..xzFy..a'
11. '..x..yz(ret^n F)..a' for '..x..yz(swop^n push^n F)..a' i.e. '..xz..yF..a' where '..y' is a list of n variables.

Note, for a predicate functor designated by 'f' the notation 'f^0' designates the same as 'f' and 'f^(n+1)' designates the same as 'f^n f' so that each predicate functor issues in its iterates in the expected way.

The pidgin 'ret' is short for 'retrojection' which I take from Quine's explanation of the permutational part of homoginization.

Now  predicates in quantificational logic are said to be of degree n when they have n variables attached to them (most often on the right hand side) e.g. 'Fxyxxzu' is of degree six and 'Gyxyxy' is of degree five.

The first step in translating a closed sentence of quantificational logic into a predicate functor is to attach the degree of each predicate in the sentence to its name as a numeral (this does not actually involve arithmetic in the notation any more than counting the number of variables attached to a predicate of quantificational logic does)e.g. 'Fxyxxzu' becomes 'F6xyxxzu' and 'Gyxyxy' becomes 'G5yxyxy'. This must be done for each predicate in the sentence being transformed. Sentence letters (e.g., often 'p', 'q', 'r') get carried over to degree zero predicates.

Next, to simplify matters, before or after attaching these numerals, the sentence is to be put into one of its equivalents which is compounded of only existential quantifiers, conjunctions, and negations.

Now, negations get taken over simply enough by bringing it into predicate functor form e.g. 'not Fxyz' becomes '(not F3)xyz' and 'some item is {x such that (not Fxyz) and (not Gux)}' becomes 'some item is {x such that (not F3)xyz and (not G2)ux}'.

Where ever there are duplicate occurrences of a variable they can be eliminated by retrojections, pops, pushes, and dups. A mechanical way of doing this to alphabatize the variables attached to a predicate and to put it in stack form (which favors the left hand side of a predicate) e.g. 'Fxyxzyz' becomes 'F6xyxzyz' then 'xxy(swap pop push^3 F6)zyz' to 'xxyyzz(pop swap pop^2 swap push^3 F6)' then to 

'(push dup push dup push dup pop swap pop^2 swap push3 F6)xyz'

which avoids retrojections and is adhoc really.

Now assuming this is done then it is only homogenization that is left and it only occurs when confronted by conjunctions e.g. given 

'Faxy and Gbz'

missing letters can be brought in by 'drop' e.g.

'bz(drop^2 F3)axy and ax(drop^2 G2)bz'

and alphabetized

'(..f drop^2 F3)abxyz and (..g drop^2 G2)abxyz'

by some appropriate sequence of functors designated by '..f' and '..g' respectively: the functors '(..f drop^2)' and '(..g drop^2)' are said to homogenize 'F' and 'G' respectively. This can obviously be generalized to connectives of more components than the two of primitive conjunction. Finally then, the 'and' is brought in so that

'((..f drop^2 F3) and (..g drop^2 G2))abxyz'.

Last, but not least, the variable of an existential quantifier is brought to the top of the left stack by appropriate pops, then 'some' is brought in e.g.

'some item is {z such that Fxyz}'

becomes 

'some item is {z such that xyz(pop^3 F3)}'

and then

'xy(some pop^3 F3)'.

Together these complete the translation to and from predicate functor logic.

## 2025 0425

### 2025 0425 2042 Reading the Frege Reader: The First Sentence
This is a record of my reading of "The Frege Reader" edited by Michael Beany.
It was recommended to me by R.P. (@ResonantPyre).
Since this is my first public reading of a primary text (I do not read secondary texts--- like I [have](#2025-0420-2247) Durant's "Story of Philosophy"--- as I might Plato's dialogues) I shall say a bit on what I do when I read.

First, this is a record of my verbal responses to my exposure to the text.
To read is to do more than listen with my eyes.
It is built upon gazing at the page and the marks upon it.
The narrower repertoire of looking, upon which the complex repertoire of seeing is built and upon which reading is built, bring the marks into focus and I can begin to see words, perhaps letters, sentences, and, the key feature of seeing, all that I see to myself which is not to be found on the page or in its marks.

To read is to go beyond seeing as listening with my eyes.
It is speaking in response to sights of, sounds of, feelings of, etc.
By uncovering what I have to say I am more likely to discover what I am thinking and feeling.
There is still much I have to learn about how best to grasp my thinking, my feeling, and what of past and present environments may be controlling it now.

Without further ado!

I opened to the page with all the publishing informatin on it.
It was first published in 1997.
Immediately, the first name of the dedication caught my eye "Peter Geach".
It occurred to me that he is mentioned by Quine, perhaps in "From stimulus to Science" where Quine discusses pronouns of laziness.
After checking the page (I can see to myself where in the book Quine would have written about that), it is found that yes, Geach is where Quine got the phrase "pronouns of laziness" (they are, for Quine, a focal point of reification).

Now I turn to the contents of "the Frege reader", and, as I very often do, I write them down:

1. Begriffsschrift (1879): Selections (preface and Part I)
2. 'Letter to Marty, 29.8.1882'
3. The Foundations of Arithmetic (1884): Selections (introduction and $$1-4, 45-69, 87-91, 104-9; with summaries of the remaining sections)
4. 'Function and Concept' (1891)
5. 'Letter to Husserl, 24.5.1891': Extract
6. 'On Sinn and Bedeutung' (1892)
7. '[Comments on Sinn and Bedeutung]' (1892)
8. 'On Concept and Object" (1892)
9. Grundgesetze der Arithmetik, Volumen I (1893): Selections (Preface, introduction, $$1-7, 26-29, 32-33)
10. 'Review of E. G. Husserl, Philosophie der Arithmetik I' (1894): Extract
11. 'Logic' (1897): Extract
12. 'On Euclidean Geometry' (c. 1900)
13. 'Letter to Russell, 22.6.1902': Extract
14. 'Letter to Russell, 28.12.1902': Extract
15. Grundgesetze Der Arithmetik, Volumen II (1903): Selections ($$55-67, 138-47, Appendix)
16. 'Letter to Russell 13.11.1904': Extract
17. 'Introduction to Logic' (1906): Extract
18. 'A brief Survey of my Logical Doctrines' (1906): Extract
19. 'Letters to Husserl, 1906'
20. 'Logic in Mathematics' (1914): Extract
21. 'Letter to Jourdain, Jan. 1914': Extract
22. 'My Basic Logical Insights' (c. 1915)
23. 'Thought' (1918)
24. 'Negation' (1918)
25. '[Notes for Ludwig Darmstaedter]' (1919)
26. 'Sources of Knowledge of Mathematics and the Mathematical Natural Sciences' (1924/5): Extract
27. 'Numbers and Arithmetic' (1924/5)

It contains selections from his larger works which I shall get at a later time (though I do wish there was a single complete volume with all his texts and relevant major correspondances).
They are presented in chronological order and are mostly extracts: I do not yet know what the difference is between an extract and a selection.

Next, the preface, though I turned to appendix 2 on Frege's logical notation to take a look at its layout.
I take Frege's notation as one of the greatest accomplishments in human history and was likely drawn to it from how often I ahve been reinforced by it in the past.

The preface says that Frege's works can not be allowed to "speak for themselves" to a student for there are subtleties that may easily go unnoticed but which are key to firmly grasping Frege's collective works.

I am impatient and shall skip any further introductory matters and go straight to Frege's first work "Begriffsschrift: a formal language of pure thought modeled on that of arithmetic".
Again, I am met with more commentary that I simply skip to get to Frege's "preface":

> "The recognition of a scientific truth generally passes through several stages of certainty."[pg. 48]

Already, the developmentalistic metaphors which plague psychology and the other nonbiological sciences appears: "passes through several stages" just as a child is said to pass through grades from kindergarten onwards and upwards.
For those interested in precise and accurate reports on changes in human behavior--- which 'recognition' purports to be--- it is not the passage of time between stages that matters (or, the metaphorical extension of the same to "certainty"), but rather what happens as time passes.

In the end, any developmentalist, when pressed, can give some description of purportedly characteristic features of any performance to be found within a deliniated stage.
Whether such characteristic descriptions are maintained or whether a given sequence of corresponding stages seperated by changes in characteristic performances are maintained is a problem of statistical aggregation.
For one aimed at something more foundational than flexable predictions relative to the overlap of an individual's behavior among differing performances in differing stages of development there is little of consequence to be kept besides the vague gesturings to unexamined environments.

It is the 'recognition' that "generally passes through stages of certainty", not the "scientific truth" itself.
Might "recognition" just as well be replaced by "cognitition"?
Are there nonscientific truths?
Is this law of recognition itself subject to a staged development?
If so, what stage is it in?
What stages are there?
Is it a spectrum from uncertainty to certainty as the word "certainty" suggests?
Is the generality here scientific or logical?

Following Skinner's principle "Etymology is the archeology of thought." from his paper "The Origin of Cognitive Thought" I shall show the etymology of "recognition" back to its proto-indo-european roots and say a bit more about why this is relevant: (each of the following selections are from <https://www.etymonline.com/> !)

1. recognition(n.)--- mid-15c., recognicion, "knowledge (of an event or incident); understanding," from Old French recognition (15c.) and directly from Latin recognitionem (nominative recognitio) "a reviewing, investigation, examination," noun of action from past-participle stem of recognoscere "to acknowledge, know again; examine" (see recognize).

   Sense of "acknowledgment of a service or kindness done" is from 1560s. Sense of "formal avowal of knowledge and approval" (as between governments or sovereigns) is from 1590s; especially acknowledgement of the independence of a country by a state formerly exercising sovereignty (1824). The meaning "a knowing again, consciousness that a given object is identical with an object previously recognized" is by 1798 (Wordsworth). The literary (especially stage) recognition scene "scene in which a principal character suddenly learns or realizes the true identity of another character" is by 1837 (in a translation from German).

2. recognize(v.)--- early 15c., recognisen, "resume possession of land," a back-formation from recognizance, or else from Old French reconoiss-, present-participle stem of reconoistre "to know again, identify, recognize," from Latin recognoscere "acknowledge, recall to mind, know again; examine; certify," from re- "again" (see re-) + cognoscere "to get to know, recognize" (see cognizance).

   With ending assimilated to verbs in -ise, -ize. The meaning "know (the object) again, recall or recover the knowledge of, perceive an identity with something formerly known or felt" is recorded from 1530s.

3. cognizance(n.)--- mid-14c., conisance, "device or mark by which something or someone is known," from Anglo-French conysance "recognition," later, "knowledge," from Old French conoissance "acquaintance, recognition; knowledge, wisdom" (Modern French connaissance), from past participle of conoistre "to know," from Latin cognoscere "to get to know, recognize," from assimilated form of com "together" (see co-) + gnoscere "to know" (from PIE root *gno- "to know").

   Meaning "knowledge by observation or notice, understanding, information" is from c. 1400. In law, "the exercise of jurisdiction, the right to try a case" (mid-15c.). Meaning "acknowledgment, admission" is from 1560s. The -g- was restored in English spelling 15c. and has gradually affected the pronunciation, which was always "con-." The old pronunciation lingered longest in legal use.

4. *gno-, Proto-Indo-European root meaning "to know." It might form all or part of: acknowledge; acquaint; agnostic; anagnorisis; astrognosy; can (v.1) "have power to, be able;" cognition; cognizance; con (n.2) "study;" connoisseur; could; couth; cunning; diagnosis; ennoble; gnome; (n.2) "short, pithy statement of general truth;" gnomic; gnomon; gnosis; gnostic; Gnostic; ignoble; ignorant; ignore; incognito; ken (n.1) "cognizance, intellectual view;" kenning; kith; know; knowledge; narrate; narration; nobility; noble; notice; notify; notion; notorious; physiognomy; prognosis; quaint; recognize; reconnaissance; reconnoiter; uncouth; Zend.

    It might also be the source of: Sanskrit jna- "know;" Avestan zainti- "knowledge," Old Persian xšnasatiy "he shall know;" Old Church Slavonic znati "recognizes," Russian znat "to know;" Latin gnoscere "get to know," nobilis "known, famous, noble;" Greek gignōskein "to know," gnōtos "known," gnōsis "knowledge, inquiry;" Old Irish gnath "known;" German kennen "to know," Gothic kannjan "to make known."

Most people are stuck on linguistics and philosophy when trying to figure out what something means.
The science of behavior skips over them and goes straight to the contingencies (a contingency is a consequence from response on occasion e.g. a door opens from a push on a lever) of which we now only remotely speak.
It is through "The Diversions of Purely" by John Horne Tooke that Skinner and so many others have come to better appreciate the role of etymology in the analysis of verbal behavior.

The control that each part of a text has over the responses of a reader are almost entirely the result of etymology as the history of the characteristic consequences of verbal responses of given form on characteristic occasions.
We now know that the wheel, horses, and most language descend from the bronze-age peoples of the eurasian steppes (see "The Horse, the Wheel and Language" by David W. Anthony 2007).
Most cultures of the world remain under the control of circumstances which are not very different from those of the bronze age people, and each word of a modern language which purports to mention or refer to a mentalistic object such as an idea or thought can be traced through to its PIE root which reveals the concrete contingencies from which we speak though we are oh so remote from such ancient speakers.

Thus, going from "recognize" to "*gno-" and back again through "can" and "could" as "have power to", "be able to", or through "acquaint" as "to be near to" or "to bring near" or "intimate with", we reach out to the concrete contingencies which control the verbal behavior that is a consequence of them.
B. F. Skinner has the following to say as an introduction to "knowing":

> "We say that a newborn baby knows how to cry, suckle, and sneeze. We say that a child knows how to walk and how to ride a tricycle. The evidence is simply that the baby and child exhibit the behavior specified. Moving from the verb to the noun, we say that they possess knowledge, and the evidence is that they possess behavior. It is in this sense that we say that people thirst for, pursue, and acquire knowledge.
> 
> But this brings us at once to the question of what it means to possess behavior. ... to say that a response is emitted does not imply that it has been inside the organism. Behavior exists only when it is being executed. Its execution requires a physiological system, including effectors and receptors, nerves, and a brain. The system was changed when the behavior was acquired, and it is the changed system which is "possessed." The behavior it mediates may or may not be visible at any given moment. There are parallels in other parts of biology. An organism "possesses" a system of immune reactions in the sense that it responds to invading organisms in a special way, but its responses are not in existence until it is being invaded. It is often useful to speak of a repertoire of behavior which, like the repertoire of a musician or a company of players, is what a person or company is capable of doing, given the right circumstances. Knowledge is possesed as a repertoire in this sense."[pg. 152 Skinner "About Behaviorism"]

And then, jumping ahead to Skinner's section "Knowledge as Power and as Contemplation"

> Much of what is called contemplative knowledge is associated with verbal behavior and with the fact that it is the listener rather than the speaker who takes action. We may speak of the power of words in affecting a listener, but the behavior of a speaker in identifying or describing something suggests a kind of knowledge divorced from practical action.
Verbal behavior plays a principal role in contemplative knowledge, however, because it is well adapted for automatic reinforcement: the speaker may be his own listener.
There are nonverbal behaviors having the same effect.
Perceptual responses which clarify stimuli and resolve puzzlement may be automatically reinforcing.
"Getting the meaning" of a difficult passage is similar.
... Contemplation of this kind would be impossible, however, without a previous exposure to contingencies in which action is taken and differentially reinforced."[pg. 155-156 Skinner "About Behaviorism"]

So it is that I return to the first sentence from Frege

> "The recognition of a scientific truth generally passes through several stages of certainty."[pg. 48]

The word "recognition" is replaced with "*gno-" and the rest comes through a careful analysis of what people do when they are said "to know".
It is not passing through "several stages of certainty" that strengthens the possession of a scientific truth as a kind of knowledge or cognitition or even a conception (of which I expect there to be much talk later), but rather the concrete contingencies to which the knowing organism is exposed and which result in the contemplative knowing which purportedly reaffirms itself as in automatic reinforcement of a trained "knower of scientific truths".

I shall not take so long with each sentence as I submit that most of them cluster around a few threads of etymology that bring us back to the science of behavior and then follow from there.
There is much more to go yet I shall leave this here for now.

Tomorrow I may even get to the second sentence :)  

### 2025 0425 2005 Some Origins of My Interst in the History of the World
My interest in the history of the world is largely the result of my interest in the history of science and technology.
Technologies are concrete artifacts of cultural practices.
They mediate conspicuous behaviors and are an easy access point to the study of social/cultural selection and variation.

The sewing machine is my goto example.
The mechanization of the behaviors which once dominated the practices of sewing make it so that the consequences of sewing appear far more often without the direct participation of individaul sewers.
Repertoires of sewing are almost extinguished as the remote control of sewing machines eliminates the contingencies that once taught the people of the world to sew.

Washing machines are perhaps better known for the labors they are said to save.
While it may come easy to us to say that labors are saevd by washing machines, especially when the push of a button has the same consequences which were once the result of a long chain of responses, it is more accurate to say that a change in the environment changed prevailing behaviors and to seperate such changes from teh value judgements that are so quickly provided by a majority of our ethical practices.

The primary reason to stick with an accurate report of behavior rather than amending it with a value judgement is that the ethical practices which bring such judgements to the judge are not yet themselves the result of a conclusion from contemplation on accurate templates of the world.
More than that, they are very rarely so: the science of behavior is new and has had little to no effect on prevailing ethical practices in any of the cultures of the world.
It is to history that I look to see the changes in cultural practices that shaped modern ethical practices specifically and human practices generally.

Our world is now one with much more than just sewing and washing machines.
We now have verbal machines that do for verbal behavior what sewing and washing machines do for sewing and washing.
Fewer people are required than ever before to respond verbally in ways which once prevailed.

Computers have long since provided remote listeners (called electrical/computer engineers) with control over the behavior of computer programmers.
Circuits, once elaborate networks of relays, mediate the consequences of a listerner's response to the speech of programmers: the programmer sets up bits as a consequence of the verbal responses we call "computer programming" and the computer mediates the consequences of having exposed teh circuit designer to those bits without the circuit designer having any direct contact with the programmer as a contemporaneous listener.

This may all seem needlessly detailed or obtuse to anyone unfamiliar with the science of behavior and, specifically, the experimental analysis of behavior in laboratory environments.
Many are apt to say that what I wrote above is already a functional part of the vernacular and goes without saying.
But, it can not be so, especially in light of human history, where we so often find similarly authoritative proclamations about this or that kind of control over the world and our ethical responses to it.
The science of behavior has had little to no effect on the practices of programming, much less on our ethical practices in response to them.

So it is that my interest in history is the systematic reevaluation of reports on history in light of the science of behavior through the experimental analysis of behavior.


### 2025 0425 1943

R.P. (@ResonantPyre) has given me a few great book recommendations over the past few months:

* "The English and their History" by Robert Tombs
* "Wandering Significance" by Mark Wilson
* "A History of Philosophy without any gaps" by Peter Adamson (of which there are seven, for now)
* "The Frege Reader" edited by Michael Beaney

I have not yet bought "The English and their History".
Wilson's "Wandering Significance" was the fist that I bought up on R.P.'s recommendation.
It was recommended as a result of some conversations we had on Quine and physics.
Wilson is not the kind of writer that I'm used to: he takes a winding path around every single point rather than sticking it in your face.
One of the reasons I so love B. F. Skinner's writings is that he goes out of his way to stick your nose in everything he has to say.

"Wandering Significance" is presented as a very long essay on the origins and operations of meaningful language (though it is often much much more than that).
There was very little of it that I read and am likely to repeat: specific examples in physics didn't line up with what I had repeatedly witnessed physicists do (inside the lab and outside it).
May sections felt like the long delays you see in a movie where a person is rushing to finish a homecooked meal before the jig is up.
Though I may come back to it later, what little it has to say about Quine and Skinner combined with its strange outlook on the practices and behaviors of phycisists left me uninterested in reading more.
That has not stopped me in the past though, so the future of this book is open.

I just got the first volume of "A history of philosophy with gaps" on "classical philosophy" by Adamson.
I've flipped through some pages, read an introduction, and was excited to see that there were many references to primary texts that were prominently placed and mentioned.
This made me very happy: I am looking for a more comlpete and comprehensive yet singularly organized outline of major philosophers and their texts and this may just be it.

As much as I enjoy clicking through a few Wikipedia articles to jog my memory when I've fogotten some detail I once knew, I can not recommend it for anything much more than that.
This seems to especially be the case when it comes to philosophy.
There's just too much that people have said, and too little cohesion between entries.
As I get through Durant's "Story of Civilization" I'll have much more to say about monolithic efforts that have a kind of grand unification without presumming cosmic authority (this problem was already brought up from my reading of Bourbaki which is a series built by committee but governed by logical and mathematical practices which do much of the heavy lifting when it comes to cosmic unifications and simplifications).

My readings of Frege are limited to "The Foundations of Arithmetic" and what can be found in "From Frege to Godel" (a text that any student of logic must have: it is truly indispensible).
"The Frege Reader" says it covers his major writings: I still must buy his other books to read them in full.

Books are some of my favorite things.

### 2025 0425 1925
While I had hoped to write every day without breaking my streak, it has been two days since I last wrote something here.
My only excuse, if there is even a need for one (which I think there is not), is that I had some wonderful conversations with people about things I will soon be writing on (I aim to summarize two of the conversations I've had), but also I have been recovering from a workout injury.

For most of my life I did little more than math, physics, piano, and clarinet.
As I have grown older I have made more friends than I ever imagined.
Some of them are so kind and so generous with their time that they teach me new things.
A few months ago I was lucky enough that a family friend had found a new way to spend his retirement by being my weightlifting coach.
He spent most of his life weightlifting and competing in Highland games and he had a gym in his basement.

All of these things came together and because of what he has learned through his life I have been slowly strengthening my body in ways similar to how I have strengthend my mind.
Sadly, I swung a kettlebell the wrong way and was given an a cute pain in my back which I am still recovering from.
Thankfully, it has gotten better by the day and I have found it easier and easier to get into and out of chairs.

I let myself indulge in a sorta lavish relaxation all in the name of "letting my back heal".
Though I did not stop writing and reading, I did stop working here at the computer keyboard.
It will be interesting to see what other gaps occur in my writings as the blizzard of life envelops me anew each day.

## 2025 0422

### 2025 0422 2322 An Incomplete Sketch of My Philosophy of Logic
The signifcance of Quine's predicate functors can not be understated by me. He showed, by way of predicate functors, that variables are not a neccessary part of predicate logic, and hence, baring the embrace of some weaker practices purporting to undermine predicate logic, no logical theory involves variables neccessarily.

Thus, given a theory and its predicate logic, the predicate functors of Quine give a way, once and for all, to rid ourselves of the lgoical import of pronomial cross reference. This eliminates, then, all problems which turn up from such cross referencing e.g. all problems of scope and bondage.

It also turns our focus back to the probelms to solve: what is the logic of the theory under investigation? In predicate (functor) logic this means settling what predicates belong to the lexicon of the theory. The requirement that the lexicon be mentioned by *listing* the predicates is preformally stated as "the lexicon is finite", though the emphasis here is on the preformality of that statement otherwise the invocation of the predicate 'is finite' quickly demands its own logical theory.

Now, with the lexicon given, the paucity of premises is among the next steps. I say "among the next steps" because it is often through a sequence of steps that the predicates and premises of a given theory evolve from the conclusions of their implication.

It is here, at implication, where the supremacy of predicate logic is without question except for those who are wedded to deviant methods that always liven the outer realms of any inquiry.

But, let me linger here lest I be seen as some immoral logical supremacist! Thanks to Quine's main method, implication can be explained entierly by two short sentences "for those abreast of the jargon, it is as follows. To prove that a given set of premises implies a contemplated conclusion, prove that the premises are inconsistent with the negation of that colclusion. Do so by putting the premises and the negated conclusion into prenex form and then accumulating a truth functional inconsistency by persistent instantiation of the universal and existential quantifiers, taking care to use a new variable for each existential instantiation."[pg. 51 Quine "From Stimulus to Science"].

Whatever else anyone says they *actually mean* by implication can only be grasped natively by foreign import i.e. paraphrase into a logical theory as above.

This is staunch. It is also repulsive to many. I accept all such responses to such declarations as relevant to a full theory of logical and scientific behavior. What can not be accepted are methods which, e.g., break extensionality of a theory by recourse to some deviant logic: "A context is *extensional* if its truth value cannot be changed by supplanting a component sentence by another of the same truth value, nor by supplanting a component predicate by another with all the same denotata, nor by supplementing a singular term by another with the same designatum. Succinctly, the three requirements are substitutivity of covalence, of coextensiveness, and of identity, salva veritate. A context is *intensional* if it is not extensional."[pg. 90 Quine "From Stimulus to Science"].

Without extensionality, intersubjective agreement by qualified witnesses is verbally bankrupt. It is already hard enough to grasp a theory with its necessary (but not sufficient) predicate logic. But, to do so without predicate logic is to plunge the world into the caprice of uncontemplated cooperation. There can be no verbal coordination without extensionality.

Note, none of this says that a theory is its predicate logic. Whatever more a theory may be is hard to say, in the same way that it is hard to say when a given definition of verbal behavior is appropriate.

With all of that wawa by the way, let me give you a quick path from familiar and traditional quantificational logic through Quine's predicate abstracts (and their principle of concretion), to predicate functors so that you might more firmly grasp where I am coming from.

First, take '{x:Fx}y' as short for 'some item is {u such that u=y and some item is {x such that u=x and Fx}}'. Elsewhere I may come back and explain how it comes that the two place predicate of identity, written as '=' short for 'is identical to', is indiscernability in that they are coextensive and indiscernability is in hand from any lexicon like those here contemplated e.g. in a lexicon with one one place prediate 'F' and one two place predicate 'G' (these are not to be taken as schematic predicate letters as are used in the remainder) 'x is indiscernable from y' is short for 'Fx if and only if Fy, and each item is {u such that Gux if and only if Guy, and Gxu if and only if Gyu}'.

The English relative clause 'who loves Dick' and the pidgin 'x such that x loves Dick' are uniformly paraphrased by the *predicate abstract* '{x: x loves Dick}' which *abstracts* 'Tom' from 'Tom loves Dick' by *binding* the *free* occurrence of 'x' in the *open* sentence 'x loves Dick' with the prefix 'x:' so that the *predication* '{x:x loves Dick}Tom' *concretes* to 'Tom loves Dick': whatever can be said of a thing can be said by predicating a predicate of it i.e. *predicational completeness*.

Predicate abstracts as the logical import of the relative clause are the import of pronomial reference and respective problems of freedom and bondage.

The principle of concretion comes along for free as follows (assuming either that the lexicon of the language is finite or there is a predicate of identity provided subject to the following familiar constraints in either case).
Godel's premises of identity are 1) identity is reflexive, i.e. 'x=x', and 2) identity is substatutive, i.e. each instance of the schema of substitutivity 'x=y and Fx, only if Fy'. They are equivalent to Wang's premises, i.e. each instance of Wang's schema 'Fx iff some item is {y such that x=y and Fy}'. By two instances of Wang's schema, 'some item is {u such that u=y and Fu} iff some item is {u such that u=y and some item is {x such that u=x and Fx}}' and 'some item is {u such that u=y and Fu} iff Fy', '{x:Fx}y iff Fy' is true.

In a truth functional and quantificational logic of predicates without a predicate of identity '{x:Fx}y' is introduced by the schema of concretion '{x:Fx}y iff Fy'. Concretion ties relative clauses as predicate abstracts into a truth functional and quantificational logic of predicates.

Predicate abstracts of many places are defined with the help of 
'..x' for 'x.0 x.1 .. x.length(x)-1' where 'x' is a list of variables and 'x..y=u..v' for 'x=u and ..y=..v' (the case where 'y' and 'v' are of different lengths is handled by '..x=..y' for 'x.0..x.min(len(x),len(y)) = y.0..y.min(len(x),len(y))' so that
'{..x:F..x}..y' for 'some item is {..u such that ..u = ..y and some item is {..x such that ..u = ..x and F..x}}'
with the schema of generalized concretion following as
'{..x:F..x}..y iff F..y'.

Predicate abstracts like '{xy:Fxy}' are predicates just like 'father (of)' or 'older than'. They are not items like sets or relations. It so happens that extensionality along with a few meager assumptions on the existence of sets-- e.g. that they yield a serviceable theory of ordered pairs-- often permits predicate abstracts to moonlight as designators of abstract items like sets. Nothing like that is found here, but shall eventually be found elsewhere when I eventually get to that.

(Note that the present methods do not incorporate the stack based notation that I've recently uncovered. I have yet to carry that efficiency through this sequence of arguments.)

The basic recombic predicate functors can now be introduced as abbreviations for the appropriate predicate abstract:

* Major Padding) 'Pad F' or 'drop F' for '{x..y:F..y}'
* Minor Padding) 'pad F' for '{..xy:F..x}'
* Reflection) 'refl F' or 'dup F' for '{x..y:Fxx..y}'
special cases e.g. the relfection of a one or no place predicate is that predicate
* Permutation [needs a better name]) 'Perm F' for '{xy..z:Fx..zy}'
* Major Inversion) 'Inv F' for '{x..y:F..yx}'
* Minor Inversion) 'inv F' for '{xy..z:Fyx..z}'
* Retrojection) 'Ret.k F' for '{x..y..z:F..yx..z}' when length(y)=k

The basic logical predicate functors are then

* Alternative Denial) 'F nand G' for '{..x: not(F..x and G..x)}'
* Complement) 'comp F' or 'not F' or '-F' for 'F nand F'
* Alternation) 'F or G' for '(not F) nand (not G)'
* Joint Denial) 'F nor G' for 'not(F or G)'
* Complementary Conditional) 'F not only if G' for 'F nor not G'
* Conditional) 'F only if G' for 'not (F not only if G)'
* Converse Conditional) 'F if G' for 'G only if F'
* Complementary Converse Conditional) 'F not if G' for 'not(F if G)'
* Conjunction) 'F and G' for 'F not if (not G)'
* Biconditional) 'F iff G' for '(F only if G) and (F if G)'
* Exclusive Alternation) 'F xor G' for 'not(F iff G)'
* Minor Existential) 'some F' for '{..x:some item is y such that Fy..x}'
* Major Existential) 'Some F' for 'some^n F' where 'F' is an n place predicate and for a predicate funtor 'f' the iterates are 'f^1' for 'f' and 'f^(n+1)' for 'f^n f'
* Minor Universal) 'each F' for 'not some not F'
* Major Universal) 'Each F' for 'not Some not F'
* Inclusion) 'F => G' for 'Each(F only if G)'
* Converse Inclusion) 'F <= G' for 'G => F'
* Proper Inclusion) 'F > G' for '(F => G) and not (F <= G)'
* Converse Proper Inclusion) 'F < G' for 'G > F'
* Coextension) 'F <=> G' for '(F <= G) and (F => G)'.

Now for the predicate functors that are usually introduced as part of set theories:
* k Place Composition) 'F^m .k G^n' or 'F^m of^k G^n' for 'some^k ((Inv^(m-k) drop^(n-k) F) and (Inv^k drop^(m-k) Inv^(n-k) G))'
* Image) 'F^m " G^n' or 'F on G' for 'F of^n G'
* item) 'item' or 'U' (or sometimes '1' or 'the universe of discourse') for 'dup =' or 'refl ='
* void) 'nonitem' 'void' or sometimes '0' for 'not item' or '-U'

Some of the predicate functors from the theory of relations
* Symmetric) 'Sym F' for 'F <=> (swap F)'
* Asymmetric) 'Asym F' for 'F <= (not swap F)'
* Transitive) 'Trans F' for 'F => (F of F)'
* Intransitive) 'Intrans F' for '(not F) => (F of F)'
* reflexive) 'reflexive F' for 'each dup F'
* equivalence) 'equiv F' for '(reflexive F) and (trans F) and (symm F)'

therefore 'equiv =' is true. The next functors bridge something like the gap between "the relational part of a set" and their origins in predicate logic:

* two place part) 'F`' for 'some^(m-2) Inv^2 F'
* k place part) 'F`k' for 'some^(m-k) Inv^k F'
Further generalizations include slices of length k, and noncontiguous parts

Unlike relations, the complement '-F' of a two place predicate 'F' is a two place predicate, and '-F' is equivalent to '(-F)`', but where predicates come to do doulbe duty as names of their extension, the complement of a relation need not be the two place part of its complement. This is a tiny example of the way in which theories like the calculus of relations obscure their (stronger) logic. With the generalization of the k place part comes the generalization of composition. Is it possible to introduce the k place part from kplace composition cross products and confinements?

* cross product) 'F cross G' for (pad^n F) and (Pad^m G)'
* iterated cross) 'F^1' for 'F' and 'F^(k+1) for 'F cross F^k'
* right confinement) 'F]G' for 'F and (U^(m-n) cross G)'
* left confinement) 'F[G' for 'F and (G cross U^(m-n))'

component confinement follows from iterated left and right confinements. Together a left and right confinement is a sliced confinement

The following definitions introduce functional predicates and all that is often said to follow from theories of functions. Here, generalized composition is avoided, and, hence, the predicates operated on by the predicate functors are now two-place predicates.

* Functional) 'Func F' for '= => (F of (swap F))'
* Left Field) 'Lfied F' for 'F on U'
* Right Field) 'Rfield F' for '(swap F) on U'
* Field) 'field F' for '(Lfied F) or (Rfield F)'

The various theorems about functions then turn out to be applications of schema of functional predicates e.g. 'Func void', 'Func =', '(Func F) and (Func G), only if (Func (F of G))', and '(Func F) only if (Func (F and G))'
Talk of correlations comes up often and is an application fo the logic of correlative predicates:

* correlation) 'Corr F' for '(Func F) and (Func swap F)'

Then similarities are relative to a corraltive predicate

* similarity) 'F <=G=> H' for '(Corr G) and (F <=> Lfield G) and (H <=> Rfield G)'

Somewhere around this point in my exploration of predicate functor logic it became clear that what goes by the name 'naive set theory' is neither naive nor set theory: it is just logic, pure predicate logic. The notational abbreviations afforded by predicate functors reveal this This part of my exploration is where I saw the damage done by Tarski's success e.g. with set theoretic simjulations of logic and with comingling predicate logic and the calculus of relations. The entire affair is summed up by the difference between 'denotes' and 'designates'.

I've still avoided introducing the singleton of a predicate. It placys the part of singular terms in a pur predicate (functor) logic. It's just russell's theory of descriptions adapted to these variableless methods.

Singular predicates play the part once played by singular terms.

* Singular) 'Sing F' for 'not some^2 ((nip F) and (drop F) and (not =))' or 'each^2((nip F) or (drop F) or =)'

But, it is not a singular predicate which exactly plays the part once played by singular terms, it is another predicate which, for now, I call 'the singleton'

* singleton) '{F}' for 'each(= iff (nip F))'
* itemization) '{F,..,G}' for '{F} or {..,G}'

these carry over a lot of what went as finite set theory

Now the components of a predicate come through the image of the singleton (this is then an operation that generalizes 'application and the 'collective relate')

* components) 'F at G' or 'F{G}' for 'F"{G}'

The arguments of a two place predicate are those items of its second component which are true of that predicate with respect to only one first component. The definition is beautiful

* arguments) 'arg F' for 'some{F}'

Just as there is a serviceable theory of identity embedded in any logic with only finitely many predicates-- the universal closure of the conjunction of each biconditional of each parallel permutation of primitive predicates-- so to is there a serviceable theory of denotation.

Much earlier, i defined the left and right confinement from a predicate of equality. That was unnecessary. Those definitions become logically equivalent to the following when a predicate of equality is present.

* left confinement) 'F[G' for 'F and drop G'
* right confinement) 'F]G' for 'F and nip G'

Descending from the beautiful definition of argument of a predicate is the notorious method of function abstraction (aka lambda abstraction)

* lambda) 'lambda F' for 'F](arg F)'

note 'lambda lambda F' is equivalent to 'lambda F'. At first this seemed like an error: we can function (lambda) abstract again and again can't we? Then it dawned on me: this theorem covers degenerate lambda abstractions in the classical case. An example of a degenerate lambda abstraction in the classical case is 'lambda x lambda x f'. A carefully designed grammar eliminates any such degenerate 'expression'. The equivalence of 'lambda^2 F' to 'lambda F' suggests a collapse to 'lambda x f' i.e. a rule of evaluation.

Happily, 'func lambda F' is true i.e. the function abstraction of a predicate is a functional predicate.

From function (lambda) abstraction, the problem is posed as to what definition of 'function application' fits. I'm torn between two alternatives. The one assumes function application is always going to occur on a functional predicate:

* appliatoin) "F'G" for '(lambda F){G}'

The traditional definition of function application (with "F'x" as 'the y such that Fyx') dissolves into my prior definition of 'component' which generalizes function application and the collective relate.

All this being said, there is a lot of work to be done to bring these things together. But that is somewhat secondary to the larger problem to solve.

My methods of logic are to be pure predicate logic: predicates and predicate functors. The challenge is to make a story of logic which is autonomous from quantificational logic. Another part of the challenge is to free the world from the confines of Tarski's theory fo models which too strongly weds logic to mathematical logic and mathematical logic to theories of sets.

The prevailing solution has been to abandon predicate logic for deviant logics and deviant foundations which purport to be unbeholden to the constraints of, e.g., smooth discourse, e.g., fascilitated by extensionality. In such methods I simply see theories from theoraticians who have yet to present their predicates and premises.

Category theorists and type theorists are the most familiar of the gang as far as I know. Some computational theorists enter upon this territory, but, e.g., automata have yet to present themselves as foundational instruments in the same way as categories and types have. The closest is calculi, but there is not yet a general flag underwhich they fly (I'd propose Fefermann's Finitary Inductively Presented Logics if pressed).

## 2025 0421

### 2025 0421 2110
I had to stop my work on the latest little lisp entry in order to make note of a profoundly interesting interview that was just shared with me.
Australian identical twins Bridgette Powers and Paula Powers were recorded after they witnessed a car crash that turned into something more.
What is interesting is that they speak in a way that presumably makes it easy to hear when each of their verbal responses is strong or weak.

When they are speaking "in sync" they are emitting, what I can only guess is, the same operant, the same unit of response.
When they speak "out of sync" you get to listen to the verbal behavior that is presumably equally strong but which, if equally strong within the same skin, could not be emitted at the same time.

I do not have time now to do more research on them, but I submit that they have been interviewed prior to this most recent event.

### 2025 0421 1928

Today was a hammond b3 organ jazzy kinda day.

### 2025 0421 1546
This continues my work on my little lisp from [202504201615](#2025-0420-1615).

> and this is a reminder that my aim is to release a method of logic programming from Quine's main method by implementing each step to it he makes in "Methods of Logic 4th Edition".

In the last entry I did not give any examples of the functions defined running because I have not yet checked if they even work in such example cases.
First, a complete summary of the code that I'm working with (eventually each of the suprisingly disparate entries will come together under a unified collection of verbal practices which, ideally, approach something like Leibniz calculus ratiocinator and characteristica universalis, but I'll have more to say on that later).

```
let run=code=>console.log(code,'\n',eval(code)) // for examples

// some basic lisp functions
, consOf = (car,cdr) => [car,cdr]
, carOf = cons => cons[0]
, cdrOf = cons => cons[1]
, nil = 'nil'
, isIdentical = (x,y) => x==y
, isNil = x => isIdentical(x,nil)
, isPair = x => Array.isArray(x)
, isAtom = x => !isPair(x)

// proper and dotted lists
, isProperList = x => isNil(x) || (isPair(cdrOf(x)) && isProperList(cdrOf(x)))
, isDottedList = x => !isProperList(x)

// some basic javascript string functions
, emptyString=''
, isIdenticalString = (x,y) => x==y
, isEmptyString = string => isIdenticalString(string,emptyString)
, concatenationOf = (...strings) => 
   strings.length ? strings.shift() + concatenationOf(...strings) : emptyString
, firstCharOf = string => isEmptyString(string) ? emptyString : string[0]
, restCharsOf = string => isEmptyString(string) ? emptyString : string.slice(1)
, isString = x => 'string' == typeof x

// basic rune functions
, runeMark = '^'
, isRuneMark = x => isIdenticalString(x,runeMark)
, isRune = x => isString(x) && isRuneMark(firstCharOf(x))
, isRunic = list => isNil(list) || (isRune(carOf(list)) && isRunic(cdrOf(list)))

// from strings to runic lists and back again
, runeOf = char => concatenationOf(runeMark,char) 
, runicListOf = string => 
   isEmptyString(string) ? nil 
   : consOf(runeOf(firstCharOf(string)), runicListOf(restCharsOf(string)))
, charOf = rune => restCharsOf(rune)
, stringOf = runicList => 
   isNil(runicList) ? emptyString
   : concatenationOf(charOf(carOf(runicList)), stringOf(cdrOf(runicList)))

// prepending proper lists clears the way for buliding 
// runic lists from runic lists
, prependedProperListOf = (properList1, properList2) =>
   isNil(properList1) ? properList2
   : consOf(carOf(properList1)
     ,prependedProperListOf(cdrOf(properList1), properList2))

// how to make atoms and dotted lists proper lists
, singletonListOf = x => consOf(x,nil)
, properListOf = x => 
   isNil(x) ? nil
   : isAtom(x) ? singletonListOf(x)
   : consOf(carOf(x), properListOf(cdrOf(x)))

// generlization of prepepending proper lists to atoms and lists
, prependedListOf = (x,y) => 
   prependedProperListOf(properListOf(x),properListOf(y))
, appendedListOf = (x,y) => prependedListOf(y,x)

// printing atoms i.e. symbols
, isSymbol = x => isString(x)
, atomicPrintOf = atom =>
   isNil(atom) ? atomicPrintOf('()')
   : isSymbol(atom) ? runicListOf(atom)
   : atomicPrintOf('!?');
```
Now, examples for the latest functions defined in the latest entry
```
isRunic(prependedListOf(runicListOf('this is a test'),runeOf('!'))) 
 true
stringOf(prependedListOf(runicListOf('this is a test'),runeOf('!'))) 
 this is a test!
stringOf(atomicPrintOf(nil)) 
 ()
stringOf(atomicPrintOf('x')) 
 x
stringOf(atomicPrintOf(3)) 
 !?
```
That solves the problem of printing atoms where atoms in my little lisp are each symbols (notice that the last example gave '!?' as a default when the atom printer doesn't know how to print the item which purports to be an atom: in a different design I have no symbols, but that is closer to the little uhdForth-like language I'll show off after I get this little lisp done).

Next is the problem of printing proper lists and dotted lists.
First I'll work on proper lists because that seems simpler.
When a proper list gets printed it starts with '(' and ends with ')'.
Everything between those parenthesis has to be printed by the main printer that dispatches the appropriate kind of printer on the argument given i.e.
```
let printOf = x =>
   isAtom(x) ? atomicPrintOf(x)
   : isProperList(x) ? properListPrintOf(x)
   : isDottedList(x) ? dottedListPrintOf(x)
   : atomicPrintOf('!?')
, properListPrintOf = properList =>
   prependedListOf(runeOf('(')
   , prependedListOf(properListPrintEachOf(properList), runeOf(')')));
```
Dotted lists can wait.
The problem is reduced to printing each of the items in the proper list.
```
let properListPrintEachOf = properList =>
   isNil(properList) ? nil
   : prependedListOf(runeOf(' ')
     , prependedListOf(printOf(carOf(properList))
       , properListPrintEachOf(cdrOf(properList))));
```
As much as I do not like these long and camel case names, it is all I have for now.
With these definitions we should be able to test out printing everything but dotted lists.

Oh! But, it just occurred to me that there is a special case of proper list that must be handled: runic lists!
There is also another special case that just occurred to me: the runes of '(' and ')' must be distinguishable from the parenthesis printed around the printings from the list!

```
let openParenthesis = runeOf('(')
, closeParenthesis = runeOf(')')
, isParenthesis = x => isIdentical(x,openParenthesis) || isIdentical(closeParenthesis);
atomicPrintOf = atom =>
   isNil(atom) ? atomicPrintOf('()')
   : isParenthesis(atom) ? prependedListOf(runeOf('^'),runicListOf(atom))
   : isSymbol(atom) ? runicListOf(atom)
   : atomicPrintOf('!?');

let runicListPrintOf = runicList =>
   prependedListOf(runeOf("'")
   , prependedListOf(runicList,runeOf("'")));
printOf = x =>
   isAtom(x) ? atomicPrintOf(x)
   : isRunic(x) ? runicListPrintOf(x)
   : isProperList(x) ? properListPrintOf(x)
   : isDottedList(x) ? dottedListPrintOf(x)
   : atomicPrintOf('!?');
```
Now for some much needed examples:
```
stringOf(printOf(nil)) 
 ()
stringOf(printOf('thisIsABigAtom')) 
 thisIsABigAtom
stringOf(printOf(runicListOf('this is a runic list'))) 
 'this is a runic list'
```
So far so good.
Next to test printing proper lists.
A much needed function for making lists is added to the mix.
```
let listOf = (...x) => x.length ? consOf(x.shift(), listOf(...x)) : nil
```
The function designated by 'listOf' takes any number of arguments and returns a proper list each item of which is the appropriate argument.
Thankfully, I ran into an error when I tried to run
```
stringOf(printOf(listOf(nil,'thisIsABigAtom',runicListOf('this is a runic list'))))
```
It said that 'dottedList' was not defined.
My definition of proper list was missing a clause!
Here is the corrected version: 
```
isProperList = x => isNil(x) || isNil(cdrOf(x)) || (isPair(cdrOf(x)) && isProperList(cdrOf(x)));
```
Another crash and another correction!
The code for detecting parenthesis was missing an argument:
```
isParenthesis = x => isIdentical(x,openParenthesis) || isIdentical(x,closeParenthesis);
```
Now for a big example that almost works exactly as expected:
```
stringOf(printOf(listOf(nil,'thisIsABigAtom'
 ,listOf(closeParenthesis, 'x', openParenthesis)
 , runicListOf('this is a ) runic ( list'))))

 ( () thisIsABigAtom ( ^^) x ^^() 'this is a ) runic ( list')
```
Everything in there was expected except for the double '^^' in front of the parentheses.
Because I handled runic lists by wrapping them in single quotes I avoided the problem of having to prepend an extra escape character to atoms that begin with them: the function designated by 'runicListOf' escapes the escape character as expected!
So a quick correction:
```
atomicPrintOf = atom =>
   isNil(atom) ? atomicPrintOf('()')
   : isSymbol(atom) ? runicListOf(atom)
   : atomicPrintOf('!?');
```
and the big example becomes
```
stringOf(printOf(listOf(nil,'thisIsABigAtom'
 ,listOf(closeParenthesis, 'x', openParenthesis)
 , runicListOf('this is a ) runic ( list'))))

( () thisIsABigAtom ( ^) x ^() 'this is a ) runic ( list')
```
which is very satisfying, but there is a space missing before a closing parenthesis:
```
let spaceRune = runeOf(' ');
properListPrintOf = properList =>
   prependedListOf(openParenthesis
   , prependedListOf(properListPrintEachOf(properList)
     ,prependedListOf(spaceRune,closeParenthesis)));
```
The big example is then
```
stringOf(printOf(listOf(nil,'thisIsABigAtom'
 ,listOf(closeParenthesis, 'x', openParenthesis)
 , runicListOf('this is a ) runic ( list'))))

 ( () thisIsABigAtom ( ^) x ^( ) 'this is a ) runic ( list' )
```

All that is left is to deal with dotted lists.
They seem to be just like proper lists but instead of checking whether the relevant item is the item designated by 'nil' check if the relevant item is an atom:
```
let  dottedListPrintOf = dottedList =>
   prependedListOf(openParenthesis
   , prependedListOf(dottedListPrintEachOf(dottedList)
     ,prependedListOf(spaceRune,closeParenthesis)))
, dotRune = runeOf('.')
, dottedListPrintEachOf = dottedList =>
   isAtom(dottedList) ?
     listOf(spaceRune,dotRune,spaceRune,atomicPrintOf(dottedList))
   : prependedListOf(spaceRune
     , prependedListOf(printOf(carOf(dottedList))
       , dottedListPrintEachOf(cdrOf(dottedList))));
```
Now for the simplest example to check for (hopefully) easy to fix bugs:
```
stringOf(printOf(consOf(closeParenthesis,openParenthesis))) 
 ( ^) . ^(,nil )
```
Oh, wow, what an interesting mistake.
Everything works right up to the space after the dot, but then something goes wrong.
That thing is also something that often goes wrong when working with lists: prepending is not the same as listing out.
Prepending gets rid of trailing empty lists when putting each of its arguments together.
This should correct things.
```
dottedListPrintEachOf = dottedList =>
   isAtom(dottedList) ? prependedListOf(
     listOf(spaceRune,dotRune,spaceRune),atomicPrintOf(dottedList))
   : prependedListOf(spaceRune
     , prependedListOf(printOf(carOf(dottedList))
       , dottedListPrintEachOf(cdrOf(dottedList))));
```
The example:
```
stringOf(printOf(consOf(closeParenthesis,openParenthesis))) 
 ( ^) . ^( )
```
Great!
Now to put everything together into one really big example:
```
stringOf(printOf(consOf(listOf(nil,'thisIsABigAtom'
 ,listOf(closeParenthesis, 'x', openParenthesis)
 ,runicListOf('this is a ) runic ( list')))),'x')))

 ( ( () thisIsABigAtom ( ^) x ( ^) . ^( ) ^( ) 'this is a ) runic ( list' ) . x )
```
Wonderful.
That's a big problem solved.
The next problem to solve is the reader: it must take strings like the ones displayed from the printer and build the appropriate lists and atoms.

Since there was a lot of editing along the way, here are all the function definitions that were finally accepted.
I changed some names in order to avoid collisions e.g. 'listOf' by 'properListOf' and 'properListOf' by 'properOf' and now 'listOf' designates a function that can be used to produce both dotted and proper lists depending on whether the last argument is the empty proper list or some other atom.
Everything below was checked on the latest big example.
```
let run=code=>console.log(code,'\n',eval(code)) // for examples

// some basic lisp functions
, consOf = (car,cdr) => [car,cdr]
, carOf = cons => cons[0]
, cdrOf = cons => cons[1]
, nil = 'nil'
, isIdentical = (x,y) => x==y
, isNil = x => isIdentical(x,nil)
, isPair = x => Array.isArray(x)
, isAtom = x => !isPair(x)

// proper and dotted lists
, isProperList = x => isNil(x) || isNil(cdrOf(x)) || (isPair(cdrOf(x)) && isProperList(cdrOf(x)))
, isDottedList = x => !isProperList(x)

// some basic javascript string functions
, emptyString=''
, isIdenticalString = (x,y) => x==y
, isEmptyString = string => isIdenticalString(string,emptyString)
, concatenationOf = (...strings) => 
   strings.length ? strings.shift() + concatenationOf(...strings) : emptyString
, firstCharOf = string => isEmptyString(string) ? emptyString : string[0]
, restCharsOf = string => isEmptyString(string) ? emptyString : string.slice(1)
, isString = x => 'string' == typeof x

// basic rune functions
, runeMark = '^'
, isRuneMark = x => isIdenticalString(x,runeMark)
, isRune = x => isString(x) && isRuneMark(firstCharOf(x))
, isRunic = list => isNil(list) || (isRune(carOf(list)) && isRunic(cdrOf(list)))

// from strings to runic lists and back again
, runeOf = char => concatenationOf(runeMark,char) 
, runicListOf = string => 
   isEmptyString(string) ? nil 
   : consOf(runeOf(firstCharOf(string)), runicListOf(restCharsOf(string)))
, charOf = rune => restCharsOf(rune)
, stringOf = runicList => 
   isNil(runicList) ? emptyString
   : concatenationOf(charOf(carOf(runicList)), stringOf(cdrOf(runicList)))

// how to make lists of lists and atoms
, listOf = (...x) => x.length > 1 ? consOf(x.shift(),listOf(...x)) : x.shift()

// how to make proper lists of lists and atoms
, properListOf = (...x) => x.length ? consOf(x.shift(),properListOf(...x)) : nil

// prepending proper lists clears the way for buliding 
// runic lists from runic lists
, prependedProperListOf = (properList1, properList2) =>
   isNil(properList1) ? properList2
   : consOf(carOf(properList1)
     ,prependedProperListOf(cdrOf(properList1), properList2))

// how to make atoms and dotted lists proper lists
, singletonListOf = x => consOf(x,nil)
, properOf = x => 
   isNil(x) ? nil
   : isAtom(x) ? singletonListOf(x)
   : consOf(carOf(x), properOf(cdrOf(x)))

// generlization of prepepending proper lists to atoms and lists
, prependedListOf = (x,y) => 
   prependedProperListOf(properOf(x),properOf(y))
, appendedListOf = (x,y) => prependedListOf(y,x)

// printer dispatch
, printOf = x =>
   isAtom(x) ? atomicPrintOf(x)
   : isRunic(x) ? runicListPrintOf(x)
   : isProperList(x) ? properListPrintOf(x)
   : isDottedList(x) ? dottedListPrintOf(x)
   : atomicPrintOf('!?')

// how to print atoms i.e. symbols
, isSymbol = x => isString(x)
, atomicPrintOf = atom =>
   isNil(atom) ? atomicPrintOf('()')
   : isSymbol(atom) ? runicListOf(atom)
   : atomicPrintOf('!?')

// how to print runic lists
, quotationMark = runeOf("'")
, runicListPrintOf = runicList =>
   prependedListOf(quotationMark
   , prependedListOf(runicList,quotationMark))

// how to print proper lists
, openParenthesis = runeOf('(')
, spaceRune = runeOf(' ')
, closeParenthesis = runeOf(')')
, isParenthesis = x => isIdentical(x,openParenthesis) || isIdentical(x,closeParenthesis)
, properListPrintOf = properList =>
   prependedListOf(openParenthesis
   , prependedListOf(properListPrintEachOf(properList)
     ,prependedListOf(spaceRune,closeParenthesis)))
, properListPrintEachOf = properList =>
   isNil(properList) ? nil
   : prependedListOf(runeOf(' ')
     , prependedListOf(printOf(carOf(properList))
       , properListPrintEachOf(cdrOf(properList))))

// how to print dotted lists
, dottedListPrintOf = dottedList =>
   prependedListOf(openParenthesis
   , prependedListOf(dottedListPrintEachOf(dottedList)
     ,prependedListOf(spaceRune,closeParenthesis)))
, dotRune = runeOf('.')
, dottedListPrintEachOf = dottedList =>
   isAtom(dottedList) ? prependedListOf(
     listOf(spaceRune,dotRune,spaceRune,nil),atomicPrintOf(dottedList))
   : prependedListOf(spaceRune
     , prependedListOf(printOf(carOf(dottedList))
       , dottedListPrintEachOf(cdrOf(dottedList))));
```


## 2025 0420

### 2025 0420 2247
This continues my read of Will Durant's "The Story of Philosophy" from [202504142055](#2025-0414-2055).

The first chapter on Plato is divided into ten sections:
1. The Context of Plato
2. Socrates
3. The Preparation of Plato
4. The Ethical Problem
5. The Political Problem
6. The Psychological Problem
7. The Psychological Solution
8. The Political Solution
9. The Ethical Solution
10. Criticism

The first section "The Context of Plato" begins with a geological description:

> "If you look at a map of europe you will observe that Greece is a skeletonlike hand stretching its crooked fingers out into the Mediterranean Sea." [pg. 5 Durant "The Story of Philosophy"]

Geology selects geography which plays a large part in the survival of species, be they human or not.
Durant begins with a geological description that has not changed much since the time of ancient Greece from 1100 BCE to 146 BCE.
We are connected to the shape of greece on which Socrates and Plato lived out their entire lives.

He then describes the surrounding geographies and narrows his view quickly upon the "city-states":

> "Greece was broken into isolated fragments by these natural barriers of sea and soil; travel and communication were far more difficult and dangerous then than now; every valley therefore developed its own self-sufficient economic life, its own sovereign government, its own institutions and dialect and religion and culture.
In each case one or two cities, and around them, stretching up the mountainslopes, an agricultural hinterland: such were the "city-states" of Euboea, and Locris, and Aetolia, and Phocis, and Boeotia, and Achaea, and Argolis, and Elis, and Arcadia, and Messenia, and Laconia--- with its Sparta, and Attica--- with its Athens."[pg. 5 Durant "The Story of Philosophy"]

Having arrived at Sparta and Athens, the Greco-Persian Wars enter the scene: "Sparta provided the army and Athens the navy."[pg.6]
Athen's navy became a merchant fleet and new cultural practices flowed through the city's ports.
Finally, Durant has made his way to philosophy:

* "Democritus (460-360 BCE)--- 'in reality there is nothing but atoms and space.'"
* Epicurus (342-270 BCE)
* Lucretius (98-55 BCE)
* The Sophists
  * Gorgias
  * Hippias
  * Protagoras
  * Prodicus

The Sophists were
> "traveling teachers of wisdom, who looked within upon their own thought and nature, rather than out upon the world of things."
>
> "They asked questions about anything; they stood unafraid in teh presence of religious or political taboos"
>
> "In politics they divided into two schools. One, like Rousseau, agrued that nature is good, and civilization bad; that by nature all men are equal, becoming unequal only by class-made institutions: and the law is an invention of the strong to chain and rule the weak.
> Another school, like Nietzsche, claimed that nature is beyond good and evil; that by nature all men are unequal; that morality is an invention of the weak to limit and deter the strong; that power is the supreme virtue and the supreme desire of man; and that of all forms of government the wisest and most natural is aristocracy."[pg. 6-7]

Next, the Peloponnesian war (430-400 BCE) begot the oligarichal control of Athens by Critias and his "Thirty Tyrants".
So it is that Critias "was a pupil of Socrates, and an uncle of Plato"[pg. 7]


### 2025 0420 2201
Here are some notes that I told my self to make:

* Jonathan Gorard (@getjonwithit) made a use-mention mistake when invoking a theory of concepts to explain his ontological stance.
  > "Even the concepts of "places", "paths", "action", "distance" (and for that matter, "two") are ultimately just mathematical abstractions.
  >
  > The fact that we privilege some abstractions as more "real" than others is a byproduct of familiarity, not fundamentality."
  > 3:27 PM - 4/19/25 @getjonwithit
  *  This was in response to Sabine Hossenfelder (@skdh) explaining her ontological stance: "I think having a mathematical description is what it means to understand something"[11:38 - 4/18/25].
I get that most people may not see this clearly as a problem of ontology.
* Jonathan Gorard gave a definition of mathematics that does not address the key problem with respect to ontology (mentioned by Quine as "Every critically massive set of truths has some nonmathematical members."[pg. 52 Quine "From Stimulus to Science"]
  >"My personal definition of mathematics (i.e. "the set of all things about which one can reasonably prove theorems") is pretty broad. If an abstraction can be made precise, then it's mathematical."
  > 5:21 PM - 4/19/25 @getjonwithit
  * My response to this was to mention the limitations of a definition of mathematics with respect to, e.g., Quine's theory of protosyntax which is a descendent of Tarski's *formalized languages*.
If I was forced to give a clear cut definition of mathematics then it would be the theory of protosyntax, but if I was allowed to do as I want it would be to given no clear cut definition: "Mathematicity is perhaps a matter of degree." [pg.55 Quine "From Stimulus to Science"]
* Abi Daker (@abidaker) responded to my explanation of abstraction by way of predicate abstracts, concretion, and predicational compelteness.
She provided me with some brilliant examples from her work on early art:
  > "early use of abstraction was paeolithic people creating different sets of symbols for different animal/bird species which were based around the creatures footprints. It comes up a lot in cave paintings" [1:11 AM - 4/20/25]
 
  > "other nice bit of symbolism is that arches in churches exist because early churches were tree glades and when they started to build dedicated spaces for ceremony/worship, they wanted to mimic the tree arches.
  >
  > the domes are symbolic of the sun, as well"[1:49 AM - 4/20/25]

* @ResonantPyre mentioned an interesting sounding book "Culture and Society" by Raymond Williams.[2:15 PM - 4/13/25]

### 2025 0420 1615
This continues the work on my little lisp from [202504181456](#2025-0418-1456).

Runic lists are the foreign companion to javascript's native strings.
Just as strings can be concatenated together, so too can proper lists.
One proper list is prepended to an other by an appropriate sequence of pairings by the function designated by 'consOf'

```
let prependedProperListOf = (properList1, properList2) =>
  isNil(properList1) ? properList2
  : consOf(carOf(properList1)
    ,prependedProperListOf(cdrOf(properList1), properList2));
```
An example:
```
isIdenticalString(concatenationOf('this is ','a test')
 ,stringOf(prependedProperListOf(runicListOf('this is ')
   ,runicListOf('a test'))))
 true
```

A method of prepending proper lists, dotted lists, and atoms follows from transforming a dotted list into its associated proper list (which then also takes care of the atomic case):

```
let singletonListOf = x => consOf(x,nil)
, properListOf = x => 
  isNil(x) ? nil
  : isAtom(x) ? singletonListOf(x)
  : consOf(carOf(x), properListOf(cdrOf(x)))
, prependedListOf = (x,y) => 
  prependedProperListOf(properListOf(x),properListOf(y));
```

A note on speed and efficiency: they are the result of shortcuts which do not mutilate the logical consequences of the definitions given.
Before such definitions are given, there are no comparisons to be made, and hence no speed or efficiency of which to speak.

Appending mirrors prepending.
```
let appendedListOf = (x,y) => prependedListOf(y,x);
```

Finally, the printer prints a list or an atom as a runic list that can be sent to the native language of javascript as a string which is then displayed.
There is first the problem of transforming a symbol (which is just a native javascript string in my little lisp) into a runic list; but, surprise, we already implemented that as the function designated by 'runicListOf'.
There is only one catch, if the atom being printed is designated by 'nil' then print a runic list that gets displayed as '()' so as not to forget what part it now plays:
```
let isSymbol = x => isString(x)
, atomicPrintOf = atom =>
  isNil(atom) ? atomicPrintOf('()')
  : isSymbol(atom) ? runicListOf(atom)
  : atomicPrintOf('!?');
```

## 2025 0419

### 2025 0419 1349 The Parts of "Introduction to Philosophy" and My Outlook
The readings in the fourth edition of Perry, Bratman, and Fischer's "Introduction to Philosophy" are divided into six parts:

1. Philosophy and the Meaning of Life
2. God and Evil
3. Knowledge and Reality
4. Minds, Bodies, and Reasons
5. Ethics and Society, and
6. Puzzles and Paradoxes.

Each part is filled with selections from primary sources.
One of my aims is to cover the breadth of these sources and to supplement that breadth with the depth of the texts from which these selections were made.

There must be no doubt as to where I am coming from when looking over the data of philosophy: they are records of responses whose origins are through reinforcing practices of verbal communities whose cultures evolved by selection from variations on cultures past.
Such reinforcing practices were and are mediated by bits of behaving biology: organizations mediated by organisms mediated by organelles and so on.

There are doubts as to what specific chains of contingencies selected these organisms, these behaviors, and these cultures.
The boundary between what is known and what is unknown is often, and perhaps unavoidably, blurry.
Science and logic have done better than the rest of human's practices to uncover the shape of our strongest doubts.
There appears to be no better reason for them than that.

What others have to say about the world, or the worlds, enters into a mix.
The soup of social behaviors is seasoned by each of us.
Our favorite flavors are contingent upon the ingredients with which we are endowed.

Philosophy is a part of science.
The behavior of the philosopher is behavior and is, hence, part of the science of behavior.
Psychology as something other than the science of behavior is part of the speculation of philosophers.
Such psychologies are a part of science as much as any pseudoscience is part of what is to be explained by a complete account of human behavior.

Just as the molecular biologist can spend their life pouring over the peculiairites of abberant corners of the ocean, so too can the experimental analyst probe the behaviors and practices of prescientific practitioners.
They are, after all, the progenitors of modern science, which, if human practices are to be taken as a going concern, is the progenitor of future sciences largely unknown or, perhaps, even the progenitors of some more effective or, goodness forbid, less effective practices beyond science itself.

To read philosophy is to triangulate: where is what you have to say located with respect to what others have to say?
How far can speaking and listening take each of us when dealing with our shared world?
These are among the questions I ask, and which others have asked and answered well before me.
What can be learned from listening to my self as I do those others?
As an other to my self, what can be said of my speech and its relation to the speech of others better spoken than mine?

With that I turn to the texts, and I shall return here to tease out more questions, and, if I am so lucky, to find the threads of wisdom that often pose as our best tentative answers to them.

## 2025 0418

### 2025 0418 1601
A new mistake I make with markdown to add to the first I identified in [202504171525](#2025-0417-1525):
> I put the link inside parenthesis and then put the display text inside square brackets AFTER the parenthesis! 

### 2025 0418 1456
This continues the work on my little lisp from [202504171613](#2025-0417-1613).

I apologize for these fragments of progress on this particular project: I've decided to just get things done as they occur to me rather than go out of my way to first make them more easily explainable up front.
Everything will still be completely explained, but I am certain there are better ways of writing this all out than what I have done thus far.

Runes are special atoms that are designated by javascript strings that start with the "runeMark":

```
let runeMark = '^'
, isRune = x => ('string'== typeof x ) && runeMark == x[0];
```

Examples:
```
isRune('^test') 
 true
isRune('test') 
 false
```

A list is runic if it is proper and each of its left parts is a rune:
```
let isRunic = list => isNil(list) || (isRune(carOf(list)) && isRunic(cdrOf(list)));
```
Runic lists shall paly the part of strings in my little lisp (for now because I dont' know what consequences may come to select a new design).
This follows the convention of [Paul Graham's Bel](https://www.paulgraham.com/bel.html) where he spoke of chars I speak of runes: for this project chars are always native javascript.
The strange distinction must be made to preserve the distinction between symbols in lisp, runic lists, and javascript strings: this is something that has already tricked me and may very likely trick you as well!

I'll write some examples for the function designated by 'isRunic' after I introduce some slightly edited string functions from [Bit Strings and Binary Trees](#2025-0413-1513-bit-strings-and-binary-trees):

```
let emptyString=''
, isIdenticalString = (x,y) => x==y
, isEmptyString = string => isIdenticalString(string,emptyString)
, firstCharOf = string => isEmptyString(string) ? emptyString : string[0]
, restCharsOf = string => isEmptyString(string) ? emptyString : string.slice(1)
, concatenationOf = (...strings) => 
   strings.length ? strings.shift() + concatenationOf(...strings) : emptyString
, isString = x => 'string' == typeof x
, isRuneMark = x => isIdenticalString(x,runeMark);

isRune = x => isString(x) && isRuneMark(firstCharOf(x));
```

This is also the first example of redefining a variable in javascript: 'isRune' comes to denote a function specified in language more like the idioms I've adopted so far with their 'of's and 'is's.
The names are much longer than I'd write if I was writing just for my self, but I'm not doing that am I?
Examples (I do not yet have a comprehensive method of testing, but have tested out a few different methods of testing and so far examples are good enough):

```
isRune(concatenationOf(runeMark,'test')) 
 true
isRune('test') 
 false
```

Next, the two functions that go from strings to runic lists and back again:
```
let runeOf = char => concatenationOf(runeMark,char) 
, runicListOf = string => 
  isEmptyString(string) ? nil 
  : consOf(runeOf(firstCharOf(string)), runicListOf(restCharsOf(string)))
, charOf = rune => restCharsOf(rune)
, stringOf = runicList => 
  isNil(runicList) ? emptyString
  : concatenationOf(charOf(carOf(runicList)), stringOf(cdrOf(runicList)));
```

Examples:
```
isRunic(runicListOf('this is a test')) 
 true
stringOf(runicListOf('this is a test')) 
 this is a test
```

Why start with strings and runic lists?
Runic lists are the foreign companion to javascript's native strings and they shall be the porthole through which the foreign items of my little lisp are displayed in our native tongue.

The first main part of my little lisp that must be made is the printer.
It has to take a pair or an atom and transform it into a runic list that can then be read by our human eyes.
Perhaps if we could see into the computer like superman with x-ray vision, then we wouldn't need to worry about making a printer.
But, making a printer also helps when making a reader which transforms key presses into foreign code.

For my little lisp there is only going to be one kind of atom: symbols.
Runes are then special symbols: those that start with the rune mark as above.
What was spoken of as 'strings starting with the rune mark' shall now be spoken of as 'symbols starting with the rune mark'.
While I'd rather not use the word 'symbol' because it leads to questions like "What does this symbol symbolize?" there is a long history of its use in lisp and I am not yet prepared to break that chain of consequences.

Here is all the code from this entry in one place:

```
let run=code=>{console.log(code,'\n',eval(code));}

let consOf = (car,cdr) => [car,cdr]
, carOf = cons => cons[0]
, cdrOf = cons => cons[1]
, nil = 'nil'
, isIdentical = (x,y) => x==y
, isNil = x => isIdentical(x,nil)
, isPair = x => Array.isArray(x)
, isAtom = x => !isPair(x);
run('isPair(nil)');
run('isAtom(nil)');

let runeMark = '^'
, isRune = x => ('string'== typeof x ) && runeMark == x[0];
run("isRune('^test')")
run("isRune('test')")

let isRunic = list => isNil(list) || (isRune(carOf(list)) && isRunic(cdrOf(list)));

let emptyString=''
, isIdenticalString = (x,y) => x==y
, isEmptyString = string => isIdenticalString(string,emptyString)
, firstCharOf = string => isEmptyString(string) ? emptyString : string[0]
, restCharsOf = string => isEmptyString(string) ? emptyString : string.slice(1)
, concatenationOf = (...strings) => 
   strings.length ? strings.shift() + concatenationOf(...strings) : emptyString
, isString = x => 'string' == typeof x
, isRuneMark = x => isIdenticalString(x,runeMark);
isRune = x => isString(x) && isRuneMark(firstCharOf(x));
run("isRune(concatenationOf(runeMark,'test'))");
run("isRune('test')");

let runeOf = char => concatenationOf(runeMark,char) 
, runicListOf = string => 
  isEmptyString(string) ? nil 
  : consOf(runeOf(firstCharOf(string)), runicListOf(restCharsOf(string)))
, charOf = rune => restCharsOf(rune)
, stringOf = runicList => 
  isNil(runicList) ? emptyString
  : concatenationOf(charOf(carOf(runicList)), stringOf(cdrOf(runicList)));
run("isRunic(runicListOf('this is a test'))");
run("stringOf(runicListOf('this is a test'))");

```


### 2025 0418 1453
The notation used to [explain The Popr Programming Language](https://www.hackerfoo.com/posts/popr-tutorial-0-dot-machines.html) is very much like the notation I devised for my own language, and is a clear indication that there is a convergence of notational practices that lend themselves to visualization near these concatenative methods.

It is sad that they are spoken of as "concatenative" when, as far as I can tell, this is just one way of elaborating on any principles of programming stack machines.
Without the concrete contingencies of stack machines there is no concatenative programming and concatenative methods may not even end up being the most appropriate for stack machines.
Again, as far as I can tell, the key to a specific method of programming is how you would write an interpreter for that programming language in that programming language.

This is not just some honorary show to LISP: it is the most accessible way to bring up the logic of the theory upon which a given method of programming is built.
It forces the programmer to confront the difference between use and mention, logic and theory, and often reveals the language's place on among models of computability.


### 2025 0418 1439
A major conclusion from the substance of [accumulating the links](#2025-0417-2020) to papers on what is called "concatenative programming languages" is that modern writers do not know the difference between a theory and its logic.
The inability to notice that the predicate functors of Quine are not combinators as in the more familiar combinatory calculi is a great sadness: both to our sciences and to our teachers.

Finally done accumulating the links that I wrote down.
Now on to my little lisp.

## 2025 0417

### 2025 0417 2026

All the links across the internet are rotting.
Searches are no longer helpful: one system often says that there is nothing relevant while another quickly brings what you're looking for to the top only to bury it later.
My saddness knows no bounds.
There are a few people that could break out of the environments which control them (and the world) in unhelpful ways, but they have no good reasons for doing so.

If your shit isn't in plain text documents then it is more likely to rot than anything else.
There are no easy ways to accept or deal with this: it is a kind of culture war both foreign and yet somehow familiar.

### 2025 0417 2020

Some links to papers and presentations on concatenative programming languages and related things:

* ["Stanford Seminar - Concatenative Programming: From Ivory to Metal" by John Purdy, November 15, 2017](https://www.youtube.com/watch?v=_IgqJr8jG8M)
* ["The Theory of Concatenative Combinators" by Brent Kerby 2002](http://tunes.org/~iepos/joy.html)
    > This is unlikely to be around for long: it will be too hard to find it.
* ["Linear logic and permutation stacks—the Forth shall be first" by Henry Baker 1994](https://dl.acm.org/doi/10.1145/181993.181999)
* ["Iota and Jot" by Chris Barker 2001](https://en.wikipedia.org/wiki/Iota_and_Jot)
   > There is a lot more to this one than can be got at right now.
   > I really do hope that someone is out there (besides just the way back machine) keeping track of things like this before all is lost.
* ['Chris Barker's Iota-Jot-Zot family of esolangs' 2020 by Ilia Chtcherbakov](http://cleare.st/code/iota-jot-zot)
   >Ilia shares my unhappiness with broken links directly to Chris Barker in this one!
* [Chris Barker's current homepage](https://cb125.github.io/) (who knows how long it will be up?)
* [The Joy Programming Language](https://hypercubed.github.io/joy/joy.html)
* ["Concatenative programming and stack-based languages" by Douglas Creager](https://www.youtube.com/watch?v=umSuLpjFUf8)
* [the online space of Douglas Creager.](https://dcreager.net/)
* [Douglas Creager links to concatenative programming stuff](https://dcreager.net/concatenative/)
* [“Factor: A Dynamic Stack-based Programming Language” 2010 by Slava Pestov, Daniel Ehrenberg, Joe Groff](https://dcreager.net/papers/Pestov2010/)
* ["A denotational semantics of a concatenative/compositional programming language" Jurij Mihelič, William Steingartner, Valerie Novitzká. 2021](https://dcreager.net/papers/Mihelic2021/)
* [Robert Kleffner. “A foundation for typed concatenative languages”. Master's thesis, Northeastern University. April 2017](https://dcreager.net/papers/Kleffner2017/)
* ["Foundations of Dawn: The Untyped Concatenative Calculus" by Maddox](https://www.dawn-lang.org/posts/foundations-ucc/)
* ["Continuation-Passing Style, Defunctionalization, Accumulations, and Associativity" by Jeremy Gibbons 2021](https://arxiv.org/abs/2111.10413)
* [The Popr Programming Language by Dusty DeWeese](https://www.hackerfoo.com/posts/popr-tutorial-0-dot-machines.html)
    * [on github](https://github.com/HackerFoo/poprc/?tab=readme-ov-file)
* [Functional Bits: Lambda Calculus based Algorithmic Information Theory John Tromp April 23, 2023](http://tromp.github.io/cl/LC.pdf)
* [Tree Calculus by Barry Jay ](https://treecalcul.us/)
* [John Earnest](http://beyondloom.com/)
* [no stinking loops](http://www.nsl.com/)

It is in large part because of the difficulty in assembling this list of links (and finding the sources mentioned by the documents pointed to from these linked sites) that I have largely committed to monolithic methods of writing.
It is not what I would prefer, but it is also not so inconvenient when what I'm ultimately going for is an accurate record of the evolution of my verbal behavior.

### 2025 0417 1613
This continues work on my little lisp from [2025 0415 1548](#2025-0415-1548).

Nil is a proper list and any pair whose right part is a pair whose right part is itself a proper list is a proper list; and a nonproper list is called a dotted list.

```
let isProperList = x => isNil(x) || (isPair(cdrOf(x)) && isProperList(cdrOf(x)))
, isDottedList = x => !isProperList = x;
```

Historically, a pair whose left part is designated by 'x' and whose right part is designated 'y' was designated by '(x . y)' and '(x y)' was short for '(x . (y . nil))' so that, in general, '(x y ...z)' is schematically short for '(x . (y . (...z . nil)...))'.
Thus, the only abbreviations which contained a dot were those whose right parts were nonnil atoms e.g. '(^a ^b . ^c)' is short for '(^a . (^b . ^c))' and '((^a ^b) . ^c)' is short for '((^a . (^b . nil)) . ^c)' where all atoms beginning with '^' designate their self.



### 2025 0417 1525

Some writing projects that I may never get around to:

* [Metamath](https://us.metamath.org/) isn't like Quine's schematic methods (even though they go into a lot of detail about how they extend Quine's schematic methods)?
    * The theory upon which metamath is based: [Megill, “A Finitely Axiomatized Formalization of Predicate Calculus with Equality,” Notre Dame Journal of Formal Logic, 36:435-453, 1995](https://us.metamath.org/downloads/finiteaxiom.pdf)
    * Some explanation of how that theory works in practice: [Metamath is a metalanguage that describes first-order logic](https://us.metamath.org/mpeuni/mmset.html#mmname)
* FORTH and the final sentence of McCarthy's [history] of LISP.
   > Who am I kidding, I'm definitely going to write about that.
* [The Kitten Programming Language](https://kittenlang.org/) and its relation to FORTH.
* [Big History](https://en.wikipedia.org/wiki/Big_History) and ages old cosmological methods.
* more to come...

> Side note. the markdown convention of using square brakets around the display text for a link which is given in curved brackets does not work: I constantly put square brackets where curved brackets go and vice versa.
> This seems like something that would have been quickly uncovered with a little "beta testing".
> Dare I say that a solution may be found in some method of slashes? 

## 2025 0416

### 2025 0416 2358
Today is the first day that a problem I've had for some time was clearly revealed to me: I'm having trouble keeping up with my self.
Hopefully tomorrow I can get out some more work that I did yesterday and today:

* Paths in binary trees and bit strings (still haven't found a beautiful bijection between bits and trees)
* I never finished the [latest entry on my little lisp](#2025-0415-1548) (even though I worked a lot more on it that day)
* I never got around to writing out what I meant by "we live in a stack based world".
* Read more of Durant's "story of philosophy" and failed to write down any of what I had to say (just because I have something to say does not imply it is even remotely important, but it is easier to make that judgement once I've said it on a page than to my self).
* Haven't even started writing about working out through setbacks: I want to capture the pain of that dip and better understand why it always seems like a bigger deal than it ever really is.
Something about that old saying "You're stronger than you think." e.g. there is greater strength which your culture has yet to release.

Oh! And the Durant reading was on Greece, Socrates, and Plato.
To my great surprise, I just discovered that Whitman's 1865 "O Captain! My Captain!" is specifically described as a "Ship of State" metaphor which is most famously mentioned in Plato's "Republic" Book 6.
This is something like an example of what I mean when I say, metaphorically, reading always makes the world seem larger and smaller than it once was.

### 2025 0416 2313
I left a note for my self on twitter.
This is me making the note I'd made a note for my self to note.

The account @ResonantPyre, a friend, wrote a few paragraphs on Wittgenstein's 1935 'Lectures on Personal Experience'.
Nonsense is under the microscope.
'Sound and Sense' was the title of a book on poetry that I was forced to buy to finish some requirement of my undergraduate education (little did I know what a wonderful friend I would find in that professor even though I was a fish out of water).
I defaced my copy by prepending a 'non' at each place where 'sense' occurred.

Meaning, sense, semantic agreement, and all other spooks of sentences are cast out, reluctantly, with the full thrust of Ramsey's proxy functions encapsulated by Quine's "cosmic complement".
Indeterminancy of reference alone does not preclude propositions (the poltergeists of sentences).
On occasions we fix our meaning of a sentence to outright intersubjective agreement: *unus pro omnibus, omnes pro uno*.

The problem with propositions--- meanings made eternal--- is their purported transcendence: everywhere forever, forever everywhere.
A well paraphrased verbal response stands for now and does well enough without proposing propositions.
We lose nothing by leaving those who are lost without them: they come back around eventually.

We go to dictionaries "to get the meaning of the word" and when we open to the right page we get on where we left off.

There's obviously much more to this all than anything like the flob above.
It's okay to have some fun when you fill it out with fuller explanations sooner rather than later.

### 2025 0416 2234 O Russell! My Russell!

When I was young, and still very much under the control of Russell's *Principles* and *Principia*, I found my way to Egner and Denonn's "The Basic Writings of Bertrand Russell" and clung to it as an other might the primary text of their religion.
Everywhere I found the methods of Principia applied to Russell's world.
That world was one I hoped to glimpse within my own.

It is only now, some fifteen years later, that I see less of Russell and more of logic in his memorabilia.
It was always the logic of his works, and not their well written projections, that caught and kept me reading.
The error of Russell's ways were somewhat corrected by him.
Clarity and exactness were logic disguised and upon confronting this perversion (he excised logic overtly in "An Inquiry into Meaning and Truth") he left the fruits of philosophy for something more attractive to modern minds.

Logic looms throughout his works, but the mind left him open to mystical influence.
He rested in a way somewhat more obscure than Descartes "I think therefore I am."
It is, for me, the difference between singular terms and Quine's elimination of them by Russell's own singular descriptions.

Expedience was never sufficient for Russell, there was always more to it than just that.
Thus, even though he is so often worshipped as a controversial figure, it has gone unnoticed that public comprehension is a halmark of convention (whether such conventions are in vogue is another matter e.g. nothing sells quite like good versus good).

Russell was no real threat to the world.
For all that was hoisted upon him through his personal affairs, he made a cuckold of Frege and logic got lost in the shuffle.
Quine remediated what Russell ruffled.
Carnap did too, but never got far enough from Russell's outlook e.g. his philosophy of physics is limp.

What once of logic had exhausted Russell was energized by Quine: sprawling theories and unspooled speculations were cut off from their metaphysical wellsprings and reality pierced the veils of clarity and exactness.
Rather than complexify the world, Quine made due with the complications of a worldclass watchmaker.
But whither the warmth of love and the sensations of the soul amongst such machinations? 
Those who clutched for them betrayed their own insecurities.

Principia Mathematica was a report from the frontlines of the final war which we wage on to this day.
It tormented him for the rest of his life: had he done more to help the world or to hurt it by having so vividly revealed the brutality of the battlefront?
There seemed to be no peace nor no hope in the trenches.
This would not keep the homefires burning: what else was he to do?

## 2025 0415

### 2025 0415 1915
As is often the case, I have added some other books to read in parallel with Druant's "The Story of Philosophy". They are
* Russell's "The History of Western Philosophy", and
* Grayling's "The History of Philosophy".

I'm pretty sure those are the only books in my library that are directly aimed at giving a sorta story or a sorta history of philosophy as a whole.
There is also the classic

* "Introduction to Philosophy: Classical and Contemporary Readings, 4th edition" edited by Perry, Bratman, Fischer

that may come up in what may be a larger piece of the reading puzzle I have to solve in order to do all that I must before I die.
As much as I recommend people go directly to primary texts, there is no good way to get started on the whole of a thing than by reading how someone else did it: there are some writers who can explain what they did as if it was done by an other and they are to be cherished.
We're all human, in the end, and that alone binds us into the most startling of enterprises. 

### 2025 0415 1557

It seems that people who were a part of history wish they had written down more of the history of which they were a part.

There is less to learn from what was forgotten than from what was remembered.

Time tramples all.

> This occurred to me while reading [John McCarthy's 1979 "History of Lisp"](https://justine.lol/sectorlisp/lisp-history.pdf).

### 2025 0415 1548

First a summary of the work done on my little LISP from [202504112248](#2025-0411-2248)

```
let consOf = (car,cdr) => [car,cdr]
, carOf = cons => cons[0]
, cdrOf = cons => cons[1]
, nil = []
, isIdentical = (x,y) => x==y
, isNil = x => isIdentical(x,nil)
, isPair = x => Array.isArray(x)
, isAtom = x => !isPair(x);
```

In English (that carefully distinguishes between quotations and their purported designations for reasons that shall be explained later),
* the function designated by 'consOf' takes two arguments and gives back a javascript array with the first argument indexed by zero and the second argument indexed by one;
* the function designated by 'carOf' takes one argument, which it expects is the result of an application of 'consOf', and returns the item of the argument indexed by zero;
* the function designated by 'cdrOf' is like the one designated by 'carOf' but it returns the item of the argument indexed by one;
* the item designated by 'nil' is a javascript array of length zero;
* the function designated by 'isIdentical' takes two arguments and returns the javascript item designated by 'true' where the first argument is `==` to the second (in the language of javascript) and returns the javascript item designated by 'false' elsewhere;
* the function designated by 'isNil' takes one argument and returns the js item deisgnated by 'true' where it is `==` to the js item designated by 'nil', and returns the js item designated by 'false' elsewhere;
* the function designated by 'isPair' takes one argument and returns the js item designated by the application of 'Array.isArray' to it; and
* the function designated by 'isAtom' takes one argument and returns the js item designated by 'false' if the js item designated by the application of the function designated by 'isPair' is designated by 'true', and the js item designated by 'true' otherwise.  

From my work on [Bit Strings and Binary Trees](#2025-0413-1513-bit-strings-and-binary-trees), the practice of designating a function by ending it in 'Of' and designating the functional representation of a predicate by beginning it with 'is' helps a lot when working out the logic of the programs under construction.
This is also a partial explanation for all the 'designated by's and quotations that spell out the letters of what most people would call the *names* of the designated functions or objects.

I'm writing here now after having written the following section to say that it goes a little too far from talk about programming a LISP in javascript and if that's all that you care about then you can just skip past this break to the next one.
What you're missing is a more detailed explanation of the general plan for dealing with syntax and semantics.
It doesn't get too deep, but does go even further out of bounds than I can presumably tolerate.

---

As much as there is the temptation to write phrases like 'the function consOf' or, better, 'the function `consOf`' which resorts to a different typeface in order to weakly emphasize that there is something different about the purported designatum of the so rendered phrase, this has only led to decades and centuries of mistakes and wasted efforts promulgated by untaught writers and readers.

This is especially the case when writing and reading programs e.g. the problems that so many people have with so called 'pointer arithmetic' is no such simple thing as a 'bad design decision of a particular programming language'.
The problems of meaning and reference, the life blood of semantics, crop up wherever languages are planted (which includes some of our most powerful social technologies).
They are fundamental problems that can not be swept under the rug before the guests arrive.

When a writer leaves it to the reader to figure out what is supposed to be a quotation and what purports to be designated by such a quotation, there is only hell to pay.
Even if the writer commands the reader to take on the responsibility of "keeping conventions in mind" this only works if there are other reasons for them to do so (I think of Kleene's 1952 "Introduction to Metamathematics").

This problem is not as unfamiliar nor as fussy as it seems e.g. classic problems of scoping in logic and in programming are fundamentally about not having resolved a concrete method of reference or meaning which deals well with syntactic and semantic agreement.

Rather than ignore the fundamental problems I'll pick the strongest, perhaps tentative, methods that are already within my reach and take it from there: nothing is as final as finality claims to be.

I have already mentioned Quine multiple times throughout these notes and to anyone who is so unfortunate to have followed me on twitter.
As much as people detest Quine's obstinate rigor (a term that I only just discovered is already stated poetically as "Ostinato Rigore"), the articulation of his uncertainty is beyond anything else that I've seen.

The plan is simple: semantics breaks into reference and meaning, reference breaks into designation and denotation, and denotation 

> "is where the action is. It takes designation in stride, for a singular term can be recast as a predicate that happens to denote just one thing if any.
> The singular term 'Boston' *designating* Boston, can be reconstrued as a predicate 'is Boston', *denoting* only Boston.
> Anything said about Boston can be paraphrased using 'is Boston'."[pg. 59 of Quine's "From Stimulus to Science"]

Quine shows just how denotation comes to be defined in consistent theories by first generalizing Tarski's method of defining truth to denotation and then admitting a hierarchy of predicates of denotation, finite in number for a given theory of denotation.
This settles, once and for all, the work of denotation in a theory which is rich enough to admit a method of semantic ascent e.g. by way of quotations.

While such a hierarchy of predicates of denotation are less than what most expect, e.g., from a comprehensive theory of truth ("Truth, one might risk being quoted as saying, is just a degenerate case of denotation."[pg. 65 Quine's "From Stimulus to Science"]) it is all that can be got without plunging any such theory into the classic antimonies.

Now, among the practices of programming there are those called "denotational semantics".
This is but one way of explaining what so many people speak of as the meaning of a given program.
It is much easier to see how all such talk paraphrases into Quine's methods by beginning with a programming language which drops variables from the beginning and hence eases the complexity of any subsequent definition of programmatic denotation as a projection from a logically consistent theory of denotation.

Sadly, now is not the time or place for me to complete such a project.
As almost always seems to happen when working on interesting things, other interesting things come up, they are temporarily entertained for the sake of maintaining momentum, and then when they start to take over and drive at a new direction, there's nothing left to do but drop it and return to where the enthusiasm last let off on the path you had original planned.

I won't leave the other part of semantics hanging though: meaning.
The simplest explanation of how I deal with it is to point again to Quine.
This is not because I accept Quine without criticism or correction, but because there is no one who has yet combined the philosophies of Quine with the philosophy of Skinner's radical behaviorism in the way that seems to be characteristic of my outlook and actions in regards to these matters.

In the classical story, sentences have meaning.
It is what a sentence means that matters most when we speak with each other.
If you don't know what a sentence means, then the sentence is effectively worthless to you.
Sentences are merely the aftereffects of meaning.
In the metaphor of traditional signals processing, it is sentences that send the message but it is the meaning that is encoded or decoded from the sentence.

Skinner's analysis of verbal behavior ends all speculation as to what potential philosophic value there can be to disputing the existence or nonexistence of meanings (or information, in the modern parlence, or propositions in more sacred realms).
But, meanings die hard, and it is harder still for most to imagine a world without meanings much less to get along in one.
While Skinner drops meanings outright and simply begins without them until he comes around to the verbal behavior that we otherwise said would not occur without the existence of meanings, Quine gives traditionalists a good shake.

He starts with "sameness of meaning" rather than with meaning.
At least we can say when two sentences "have the same meaning" if we can not say what the meaning is, or if we might somehow get along without purporting anything more than a predicate "means the same as".
Just as in the case of denotation, I can not go through how "sameness of meaning" goes broke.
The concluding outlook is this:

* reification through joint reference in that weaker form of universal closures of conditionals that Quine calls "focal observation categoricals" and which burgeon into micro spatiotemporal theories of the world cover their own ground, i.e. we are justified in speaking of "sameness of reference" between the pronouns of focal observation categoricals,
* these very grounds give us the power of intersubjective sameness of reference with repsect to gross bodies (this being little more than the classic "by ostention"), and
* abstract items, such as orderd pairs, receive only the most conservative of treatments
   > "I submit that intersubjective sameness of reference makes no sense, as applied to abstract objects, beyond what is reflected in successful dialogue." [pg. 70 Quine's "From Stimulus to Science"].

For more detail on any of these things I can point to the text quoted, oh so many times in this entry, Quine's 1995 refinement of his 1990 talk "From Stimulus to Science".
It is a hard read, something which I have said many times before, but the value of each of its words and sentences is second only to the value of his "Methods of Logic 4th edition".

That all being said...

---

Thankfully I wrote up all that thinking because, for some reason which is not clear to me, it let me see that 'nil' is to designate a symbol/atom and that should help me with the design of this little LISP!

Here is the code from above revised 

```
let consOf = (car,cdr) => [car,cdr]
, carOf = cons => cons[0]
, cdrOf = cons => cons[1]
, nil = 'nil'
, isIdentical = (x,y) => x==y
, isNil = x => isIdentical(x,nil)
, isPair = x => Array.isArray(x)
, isAtom = x => !isPair(x);
```

This entirely avoids the problems that come with 'nil' designating a javascript array.
So the key examples that show the critical change are:

```
isPair(nil) 
 false
isAtom(nil) 
 true
```

The 'LISt' in 'LISt Processing' is defined inductively as

1. the item designated by 'nil' plays the part of the empty list
2. a list whose first item is designated by 'x' and the rest of whose items are listed by the item designated by 'y' is designated by 'consOf(x,y)'.

Recursively we define proper lists as 
```
let isProperList = x=> isNil(x) || (isPair(cdrOf(x)) && isProperList(cdr(x)))
, isDottedList = x => !isProperList(x);
```

As the second definition suggests, a list which is not proper is called dotted because of the historic practice of writing a pair whose left part is x and whose right part is y as '(x . y)'.
This matters because when printing atoms and pairs there will be times when a dotted list must be constructed so that the person looking at the printed result can tell the difference between '(^a ^b)' and '(^a . ^b)'.

## 2025 0414

### 2025 0414 2055
This entry continues my read of Will Durant's "The Story of Philosophy" from [202504132323](#2025-0413-2323).

The introduction hovers around a familiar analogy: science is to knowledge as philosophy is to wisdom.
He says that science analyzes and philosophy synthesizes; that science takes apart what philosophy must put back together; that science gives us the power to do only what philosophy can tell us is worth doing; that science without philosophy has no value in that science without philosophy is like a fact without a feeling; that we must not forget that science is a descendant of philosophy; and that philosophy brought science into this world and can just as easily take it out of this world.

Next, he breaks apart philosophy in the traditional way, only to follow his own advice and focus on "the great men of philosophy" rather than the great fields of philosophy:
* Logic "is the study of the ideal method in thought and research: observation and introspection, deduction and induction, hypothesis and experiment, analysis and synthesis" [pg. 3]
* Esthetics (or Aesthetics) "is the study of ideal form, or beauty; it is the philosophy of art" [pg. 3]
* Ethics "is the study of ideal conduct; the highest knowledge, said Socrates, is the knowledge of good and evil, the knowledge of the wisdom of life" [pg. 3]
* Politics "is the study of ideal social organization (it is not,a s one might suppose the art and science of capturing and keeping office); monarchy, aristocracy, democracy, socialism, anarchism, feminism--- these are the *dramatis personae* of political philosophy." [pg. 3]
* Metaphysics (which he disdains as you will see) "And lastly, *metaphysics* (which gets into so much trouble because it is not, like the other forms of philosophy, an attempt to coordinate the real in the light of the ideal) is the study of the "ultimate reality" of all things: of the real and final nature of "matter" (ontology), of "mind" (philosophic psychology), and of the interrelation fo "mind" and "amtter" in the process of perception and knowledge (epistomology)." [pg. 3]

The chapters outline the path Will takes through the story of philosophy:

1. Plato
2. Aristotle and Greek Science
3. Francis Bacon
4. Spinoza
5. Voltaire and the French Enlightenment
6. Immanuel Kant and German Idealism
7. Schopenhauer
8. Herbert Spencer
9. Friedrich Nietzsche
10. Contemporary European Philosophers
11. Contemporary American Philosophers

In the introduction to the second edition Will makes repeated apologies for what he has already apologized for in the main text: it is riddled with sentences that would have experts on the relevant topics fuming and it leaves out Eastern philosophers entirely (which he tried to correct in the first volume of "The Story of Civilization").

There is no book on philosophy as a whole which has not failed to present that whole without holes.


### 2025 0414 2041
Yesterday, I stumbled on <https://www.game-cities.com/> by Konstantinos Dimopoulos who shared a link to [Ultima and Worldbuilding in the Computer Role-Playing Game
Carly A. Kocurek and Matthew Thomas Payne](https://services.publishing.umich.edu/Books/U/Ultima-and-Worldbuilding-in-the-Computer-Role-Playing-Game) that can be read online for free.

It's a short and quick read.
I'm always left wanting more from the books I read on the history or philosohpy of various video games.
In the end, the books that I want on my favorite games are basically literate programs that give an elaborate history of each design decision.
That is not yet the kind of book that is easy to get your hands on.

I hope to come back to Dimopoulos' site and read some of his [articles](https://www.game-cities.com/articles-talks) on urban design in video games.


### A Stack Notation for Predicate (Functor) Logic 2025 0414 1626
A way of eliminating variables from predicate logic which combines the methods of Quine's predicate functors with the practices of programming in J and FORTH permits predicate letters which, initially, generalize the familiar two place predicate notation--- e.g. 'x is identical to y', 'x belongs to y', and 'x is the father of y'--- by schema such as 'xyzFstuvw' so that each predicate has a number of left places--- three in the example schema--- and a number of right places--- five in the example.
Thus, the predicate 'x pairs y with z' is rendered perspicuously as 'xPyz'.
The left places are referred to as the pile of the predicate and the right places are the list of the predicate.

The predicate functors of recombination (which generalize the stack notation of FORTH to predicate logic) are 

1. '...xy(DROP F)...z' for '...xF...z'
2. '...wxy(HEM F)...z' for '...wxyFx...z'
3. '...x(PUSH F)y...z' for '...xyF...z'

from which all others are defined e.g.

4. 'OVER F' for 'HEM PUSH F'
5. 'OVER2 F' for 'OVER OVER F'
6. 'OEM F' for 'OVER HEM F'
7. 'DSH F' for 'DROP PUSH F'
8. 'DUP F' for 'OEM DSH F'
9. 'DROP2 F' for 'DROP DROP F'
10. 'POP F' for 'OEM DROP2 F'
11. 'NIP F' for 'POP DSH F'
12. 'HIP F' for 'HEM NIP F'
13. 'HIP2 F' for 'HIP HIP F'
14. 'SWAP F' for 'HIP PUSH F'
15. 'PUSH2 F' for 'PUSH PUSH F'
16. 'TOR F' for 'HIP2 PUSH2 F'
17. 'ROT F' for 'TOR TOR F'

and so on.
The remaining predicate functors (which generalize the tacit notation of the J programming language to predicate logic) are defined from

18. '...x(NOT F)...y' for 'not ...xF...y'
19. '...x(F AND G)...y' for '...xF...y and ...xG...y'
20. '...x(SOME F)...y' for 'some item is (z such that ...xzF...y)'.

For more on predicate functors and the method of homogenization which permits translation back and forth between predicate functor logic and 'predication' logic see Quine's "Methods of Logic 4th edition".

> Predication logic is the most appropriate name for what is unhappily called 'first-order logic' or otherwise called 'predicate logic'.
> As I see it, with predicate functors elimination of variables, the only items in the lexicon of a logical theory are the predicates and hence 'predicate logic' is what might otherwise be called 'predicate functor logic'.

There are two things forthcoming:
1. an implimentation of the algorithms that transform a pure predicate functor term into a sentence (perhaps open or perhaps closed) of predicate logic, and vice versa; and
2. a further generalization of the above notation which incorporates all four of the following:
    * Quine's predicate functor notation,
    * FORTH's stack notation,
    * J's tacit notation, and
    * LISP's list notation.

There are additional changes which permit a full generalization of APL and J's functional operators that generalize e.g. matrix multiplication.
These correspond to predicate functor functors: you can continue to go up and up the grammatical hierarchy and even contemplate predicate functor functor functors if you desire.

Though, it appears as if there is a limit to how high you can go and still get something helpful out of it: it seems to me that predicate functor functors are as far as you can go since after that everything else looks exactly like the first three levels of operations on the grammatical hierarchy of a logical language.

### 2025 0414 1605
Today is a Nat King Cole day.
His voice, his piano, and his trio have brought me hours of delight.

Some things I'd like to get done today:
1. Finish the very short paper on "A Stack Notation for Predicate (Functor) Logic"
2. Finish my little parenthetical space sperated language
3. Read some more of "The Rise and Fall of the Third Reich" by Shirer and published October 17, 1960.

I've read that Shirer's account is 'outdated' or 'narrow' or 'not historically accurate'.
No book is historically accurate: we wouldn't know how to check for historic accuracy even if our life dependend on it.
That is a bit of an overexaggeration: it is largely the result of the metaphorical distance between our records of day to day life and works like "Schedules of Reinforcement" by Skinner and Ferster where the challenges of keeping an accurate record of behavior are uncovered in all their ugly details.

Whether Shirer's book is adequate for an understanding of the third reich is moot: no book is going to give "the whole story" as if there was some big story in the sky that would truly enlighten us if we could just reach it.
I see what I've read of Shirer so far as a starting point: it is so well written that you would be hard pressed NOT to finish it.
Not all history books have that same luxury.

So far, I've read that Richard J. Evan's "Third Reich Trilogy":

1. "The Coming of the Third Reich" published October 2003
2. "The Third Reich in Power" published October 2005
3. "The Third Reich at War" published October 2008

is a much more accurate report on the contingencies that selected and ultimately extinguished the third reich.

I have so many books to read that it is hard to see how I'll ever get to reading them all. 

## 2025 0413

### 2025 0413 2353
While skimming over [John McCarthy's 1979 "History of Lisp"](https://justine.lol/sectorlisp/lisp-history.pdf) two things caught my attention:
1. he mentioned that Quine had used prefix notation in some thing he or those around him had read and that this had some influence on LISP's prefix notation (pg. 7)
2. He mentioned a paper he wrote with Cartwright (1978) that "show how to represent pure LISP programs by sentences and schemata in first order logic and prove their properties"(pg. 8)

He also mentioned something about LISP having no effect on those working in recursion theory, but that now seems to be a historical hiccup.

My interest in (1) is much less than my interest in (2).
I am reading the mentioned paper now: ["First Order Programming Logic" by Cartwright and McCarthy in 1979](https://dl.acm.org/doi/10.1145/567752.567759).

I just finished reading it (202504140019) and am both happy and sad.
My interest was primarly the result of being consumed by the methods of logic programming.
But, alas, "programming logic" and "logic programming" though just one swap away from each other does not bring them together as one.
That is the sadness: this paper does not not reduce pure LISP to the methods of logic programming.

What it does do though is still very important, and I will have to read this paper again before I can explain it and the many things that it brings up as they relate to logic programming: it calms the anxieties of those who continue to seek something beyond predicate logic when making proofs about programs.

Cartwright shows how to set up a theory (kind of like the traces in what Hoare first introduced as [1978 "Communicating Sequential Processes"](https://dl.acm.org/doi/10.1145/359576.359585)) that transforms partial functions into total functions allowing for the introduction of an equivalence axiom that permits convenient proofs of properties of the original partial function e.g. an interpreter.

McCarthy's contribution is a minimization schema for each partial function (this is very likely akin to the schematic method I devised for setting up a schematic theory of transitive closures of an unspecified predicate of the lexicon of the theory) that ends up being equivalent to Cartwright's method.

Together these methods indicate clearly and exactly that no more than predicate logic is needed to reason about recursive programs, even those once as unfamiliar as interpreters/compilers.

These are happy things to know because it strengthens the general significance of my proposal that the practices of programmign are simply a subcollection of the practices of predicate logic.

The significance to this specific paper, and any others like it, is that such formalisms are no more a surprise than a formalism of predicate logic itself.


### 2025 0413 2323
I got a copy of Will Durant's "The Story of Philosophy" some months ago from a book fair.
Though I read some of it back then, I do not recall any of it now.
One of the joys of reading SO MANY *fundamentally interesting* things (sometimes over and over again) is that you can actually forget some of them!

Most of what I read is nonfiction, but I've started reading fiction recently e.g. Agatha Christe, le Carre, Chaucer, and Dante.
Most of the fiction I've read in my life is from when I was less than fifteen years old e.g. Asimov, Pullman, Tolkein, Salinger, Wells, Orwell, and Shelly.
Since I haven't reread those books I'm uncertain whether I have forgotten them or not.

Anyway, I finished reading the introduction to Will Durant's "The Story of Philosohpy" and had some things to say (apparently).

---

The success of Will's book "The Story of Philosophy" allowed him and his wife to write "The Story of Civilization" which I have yet to finish reading (in fact, I still have yet to finish the first book in the series!).
I am drawn to the Durants because they wished to see the world as a whole: to put Humpty Dumpty back together again.

Mine is the second edition and the eighth printing: copyrighted last in 1933 and published no later than 1953 by Simon and Schuster.

The introduction to this second edition echos my own outlook on our modern world: although we are technically more interconnected now than ever before, there is less wisdom among us than ever there has been.
Much of our world was designed and built by a few of us.
They have drained knowledge of all that makes it rich.

To be wise is to threaten the prevailing world order.
The wise see beyond the shades of political esotericism through to the fundamental conflict between controllers and countercontrollers.
Hope has not died, and it is still expected by a few of us that beyond the bounds of individualism and collectivism there is something like a peaceful culture: one that does the controlling and countercontrolling through us in only the most conspicuous of ways.


### 2025 0413 2249
I finally finished my entry on [Bit Strings and Binary Trees](#2025-0413-1513-bit-strings-and-binary-trees)!
It took longer than I expected, but I also uncovered some unexpected things along the way e.g. a quick and easy way to find shorter bit strings for a given binary tree.

Something I also learned: there is no faster way to catch bad writing than to publish it.

Something else I learned: if you don't end a markdown list with two returns it thinks the trailing line of text is part of the last entry in the list.

### 2025 0413 1754
A friend just introduced me to two great bits of music:
* [Alberto Ginastera](https://en.wikipedia.org/wiki/Alberto_Ginastera) - Harp Concerto (1956)
* [Seru Giran](https://en.wikipedia.org/wiki/Ser%C3%BA_Gir%C3%A1n_(album)).

I was able to guess that Ginastera had learned from Copland.
I described Ginastera as a "faster Copland".

Seru Giran was too short: I wanted more after listening to it.
It is a very comforting album that seemed to combine popular styles from both North America and England.

### 2025 0413 1644
If I could just finish one thing before finishing another thing then I would be able to finish more things that people care about now than things they'll care about later.
This is a roundabout explanation of why I work the way that I do.

### 2025 0413 1634 Buffett's Jet and Apple's Three Jets Full of iPhones

Warren Buffett wrote about selling and buying a jet in his [letter to Berkshire sharehodlers in 1989](https://www.berkshirehathaway.com/letters/1989.html).
On more than one occasion I have had to explain to people that it is more than a luxury, though that is mostly what the shareholder letters show off as the big problem.

There is almost no limit to the potential value of being able to go from point A to point B around the world with tangable objects in tow.
Apple's three jets full of iPhones should help people to more easily grasp why Warren went from naming his jet the 'indefensible' to the 'indispensable'.

### 2025 0413 1513 Bit Strings and Binary Trees
Bytes of memory stored in a computer can be treated as strings of binary digits or as little binary trees.
Let me explain what I mean while I give you a javascript program that decodes a binary tree into a bit string and encodes a bit string into a binary tree.
(Most people would swap 'encode' and 'decode' in that sentence.)

Computers tend to use bits, bytes, and sometimes other more exotic words to do either place value arithmetic (binary in almost all cases, ternary in some rare cases) or to address other words of memory.

> Technically they do place value arithemetic of remainders (sometimes with respect to the word size, and othertimes in less obvious ways that are otherwise more familiar as "logical operations").

Storing addresses is an easy way to avoid having to copy words to new locations: go to where the relevant data is rather than waste the time and space to replicate it somewhere else.

Words also have another vital role to play in computers: they store commands that end up telling the computer what to do with all those addresses and numbers.

There is an alternative way of looking at words as lists of ones and zeros: each string of bits corresponds to a binary tree and each binary tree corresponds to a string of bits.
It is easier to decode a binary tree into its binary string than it is to encode a binary string into its tree.

Since the binary trees contemplated here are all dirty--- they are all built from pairs whose subcomponents are pairs or the nil pair--- there is only one terminal condition which must be checked in any recursive definition: is this the nil tree?
Before that, here is the code for building pairs from their left and right parts and for getting the left and right parts from a pair.

```
let pair=(leftPart,rightPart)=>({leftPart,rightPart})
, leftPartOf=pair=>pair.leftPart
, rightPartOf=pair=>pair.rightPart;
```

The nil pair is not to be confused with the empty pair: the empty pair has no parts, but the nil pair is a special pair (or what some might call an atom) that is identical to its left and right parts (see [2025 0411 2248](#2025-0411-2248) for more on the logic of atomic and empty items).
The way we make an object like that in javascript is a bit esoteric:

```
let nil={get leftPart(){return this},get rightPart(){return this}};
```

Setting up getters (and setters) with javascript objects is just weird and I really only read the documentation for such things when I already really know what I want.
Otherwise, you will get lost trying to make sense of the design choices that brought us the programming language of the internet.
There is an alternate world where LISP was the language of the internet: that world would have been better than this one.
An even better world is one where it was FORTH, but I digress.

The above definition of 'nil' makes it so that 

```
nil == leftPartOf(nil) && nil == rightPartOf(nil)
  true
```

which happens to be the defining feature of atoms in a more general setting:

```
let atom=x=> x==leftPartOf(x) && x==rightPartOf(x);
```

Sadly, we won't be needing to talk any more about atoms.
We just need to know whether a given tree is nil or not:

```
let isNil=x=> x==nil;
```

Oh, I should explain the difference between 'dirty trees' and 'pure trees' because most people are familiar enough with 'pure trees' and few people use the phrase 'dirty trees'.
Dirty trees are ones that include something other than pairs or the empty pair among their subcomponents.
Pure trees are ones that don't include anything other than pairs or the empty pair among their components.
They are pure in more ways than one: they do not admit of talk of atoms, and if you go down one branch or another you eventually hit the empty tree (but it may take infintely long to get there in some cases, those also being cases that I would say are inappropriate).

So now we have all we need in order to talk about binary trees.
Onto what we need to talk about bit strings:

```
let zero='.'
, one=','
, concatenate=(...strings)=> strings.length ? strings[0]+concatenate(...strings.slice(1)) : '';
```

There are two building blocks to bit strings: zero and one.
They are to be distinct:

```
zero != one
  true
```

and they can be combined by concatenation.
The principles governing concatenation, like the principles governing binary trees, are actually a lot more elusive than it first seems: thankfully, if we just go with what javascript gives us there is no reason to get stuck in the theories of strings and trees, even though those can be fun places to be stuck.

The function designated by 'concatenation' takes any number of arguments that are strings and puts them together in the order they are taken (from left to right).

```
concatonate(zero,one,zero,one,one)
  .,.,,
```

Why use '.'s for zeros and ','s for ones?
Because then the binary decoding of a tree is also the program for the stack machine that actually constructs the binary tree it decodes!
But first, lets decode some trees:

```
let decode= tree => isNil(tree) ? zero 
: concatenate(decode(leftPartOf(tree)), decode(rightPartOf(tree)), one);
```

In English, decode takes a tree and first checks if it is identical to nil.
If it is then we already know how to decode that: it's just a zero.
If it isn't nil then we have already made the assumption (quite a strong one really) that it must be a pair with a left part and a right part.
So, all we need to know to write out the decoded tree is to
1. decode the left part of the tree and
2. decode the right part of the tree.

Once we have those two things we just write out the code for the left tree, followed by the code for the right tree, and cap it all off at the end with a one.

It will be shown that the decoded strings are almost always longer than they need to be: any initial zeros can be trimmed from the front of the decoded string without any loss to subsequent encoding.

Some examples

```
decode(nil) 
   .
decode(pair(nil,nil)) 
   ..,
decode(pair(nil,pair(pair(nil,nil),nil))) 
   ...,.,,
```

The decode function is one-to-one (injective i.e. each string that comes out of it is only the result of decoding one and only one tree).
There are some bit strings that will never be the result of such decoding e.g. ',' and '.,'.
Thus the method of decoding is not a one-to-one correspondence (bijection i.e. each tree matches to one and only one string and each string matches to one and only one tree).
In general, there is a philosophic simplicity to establishing one-to-one correspondences from one universe of items to another.
For more on the fruits of such labors see Quine's 1946 "Concatenation as a Basis for Arithmetic".

When treated as postfix notation (as in reverse [Łukasiewicz notation](https://en.wikipedia.org/wiki/Polish_notation)) the ',' comes to work as the function 'pairUp' of a [stack machine](https://en.wikipedia.org/wiki/Stack_machine).
So many of the practices of programming are described by tiny stack machines that there is great value to being familiar with them perhaps without ever programming a real stack machine.

From the method of decoding it can be seen that the last ',' pairs together the left and right parts of the tree.
This is how the encoder works!
It simulates a little stack machine and uses the bit string as the list of commands necessary to construct the decoded tree.

First, we can  use a tree to simulate a stack.
All we need is a way to push, pop, and maybe peek at the top of the stack.
When I implement stacks with trees, I always take the top item to be the right part of the tree simulating the stack.
Some people prefer to take the left part of the simulating tree as the top, and there's nothing mathematically wrong with that (perhaps not even something logically wrong with that), but the programming language LISP has long since forced us to interpret such a setup as a list and not a stack i.e. the left part of a tree is the first item in a tree simulating a list.

```
let push= (stack,item) =>pair(stack,item)
, pop= stack => leftPartOf(stack)
, peek = stack => rightPartOf(stack);
```

If instead of allowing nil into our universe of pairs we had stuck ourselves with a pure theory of pairs, then 'peek' and 'pop' would have to be written differently e.g.

```
let error = message => console.log(message)
, emptyPair=[]
, isEmpty = pair => pair.length == 0
, purePop = stack => isEmpty(stack) ? emptyPair : leftPartOf(stack)
, purePeek= stack => isEmpty(stack) ? error('empty stack') : rightPartOf(stack);
```

Most people teach and prefer the implementation that includes error messages.
They want the machine to tell them when something is wrong by anticipating how it could go wrong.
My preference is for a crash based method of design: rather than go looking for errors leave as much of the design as possible out on the rim of "don't cares".
Not only does this make your code a lot shorter (which makes it so there are fewer places where things could ever possibly go wrong), it also releases you from the anxiety of trying to catch your mistakes before you make them.

There are some standard techniques that we pick up over time for avoiding problems that have already happened again and again in the past.
Some people are taught by others to avoid them and some are taught by the immediate consequences of having made such mistakes.
The latter almost always teaches better than the former, but that is not the end of this general problem as to how to practice programming.

There is no universal way to detect logical errors in programs.
No one is there to look over your shoulder and tell you "Hey, this isn't actually what you wanted to make in the first place."
Though we have new machines that speak to us in such conversational tones--- after having been forcefed some scrap of code--- there is no part of such machines that prepares them for anything more than a world like the selecting past (but that goes a much longer way than most people once supposed, e.g., half a decade ago).

But, there is a way to entirely avoid logical errors: that is what logic itself is for!
Thus, rather than make trouble for ourselves by starting with the logic of pure binary trees (where, e.g., the definition of 'properPop' deals clumsily with degenerate cases), we begin with a theory that admits no such road bumps even if, in degenerate cases, it gives us unfamiliar results (e.g. the definition of 'pop' I have adopted here gives nil when you pop nil as the simulation of the empty stack).

Back to encoding bit strings to trees!
I already gave away the key to solving this problem: the bit string is a program that tells a tiny stack machine how to build the encoded tree.
Thus, we go along the bits, from left to right, and when we hit a zero we push a nil onto the stack, and when we hit a one we pop the top two trees on the stack and pair them into a new one that we push back on the stack.

Unlike pairs, there are no standard ways of getting at the anatomy of a string (there is much more to say about that and about Quine's protosyntax in general).
Thus, we go with some traditional methods:

```
let emptyString=''
, isEmptyString = string => string==emptyString
, isZero = string => string == zero
, isOne = string => string == one
, firstOf = string => isEmptyString(string) ? emptyString: string[0]
, restOf = string => isEmptyString(string) ? emptyString : string.slice(1);
```
Some examples:
```
isEmptyString(emptyString) 
  true

isEmpty(concatenate(zero,emptyString,one,one)) 
 false

isZero(firstOf(concatenate(zero,one,one,emptyString)))
  true

isOne(firstOf(restOf(concatenate(zero,one,one,emptyString))))
  true

concatenate(zero,one,zero)==restOf(concatenate(one,zero,one,zero))
  true
```

Not all of those string functions are really needed in the encoder, but there's nothing wrong with covering familiar ground when it doesn't take you far from your path.
There are two operations that our little simulation of a stack machine has to accommodate: push a nil and push a pair made from popping the top two items from the stack.
The first is simple enough:
```
let pushNil=stack=>push(stack,nil);
```
It is the second that poses a few problems (but not anything too troublesome).
Rather than explain it, I'll just show the code and go from there:
```
let emptyStack = nil
, isEmptyStack = stack => stack==emptyStack
, topOf = stack => peek(stack)
, secondOf= stack => peek(pop(stack))
, pop2 = stack => pop(pop(stack))
, pairUp=stack=>push(pop2(stack),pair(secondOf(stack),topOf(stack)));
```
> For those familiar with stack operations in other circumstances, it may be strange to take a functional view of stacks: popping a stack returns a stack without its top, it doesn't change the state of some stack accessible outside of the scope of the current executing function.

Now all that is left is to break the encoder into a function that gets the ball rolling with an empty stack and one that takes a string and a stack and gets to work:

```
let encodeHelperBeta = (string, stack) =>
  isEmptyString(string) ? topOf(stack)
  : isZero(firstOf(string)) ? encodeHelperBeta(restOf(string), pushNil(stack))
  : isOne(firstOf(string)) ? encodeHelperBeta(restOf(string), pairUp(stack))
  : encodeHelperBeta(restOf(string),stack)
, encodeBeta = string => encodeHelperBeta(string,emptyStack)
```
Lo, this is not the last version of 'encode' and that is why it is called 'encodeBeta'.
There are three things of note in the beta definition given:

1. it asks if the string is empty and when it is it returns the top of the stack,
2. it checks if the first item of the string is zero or one and executes the corresponding operation on the stack (pushing a nil on top of the stack or pairing up the top two items on the stack), and
3. if it runs into an item of the string that we haven't talked about yet it does nothing and goes on its merry way.

Here are some examples, but because we have no way of seeing the encoded tree I have to put it through the decoder--- this ends up being helpful because it should decode into a string that we could put back into the encoder to make the tree all over again.
```
decode(encodeBeta(concatenate(zero,zero,one)))
  '..,'
decode(encodeBeta(decode(encodeBeta(concatenate(zero,zero,one)))))
  '..,'
decode(encodeBeta(concatenate(zero,zero,one,zero,one)))
  '..,.,'
decode(encodeBeta(concatenate(one,one)))
  '...,,'
```
> Alternatively, we could have built up the parenthetical notation familiar to most people for ordered pairs e.g. where '(x,y)' is short for "the pair whose left part is x and whose right part is y".
> It would also be helpful to know when two trees are equal so that we might be able to write out the construction of a tree and compare it to the encoding of its abbreviation as a bit string.
> While javascript comes with its own built in string functions, it doesn't give us built in pairs or functions for working with pairs.
> Given what is known from Solomon Fefferman's work on [Finitary Inductively Presented Logics](http://virtualmath1.stanford.edu/~feferman/papers/presentedlogics.pdf), it is probably a bad idea to design a language that doesn't work with ordered pairs (and for those less theoretically minded, there's always the conveniences of LISP to look at).

That last example is well worth looking at more closely: it took the concatenation of one with one (which is not a string that will ever come out of the decoder) and it encoded just fine and when we sent that tree through the decoder it returned the concatenation of zero, zero, zero, one, and one!
So we have a much shorter way of decoding this particular tree: we can write it as ',,' instead of '...,,' when we have said how 'encoderBeta' works!

The following example shows why it is still called 'encoderBeta':
```
decode(encodeBeta(concatenate(zero,zero,zero,zero)))
  '.'
```
Where did all those other zeros go that we concatenated together?
How is it that we only got a single zero when we decoded the encoded tree?

After the encoder checks that the string is empty it peeks at the top of the stack and returns it as the result.
Since there are no ones in that concatenation, there are no pairs made out of the nil trees on the stack (of which there are four before it shows us the top nil).

There's nothing wrong with the beta encoder except that we can make a new one that doesn't leave anything on the stack.
Presumably, doing so will give us some new ways of abbreviating binary trees as bit strings!
The simplest change is to pair up everything that's left on the stack and then return that.
```
let pairUpEverything = stack =>
  isEmptyStack(pop(stack)) ? topOf(stack)
  : pairUpEverything(pairUp(stack))

, encoderHelper = (string, stack) =>
  isEmptyString(string) ? pairUpEverything(stack)
  : isZero(firstOf(string)) ? encodeHelper(restOf(string), pushNil(stack))
  : isOne(firstOf(string)) ? encodeHelper(restOf(string), pairUp(stack))
  : encodeHelper(restOf(string),stack)
, encode = string => encodeHelper(string,emptyStack);
```

Everything on the stack gets paired up by repeatedly applying 'pairUp' to the stack until there is only one item left on it (this is checked by seeing if popping the stack leaves only the empty stack behind).
Some examples:
```
decode(encode(concatenate(zero,zero,one)))
  '..,'
decode(encode(concatenate(zero,zero,zero,zero)))
  '....,,,'
decode(encode(concatenate(one,zero,one,zero,zero,zero)))
  '..,.,...,,,'
decode(encode(concatenate(zero,zero,one,zero,one,zero,zero,zero,one,one,one)))
  '..,.,...,,,'
```
The last two examples suggest a simple way of getting the shorter decoded string from the longer decoded string of an encoded tree without having to guess and check encoding and decoding:

1. if the long code is a concatenation of all zeros or all ones then you already have the shorter bit string you're looking for, but
2. if at least one zero and one one both occur in the long code then trim off any zeros on the left and any ones on the right.

There are some more complicated javascript functions being used to define the operations of trimming and checking for occurrences of ones and zeros, but no more than what you can figure out for yourself from context clues or a quick search:

```
let occursIn = (string, item) => string.includes(item)
, trimLeftZeros = string => string.slice(string.indexOf(one))
, trimRightOnes = string => string.slice(0,1+string.lastIndexOf(zero))
, trim = string => trimLeftZeros(trimRightOnes(string))
, shorten = string =>
  isEmptyString(string) ? '.'
  : occursIn(string,zero) && occursIn(string,one) ? trim(string)
  : string;
```

And, here are some examples:

```
shorten(concatenate(zero,zero,one,zero,one,zero,zero,zero,one,one,one))
  ',.,...'
decode(encode(shorten(concatenate(zero,zero,one,zero,one,zero,zero,zero,one,one,one))))
  '..,.,...,,,'
```

There are a few ways that these shorter decoded bit strings help out e.g. they let us fit more trees within a single word of memory and, if we are silly enough to do so, fit multiple tiny trees within a single word of memory.

> Future John here (from 202504272330): the shortener doesn't work in the case where there is a one, a zero, and the first occurrence of a one is the last occurrence of a zero e.g. '00000111111'.
> You can fix this by checking for this exact degeneracy and then keep whichever block of identical digits is shorter.

Next I'll come up with a way to showcase how each bit string or binary tree locates chuncks, down to words, of memory.
After that it will probably be time for some bit string and binary tree arithmetic.
No promises!

Here's all the code I used to write this whole note:
```
let pair=(leftPart,rightPart)=>({leftPart,rightPart})
, leftPartOf=pair=>pair.leftPart
, rightPartOf=pair=>pair.rightPart;
let nil={get leftPart(){return this},get rightPart(){return this}};
let run=code=>{console.log(code,'\n  ',eval(code));}
run('nil == leftPartOf(nil) && nil == rightPartOf(nil)');

let atom=x=> x==leftPartOf(x) && x==rightPartOf(x);
run('atom(nil)');

let isNil=x=> x==nil;
let zero='.'
, one=','
, concatenate=(...strings)=> strings.length ? strings[0]+concatenate(...strings.slice(1)) : '';
run('zero != one');
run('concatenate(zero,one,zero,one,one)');

let decode= tree => isNil(tree) ? zero 
: concatenate(decode(leftPartOf(tree)), decode(rightPartOf(tree)), one);
run('decode(nil)');
run('decode(pair(nil,nil))');
run('decode(pair(nil,pair(pair(nil,nil),nil)))');

let push= (stack,item) =>pair(stack,item)
, pop= stack => leftPartOf(stack)
, peek = stack => rightPartOf(stack);

let error = message => console.log(message)
, emptyPair=[]
, isEmpty = pair => pair.length == 0
, purePop = stack => isEmpty(stack) ? emptyPair : leftPartOf(stack)
, purePeek= stack => isEmpty(stack) ? error('empty stack') : rightPartOf(stack);

let emptyString=''
, isEmptyString = string => string==emptyString
, isZero = string => string == zero
, isOne = string => string == one
, firstOf = string => isEmptyString(string) ? emptyString: string[0]
, restOf = string => isEmptyString(string) ? emptyString : string.slice(1);
run("isEmptyString(emptyString)");
run("isEmpty(concatenate(zero,emptyString,one,one))");
run("isZero(firstOf(concatenate(zero,one,one,emptyString)))");
run("isOne(firstOf(restOf(concatenate(zero,one,one,emptyString))))")
run("concatenate(zero,one,zero)==restOf(concatenate(one,zero,one,zero))");

let pushNil=stack=>push(stack,nil);
let emptyStack = nil
, isEmptyStack = stack => stack==emptyStack
, topOf = stack => peek(stack)
, secondOf= stack => peek(pop(stack))
, pop2 = stack => pop(pop(stack))
, pairUp=stack=>push(pop2(stack),pair(secondOf(stack),topOf(stack)));
let encodeHelperBeta = (string, stack) =>
  isEmptyString(string) ? topOf(stack)
  : isZero(firstOf(string)) ? encodeHelperBeta(restOf(string), pushNil(stack))
  : isOne(firstOf(string)) ? encodeHelperBeta(restOf(string), pairUp(stack))
  : encodeHelperBeta(restOf(string),stack)
, encodeBeta = string => encodeHelperBeta(string,emptyStack);
run("decode(encodeBeta(concatenate(zero,zero,one)))");
run("decode(encodeBeta(decode(encodeBeta(concatenate(zero,zero,one)))))")
run("decode(encodeBeta(concatenate(one,one)))")
run("decode(encodeBeta(concatenate(zero,zero,zero,zero)))");

let pairUpEverything = stack =>
  isEmptyStack(pop(stack)) ? topOf(stack)
  : pairUpEverything(pairUp(stack))
, encodeHelper = (string, stack) =>
  isEmptyString(string) ? pairUpEverything(stack)
  : isZero(firstOf(string)) ? encodeHelper(restOf(string), pushNil(stack))
  : isOne(firstOf(string)) ? encodeHelper(restOf(string), pairUp(stack))
  : encodeHelper(restOf(string),stack)
, encode = string => encodeHelper(string,emptyStack);
run("decode(encode(concatenate(zero,zero,one)))")
run("decode(encode(concatenate(zero,zero,zero,zero)))")
run("decode(encode(concatenate(one,zero,one,zero,zero,zero)))");
run("decode(encode(concatenate(zero,zero,one,zero,one,zero,zero,zero,one,one,one)))");

let occursIn = (string, item) => string.includes(item)
, trimLeftZeros = string => string.slice(string.indexOf(one))
, trimRightOnes = string => string.slice(0,1+string.lastIndexOf(zero))
, trim = string => trimLeftZeros(trimRightOnes(string))
, shorten = string =>
  isEmptyString(string) ? '.'
  : occursIn(string,zero) && occursIn(string,one) ? trim(string)
  : string;
run("shorten(concatenate(zero,zero,one,zero,one,zero,zero,zero,one,one,one))");
run("decode(encode(shorten(concatenate(zero,zero,one,zero,one,zero,zero,zero,one,one,one))))");
```

## 2025 0412

### 2025 0412 2335

It is late and it feels like there is a fat tire tightening around skull.
My temples throb and my eyes hurt.
Nothing comes from closing my eyes but the feeling of pains marching across my face.
If I don't tell myself to relax then I'll soon be clenching my teeth until my ears start to ring.
A few deep breaths may loosen my neck and shoulders, but my forehead remains scrunched.

Despite these unfavorable conditions, I have added some more to the [2025 0412 1422 Paper on Logic](#2025-0412-1422-paper-on-logic).

### 2025 0412 1619
I'm making a note here because I am so happy to have a single text document that I can easily copy and paste onto a single monolithic website.
This is the closest I have come to explaining myself to others in a way that is mostly accurate and mostly accessible.

For now there are only a list of timestamped entries to show you.
Soon these shall evolve into more substantial blocks of templates, contemplations, and conclusions.
As what I have to say reaches a critical mass, it bubbles up into a more solid and long standing brick in the foundation of my principles and practices.
I am glad to share these steps in the evolution fo my behavior.

### 2025 0412 1422
It has been a while since I've worked directly on my paper on logic.
In the past I have had to shove what little I can into a thin thread of tweets.
Now I am not stuck in a tight corner and can let my explanations relax into the more casual style that my friends and family are subjected to when I trap them in a "Johnversation".

My outlook on logic is now almost entirely that of Quine's (amended by Skinner's outlook on verbal behavior).
His "Methods of Logic 4th Edition" (and you need the 4th edition!) has almost split in half from how many times I've read it.
It doesn't help that Quine's main method of proof for predicate logic is about half way through the book and that it is the one section I've probably read and reread the most.

Whereas Quine's "Methods of Logic 4th Edition" narrowly focuses on the mechanics of logical practices, his "From Stimulus to Science" broadly applies those mechanics to, what Quine called, a "Breast Pocket Theory of the World".
As much as I disagree with the details of Quine's theory, e.g. he mistakes something like Skinner's experimental analysis of behavior for a dispositional stimulus-response theory, I agree with his methods.

Where Quine goes wrong, Skinner often goes right, and where Skinner goes wrong, Quine is there to clearly identify the problems if not also to provide compact solutions.

That is enough on where I'm coming from when I talk about logic.
Now for parts of my paper on logic.

***

There are a few unique things about my methods of logic.
The most important is that I aim to present predicate functor logic not as the elimination of variables from predicate logic, but as an autonomous system independent of predicate logic.

Whereas predicate logic is a consequence of grammar and truth, predicate functor logic is a consequence of grammar and denotation.
Closed sentences are true, predicates denote, and closed sentences are a degenerate form of predicate: they are zero place predicates.
Correspondingly, truth is a degenerate form of denotation.
In predicate functor logic we can define truth of a zero place predicate as the universal (or existential closure) of the quotation of its padding denoting.

> The phrases "existential closure", "quotation", "padding", and "denoting" are all technical terms and I have used them as such with full knowledge that they are foreign to most people e.g. "padding" is most likely unknown to anyone who is not yet familiar with Quine's predicate functors.

The first sentence of the paper, so far, is little more than the opening of a mystery:

> Logic is the science of validity as a consequence of grammar and denotation.

It is a sad sentence, and I am unhappy with it.
Simpler alternatives occur to me e.g.

> Logic is a consequence of grammar and denotation.

or perhaps

> Logic is that science which is the consequence of grammar and denotation.

but neither of those, nor the first, really clear the way for what is to come.
Such introductory sentences tend to come much later in my writing of a paper.
They are a summary of what is to come, and when what is to come has not yet come, there is nothing to concretely summarize.

The key components to the introductory sentence shall remain 'grammar' and 'denotation'.
They are an echo of Quine's "Philosophy of Logic 2nd Edition" which clearly shows the ways in which logic is the sum of grammar and truth.
He has a beautiful metaphor:

> Logic chases truth up the tree of grammar. [pg. 35]

and

> The logician talks of sentences only as a means of achieving generality along a dimension that he cannot sweep out by quantifying over objects.
> The truth predicate then preserves his contact with the world, where his heart is. [pg. 35]

Together these give a clear hint as to where Quine is coming from when he talks about the methods of logic.
The problem of "achieving generality" by talking of sentences is a critical part of Quine's method of semantic ascent:

> The move is what I call *semantic ascent*: mentioning an expression by name instead of using it as a component clause. [pg. 92 of From Stimulus to Science]

In "From Stimulus to Science" Quine does a better job of showing how quotation is just one potential method of semantic ascent by showing how it can be extracted from 'that' clauses e.g. 

- perceives that
- thinks that
- it occurred to him that
- believes that
- doubts that
- expects that
- hopes that
- fears that
- regrets that
- says that
- denies that
- predicts that
- strives that

[pg. 90 of From Stimulus to Science].
For most people familiar with Quine, semantic ascent is intertwined with his theory of propositional attitudes (as the examples amply indicate).
But, there is nothing so grand in the method: it is a technical solution to a technical complication:

> We can generalize on 'Tom is mortal', 'Dick is mortal', and so on, without talking of truth or of sentences; we can say 'All men are mortal'.
> We can generalize similarly on 'Tom is tom.', 'Dick is Dick', '0 is 0', and so on, saying 'Everything is itself'.
> When on the other hand we want to generalize on 'Tom is mortal or Tom is not mortal', 'Snow is white or snow is not white', and so on, we ascend to talk fo truth and of sentences, saying "Every sentence of the form 'p or not p' is true', or 'Every alternation of a sentence with its negation is true'.
> What prompts semantic ascent is not that 'Tom is mortal or Tom is not mortal' is somehow about sentences while "Tom is mortal' and 'Tom is Tom' are about Tom.
> All three are about Tom.
> We ascend only because of the oblique way in which the instances over which we are generalizing are related to one another. [pg. 11 Philosophy of Logic]

Quotations as a method of semantic ascent are haunted by philosophies of the past which are unable to distinguish between schematics and the items they purport to schematize.
For example, "of the form 'p or not p'" is usually written without the interior quotes, that is as "of the form p or not p", and this is otherwise excused as an expedient to effective writing where the reader is expected to know where the invisible quotations are to be put.
This has continued to confuse people who then take the occurrence of the letter pee in "of the form p or not p" as a pronoun referring to some otherwordly item called a proposition that somehow is *the meaning* of what a sentence like "Tom is Tom" says.

To be trapped down such dark allys of past mistakes is as silly as being stuck doing arithmetic with roman numerals.
We can look back and marvel at our mistakes and laugh at the labors we now save by teaching better methods.

All of this is to say that the single sentence

> Logic is the sciene of validity and validity is a consequence of grammar and denotation.

which is the first in my paper on logic is woefully inadequate as a summary of even what little I have said above and what little I have yet to say on the rest of logic.

***

Perhaps the proper "first sentence" is the second one

> Compounding is (denotative) functional when, exclusively, each like compound denotes or each like compound does not denote, where and only where (waow), exclusively, each like component denotes or each like component does not denote.
> Chains are compounds compounded functionally.

The phrase "denotative functional" generalizes to predicates what is said as "truth functional" of sentences.
The definition of denotational compound is clumsy with its "exclusively"s and "like"s, but the alternatives are woefully inaccurate and undermine the task at hand.
Logic does not have the benefit of its fruits before they have ripened.
As much as I would love to say that whether a compound denotes or not is a function of whether its components denote or not, there is no thing that can yet be spoken of as "a function".

It is only from the methods of logic that we come to pin down the items of the world.
While there are many who say that they are in touch with the world in some direct way, e.g. Russell repeatedly leaves room for singular terms so that they can be used to designate particulars of immediate experience, there is not yet a way that I have come to be in touch with the world as such others describe.
It is only through their reports that I am even confronted with the problems of such seemingly personalized revelations.

There is another problem that sometimes occurs when making such definitions: do I not appear to be speaking of "exclusive alternations" when I have not said what it means to say an instance of the sentence schema "exclusively p or not p"?
Is there not a circularity to my methods?

Quine was confronted with the same problem so often in the first publication of his early text "Mathematical Logic" that he had to add an appendix to help the anxious reader.
There he explains the difference between "theorem" and "metatheorem" as well as "deduction" and "formal deduction".
There (and in his Methods of Logic) he is confined to philosophic methods and only indirectly mentions in Methods of Logic the scientific origins of logical practices.

I have gone out of my way to use the phrase 'where and only where' or 'when and only when' to distinguish the preformal/informal or preschematic methods from the schematic methods of logic.
To explain the origin of verbal behavior with the forms "Exclusively Tom is wet or Tom is dry" is to leave the science of logic for the science of verbal behavior more generally.
Just as humans spoke grammatically before speaking of grammar, they spoke logically before speaking of logic.

At this time I do not have a better explanation as to why I can use phrases that appear to have the same form as those said to be defined e.g. 

> Joint denials denote waow each of their components do not.
> Negations are self joint denials: they denote waow their component does not.
> Alternations are negations of joint denials: they denote waow some of their components do.
> Conjunctions are joint denials of negations: they denote waow each of their components do.

***

With all that said, I have only introduced the following sentences from my paper on logic

> Logic is the science of validity and validity is a consequence of grammar and denotation.
>
> Compounding is (denotative) functional when, exclusively, each like compound denotes or each like compound does not denote, where and only where (waow), exclusively, each like component denotes or each like component does not denote.
> Chains are compounds compounded functionally.
>
> Joint denials denote waow each of their components do not.
> Negations are self joint denials: they denote waow their component does not.
> Alternations are negations of joint denials: they denote waow some of their components do.
> Conjunctions are joint denials of negations: they denote waow each of their components do.

Here is all that I have so far:

#### 2025 0412 1422 Paper on Logic

Logic is the science of validity and validity is a consequence of grammar and denotation.

##### Functional Compounding and Chains

Compounding is (denotative) functional when, exclusively, each like compound denotes or each like compound does not denote, where and only where (waow), exclusively, each like component denotes or each like component does not denote.
Chains are compounds compounded functionally.

##### Example Chains: Joint Denials, Negations, Alternations, and Conjunctions

Joint denials denote waow each of their components do not.
Negations are self joint denials: they denote waow their component does not.
Alternations are negations of joint denials: they denote waow some of their components do.
Conjunctions are joint denials of negations: they denote waow each of their components do.

##### Subcompounds and Functional Substitutions

Subcompounds of compounds are their self or those of their components.
Substitutions of like compounds for like nonchain subcompounds are (denotative) functional.
Functional substitutions of functional substitutions of compounds are functional substitutions of their self.

##### Functional Validity, Consistency, Implication, and Equivalence

Compounds are (functionally)
* valid waow each of their functional substitutions denote,
* consistent waow their negation is nonvalid (i.e. soem of their functional substitutions denote),
* implied by others waow the conjunction of their self (the conclusion) with the negation of the other (the premise) is nonconsistent (i.e. each of their functional substitutions denotes where the same of the other does), and
* equivalent to others waow they are mutually implicative (i.e. each of their functional substitutions denotes waow the same of the other does).
[See pg. 36 of POL]

##### Example Validities and (Non)consistencies: Laws of Excluded Middle, Contradiction, Self Implication, and Self Equivalence

Alternations of compounds with their negations are valid (they denote waow the compound does or its negation does, i.e. waow it does or does not, so, each functional substitution denotes).
Conjunctions of compounds with their negations are nonconsistent (they denote waow their compound does and its negation does i.e. waow it does and does not, so, each functional substitution does not denote).
Compounds are implied by and equivalent to their self.

##### Functional Substitutions Keep Validity, Nonconsistency, Implication and Equivalence

Functional substitutions in
* validities are validities (each functional substitution of the functional substitution of the validity is a functional substitution of the validity and hence denotes),
* nonconsistencies are nonconsistencies (each function substitution of the negation of the functional substitution of the nonconsistency is a functional substitution of the negation of the nonconsistency i .e. is a functional substitution of a validity and hence the negation of the functional substitution of the nonconsistency is valid so that the function substitution of the nonconsistency is nonconsistent),
* implications are implications (the conjunction of the conclusion with the negation of the premise is nonconsistant and hence its functional substitution is nonconsistent and identical to the conjunction of the function substitution of the conclusion with the negation of the functional substutituion of the premise), and
* equivalences are equivalences (functional substitutions of mutual implications are mutual implications).

##### Interchanges of Equivalents are Equivalent
Interchanges of equivalents in a compound are equivalent to that compound (each functional substitution of a compound matches the same of its interchange, except perhaps for the same of the equivalents which otherwise denote in tandem, so each denotes waow the other does i.e. they are equivalent).

##### Interchnage of Equivalents Keeps Validity, Nonconsistency, Implication, Equivalence, Nonvalidity, Consistency, Nonimplication, and Nonequivalence

Interchanges of equivalents in
* validities are validities (each functional substitution of the interchange denotes waow the same of the validity does),
* nonconsistencies are nonconsistent (their negation is a validity and so the interchange in the negation is a validity),
* implications are implications (interchange into the nonconsistency is a nonconsistency),
* equivalents are equivalents (interchange of mutual implications are mutual implications),
* nonvalidities are nonvalidities (a compound is nonvalid waow some functional substitution does not denote, i.e. some functional substitution of its negation denotes, i.e. its negation is consistent, and since the negation of the interchange is identical tot he interchange of the negation which is consistent and consistency is kept by interchange then the negation is consistent i.e. it is nonvalid)
* consistencies are consistencies (the negation of the interchange is identical to the interchange of the negation which is nonvalid hence it is nonvalid),
* nonimplications are nonimplications (nonimplication is consistency of the conjunction ...)
* nonequivalences are nonequivalences (one is a nonimplication ...).

##### Equivalents of Identity
Compounds are equivalent to
* their double negation (which denotes waow the negation of the compound does not, i.e. waow it does, so, each functional substituion of it denotes waow the same of its double negation does),
* their self alternation/conjunction (which denotes waow some/each of its components does i.e. waow the compound does), and 
* their alternation/conjunction with nonconsistencies/validities.

##### Equivalents of Distributivity of Conjunctions and Alternations



### 2025 0412 1349
I have found that it is easier to be wrong than it is to be right and that it is easier to be right after having been wrong.
If there was a way to avoid being wrong then I would only act in that way.
Logic helps but is slow.
Sorting through the wrong bits takes more than what logic has to offer.

I see logic as part of science and see science as picking up what logic fumbles and drops.
Logic works on the theoretical side of science and laboratories work on the experimental side.
As I have said many times before "Libraries select theoretical practices and laboratories select experimental practices".
Science is compounded of theoretical and experimental practices.

One of the reasons I've allowed myself to write in this longer form is to make it easier for other people to see that I am often wrong, and that it is only from first being wrong that I get anywhere near being right.
The definitions, the natures, or the essences of what is right and what is wrong are not easy to suss out.

Many people have spent their whole lives trying and failing to establish what is right and what is wrong.
As much as I have a sort of optimism that if I work hard enough then I may one day uncover some piece of a complete theory of right and wrong, I am not so foolish to think that I shall do anything like what the best already have.

With that in mind, I offer these writings that are most certainly mistaken for no other reason than that they occurred to me.

***

I talk about behavior a lot.
Does that imply that my behavior is under my careful control?
No.
It is one thing to know a science and another thing to have a technology in hand.
The things of technology are the consequences of science, more or less.
But, the knowledge of science lacks the concrete dimensions of levers, pullies, medicines, and computers.
All that goes into scinece is often ephemeral: here for but a moment and gone forever, lost to our remote contact with the past through fragmentary records that soon fall apart.

Those who manufacture products, be they assemblies of machines, or chemicals, or organisms, or programs, or etc., work in ways often mistaken as scientific.
They manufacture with the controls offered by technologies i.e. with the descendants of scientific practices.

The difference between science and such manufactureres is the difference between sensitivity and insensitivity.
The manufacturer is deliberately insensitive to the strange variations responsible for science.
This is not what most people think or say: they say that business is as much a science and an art as physics or biology.
They insist that it is so in large part because the best businesses are almost always the result of some new technological consequence from science.

This does not always seem to be the case.
Some businesses appear to survive despite science.
Much that is "made for TV" objects to being called pseudoscience.
It sells, and there is a science to sales!

These are all problems of control: can you make a person buy your product?
Some say you can't, others say you can.
Some say that people only buy what is an intrinsically good product: you can't trick your way into becoming a good business when your product is technologically sophistocated.

***

What's with people who make a living as oracles?
They make predictions, often asserted with the authority of a proclamation, and profit, one way or another, from betting that they're right and ultimately being right.
But, are they betting on what they think they are?
Are they right when their bet pays off?
There seems to be some difference between constructing bets that are verly likely to pay off and making predictions like those people assume come from our best sciences.

How do we know the future?
When we speak of the future, as in predictions, what is it that we are speaking of?
Can speach actually be said to be about anything?
Certainly the speaking occurrs, but what good reasons do we have for saying that such speach is about this, that, or the future?

### 2025 0412 0119
[Matios Berhe](https://www.twitter.com/MatiosTV) suggests I read
- Kant's Critique of Pure Reason, chapter 2, section 12
- Kant's Critique of Pure Judgement, Critique of the Teleological Judgement
- Locke's Essay Concerning Human Understanding

I have now added these to my long list of "stuff to read".
Some day, sooner rather than later, I shall provide a complete list of texts in my library and the schedule on which I read them.
There are two main reasons to do so:
1. to point up the origins of what I have to say (so that others do not suspect me of originating, initiating, or creating my verbal behavior)
2. to better scheduling my reading and writing.

I already have a copy of Kant's Critique of Pure Reason that I read first when I was in high school.
There are a few texts from that time which have survived long enough to find themselves on my shelves.
One of the treasures from that time is Russell's "Problems of Philosophy".
In no way did I learn all that Russell and Kant had to teach in their books when I first read them so very long ago.
There is something special though about reading things that you do not yet understand.

The more I read the more I don't at first understand, and yet, at the same time, the more I learn about the world as a whole.
There is something about reading which makes the world bigger and smaller than it once seemed.
When I'm not in such a metaphorical mood, I am quick to say the world is no bigger nor no smaller than it is: to say otherwise is to plunge such a theory into contradiction from which no logic prevents each and every imaginable conclusion.

People are usually scared by contradiction for the wrong reasons: this is somewhat corrected by phrases like "law of explosion".
The problem with contradiction is not really a problem: when you have a contradiction you find yourself without the benefits of a consistent theory.
Some people are ok with that.

### 2025 0412 0102
I finally have a place to put all my daft drafts without chopping them into little tweet size pieces.
Why has it taken me so long to return to this long form method of writing?
It's mostly that strange feeling I get that this is like a message in a bottle sent out into an enormous sea of nothing.

Once I figured out that it wasn't sending the message that mattered most, I warmed up to the practice of writing from my little island on the internet.

## 2025 0411

### 2025 0411 2248
I'm implementing a little LISP.
Like the LISPs of yore, there are two basic items: pairs and atoms.

Pairs split into left and right parts.
Paul Graham wrote of "halves" when explaining pairs in [Bel](https://www.paulgraham.com/bel.html).

Whether you talk about the anatomy of a pair by "breaking it into pieces", "splitting it into halves", or "decomposing it into components", there is one rule that governs the logic of pairs: 

> pairs are identical where and only where their left components are identical and their right components are identical.

This is otherwise know as "the law of extensionality of ordered pairs".
It is a fancy way of mentioning the precise conditions upon which to count two items in a universe of ordered pairs as identical.
The single premise "each item is (a,b,c,x,y, and z such that a pairs b with c, x pairs y with z, and a is identical to x, if and only if b is identical to y and c is identical to z)" logically implies each conclusion of *any* theory of ordered pairs.

Now is not the time or place to get distracted by problems of logic, theories, and ordered pairs.
As much as I would enjoy a full digression on the origins of logic, the extensionality of effective theories, and ordered pairs as a paradigm of philosophic investigation, those are not the problems that I am presented with here and that I have to solve now.

Back to my little LISP.

Pairs are identical where and only where their parallel components are.
For now, "(x,y)" is short for "the pair whose left part is x and whose right part is y".
Thus, (x,y) is identical to (a,b) where and only where x is identical a and y is identical to b.

Since I'm coding up my little LISP in javascript I'm using the following definitions:

```
let cons=(x,y)=>[x,y]
, car=x=>x[0]
, cdr=x=>x[1]
```

These just say that 'cons' is the name of a function that takes two arguments and returns an array whose zeroth component is the first argument and whose first component is the second argument (note this confusing sentence is why some people prefer different methods of indexing into arrays of data).
The function named 'car' takes one argument and returns the item at index zero of it.

> If you do not already know how javascript arrays work, and how indexing into javascript arrays works, then these explanations are not helpful.
> Thankfully, if you do not already know how javascript arrays work, you get the joy of learning that first before coming back here.
>
> I have not yet found a faster way to introduce programming than to present it after teaching logic up to and including Quine's main method (of proof for predicate logic).
> Since more people are familiar with javascript than they are with Quine's main method--- or with the methods of logic--- I shall continue to avoid what might otherwise appear as an entirely roundabout way of teaching programming from scratch.

The function named 'cdr' takes one argument and returns the item at index one of it.

Both 'car' and 'cdr' are names inherited from working with programmable machines (aka "computers") during the late 1950s and early 1960s.
If you want to learn more about where 'car' and 'cdr' come from, and perahsp even how to pronounce 'cdr' (which I say as "could-er"), then track down [John McCarthy's 1979 "History of Lisp"](https://justine.lol/sectorlisp/lisp-history.pdf) and give it a compassionate read.

With 'cons', 'car', and 'cdr' we can make pairs, get their left part, and their right part respectively.
There are algebraic or equational ways of introducing these basic functions e.g.

```
car(cons(x,y))==x
cdr(cons(x,y))==y
'car(x)==car(y) and cdr(x)==cdr(y)' is equivalent to 'x==y'
```

but again this is just a distraction from the problem to solve (implementing a little lisp like language).
Note, such purportedly algebraic or equational theories appear to follow entirely from pure logic when Quine's method of schematic theories is extended to schematic theories of functional predicates (see Quine's 'Philosophy of Logic' for a schematic theory of identity).

Following the conventions established in the wonderful book "The Little Schemer" by Friedman and Felleisen, cdr and car are only used in contexts where their arguments are pairs with left and right parts.
There is one pair without any parts.
It is unique (in more ways than one).

```
let nil=[]
, id=(x,y)=>x==y
, empty=x=>id(x,nil)
```

The item named 'nil' is an empty javascript array.
Not all empty arrays are equal in javascript.
Equality, identity, and indiscernibility are hard to deal with when you don't start with logic.
Surprisingly, 'id' works almost like it does in a language like [Bel](https://www.paulgraham.com/bel.html) even though it is not obvious that it should.
Remember the 'law of extensionality of ordered pairs'?
Well, it is violated by almost all programming languages.

There is a silent assumption that is among the many hidden away behind any explanation of a foreign programming language: they actually run on real machines.
On most machines there are places and objects at those places.
Objects in different places can't be identical!
(Think that over: identical things can't differ in place apparently.)
And yet, sometimes they are!

Pairs may have parallel components that are identical without being identical themselves.
This happens when two pairs are silently assumed to be in different places but each of them places their components *in the same places*!
There are lots of solutions to this problem of identity.
Most of them are needlessly complex and overburdened with crusty philosophies.
My solution is as sad as it is simple: just go with what javascript gives you.

My real solution is to go with logic: identity is indiscernibility in that any two predicates which fill out each instance of the premises of identity are coextensive and indiscernibiltiy is one such predicate (in a logical theory with a finite lexicon).
For more on this see Quine's "Philosophy of Logic".

Atoms are not pairs (and pairs are not atoms).

```
let pair=x=>Array.isArray(x)
, atom=x=>!pair(x)
```

Is nil an atom?
No, not here.
Is nil an atom elsewhere?
Sometimes.
Early LISPs only had pairs and symbols.
Symbols are often a large part of LISPs.
They are the workhorses of reference: they end up sometimes playing the part of proper nouns and sometimes playing the part of pronouns.
Here again, there is much to be learned from predicate logic.
Here again, I must avoid dipping into such unfamiliar wells of simplicity.

The general problem of atomic items provides ample opportunity to make use of degenerate cases of common objects e.g. Quine provides for atoms in his "Set Theory and its Logic" by picking out atoms with the reflection of membership i.e. 'x is atomic' is short for 'x belongs to x'.
Most people avoid making convenient use of alarming degeneracies: they're really missing out on some good fun.

Atomic items and empty items have a long and sordid history.
Atoms are not supposed to have parts, and yet we are often forced to say that they do.
Empty items always end up behaving like atoms in many respects--- e.g. they are empty and presumably "don't have parts"--- but they almost always tend to be unique where as there tend to be many many atoms in any given theory.

The only place I have found clarity on these boundary problems is, you guessed it, logic.
In a theory of ordered pairs, it is enough to provide for the existence of an ordered pair that neither has a left part nor has a right part.
It is then unique since any two purportedly distinct empty pairs have identical parallel components (vacuously).
Atoms are then accomodated by one of a few methods, each of which match up with Quine's atomic methods in "Set Theory and its Logic".
An item of a theory of ordered pairs is atomic when it is identical to its components (or, just one of its components, in which case the other components allows us to distinguish between atoms with, e.g., identical left components: are these even atoms?).

Sadly neither of these principles are used in LISPs.
Again, this is because there is that silent assumption working in the background of most explanations: the programming language runs on machines in our shared material world.

This is enough thinking on this for now.

### 2025 0411 2217
I can not guarentee that there will be no spelling errors in what I write.
Rarely do I spell well, and rarely do I take the time to check my spelling.
Rather than check my spelling I would keep on thinking.
Spell checking stops thinking except when looking up the word to be spelled spurs some better bits of writing.
Definitions have the power to prompt better writing when they present short and sticky explanations.
Those sensitive to spelling errors will be greatly disappointed (that is until I go back through and spell check this document, that is, if I ever get around to doing that).

### 2025 0411 2158
I do not have much familiarity with using this software and can not be certain how what I've written here will appear.
I am also not familiar with markdown.
Will there be new lines in the final document if I put them here?
The answer is no.

Does this start a new paragraph? The answer is yes.

Does this [20250411](#2025-0411) automatically.

ok, rather than get lost in the details of editing I will just go on and start writing.
